6
1
0
2

 
r
a

 

M
7
1

 
 
]

.

A
F
h
t
a
m

[
 
 

1
v
0
2
6
5
0

.

3
0
6
1
:
v
i
X
r
a

A MOMENT MAJORIZATION PRINCIPLE FOR RANDOM MATRIX

ENSEMBLES WITH APPLICATIONS TO HARDNESS OF THE

NONCOMMUTATIVE GROTHENDIECK PROBLEM

STEVEN HEILMAN AND THOMAS VIDICK

Abstract. We prove a moment majorization principle for matrix-valued functions with
domain {−1, 1}m, m ∈ N. The principle is an inequality between higher-order moments of
a non-commutative multilinear polynomial with diﬀerent random matrix ensemble inputs,
where each variable has small inﬂuence and the variables are instantiated independently.

This technical result can be interpreted as a noncommutative generalization of one of the
two inequalities of the seminal invariance principle of Mossel, O’Donnell and Oleszkiewicz.
Our main application is sharp Unique Games hardness for two versions of the noncom-
mutative Grothendieck inequality. This generalizes a result of Raghavendra and Steurer
who established hardness of approximation for the commutative Grothendieck inequality. A
similar application was proven recently by Bri¨et, Regev and Saket using diﬀerent techniques.

1. Introduction

1.1. A noncommutative moment majorization theorem. We study matrix-valued
functions f with domain {−1, 1}m within the context of probability theory and Fourier
analysis. More speciﬁcally, we study functions f such that, for every σ ∈ {−1, 1}m, the
operator norm of f (σ) is at most 1. The special case when f is valued in the two-point
space {−1, 1} has been studied extensively within theoretical computer science [KKL88],
but also in diverse areas such as combinatorics, isoperimetry [Tal94], or social choice the-
ory [Kal02, MOO10, MN15]. (For a more comprehensive list of references and discussion,
see e.g. the survey [O’D14b].) In applications to theoretical computer science, a function
f : {−1, 1}m → {−1, 1} can be used to represent an instance of a combinatorial optimization
problem. That is, the function f can be thought of as a list of elements of {−1, 1}, seen
as a Boolean assignment to the 2m variables of some constraint satisfaction problem. Func-
tions with domain {−1, 1}m and range the simplex {(x1, . . . , xn) ∈ Rn : Pn
i=1 xi = 1, x1 ≥
0, . . . , xn ≥ 0} have also been considered [KN09, KN13, IM12]. Projecting f onto each co-
ordinate gives a family of functions with range [0, 1], so that similar tools to the Boolean
case can be applied. Here our main application, and motivation, is to the noncommutative
Grothendieck inequality (NCGI), an inequality which involves two orthogonal (in the real
case) or unitary (in the complex case) matrix variables of ﬁxed dimension. This setting
leads us to consider matrix-valued functions with domain {−1, 1}m (considering m > 1 will
allow us to “combine” multiple instances of NCGI acting on partially overlapping sets of
variables).

2010 Mathematics Subject Classiﬁcation. 68Q17,60E15,47A50.
Key words and phrases. invariance principle, moment majorization, Lindeberg replacement, noncommu-

tative Grothendieck inequality, Unique Games Conjecture, dictators versus low inﬂuences.

1

In many of the applications listed above a standard manipulation is to extend f to a multi-
linear polynomial, so that the distribution of f can be studied under diﬀerent distributions on
its domain, such as the standard Gaussian distribution. In our setting it is natural (and, as
we will see, for our purposes necessary) to investigate the behavior of matrix-valued functions
under distributions on their domain that allow the possibility for matrix variables. For any
set S, let Mn(S) denotes the n × n matrices with entries in S. Any f : {−1, 1}m → Mn(C)
can be extended to a multilinear polynomial in m noncommutative variables with matrix
coeﬃcients. Consider for instance the case m = 2 and the polynomial f (σ1, σ2) = σ1σ2,
where σ1, σ2 ∈ {−1, 1}. Since the variables σ1, σ2 commute, it is not necessary to specify
the order in which the product of the variables is taken in f . However, once f is extended
to matrix variables X1, X2, an ordering needs to be speciﬁed. We adopt the convention of
ordering matrix variables by increasing order, e.g. f (X1, X2) = X1X2.

Let d, m, n be positive integers. For us, a noncommutative multilinear polynomial of

degree d in m variables can be expressed as

Q(X1, . . . , Xm) = XS⊆{1,...,m} : |S|≤d bQ(S) Yi∈S

Xi,

where bQ(S) is an n × n complex matrix for every S ⊆ {1, . . . , m}, X1, . . . , Xm are non-
commutative n × n matrix variables, and the product Qi∈S Xi is always taken in increasing
order. For example, Qi∈{1,2}

Xi = X1X2. Noncommutative polynomials appear in many
other contexts, most notably, within free probability [Voi91, Theorem 3.3]. In addition there
is a general theory of so-called nc-functions [KVV14], but this theory does not seem to apply
to the noncommutative polynomials we consider here. (An nc function h is a function deﬁned
on matrices of any dimension, such that, for any n ≥ 1, and for any n × n matrices A, B, C
such that C is invertible, h(CAC−1) = Ch(A)C−1 and h(A ⊕ B) = h(A) ⊕ h(B). Nei-
ther property is satisﬁed by a general matrix-valued non-commutative polynomial as deﬁned
below.)

Our main goal consists in bounding the moments of Q for diﬀerent random matrix distri-
butions in the domain, when all partial derivatives of Q are small (i.e. when Q has small
inﬂuences).
In particular, we would like to say that the moments of polynomials Q with
small inﬂuences under Gaussian random matrix inputs are close to the moments of Q under
uniform {−1, 1}m inputs. Unfortunately, this task is in general impossible. For example,
consider the linear polynomial Q(X1, . . . , Xm) = (X1 + · · · + Xm)/√m. For any square
matrix A, let |A| = (AA∗)1/2. Let b1, . . . , bm be i.i.d. uniform random variables in {−1, 1},
and let I denote the n × n identity matrix. Then E(1/n)Tr|Q(b1I, . . . , bmI)|4 = 3 − 2/m.
On the other hand, let G1, . . . , Gm be n × n independent Wigner matrices with real Gauss-
In this case Q(G1, . . . , Gm) is equal in distribution to G1, and in particular
ian entries.
limn→∞(1/n)Tr|Q(G1, . . . , Gm)|4 = 2, by the semicircle law. Thus even though the ﬁrst and
second moments of the input distributions match, i.e. Eb1 = 0, Eb2
1 = 1, EG1 = 0 and
EG1G∗1 = I, the associated moments of Q can be very diﬀerent.

In summary, a general invariance principle cannot hold in this noncommutative setting.
We could instead try to prove a weaker statement such as: the moments of Q under non-
commutative inputs with EG1 = 0 and EG1G∗1 = I are bounded by the moments of Q under
Boolean inputs. We call such a statement a moment majorization theorem. Unfortunately,
this is also not true in general, as we now show. Let A be an n × n matrix whose only

2

nonzero entry is a 1 in the top left corner, and let B be an n × n cyclic permutation matrix.
Then for any 0 ≤ j, k < n with j 6= k we have BjABkA = 0 and Tr(BjA) = 1. Consider the
linear polynomial

Q(X1, . . . , Xn) =

n−1Xi=0

BiAXi,

which is such that ETr |Q(b1, . . . , bn)|4 = n. Now let H1 be a uniformly random Haar dis-
tributed n × n unitary matrix, and let H2, . . . , Hn be independent copies of H1. Then
ETr|Q(H1, . . . , Hn)|4 = nETr |AH1|4 + n(n− 1)ETr(A2H1A2H∗1 ) = n + n(n− 1)/n = 2n− 1.
Since a general noncommutative majorization principle cannot hold we instead establish a
limited moment majorization theorem, which will nevertheless be suﬃcient for our applica-
tions. We make two changes. We ﬁrst increase the dimension of the random matrix inputs
G1, . . . , Gm. That is, we allow the variables of Q to take values in the set of p × p matrices
with p > n by deﬁning the p × p matrix

for any n × n matrix A, and

ι(A) = (cid:18)A 0
0 0(cid:19)
ι(cid:0)bQ(S)(cid:1) Yi∈S
Qι(X1, . . . , Xm) = XS⊆{1,...,m}

Xi,

where X1, . . . , Xm are noncommutative p × p matrix variables. Second, we randomly rotate
G1, . . . , Gm by p × p Haar-distributed random unitary matrices H1, . . . , Hm.
We state one particular variant of our noncommutative moment majorization theorem.
Let p, n be integers. We write H ∼ H to denote a p × p Haar-distributed random unitary
matrix, b ∼ B for a uniformly random b ∈ {−1, 1}, and G ∼ G for any random variable
taking values in Mn(C) such that EG = 0 and EGG∗ = I. We also write G′ ∼ ι(G) to
denote G′ = ι(G) with G ∼ G. We use the succinct notation Gi ∼ G to denote a collection
G1, . . . , Gm of indepenent random matrices with distribution G, and denote Q(G1, . . . , Gm)
as Q{Gi}. The operator norm of a matrix A is denoted kAk.
Theorem 1.1 (Noncommutative Fourth Moment Majorization). Let Q be a non-
commutative multilinear polynomial of degree d in m variables such that kQ(σ)k ≤ 1 for all
σ ∈ {−1, 1}m. Let τ := maxi=1,...,mPS⊆{1,...,m} : i∈S Tr(bQ(S)bQ(S)∗) be the maximum inﬂu-
ence of Q. Let Gi ∼ G and c2 ≥ 1 such that kE(G1G∗1)2k ≤ c2. Then

1
n

E

Gi∼ι(G)
Hi∼Hp

Tr|Qι{GiHi}|4 ≤ E
bi∼B

1
n

Tr|Q{bi}|4 + 8(8c2)4dn4τ 1/4 + Om,n(p−1/2).

(1)

Remark 1.2. This Theorem is a special case of Theorem 3.9 below. The term 8(8c2)4dn4τ 1/4
on the right-hand side of (1) can be replaced by (8c2)4dτ by additionally assuming that
Ea1a2a3 = 0 for any a1, a2, a3 which are (possibly repeated) entries of G1. We omit the
proof of this strengthened statement, since the details are essentially identical to the proof
of Theorem 3.5.

3

Remark 1.3. Note that, although Qι takes values in the set of p × p matrices, the trace is
normalized by 1/n, and Qι still “acts like” an n × n matrix. In particular the moments of
Qι do not become arbitrarily small in general; for instance it holds (see Lemma 2.6 below)
that

1
n

E

Gi∼ι(G)
Hi∼Hp

Tr|Qι{GiHi}|2 = E
bi∼B

1
n

Tr|Q{bi}|2 .

Theorem 1.1 shows that the fourth moment of Q with appropriate random matrix inputs
is bounded by the fourth moment of Q with Boolean inputs. We provide a variant of
this majorization principle for higher order moments (Theorem 3.9) and for increasing test
functions (Theorem 3.5). These majorization principles all have dependence on the degree of
the polynomial, but we also give degree-independent bounds for polynomials whose higher
order coeﬃcients decay at an exponential rate (Corollary 3.8).

Although majorization principles such as Theorem 1.1 involve the trace norm of a poly-
nomial, we can obtain bounds on the operator norm of Q in the following way. Let t ∈ R
and consider the function t 7→ (max(0,|t| − 1))2. This function can be applied to self-
adjoint matrices via spectral calculus, and A 7→ (max(0, (AA∗)1/2 − 1))2 = 0 if the singular
values of A are all bounded by 1. In particular, if kQ(σ)k ≤ 1 for all σ ∈ {−1, 1}m then
(max(0, (Q(σ)Q(σ)∗)1/2−1))2 = 0 for all σ ∈ {−1, 1}m. The following majorization principle
gives control on the operator norm of Q when we substitute appropriate random matrices
into the domain of Q.

Theorem 1.4 (Noncommutative Operator Norm Majorization). Let Q be a noncom-
mutative multilinear polynomial of degree d. Suppose kQ(σ)k ≤ 1 for all σ ∈ {−1, 1}m. Let
τ := maxi=1,...,mPS⊆{1,...,m} : i∈S Tr(bQ(S)bQ(S)∗) be the maximum inﬂuence of Q. Let Gi ∼ G
and c2, c3 ≥ 1 such that kE(G1G∗1)2k ≤ c2 and kE(G1G∗1)3k ≤ c3. Then

1
n

E

Gi∼ι(G)
Hi∼Hp

Tr(cid:0) max(0,|Qι{GiHi}| − 1)(cid:1)2 ≤ (8c2c3)9dn1/2τ 1/6 + Om,n(τ−1/3p−1/2).

(2)

This Theorem appears below in Theorem 3.7. We will use this theorem to argue that
under the proper normalization condition a low-inﬂuence polynomial typically maps random
Gaussian matrix inputs to matrices of norm not much larger than 1. The ensemble that will
be of most interest for us is the following.

Example 1.5. Let N > 0. Let V1, . . . , VN be n×n complex matrices such thatPN
and E|g1|2 = 1.) Deﬁne G1 = PN

i=1 ViV ∗i =
1. Let g1, . . . , gN be i.i.d. standard complex Gaussian random variables (so that Eg1 = 0
i=1 giVi, and let G2, . . . , Gm be independent copies of G1.
Then it follows from [HT99, Corollary 2.8] that the random matrices G1, . . . , Gm satisfy the
hypothesis of Theorem 1.1 with c2 = 2. In the case that V1, . . . , VN are real matrices and
g1, . . . , gN are i.i.d. standard real Gaussians, the hypothesis of Theorem 1.1 is satisﬁed with
c2 = 4 · 2 = 8, as follows from the complex case and the inequality kE(ℜ(G1)ℜ(G1)∗)2k ≤
kE(G1G∗1)2k ≤ 2 when g1, . . . , gN are complex.

Here are some other examples of random matrix ensembles satisfying the hypothesis of

Theorem 1.4.

4

Example 1.6. Example 1.5 speciﬁcally applies to Gaussian Wigner matrices as follows. Let
U1, . . . , Un2 be n × n matrices such that these matrices are the standard orthonormal basis
of Cn2. Then G1 is a Wigner matrix and the hypothesis of Theorem 1.1 is satisﬁed with
c2 = 2. Similarly, let U1, . . . , Un(n+1)/2 be n × n matrices such that these matrices are the
standard orthonormal basis of symmetric n × n matrices. Then G1 is a Wigner matrix and
the hypothesis of Theorem 1.1 is satisﬁed with c2 = 2 (see [DS01, Theorem II.11] or [Ver12,
Theorem 5.32]).

Example 1.7. Let G1, . . . , Gm be i.i.d. n × n Haar-distributed random unitary matrices.
Then the random matrices G1, . . . , Gm satisfy the hypothesis of Theorem 1.1 with c2 = 1.

We give a brief overview of the strategy of the proof of Theorem 1.4 and its generalization,
Theorem 3.9. In order to prove the majorization principle, we ﬁrst establish some basic facts
about Fourier analysis of matrix-valued functions f in Section 2. In particular, starting from
a function f : {−1, 1}m → Mn(C), we extend f to a noncommutative multilinear polynomial
Q = Qf of m variables.
For these polynomials Q, we consider a few diﬀerent inner products, norms, derivatives,
Plancherel identities, and we also deﬁne the Ornstein-Uhlenbeck semigroup. We then prove a
noncommutative hypercontractive inequality for such polynomials Q. This hypercontractive
inequality is developed in Section 3 and proven in Theorem 3.1. It can also be considered a
polynomial generalization of the matrix Khintchine inequality.

To prove the noncommutative majorization principle, we use the Lindeberg replacement
method from [MOO10, Cha06], but in our (matrix-valued) polynomials, we are replacing one
random variable with one random matrix, one at a time. The “second order” terms in this
replacement have a noncommutative nature which introduces an error.
Instead of trying
to bound this error (which seems diﬃcult or impossible in general), we choose particular
random matrices such that the “second order” terms are small. This is accomplished by

replacing an n × n random matrix X with a much larger p × p matrix(cid:18)X 0
0(cid:19) H, where H
is a uniformly random p × p unitary matrix and p > n. When p → ∞, the noncommutative
“second order” errors in the Lindeberg replacement vanish.

0

1.2. Application: computational hardness for the noncommutative Grothendieck
inequality. The commutative Grothendieck inequality relates two norms on a tensor prod-
uct space. It can be stated as follows.

Theorem 1.8 (Grothendieck’s Inequality, [Gro53, LP68, AN06, BMMN13]). There ex-
ists 1 ≤ KG <
such that the following holds. Let n ∈ N and let A = (aij)1≤i,j≤n be
an n × n real matrix. Then

2 log(1+√2)

π

max

x1,...,xn,y1,...,yn∈R2n−1

hxi,xii=hyi,yii=1 ∀ i∈{1,...,n}

nXi,j=1

aijhxi, yji ≤ KG

max

ε1,...,εn,δ1,...,δn∈{±1}

aijεiδj.

(3)

nXi,j=1

Moreover, a similar bound (with a diﬀerent constant) holds for complex scalars, replacing
R2n−1 with C2n−1 and {−1, 1} with complex numbers of modulus 1.

For two recent surveys on Grothendieck inequalities, see [KN12] and [Pis12].

5

Let n, m, N be positive integers. For x = (x1, . . . , xN ), y = (y1, . . . , yN ) ∈ CN , deﬁne
hx, yi :=PN

i=1 xiyi. Given U ∈ Mn(CN ), deﬁne two matrices UU∗ and U∗U ∈ Mn(C) by
∀ i, j ∈ {1, . . . , n},

hUki, Ukji.

hUik, Ujki

(U∗U)ij :=

(UU∗)ij :=

NXk=1

NXk=1

Deﬁne

On(RN ) := {U ∈ Mn(RN ) : UU∗ = U∗U = I},
Un(CN ) := {U ∈ Mn(CN ) : UU∗ = U∗U = I},

and On := On(R), Un := Un(C).
proven in [Pis78, Kai83]. It states the following.

The noncommutative Grothendieck inequality (NCGI) was conjectured in [Gro53] and

Theorem 1.9 (Noncommutative Grothendieck Inequality, [Pis78, Haa85, NRV14]).

There exist constants 1 ≤ KR ≤ 2√2 and KC = 2 such that the following holds. Let n ∈ N
and let M = (Mijkℓ)1≤i,j,k,ℓ≤n ∈ Mn(Mn(C)). Then
MijkℓhUij, Vkℓi ≤ KC ·

MijkℓXijYkℓ.

sup

sup

(4)

nXi,j,k,ℓ=1

nXi,j,k,ℓ=1

X,Y ∈Un

U,V ∈Un(CN )

Let n ∈ N and let Mijkℓ ∈ Mn(Mn(R)). Then

sup

U,V ∈On(RN )

nXi,j,k,ℓ=1

MijkℓhUij, Vkℓi ≤ KR ·

nXi,j,k,ℓ=1

sup

X,Y ∈On

MijkℓXijYkℓ.

(5)

The constant KC = 2 is the smallest possible in (4). The smallest possible constant in (5)

is still unknown [NRV14].

Remark 1.10. In Theorem 1.9, if we choose M “diagonal”, i.e. so that Mijkℓ = Miikk for
all i, j, k, ℓ ∈ {1, . . . , n}, then we recover Theorem 1.8. That is, (5) is a generalization of (3).
Remark 1.11. For U, V ∈ Mn(CN ) vector-valued matrices deﬁne U ⊙ V ∈ Mn×n(C) by
(U ⊙ V )(i−1)n+k,(j−1)n+ℓ := hUij, Vkℓi for all i, j, k, ℓ ∈ {1, . . . , n}. Let M ∈ Mn×n(C). Then
(4) has the equivalent form

sup
X,Y ∈Un
which is the formulation we will mainly use in the paper.

Tr(M(U ⊙ V )) ≤ KC ·

U,V ∈Un(CN )

sup

Tr(M(X ⊗ Y )),

The commutative Grothendieck inequality, Theorem 1.8, was given an algorithmic in-
terpretation in [AN06], where it is shown that the left-hand side of (3) can be computed
in polynomial time and any optimal solution “rounded” to a near-optimal (up to the ap-
proximation constant) choice of signs ε1, . . . , εn, δ1, . . . , δn for the right-hand side; see also
[BMMN13, NR14] for more recent works. Moreover, assuming a standard conjecture in
complexity theory, the Unique Games Conjecture (UGC, see Section 2.12 for the deﬁnition),
Raghavendra and Steurer [RS09] showed that no better approximation to the left-hand side
of (3) could be computed in polynomial time, unless P=NP.1

1 Other sharp Unique Games hardness results for variants of (3) were proven by Khot and Naor [KN09,

KN13].

6

Following the same path, in [NRV14] it was shown that the noncommutative Grothendieck
inequality, Theorem 1.9, could be made algorithmic as well, in the sense that the left-hand
side of each inequality can be eﬃciently approximated in polynomial time. And near-optimal
solutions can be eﬃciently “rounded” to an assignment of variables to the right-hand side
with a multiplicative loss in the objective value corresponding to the constants KC and KR
respectively. Furthermore, very recently Bri¨et, Regev and Saket [BRS15] showed that ap-
proximations within a factor less than KC = 2 in (4) cannot be found in polynomial time
unles P=NP. Their proof replace the standard “dictatorship versus low inﬂuences” machin-
ery with a clever construction of a linear transformation using the Cliﬀord algebra, thereby
entirely avoiding the use of invariance principles, majorization principles and hypercontrac-
tivity. In fact, as [BRS15] mentions, “Our attempts to apply these techniques here failed.”
Our main application for our moment majorization theorem states that the approximation
given in (4) is the best achievable in polynomial time, assuming the Unique Games conjecture.
Although this is a weaker result than [BRS15] (since they do not need to assume UGC), our
proof arguably has the advantage of following the same general structure as the hardness
result for the commutative Grothendieck inequality proved in [RS09]. As such our proof
technique demonstrates that the noncommutative generalizations of the majorization and
hypercontractivity principles provided in this paper can lead to successful extensions of their
use as key tools in hardness of approximation results and more generally analysis of Boolean
functions. We expect the principles to ﬁnd more applications; for example, it is relatively
easy for us to prove Theorem 1.16 below by imitating the proof of Theorem 1.14. The proof
of [BRS15] seems to provide no such ﬂexibility. (In light of our results it would nevertheless
be interesting to try to connect their methods with the hypercontractive inequality of [CL93],
since both use Cliﬀord algebras.)

Theorem 1.12 (Unique Games Hardness for NCGI). Assuming the Unique Games
Conjecture, it is NP-hard to approximate the quantity

OPT(M) := sup
X,Y ∈Un

nXi,j,k,ℓ=1

MijkℓXijYkℓ

(6)

within a multiplicative factor smaller than KC = 2.

The proof of Theorem 1.12 considers a restricted variant where M is a positive semideﬁnite

4-tensor, in the following sense.
Deﬁnition 1.13. Let M ∈ Mn×n(C) = Mn(Mn(C)). We say that M is positive semideﬁnite,
or PSD (as a 4-tensor), if there exists M1, . . . , MN ∈ Mn(C) such that M =PN
i=1 Mi ⊗ Mi.
Theorem 1.14 (Unique Games Hardness for Positive Semideﬁnite NCGI). Let
K > 0 be the inﬁmum over all constant KC such that (4) holds for all M ∈ Mn×n(C) such
that M is PSD. Then, assuming the Unique Games Conjecture, no polynomial time algorithm
(in n) can approximate the quantity

OPT(M) = sup
X,Y ∈Un

nXi,j,k,ℓ=1

MijkℓXijYkℓ

within a multiplicative factor smaller than K.
7

As shown in [BRS15, Theorem 1.2] the constant K in Theorem 1.14 satisﬁes K = KC = 2,

so that Theorem 1.12 follows from Theorem 1.14.

We consider a last variant of NCGI, introduced in [BKS16].

Theorem 1.15 (Positive Semideﬁnite Variant of NCGI). Let d ∈ N. Then there
exists K(d) > 0 such that the following holds. Let n ∈ N and let M be a symmetric positive
semideﬁnite nd×nd complex matrix. (That is, M is positive semideﬁnite in the usual sense.)
For each i, j ∈ {1, . . . , n}, let (Mij) denote the d × d matrix {Md(i−1)+u,d(j−1)+v}d
u,v=1. Then

nXi,j=1

sup

V1,...,Vn∈Cd×dn :

ViV ∗
i =1d×d
∀ i∈{1,...,n}

Tr((Mij)T ViV ∗j ) ≤ K(d) ·

sup

X1,...,Xn∈Ud

nXi,j=1

Tr((Mij)T XiX∗j ).

As a demonstration of their ﬂexibility, the proofs of Theorem 1.12 and Theorem 1.14

readily extends to this context.

Theorem 1.16 (Unique Games Hardness for Positive Semideﬁnite NCGI Vari-
ant). Let M be as in Theorem 1.15. Let K be the inﬁmum over all K(d) > 0 such that
Theorem 1.15 holds. Then, assuming the Unique Games Conjecture, no polynomial time (in
n) algorithm can approximate the quantity

sup

X1,...,Xn∈Ud

nXi,j=1

Tr((Mij)T XiX∗j ).

within a multiplicative factor smaller than K.

As shown in [BKS16], we have pK(d) = E(cid:16) 1

dPd

i=1 λi(G)(cid:17), where G is a d × d matrix

with complex Gaussian i.i.d. entries with mean zero and variance 1/d, and λi(G) is the ith
singular value of G. It is known that K(1) = π/4 and limd→∞ K(d) = (8/(3π))2.
They are given in Section 5.

The proof of Theorems 1.14 and 1.16 rely on our noncommutative majorization principle.

It seems conceivable that Theorems 1.14 and 1.16 can be extended to handle real scalars

instead of complex scalars. We leave this research direction to future investigation.

1.3. Other applications. The commutative invariance principle [Rot79, Cha06, MOO10]
implies that if Q is a commutative multilinear polynomial with small derivatives (i.e. small
inﬂuences), then the distribution of Q on i.i.d. uniform inputs in {−1, 1} is close to the
distribution of Q on i.i.d. standard Gaussian random variables. A more general statement
can be made for more general functions and distributions; for details see e.g. [MOO10]. An
invariance principle can also be considered as a concentration inequality, generalizing the
central limit theorem with error bounds (i.e. the Berry-Ess´een Theorem). For other variants
of invariance principles, see [Mos10, IM12].

The form of the invariance principle given in [MOO10] is proven by a combination of the
Lindeberg replacement argument and the hypercontractive inequality [Bon70, Nel73, Gro75].
That is, one replaces one argument of Q at a time, adding up the resulting errors and
controlling them via the hypercontractive inequality. One version of hypercontractivity says
that a higher Lq norm of a polynomial is bounded by a lower Lp norm of that polynomial,

8

where q > p, with a bound dependent on the degree of the polynomial Q. For example, if Q
has degree d, then the L4 norm of Q is bounded by 9d times the L2 norm of Q.

The commutative invariance principle has seen many applications [O’D14b, O’D14a] in
recent years. Here is a small sample of such applications and references: isoperimetric prob-
lems in Gaussian space and in the hypercube [MOO10, IM12], social choice theory, Unique
Games hardness results [KKMO07, IM12], analysis of algorithms [BR15], random matrix
theory [MP14], free probability [NPR10], optimization of noise sensitivity [Kan14]. The Lin-
deberg replacement argument itself has many applications, e.g. in proving the universality
of eigenvalue statistics for Wigner matrices [TV11, Theorem 15].

We anticipate that our noncommutative majorization principle will ﬁnd similar appli-
cations. Even though it is impossible to prove a noncommutative invariance principle in
general, most applications of the commutative invariance principle only involve one direc-
tion of the inequality. That is, most applications of the invariance principle are really just
applications of a majorization principle such as Theorem 1.1 or 1.4.

To demonstrate further applications of our majorization principle, we show in Section
6 that one of the two main parts of the proof of the Majority is Stablest Theorem from
[MOO10] can be extended to the noncommutative setting. Then, in Section 7, we demon-
strate a (probably sub-optimal) anti-concentration estimate for noncommutative multilinear
polynomials. Both of these results proceed as in [MOO10] by replacing their invariance
principle with our majorization principle.

Since majorization principles such as Theorems 1.1 and 1.4 show the closeness of one
distribution to another, these statements could be ﬁt into the “concentration of measure”
paradigm. The paper [MS12] proves a concentration inequality for noncommutative polyno-
mials, but these methods seem insuﬃcient to prove a majorization principle. An invariance
principle has been proven in the free probability setting [DN14], but the details of exactly
what polynomials can be dealt with, and which distributions can be handled, seem incom-
parable to our majorization principle.

Remark 1.17. We remark that, although it may be tempting to try to prove Theorem 1.1
from the commutative invariance principle of [MOO10], there seems to be no straightforward
way to accomplish this task. For example, we could interpret each entry in the output of
a noncommutative polynomial Q as a commutative polynomial function of the inputs. But
then in order to control Tr|Q|4, we would need information on the joint distribution of the
entries of Q, which is not provided by the invariance principle of [MOO10].

2. Definitions, background and notation

2.1. Matrices. For n ∈ N we denote the set of n by n matrices by Mn(C). We use Mn×n(C)
in place of Mn2 to denote n2 by n2 matrices when we wish to emphasize that a speciﬁc tensor
decomposition of the space Cn2 = Cn ⊗ Cn on which the matrix acts has been ﬁxed.
For A ∈ Mn(C), kAk is the operator norm of A (the largest singular value). We denote
by A∗ the conjugate-transpose. The absolute value is |A| = (AA∗)1/2. We use I to denote
the identity matrix.
Any real function f : R → R can be applied to a Hermitian matrix A by diagonalizing
A and applying f to the eigenvalues of A. Deﬁne Chop : R → R as Chop(t) = t if |t| ≤ 1,
Chop(t) = 1 if t ≥ 1, and Chop(t) = −1 if t ≤ −1.

9

2.2. Asymptotic Notation. Let a, b, c ∈ R. We write a = Oc(b) if there exists a constant
C(c) > 0 such that |a| ≤ C(c)|b|. We write a ≤ Oc(b) if there exists a constant C(c) > 0
such that a ≤ C(c)b.

2.3. Random variables and expectations. The following notational conventions will be
useful when working with functions of multiple variables. Let m, n ∈ N and let S, T be
arbitrary sets. Let f : Sm → T and let X1, . . . , Xn be independent random variables
with the same distribution X taking values in S. Then we will denote Ef (X1, . . . , Xm)
by EXj∼X f{Xj}; more generally the curly bracket notation f{Aj} will be used to denote
f (A1, . . . , Am).
We will use the following ensembles. Let p, N ∈ N. H ∼ H denotes a p×p Haar-distributed
random unitary matrix, where the dimension p will always be clear from context. b ∼ B
denotes a uniformly random b ∈ {±1}. G ∼ G denotes any random variable taking values in
Mn(C) such that EG = 0 and EGG∗ = I. And G′ ∼ ι(G) denotes G′ = ι(G), where G ∼ G.
G ∼ V denotes a random variable distributed as G = PN
i=1 giVi, where g1, . . . , gN are i.i.d.
standard complex Gaussian random variables and V1, . . . , VN are n × n complex matrices
satisfying PN
i=1 V ∗i Vi = In. And G′ ∼ ι(V) denotes G′ = ι(G), where G ∼ V.
Whenever V is used the matrices V1, . . . , VN will be clear from context. We also sometimes
write Gj ∼ D to mean G1, . . . , Gm are independent random variables with distribution D,
where again m will always be clear from context.

i=1 ViV ∗i = PN

2.4. Fourier expansions. Let n, m ∈ N and f, h : {−1, 1}m → Mn(C). We consider the
inner product

Tr(cid:0)f{bj}(h{bj})∗(cid:1) = 2−m Xσ∈{−1,1}m
Given S ⊆ {1, . . . , m} and σ = (σ1, . . . σm) ∈ {−1, 1}m, deﬁne

hf, hi := E
bj∼B

WS(σ) :=Yi∈S

σi.

Tr(f (σ)h(σ)∗).

The set of functions {WS}S⊆{1,...,n} forms an orthonormal basis for the space of functions
from {−1, 1}m to Mn(C), when it is viewed as a vector space over C with respect to the
inner product h·,·i.
Let bf (S) := 2−mPσ∈{−1,1}m f (σ)WS(σ) be the Fourier coeﬃcient of f associated to S.
Note that bf (S) ∈ Mn(C). Then f =PS⊆{1,...,m} bf (S)WS, and
2−m Xσ∈{−1,1}m

f (σ)(g(σ))∗ = 2−m Xσ∈{−1,1}m XS,S ′⊆{1,...,m} bf (S)(bg(S′))∗WS(σ)WS ′(σ)

= XS,S ′⊆{1,...,m} bf (S)(bg(S′))∗ · 2−m Xσ∈{−1,1}m
= XS⊆{1,...,m} bf (S)(bg(S))∗.

10

WS(σ)WS ′(σ)

(7)

2.5. Noncommutative polynomials. Let n, m ∈ N be integers. We consider noncommu-
tative multilinear polynomials Q ∈ Mn(C)[X1, . . . , Xm], where X1, . . . , Xm are noncommu-
tative indeterminates. Monomials are always ordered by increasing order of the index, e.g.
X1X2 and not X2X1. Any such polynomial can be expanded as

Q(X1, . . . , Xm) = XS⊆{1,...,m} : |S|≤d bQ(S)Yi∈S

Xi,

(8)

where bQ(S) ∈ Mn(C) for all S ⊆ {1, . . . , m} and 0 ≤ d ≤ m is the degree of Q, deﬁned as
max{|S| : bf (S) 6= 0, S ⊆ {1, . . . , m}}.
Let f : {−1, 1}m → Mn(C). Deﬁne the (non-commutative) multilinear polynomial Qf ∈

Mn(C)[X1, . . . , Xm] associated to f by

Qf (X1, . . . , Xm) := XS⊆{1,...,m} bf (S)Yi∈S

Xi.

(9)

2.6. Partial Derivatives. Let f : {−1, 1}m → Mn(C). Let i ∈ {1, . . . , m}. Deﬁne the ith
partial derivative of f by

1
2

(10)

(11)

and the ith inﬂuence of f by

(f (σ) − f (σ1, . . . ,−σi, . . . , σm)),

∂if (σ) := XS⊆{1,...,m} : i∈S bf (S)WS(σ) =
Inf if := XS⊆{1,...,m} : i∈S
Tr(bf (S)bf (S)∗).
i=1 Inf if =PS⊆{1,...,m} |S| Tr(bf (S)(bf (S))∗).
Note that by (7), Inf if = h∂if, ∂ifi and Pm
2.7. Ornstein-Uhlenbeck semigroup. For f, h : {−1, 1}m → Mn(C) deﬁne their convo-
lution f ∗ h by
f∗h(σ) := 2−m Xω∈{−1,1}m
Here σ·ω denotes the componentwise product of σ and ω and ω−1 denotes the multiplicative
inverse of ω, so that ω · ω−1 = (1, . . . , 1).
Let 0 < ρ < 1. For any σ = (σ1, . . . , σm) ∈ {−1, 1}m, let
(1 + ρσj) = XS⊆{1,...,m}

f (σ·ω−1)h(ω) = XS⊆{1,...,m} bf (S)bh(S)WS(σ),

∀ σ ∈ {−1, 1}m. (12)

Rρ(σ) :=

mYj=1

ρ|S|WS(σ).

(13)

Let f : {−1, 1}m → Mn(C). Deﬁne the Ornstein-Uhlenbeck semigroup Tρf by
∀ σ ∈ {−1, 1}m.

Tρf (σ) := XS⊆{1,...,m}

ρ|S|bf (S)WS(σ) = f ∗ Rρ(σ),

11

(14)

2.8. Truncation of Fourier Coeﬃcients (or Littlewood-Paley Projections). Let
f : {−1, 1}m → Mn(C), and Qf ∈ Mn(C)[X1, . . . , Xm] the multilinear polynomial associ-
ated to f . Let d ∈ N. Let Pd denote projection onto the level-d Fourier coeﬃcients. That
is, ∀ σ ∈ {−1, 1}m, ∀ X1, . . . , Xm ∈ Mn(C),

Pdf (σ) := XS⊆{1,...,m} : |S|=d bf (S)WS(σ),
PdQf (X1, . . . , Xm) := XS⊆{1,...,m} : |S|=d bf (S)Yi∈S
Let P≤d := Pi≤d Pi denote projection onto the Fourier coeﬃcients of degree at most d.
Denote P>df := f − P≤df , P>dQf := Qf − P≤dQf .
2.9. Embeddings.

Xi.

Deﬁnition 2.1 (Matrix embedding). Let A ∈ Mn(C) and p ≥ n. Deﬁne the embedding
ιp : Mn(C) → Mp(C) by

0 0(cid:19) ∈ Mp(C).
k=1 Ck ⊗ Dk ∈ Mn×n(C), extend this deﬁnition to

ιp(A) :=(cid:18)A 0

If B =PN

ιp(B) :=

NXk=1

ιp(Ck) ⊗ ιp(Dk) ∈ Mp×p(C).

Lastly, if f : {−1, 1}m → Mn(C) deﬁne ιp(f ) : {−1, 1}m → Mp(C) by

ιp(f )(σ) = ιp(f (σ))

∀ σ ∈ {−1, 1}m.

We will sometimes denote the same quantities by Aι, Bι and f ι respectively, leaving the
dependence of ι on p and n implicit for clarity of notation.

Note that if Q = PS⊆{1,...,m} bQ(S)Qi∈S Xi ∈ Mn(C)[X1, . . . , Xm] is a noncommutative

polynomial the last item in Deﬁnition 2.1 is equivalent to deﬁning

If moreover Qf is deﬁned from f : {−1, 1}m → Mn(C) as in (9) then

Qι = XS⊆{1,...,m}
f = XS⊆{1,...,m}

Qι

ιp(bQ(S))Yi∈S
ιp(bf (S))Yi∈S

Xi ∈ Mp(C)[X1, . . . , Xm].

Xi ∈ Mp(C)[X1, . . . , Xm].

2.10. Coordinate projections.
Deﬁnition 2.2. Let U ∈ Mn(CN ) with UU∗ = U∗U = I and Q ∈ Mn(C)[X1, . . . , Xm]
a noncommutative polynomial. We denote the Gaussian L2 norm of Q associated to the
ensemble G by

kQkL2,G

:=(cid:16) E
Gj∼GHj∼Hp

Tr(cid:12)(cid:12)Qι{GjHj}(cid:12)(cid:12)2(cid:17)1/2

.

12

Let N (0, 1) denote the standard complex Gaussian distribution for a random variable. For
integers m, N let Egij∼N (0,1) denote expectation with respect to g1,1, . . . , gm,N i.i.d. N (0, 1).
For any function R in the mN random variables gi,j ∼ N (0, 1), for any i = 1, . . . , m, let

PiR(g1,1, . . . , gm,N ) =

NXj=1

Eg′

i,j∼N (0,1)(cid:2)(R{g′i,j})g′i,j(cid:3)gi,j

(15)

be the projection of R onto the linear span of the {gij}j=1,...,N . The projection Pi is naturally
extended to noncommutative polynomials R ∈ Mn(C)[X1, . . . , Xm] by applying it to each
matrix entry of R (when the variables X1, . . . , Xm are themselves matrix-valued functions
of the gi,j). We note the following facts:

Lemma 2.3. Let b : (Mp(C))m → Mp(C) satisfy kb(x)k ≤ 1 for all x ∈ (Mp(C))m.

(1) For any PSD matrix M ∈ Mn×n(C),

E

Gj∼ι(V)
Hj,Jj∼Hp

mXi=1
Tr(cid:0)M ι · (Pib{GjHj} ⊗ Pib{GjJj}) · (H∗i ⊗ J∗i )(cid:1)
Tr(cid:0)M ι · (b{GjH} ⊗ b{GjJ}) · (H∗ ⊗ J∗)(cid:1).
≤ E

Gj∼ι(V)
H,J∼Hp
i=1 PibkL2,G ≤ kbkL2,G

.

(2) kPm
Proof. We begin with (1). Recall that V is deﬁned so that Gj = PN
i=1 gi,jVi = hgj, V i,
where {gi,j}1≤i≤N,1≤j≤m are i.i.d. standard complex Gaussian random variables and gj =
(gj,1, . . . , gj,N ) for any j ∈ {1, . . . , m}. By a density argument, it suﬃces to prove (1) when
b is a polynomial in the entries of its matrix variables. Then we can write

b(hg1, ι(V )iH, . . . ,hgm, ι(V )iH) =

ra ∈ Mp(C), di ∈ N.
(16)
where the coeﬃcients ra do not depend on g1, . . . , gm, but they can depend on H. Note that
for any i ∈ {1, . . . , m} and for any j ∈ {1, . . . , N}, if eij ∈ Nm denotes the vector whose only
nonzero entry is a j in the ith entry, then

a=(a1,...,am)∈{1,2,...,N}m

X

ra

mYi=1

gdi
i,ai,

Eg1,...,gmb(hg1, ι(V )iH, . . . ,hgm, ι(V )iH)g∗i,j = reij .

So,

NXj=1(cid:2)Eg′

1,...,g′

mb(hg′1, ι(V )iH, . . . ,hg′m, ι(V )iH)(g′i,j)∗(cid:3) gi,j =

NXj=1

reij gi,j.

We can express b(hg1, ι(V )iH, . . . ,hgm, ι(V )iH)H∗ as the sum of two terms A and B such
that A contains only linear terms in gi,j where 1 ≤ i ≤ m and 1 ≤ j ≤ N, and B contains

13

only higher order terms in gi,j, as follows,
b(hg1, ι(V )iH, . . . ,hgm, ι(V )iH)H∗

=

1,...,g′

mb(hg′1, ι(V )iH, . . . ,hg′m, ι(V )iH)(g′i,j)∗(cid:3) gi,jH∗

NXj=1(cid:2)Eg′

mXi=1
+(cid:16)(b(hg1, ι(V )iH, . . . ,hgm, ι(V )iH)H∗

mXi=1

NXj=1(cid:2)Eg′

−

1,...,g′

mb(hg′1, ι(V )iH, . . . ,hg′m, ι(V )iH)(g′i,j)∗(cid:3) gi,jH∗(cid:17),

where A corresponds to the term on the second line and B to the terms on the third and
fourth line. For g a standard complex Gaussian random variable, we have E|g|2 gk = 0 for
any positive integer k, thus E(A ⊗ B) = E(B ⊗ A) = 0, and

ETr(ι(M)(A + B)) ⊗ (A + B) = ETr(ι(M)(A ⊗ A)) + ETr(ι(M)(B ⊗ B)).

Since M is PSD, ι(M) is PSD, so we have ETr(ι(M)(B ⊗ B)) ≥ 0, so that

ETr(ι(M) · (b(hg1, ι(V )iH, . . . ,hgm, ι(V )iH)H∗ ⊗ b(hg1, ι(V )iJ, . . . ,hgm, ι(V )iJ)J∗))
mXi=1
≥

ETr(ι(M) · (

1,...,g′

mb(hg′1, ι(V )iH, . . . ,hg′m, ι(V )iH)(g′i,j)∗(cid:3) gi,jH∗
mb(hg′1, ι(V )iJ, . . . ,hg′m, ι(V )iJ)(g′i,j)∗(cid:3) gi,jJ∗))

NXj=1(cid:2)Eg′
NXj=1(cid:2)Eg′
⊗

1,...,g′

=

mXi=1

ETr(ι(M) · (Pib(hg′1, ι(V )iH1, . . . ,hg′m, ι(V )iHm)H∗i

⊗ Pib(hg′1, ι(V )iJ1, . . . ,hg′m, ι(V )iJm)J∗i )),

where the last equality uses the deﬁnition (15) of Pi and the fact that Pib only depends on
the ith variable of b, so that

EHi∼HPib{hg′j, ι(V )iHj}H∗i = EH∼HPib{hg′j, ι(V )iH}H∗.

Item (2) is proven similarly, expanding

b(hg1, ι(V )iH1, . . . ,hgm, ι(V )iHm)

=

mb(hg′1, ι(V )iH1, . . . ,hg′m, ι(V )iHm)(g′i,j)∗(cid:3) gi,j

1,...,g′

NXj=1(cid:2)Eg′

mXi=1
+(cid:16)b(cid:0)hg1, ι(V )iH1, . . . ,hgm, ι(V )iHm(cid:1)

mXi=1

NXj=1(cid:2)Eg′

−

1,...,g′

mb(cid:0)hg′1, ι(V )iH1, . . . ,hg′m, ι(V )iHm(cid:1)(g′i,j)∗(cid:3) gi,j(cid:17)

14

and proceeding as in the proof of (1), so that A is the term on the second line and B the
terms on the third and fourth lines above, and we use

ETr((A + B)(A + B)∗) = ETr(AA∗) + ETr(BB∗) ≥ ETr(AA∗).

(cid:3)

2.11. Bounds on random polynomials. The key diﬀerence between the random matrices
Gi ∼ G and ι(Gi)Hi where Hi ∼ Hp is that the matrices ι(Gi)Hi behave well with respect
to matrix products. This property is exploited in Corollary 2.5 below.
Lemma 2.4. Let p ≥ n, let A, B ∈ Mn(C) be positive semideﬁnite. Then

EH∼Hp(cid:13)(cid:13)AιHBι(cid:13)(cid:13)2 ≤

n2
p kAk2kBk2.

Proof. The nonzero eigenspace K of HBι(Bι)∗H∗ is a uniformly distributed subspace of
dimension at most n of Cp. Given a unit vector x, the squared norm of the projection of x
on K has expectation at most n/p. Applying this to the eigenvectors of A,

EH∼HpTr(AιHBι(Bι)∗H∗(Aι)∗) ≤

n
pkBk2Tr(AA∗) ≤

n2
p kBk2 kAk2 .

To conclude, use that kXk2 ≤ Tr(XX∗), for X = AιHBι.
Corollary 2.5. Let R, S ∈ Mn(C)[X1, . . . , Xm] be multilinear polynomials not depending on
the j-th variable such that Ebj∼B

n Tr|R{bj}|2 ≤ 1 and Ebj∼B

n Tr|S{bj}|2 ≤ 1. Then

(cid:3)

1

1

E

Hi∼Hp (cid:13)(cid:13) (S{GiHi}) GjHj (R{GiHi})∗(cid:13)(cid:13) = On,m(cid:0)p−1/2(cid:1),

Gi∼ι(G)

where the implicit constant may depend on n and m.

Proof. Write S = S0 +Pk6=j SkXk and R = R0 +Pk6=j RkXk, where ∀ k ∈ {1, . . . , m}\{j},
Sk, Rk ∈ Mn(C)[X1, . . . , Xm] depend neither on the k-th or the j-th variable, and S0, R0 ∈
Mn(C). Then for any k 6= j,

E

Gi∼ι(G)
Hi∼Hp

kSGjHjH∗k G∗kR∗kk ≤ E
Gi∼ι(G)
Hi∼Hp
≤(cid:16) E
√p(cid:16) E

kSkkGjHjH∗k G∗kkkR∗kk
kSk2(cid:17)1/2(cid:16) E
Gj kGjk2 E

Gk kG∗kk2(cid:17)1/2

Gi∼ι(G)
Hi∼Hp

Gj ,Gk
Hj ,Hk

≤

n2

,

kGjHjH∗kG∗kk2 E
Gi∼ι(G)
Hi∼Hp

kR∗kk2(cid:17)1/2

where for the last inequality we used Lemma 2.4, the normalization assumption on R, S,
and (17) from Lemma 2.6 below. To conclude use EG∼GkGk2 ≤ EG∼GTr(GG∗) = n.
(cid:3)
Lemma 2.6. Let f : {−1, 1}m → Mn(C) and Q ∈ Mn(C)[X1, . . . , Xm] the multilinear poly-
nomial associated to f . Let 0 ≤ k ≤ m. Let Gi ∼ G and bi ∼ B. Let X = (G1, . . . , Gk) and
Y = (bk+1, . . . , bm). Then

Exi∼X ,yj∼Y |Q{xi, yj}|2 = Ebi∼B |Q{bi}|2 .
Inf i(Qι) = Inf i(Q),
∀ i ∈ {1, . . . , m}.

15

(17)

(18)

Ex∼X ,y∼Y(Q(x, y)(Q(x, y))∗)

(cid:16)bQ(S)(cid:0) Yi∈S : i≤k
= E XS⊆{1,...,m}
= XS⊆{1,...,m} bQ(S)(bQ(S))∗
= Ebi∼B |Q{bi}|2 .

(7)

Gi Yi∈S : i>k

bi(cid:1)(cid:0) Yi∈S : i≤k

Gi Yi∈S : i>k

bi(cid:1)∗(bQ(S))∗(cid:17)

1
n

1
Ebi∼BTr|Qι{bi}|2 =
n
Tr|Qι{GiHi}|2 =

E

1
n

Gi∼ι(G)
Hi∼Hp

Ebi∼BTr|Q{bi}|2 .
1
EGi∼GTr|Q{Gi}|2 .
n

(19)

(20)

Proof. Recall the variables Gi ∼ G are independent, EGi = 0 and EGiG∗i = I for all
1 ≤ i ≤ k. Similarly, the bi ∼ B are independent with Ebi = 0 and Ebib∗i = 1 for all
k + 1 ≤ i ≤ m. So,

Then (17) follows. Equation (18) follows from (11), Deﬁnition 2.1, and Lemma 2.6. Equali-
ties (19) and (20) follow from Deﬁnition 2.1.
(cid:3)

2.12. Unique Games Conjecture. The Unique Games Conjecture is a commonly assumed
conjecture in complexity theory, though its current status is unresolved.
Deﬁnition 2.7 (Unique Games). Let m ∈ N. Let G = G(S,W,E) be a bipartite
graph with vertex sets S and W and edge set E ⊆ S × W. For all (v, w) ∈ E,
let
πvw : {1, . . . , m} → {1, . . . , m} be a permutation. An instance of the Unique Games prob-
lem is L = (G(S,W,E), m,{πvw}(v,w)∈E ). m is called the alphabet size of L. A labeling
of L is a function η : S ∪ W → {1, . . . , m}. An edge (v, w) ∈ E is satisﬁed if and only
if η(v) = πvw(η(w)). The goal of the Unique Games problem is to maximize the fraction
of satisﬁed edges of the labeling, over all such labelings η. We call the maximum possible
fraction of satisﬁed edges OPT(L).
Deﬁnition 2.8 (Unique Games Conjecture, [Kho02, KKMO07]). For every 0 < β <
α < 1 there exists m = m(α, β) ∈ N and a family of Unique Games instances (Ln)n≥1 with
alphabet size m such that no polynomial time algorithm can distinguish between OPT(Ln) <
β or OPT(Ln) > α.

3. Majorization Principle

3.1. A noncommutative hypercontractive inequality. One of the main tools used in
the proof of our majorization principle is a hypercontractive inequality for noncommutative
multilinear polynomials. The inequality bounds the 2K-norm, for K ≥ 1 an integer, of a
polynomial Q by the 2-norm of Q . We refer to this inequality as a (2K, 2) hypercontrac-
tive inequality; it can be considered as a polynomial generalization of the noncommutative
Khintchine inequality between the 2K norm and the 2 norm. (The Khintchine inequality
corresponds to the polynomial Q(X1, . . . , Xm) = X1 and G1 ∼ V; see [MJC+14, Corollary
7.3] or [DR13].)

Recall the deﬁnition of the ensemble G in Section 2.3.

16

Theorem 3.1 ((2K, 2) Hypercontractivity). Let K ∈ N. Let Q be a noncommutative
multilinear polynomial of degree d ∈ N, as in (8). Let Gi ∼ G, where G is such that

(cid:13)(cid:13)E(G1G∗1)K(cid:13)(cid:13) ≤ cK for some cK ≥ 1. Then

E
Gj∼V

Tr|Q{Gj}|2K ≤ (2K − 1)dKcd

K( E
Gj∼V

Tr|Q{Gj}|2)K.

Remark 3.2. As mentioned in [MOO10, Theorem 3.13] or [Nel73, Theorem 4], the best
possible constant in this hypercontractive inequality is (2K − 1)dK in the case that Gi ∼ G
are replaced with bi ∼ B. So, we achieve the optimal constant in this case, since we can use
cK = 1 for all K ∈ N in this case.
The result that hypercontractivity also holds for the variables G1 = b1 ∼ B generalizes a
result of [Gro72, Lemma 6.1].

Proof. It suﬃces to prove the following hypercontractive estimate:
(2K − 1)−1/2c−1/(2K)

, then

K

if ρ ≥ 0 satisﬁes ρ ≤

E
Gj∼V

Tr|TρQ{Gj}|2K ≤ ( E
Gj∼V

Tr|Q{Gj}|2)K.

(21)

K

, and observe that

2K

(21)

E
Gj∼V

= E
Gj∼V

Tr|Q{Gj}|2K (14)

To see that (21) implies the theorem, choose ρ = (2K − 1)−1/2c−1/(2K)
Gi(cid:17)(cid:12)(cid:12)(cid:12)
ρ−|S|bQ(S)Yi∈S
Gi(cid:12)(cid:12)(cid:12)
2(cid:17)K
ρ−|S|bQ(S)Yi∈S
ρ−2|S|Tr|bQ(S)|2(cid:17)K
Tr|bQ(S)|2(cid:17)K

Tr(cid:12)(cid:12)(cid:12)Tρ(cid:16) XS⊆{1,...,m} : |S|≤d
Tr(cid:12)(cid:12)(cid:12) XS⊆{1,...,m} : |S|≤d
≤ (cid:16) E
= (cid:16) XS⊆{1,...,m} : |S|≤d
≤ ρ−2dK(cid:16) XS⊆{1,...,m} : |S|≤d
(17)∧(7)
= (2K − 1)dKcd

K( E
Gj∼V

Tr|Q{Gj}|2)K.

(17)∧(7)

Gj∼V

(22)

(23)

The proof of (21) is by induction on the number m of variables of Q. If m = 0 then there
are no variables and the inequality follows from the elementary inequality Pn
i=1(λi(Q))2K ≤
(Pn
i=1(λi(Q))2)K applied to the singular values of the (deterministic) matrix Q. To establish
the inductive step, write Q = R0 + R1Xm, where R0, R1 depend on at most m − 1 variables
each (for clarity we suppress this dependence from the notation). Note that TρQ = TρR0 +
ρ(TρR1)Xm We begin with a binomial expansion

|TρQ(X1, . . . , Xm)|2K = X(a1,...,a2K )∈{0,1}2K

KYi=1

ρa2i−1TρRa2i−1 X a2i−1

m (X∗m)a2iρa2i(TρRa2i)∗. (24)

17

Any term in the sum for which aj = 0 for an odd number of elements j ∈ {1, . . . , 2K} has
expectation zero. Applying H¨older’s inequality,

Tr|TρQ|2K ≤ ETr|TρR0|2K +

E
Gj∼V

X(a1,...,a2K )∈{0,1}2K :

1

2 P2K

i=1 ai is a positive integer

2KYi=1

ρaihETr|(TρRai)Gai

m|2Ki 1

2K .

(2K)!

(25)
i=1 ai is a positive integer. The number of
2K is repeated in the sum in (25) is

i=1 ρaihETr(cid:16)TρRaiGai
KXℓ=1

Let (a1, . . . , a2K) ∈ {0, 1}2K such that ℓ := 1
times the term Q2K
m(Gai
(cid:0)2K
2ℓ(cid:1) =

2P2K
m)∗TρR∗ai(cid:17)Ki 1
(2K−2ℓ)!(2ℓ)! . That is, (25) can be rewritten as
2ℓ(cid:19)hETr |(TρR1)Gm|2Ki ℓ
ρ2ℓ(cid:18)2K
KhETr |TρR0|2Ki K−ℓ
Tr|TρQ|2K ≤ ETr|TρR0|2K +
For any A, B ∈ Mn(C) it holds that Tr(cid:12)(cid:12)|B| A∗A|B|(cid:12)(cid:12)K ≤ Tr(cid:12)(cid:12)|B|K (A∗A)K |B|K(cid:12)(cid:12) (see

e.g. [Bha97, Theorem IX.2.10]), hence

E
Gj∼V

(26)

K .

Tr(ABB∗A∗)K = Tr(A∗ABB∗)K ≤ Tr((BB∗)K(A∗A)K).

Starting from (26) and applying (27) to each of the inner terms,

(27)

E
Gj∼V

ρ2ℓ(cid:18)2K
KXℓ=1
Tr|TρQ|2K ≤ ETr|TρR0|2K +
ρ2ℓ(cid:18)2K
KXℓ=1
where the second inequality is obtained by applying the inductive hypothesis. For any odd
(J − 2i), or J!! = 1 if (J − 1)/2 < 1. Now if 1 ≤ ℓ ≤ K,
ℓ!

≤ [ETr|R0|2]K +
integer J we denote J!! =Q(J−1)/2

K hETr|TρR1|2Ki ℓ
K hETr |R1|2iℓhETr|R0|2iK−ℓ

KhETr |TρR0|2Ki K−ℓ

2ℓ(cid:19)cℓ/K
2ℓ(cid:19)cℓ/K

(2K)!

(28)

i=0

,

K

2K(2K − 1)!!

2ℓ(cid:19)(cid:18)K
(cid:18)2K

ℓ(cid:19)−1

=

(K − ℓ)!
(2(K − ℓ))!

=

K!

(2ℓ)!
(2K − 1)!!

2ℓ(2ℓ − 1)!!2K−ℓ(2(K − ℓ) − 1)!!
ℓ−1Yi=0
2K − 2i − 1
2ℓ − 2i − 1 ≤ (2K − 1)ℓ.
=
Using this inequality and 0 ≤ ρ ≤ (2K − 1)−1/2c−1/(2K)
ℓ(cid:19),
ℓ(cid:19) ≤(cid:18)K

2ℓ(cid:19)cℓ/K

ρ2ℓ(cid:18)2K

∀ 1 ≤ ℓ ≤ K.

we get

=

Applying this inequality to (28),

Tr|TρQ|2K ≤

E
Gj∼V

K

(2ℓ − 1)!!(2(K − ℓ) − 1)!!
K (cid:18)K
K ≤ ρ2ℓ(2K − 1)ℓcℓ/K
ℓ(cid:19)h E
(cid:18)K
KXℓ=0
=(cid:0) 1Xi=0

E
Gj∼V

Gj∼V

Tr|R1|2iℓh E
Tr(RiR∗i )(cid:1)K = [ E

Gj∼V

Gj∼V

18

Tr|R0|2iK−ℓ

Tr(QQ∗)]K,

where the last equality follows from EG1G∗1 = I.
Corollary 3.3 ((2K, 2) Hypercontractivity). Let K ∈ N. Let Q be a noncommutative
multilinear polynomial of degree d ∈ N, as in (8). Then

(cid:3)

E

Gj∼ι(V)
Hj∼Hp

Tr|Q{Gj}|2K ≤ (2K − 1)dK(K!)d( E
E
Gj∼V
Gj∼V
Tr|Qι{GjHj}|2K ≤ (2K − 1)dK(K!)d( E
Gj∼ι(V)
Hj∼Hp
Tr|Q{bj}|2)K.

Tr|Q{bj}|2K ≤ (2K − 1)dK( E
bj∼B

Tr|Q{Gj}|2)K.
Tr|Qι{GjHj}|2)K.

E
bj∼B

Proof. The ﬁrst inequality follows from Theorem 3.1 using [HT99, Corollary 2.8] to show

that for G ∼ V and any ℓ ∈ N, (cid:13)(cid:13)E(GG∗)ℓ(cid:13)(cid:13) ≤ ℓ! .
To prove the second inequality, we follow the proof of Theorem 3.1 using (cid:13)(cid:13)E(GG∗)ℓ(cid:13)(cid:13) ≤ ℓ!

for any G ∼ ι(V), where the Q used in the proof becomes Qι. Writing Qι = Rι
1Xm, there
are only two required changes. First, the equalities (22) and (23) are justiﬁed by combining
(20) with (17) and (7). For example, (22) is justiﬁed by

0 +Rι

(cid:16) E

Gj∼ι(V)
Hj∼Hp
(20)

Gι

Tr(cid:12)(cid:12)(cid:12) XS⊆{1,...,m} : |S|≤d
= (cid:16) E
= (cid:16) XS⊆{1,...,m} : |S|≤d

Tr(cid:12)(cid:12)(cid:12) XS⊆{1,...,m} : |S|≤d

ρ−|S|cQι(S)Yi∈S

iHi(cid:12)(cid:12)(cid:12)
2(cid:17)K
Gi(cid:12)(cid:12)(cid:12)
2(cid:17)K
ρ−|S|bQ(S)Yi∈S
ρ−2|S|Tr|bQ(S)|2(cid:17)K

Gj∼V

(17)∧(7)

.

And (23) is justiﬁed in the same way. Similarly, the last inequality in the proof is justiﬁed

as

Tr|TρQι|2K ≤

E

Gj∼ι(V)
Hj∼Hp

1|2iℓh E
Tr|Rι
Tr|R1|2iℓh E

Gj∼V

0|2iK−ℓ
Tr|Rι
Tr|R0|2iK−ℓ

Gj∼ι(V)
Hj∼Hp

Gj∼ι(V)
Hj∼Hp

ℓ(cid:19)h E
(cid:18)K
KXℓ=0
ℓ(cid:19)h E
(cid:18)K
KXℓ=0
Gj∼V
Tr|Q|2)K

(20)
=

= ( E
Gj∼V

(20)

= (cid:16) E

Gj∼ι(V)
Hj∼Hp

Tr|Q|2(cid:17)K

.

The last inequality in the Corollary follows directly from Theorem 3.1.

(cid:3)

In summary, Q is hypercontractive when we substitute into Q the noncommutative random
variables GiHi. Since Q is also hypercontractive when we substitute into Q commutative
random variables distributed uniformly in {−1, 1}, we get the standard consequence that Q is
hypercontractive when we substitute into it a mixture of commutative and noncommutative
random variables.

19

Corollary 3.4 ((2K, 2) Hypercontractivity for mixed inputs). Let Gi ∼ G be i.i.d.
random n × n matrices and let Q be a noncommutative multilinear polynomial of degree
d ∈ N such that Q satisﬁes (2K, 2) hypercontractivity for some K ∈ N. That is, assume
there exists cK ≥ 1 such that

Let X = (G1, . . . , Gk). Let bi ∼ B and let Y = (bk+1, . . . , bm). Then

Proof. For any p ≥ 1, let k·kp,Z

= (Ez∼Z kQ(z)kp

p)1/p.

kQk2K,X∪Y =(cid:13)(cid:13)(cid:13) XS⊆{1,...,m} bQ(S) Yi∈S : i≤k
=(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) XS⊆{1,...,m} bQ(S) Yi∈S : i≤k

≤ (2K − 1)k/2ck/(2K)

K

denote the norm kQkp,Z

EGi∼GTr|Q{Gi}|2K ≤ (2K − 1)dKcd

K(EGi∼GTr|Q{Gi}|2)K.
K(cid:0)Ex∼X ,y∼YTr|Q(x, y)|2(cid:1)K.
Ex∼X ,y∼Y Tr|Q(x, y)|2K ≤ (2K − 1)dKcd
bi(cid:13)(cid:13)(cid:13)2K,X∪Y
Gi Yi∈S : i>k
bi(cid:13)(cid:13)(cid:13)2K,X(cid:13)(cid:13)(cid:13)2K,Y
Gi Yi∈S : i>k
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) XS⊆{1,...,m}
(cid:16)bQ(S) Yi∈S : i>k
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) XS⊆{1,...,m}
(cid:16)bQ(S) Yi∈S : i>k
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) XS⊆{1,...,m}
(cid:16)bQ(S) Yi∈S : i≤k
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) XS⊆{1,...,m}
(cid:16)bQ(S) Yi∈S : i≤k
(cid:13)(cid:13)(cid:13) XS⊆{1,...,m} bQ(S) Yi∈S : i≤k

Gi(cid:13)(cid:13)(cid:13)2,X(cid:13)(cid:13)(cid:13)2K,Y
bi(cid:17) Yi∈S : i≤k
Gi(cid:13)(cid:13)(cid:13)2K,Y(cid:13)(cid:13)(cid:13)2,X
bi(cid:17) Yi∈S : i≤k
bi(cid:13)(cid:13)(cid:13)2K,Y(cid:13)(cid:13)(cid:13)2,X
Gi(cid:17) Yi∈S : i>k
bi(cid:13)(cid:13)(cid:13)2,Y(cid:13)(cid:13)(cid:13)2,X
Gi(cid:17) Yi∈S : i>k
bi(cid:13)(cid:13)(cid:13)2,X∪Y
Gi Yi∈S : i>k

≤ (2K − 1)d/2cd/(2K)

K

= (2K − 1)k/2ck/(2K)

K

kQk2K,X∪Y ≤ (2K − 1)k/2ck/(2K)

K

by Theorem 3.1. Next, using Minkowski’s inequality, from the above we get

= (2K − 1)d/2cd/(2K)
= (2K − 1)d/2cd/(2K)
where the third line is by Theorem 3.1.

K

K

kQk2,X∪Y,

,

(cid:3)

3.2. Majorization principle.

1

Theorem 3.5 (Noncommutative Majorization Principle for Increasing Test Func-
tions). Let Q ∈ Mn(C)[X1, . . . , Xm] be a noncommutative multilinear polynomial of degree d
n Tr(Q{bj}Q{bj}∗)) ≤ 1. Let τ := max1≤j≤m Inf jQ. Let Gi ∼ ι(V). Assume
such that Ebj∼B
kE(G1G∗1)2k ≤ c2 and kE(G1G∗1)3k ≤ c3 with c2, c3 ≥ 1.
Let ψ : [0,∞) → R be a function with three continuous derivatives such that ψ′(t) ≥ 0 for
all t ≥ 0. Let a2 = supt≥0 |ψ′′(t)| and a3 = supt≥0 |ψ′′′(t)|. Then

1
n

E

Gi∼ι(G)
Hi∼Hp

Trψ|Qι{GiHi}|2 ≤ E
bj∼B

1
n

Trψ|Q{bj}|2 + a3n3/2(53c2c3)dτ 1/2 + On,m(a2p−1/2).

20

Proof. We show the bound using the Lindeberg replacement method, replacing the m vari-
ables GjHj by bj one at a time, starting from the last, for each j ∈ {1, . . . , m}. Suppose
variables j + 1, j + 2, . . . , m have already been replaced, and write Qι = R + SXj where R, S
do not depend on the jth variable.

Any three times continuously diﬀerentiable F : [0,∞) → R has a Taylor expansion

F (1) = F (0) + F ′(0) +

2Z 1
Let F (t) = Trψ((R + tSX)(R + tSX)∗) for t ∈ [0, 1]. Then

F ′′(0) +

1
2

1

0

(1 − s)2F ′′′(s)ds.

(29)

F (0) = Trψ(RR∗),
F ′(0) = Tr(ψ′(RR∗)(SXR∗ + RX∗S∗)),
F ′′(0) = Tr(ψ′(RR∗)2SXX∗S∗ + ψ′′(RR∗)(SXR∗ + RX∗S∗)2),
F ′′′(t) = Tr(ψ′′((R + tSX)(R + tSX)∗)((SXR∗ + RX∗S∗) + 2tSXX∗S∗)(SXX∗S∗))
+ Tr(ψ′′((R + tSX)(R + tSX)∗)2((SXR∗ + RX∗S∗) + 2tSXX∗S∗)(SXX∗S∗))
+ Tr(ψ′′′((R + tSX)(R + tSX)∗)2((SXR∗ + RX∗S∗) + 2tSXX∗S∗)3).

(30)

(31)

(32)

(33)

For any t ∈ [0, 1], let F1(t) = Trψ((R+tSbj)(R+tSbj)∗) and F2(t) = Trψ((R+tSGjHj)(R+
tSGjHj)∗). From (29),

EF2(1)−EF1(1) = EF ′′2 (0)−EF ′′1 (0)+E

1

2Z 1

0

(1−s)2F ′′′2 (s)ds−E

1

2Z 1

0

(1−s)2F ′′′1 (s)ds, (34)

where we used that EF2(0) = EF1(0) and EF ′2(0) = EF ′1(0). We bound the two diﬀerences
on the right-hand side of (34) separately.

For the ﬁrst, using that ψ′(RR∗) is positive semideﬁnite the ﬁrst term can be bounded as

ETr(ψ′(RR∗)2SGjHjH∗j GjS∗) = ETr(ψ′(RR∗)2S(cid:18)I 0
≤ ETr(ψ′(RR∗)2SS∗)
= ETr(ψ′(RR∗)2Sbjb∗j S∗).

0 0(cid:19) S∗)

(35)

The second term in (32) is readily bounded using Corollary 2.5, from which it follows that
EkSGjHjR∗ + RH∗j G∗j S∗k = On,m(p−1/2), and |ψ′′(t)| ≤ a2 for all t ≥ 0. Combining the two
bounds,

EF ′′2 (0) − EF ′′1 (0) ≤ On,m(cid:0)a2p−1/2(cid:1).

For the second diﬀerence on the right-hand side of (34), there are two terms, corresponding
to the ﬁrst two lines of (33) and the third line respectively. For the ﬁrst two terms we apply
the Cauchy-Schwarz inequality, isolating the last factor SXX∗S∗ and using |ψ′′(t)| ≤ a2 for

21

all t ≥ 0 to bound them by
3a2(cid:16)ETr(cid:0)(SXX∗S∗)2(cid:1)(cid:17)1/2(cid:16)ETr(cid:0)(SXR∗ + RX∗S∗ + 2tSXX∗S∗)2(cid:1)(cid:17)1/2
≤ 3a2(32c2)d(cid:16)ETr(SS∗)(cid:17)(cid:16)6ETr(cid:0)SXR∗RX∗S∗(cid:1) + 12t2ETr(cid:0)(SXX∗S∗)2(cid:1)(cid:17)1/2
≤ 3a2(32c2)d(cid:16)ETr(SS∗)(cid:17)(cid:16)ETr((SXX∗S∗)2)(36ETr((RR∗)2 + 122t2ETr((SXX∗S∗)2))(cid:17)1/4
≤ 36a2(32c2)dn1/2(cid:16)ETr(SS∗)(cid:17)3/2

,

where for the ﬁrst inequality, the ﬁrst term is bounded using Corollary 3.4 (ﬁrst using
Corollary 3.3 to show that hypercontractivity holds for Qι and then applying Corollary 3.4
with Q = Qι) and EXX∗ ≤ I, and the second term is bounded using (A + B + C)(A + B +
C)∗ ≤ 4(AA∗ + BB∗ + CC∗), the second inequality uses Cauchy-Schwarz, and the last again
Corollary 3.4, EXX∗ ≤ I, and ERR∗ ≤ I.
Finally we turn to the second term which appears in the expansion of the second diﬀerence
on the right-hand side of (34) according to (33), corresponding to the third line of (33).
Letting P = SXR∗+RX∗S∗+2tSXX∗S∗, the term can be bounded using H¨older’s inequality
by

a3ETr|P|3 = O(a3)(cid:0)ETr(cid:12)(cid:12)RXS∗(cid:12)(cid:12)3

+ ETr(cid:12)(cid:12)SXX∗S∗(cid:12)(cid:12)3(cid:1)

= O(a3)(cid:0)ETr|SX|6(cid:1)1/2(cid:16)(cid:0)ETr|R|6(cid:1)1/2
= O(a3)n3/2(53c3)d(cid:0)ETr(cid:12)(cid:12)S|2(cid:1)3/2,

+(cid:0)ETr|SX|6(cid:1)1/2(cid:17)

where the last line uses Corollary 3.4 (applied as above) and ESS∗ ≤ I, ERR∗ ≤ I.

Combining all error estimates and using ETrSS∗ = Inf j(Q) we obtain

F1(1) + On,m(p−1/2) + E

1

2nZ 1

0

(1 − s)2|F ′′′2 (s)|ds + E

1

2nZ 1

0

(1 − s)2|F ′′′1 (s)|ds

E

1
n

F2(1)

1
n
1
n

≤ E
≤ E

F1(1) + On,m(a2p−1/2) + a3n1/2(53c3)d(Inf j(Q))3/2.

(36)

Iterating over all m variables,

1
n

E

Gi∼ι(G)
Hi∼Hp

1
n

Trψ |Qι{GiHi}|2 − E
bi∼B
≤ a3n1/2(53c2c3)d(cid:0) max
≤ a3n3/2(53c2c3)d(cid:0) max

1≤j≤m

1≤j≤m

Trψ |Q{bi}|2
Inf j(Q)(cid:1)1/2(cid:16) mXj=1
Inf j(Q)(cid:1)1/2 + On,m(a2p−1/2).

Inf j(Q)(cid:17) + On,m(p−1/2)

22

(cid:3)

Let ψ : R → R be Lipschitz, so that supx6=y∈R |ψ(x)−ψ(y)|
e−x2/2/√2π, φ : R → R. For any λ > 0, deﬁne φλ(x) = λ−1φ(x/λ). Deﬁne

|x−y| ≤ 1. Let x ∈ R and let φ(x) =

ψλ(x) = ψ ∗ φλ(x) =ZR
Then |ψ(x) − ψλ(x)| < λ for all x ∈ R, and | dk

(cid:12)(cid:12)(cid:12)

dk

dxk ψλ(x)(cid:12)(cid:12)(cid:12) ≤ 3λ1−k,

ψ(t)φλ(x − t)dt.

(37)

dxk φλ(x)| ≤ λ1−k for all x ∈ R, so that
∀ x ∈ R,

∀ 1 ≤ k ≤ 3.

Lemma 3.6. Let λ > 0. If ψ is convex, then ψλ is convex, and ψ(x) ≤ ψλ(x) for all x ∈ R.
Proof. The ﬁrst property is a standard diﬀerentiation argument for convolutions. Since
ψ(x+h)+ψ(x−h)−2ψ(x) ≥ 0 for all x, h ∈ R, we also have ψλ(x+h)+ψλ(x−h)−2ψλ(x) ≥ 0.
The second property follows from Jensen’s inequality.

(cid:3)

Let ψ : R → R be deﬁned by

ψ(t) = max(cid:0)0,|t| − 1(cid:1),

∀t ∈ R.

(38)

Theorem 3.7. Let Q ∈ Mn(C)[X1, . . . , Xm] be a noncommutative multilinear polynomial
nTr(|Q{bj}|2) ≤ 1, and let τ = maxi=1,...,m Inf i(Q). Let Gi ∼ ι(G).
of degree d with Ebj∼B
Assume kE(G1G∗1)2k ≤ c2 and kE(G1G∗1)3k ≤ c3 with c2, c3 ≥ 1. Let ψ be as in (38). Then

1

1
n

E

Gi∼ι(G)
Hi∼Hp

Trψ|Qι{GiHi}|2 ≤ E
bj∼B

1
n

Trψ|Q{bj}|2 + n1/2(53c2c3)dτ 1/6 + Om,n(τ−1/3p−1/2).

Proof. Let λ > 0, and deﬁne ψλ as in (37). From Lemma 3.6, ψλ(x) ≥ ψ(x) ≥ 0 for all
x ∈ R. So,

E

Gi∼ι(G)
Hi∼Hp

From Theorem 3.5,

1
n

Trψ |Qι{GiHi}|2 ≤ E
Gi∼ι(G)
Hi∼Hp

1
n

Trψλ |Qι{GiHi}|2 .

1
n

E

Gi∼ι(G)
Hi∼Hp

Trψλ |Qι{GiHi}|2 ≤ Ebi∼B

1
n

Trψλ |Q{bi}|2

+ λ−2(cid:0)O(n3/2)(53c2c3)dτ 1/2 + On,m(p−1/2)(cid:1) .

Using ψλ(x) ≤ ψ(x) + λ for all x ≥ 0,
1
n

Trψλ |Q(σ)|2 ≤

1
n

Trψ |Q(σ)|2 + λ,

∀ σ ∈ {−1, 1}m.

Combining (39), (40) and (41) completes the proof, with a choice of λ such that λ3 =
Θ(n3/2τ 1/2).
(cid:3)

Recall the deﬁnition of Tρ in (14) and the function Chop : R → R, Chop(t) = t if |t| ≤ 1,

Chop(t) = 1 if t ≥ 1, and Chop(t) = −1 if t ≤ −1.

23

(39)

(40)

(41)

Corollary 3.8 (Smoothed Version of Theorem 3.7). Suppose f : {−1, 1}m → Mn(C)
with kf (σ)k ≤ 1 for all σ ∈ {−1, 1}m. Let 0 < ρ < 1 and let τ := maxi=1,...,m Inf if . Assume
kE(G1G∗1)2k ≤ c2 and kE(G1G∗1)3k ≤ c3 for some c2, c3 ≥ 1. Then kQfkL2,G ≤ 1 and

1

n(cid:13)(cid:13)TρQι

f(cid:13)(cid:13)2
L2,G ≤ 10n1/2τ
f − ChopTρQι

1−ρ

30(c2 c3) + Om,n(τ−1/3p−1/2).

(42)

Proof. Using the elementary inequality [max(0, t − 1)]2 ≤ ψ(t2) for all t ≥ 0, where ψ is
deﬁned in (38), applied to the singular values of TρQι
f ,
1
n

(43)

1

n(cid:13)(cid:13)TρQι

f(cid:13)(cid:13)2
f − ChopTρQι

L2,G ≤ E
Gi∼ι(G)
Hi∼Hp

Trψ(cid:12)(cid:12)TρQι

f{GiHi}(cid:12)(cid:12)2 .

We ﬁrst apply Theorem 3.7 to P≤d(TρQι
0 < ρ < 1, (14), (11) imply that

f ), where d ∈ N is to be determined later. Since

max
i=1,...,m

Inf iP≤dTρQf ≤ max

i=1,...,m

Inf iP≤dQf ≤ max

i=1,...,m

Inf iQf ,

and we get by Theorem 3.7,

1
n

E

Gi∼ι(G)
Hi∼Hp

Trψ|P≤dTρQι

f{GiHi}|2 ≤ Ebi∼B

1
n

Trψ|P≤dTρQf{bi}|2

+ n1/2(53c2c3)dτ 1/6 + Om,n(τ−1/3p−1/2).

(44)
For any a, b ∈ R, |ψ((a + b)2) − ψ(a2)| ≤ 2|a| |b| + 2|b|2 follows by |ψ((a + b)2) − ψ(a2)| ≤
dt ψ(t2)(cid:12)(cid:12) ≤ |b| 2(|a| + |b|). Combining with the Cauchy-Schwarz inequality,
|b| maxt∈[a,a+b](cid:12)(cid:12) d
Trψ(cid:12)(cid:12)TρQι
f|2.
f|2)1/2 + 2Tr|TρP>dQι

f(cid:12)(cid:12)2
f(cid:12)(cid:12)2 − Trψ(cid:12)(cid:12)P≤dTρQι

f(cid:12)(cid:12)2
f(cid:12)(cid:12)2 − Trψ(cid:12)(cid:12)TρP≤dQι

= Trψ(cid:12)(cid:12)TρP≤dQι
≤ 2(Tr|TρP≤dQι

f + TρP>dQι
f|2)1/2(Tr|TρP>dQι

Taking expectation values and using (20), (17) and (14), which imply that

Tr|TρP>dQι

f|2 = E
Gj∼G

Tr|TρP>dQι
f|2

E

Gj∼ι(V)
Hj∼Hp

we get

1

n(cid:12)(cid:12)(cid:12)Ebi∼B(cid:16)Trψ(cid:12)(cid:12)TρQι
(cid:16)Trψ(cid:12)(cid:12)TρQι

1
n| E
Gi∼ι(G)
Hi∼Hp

= E
bj∼B

f|2
Tr|TρP>dQι
= XS⊆{1,...,m} : |S|>d
ρ2|S|Tr(bf (S)(bf (S))∗)
≤ ρ2d XS⊆{1,...,m}
Tr(bf (S)(bf (S))∗)
≤ nρ2d,
f{bi}(cid:12)(cid:12)2 − Trψ(cid:12)(cid:12)P≤dTρQι
f{GiHi}(cid:12)(cid:12)2 − Trψ(cid:12)(cid:12)P≤dTρQι

f{bi}(cid:12)(cid:12)2(cid:17)(cid:12)(cid:12)(cid:12) ≤ 4ρd,
f{GiHi}(cid:12)(cid:12)2(cid:17)| ≤ 4ρd.

24

(45)

(46)

Using t ≤ |t| for any t ∈ R,

1

1

f{bi}(cid:12)(cid:12)2(cid:17)
Trψ(cid:12)(cid:12)TρP≤dQι

f{GiHi}(cid:12)(cid:12)2 − Ebi∼BTrψ(cid:12)(cid:12)TρQι

Trψ(cid:12)(cid:12)TρQι

n(cid:16) E
Trψ(cid:12)(cid:12)TρQι
Gi∼ι(G)
Hi∼Hp
n(cid:12)(cid:12)(cid:12) E
f{GiHi}(cid:12)(cid:12)2 − E
≤
n(cid:16) E
Trψ(cid:12)(cid:12)TρP≤dQι
n(cid:12)(cid:12)(cid:12)Ebi∼BTrψ(cid:12)(cid:12)TρP≤dQι

Gi∼ι(G)
Hi∼Hp
1

Gi∼ι(G)
Hi∼Hp

Gi∼ι(G)
Hi∼Hp

f{GiHi}(cid:12)(cid:12)2(cid:12)(cid:12)(cid:12)
f{bi}(cid:12)(cid:12)2(cid:17)
f{GiHi}(cid:12)(cid:12)2 − Ebi∼BTrψ(cid:12)(cid:12)TρP≤dQι
f{bi}(cid:12)(cid:12)2(cid:12)(cid:12)(cid:12)
f{bi}(cid:12)(cid:12)2 − Ebi∼BTrψ(cid:12)(cid:12)TρQι

≤ n1/2(53c2c3)dτ 1/6 + 8ρd + Om,n(τ−1/3p−1/2),

+

+

1

(47)

where the last inequality uses (46) to bound the ﬁrst term, (44) for the second and (45)
for the third. From (13) and (14) we get kTρQf (σ)k ≤ 1 for all σ ∈ {−1, 1}m, so by
deﬁnition of ψ we have Ebi∼BTrψ(cid:12)(cid:12)TρQι
d = min(max(1,− log(τ )
1).

f{bi}(cid:12)(cid:12)2 = 0. Combining (43) with (47) and choosing

30(c2c3) ), m) completes the proof (using − log ρ ≥ 1 − ρ for all 0 < ρ <

(cid:3)

3.3. Moment Majorization. Theorem 3.5 implies that the even moments of a noncom-
mutative multilinear polynomial follow a majorization principle. Although we will not make
use of Theorem 3.9 for the applications in this paper, we include it as the statement could
be of independent interest; the theorem has analogues in both the commutative [MOO10]
and free probability settings [DN14].

Theorem 3.9 (Noncommutative Majorization Principle for 2K th Moments). Let Q
be a noncommutative multilinear polynomial of degree d in m variables, as in (8). Suppose
kQ(σ)k ≤ 1 for all σ ∈ {−1, 1}m. Let p > n, and let Qι be the zero-padded extension of
Q, as deﬁned in Deﬁnition 2.1. Let τ := maxi=1,...,m Inf iQ. Let Gi ∼ G. Assume that

(cid:13)(cid:13)E(G1G∗1)K(cid:13)(cid:13) ≤ cK, for some K ∈ N and cK ≥ 1. Then

1
n

Tr|Qι{GiHi}|2K

E

Gi∼ι(G)
Hi∼Hp

≤ Ebi∼B

1
n

Tr|Q{bi}|2K + K 3(2K − 1)dKcd

Kn2Kτ 1/4 + Om,n(p−1/2K 2τ−1/4).

Proof. We begin with an upper tail estimate for Qι. From Markov’s inequality,

PGi∼ι(G)

Hi∼Hp(cid:0)Tr|Qι|2 > t(cid:1) = PGi∼ι(G)

Hi∼Hp(cid:0)(cid:0)Tr|Qι|2(cid:1)K > tK(cid:1) ≤ t−K E

Hi∼Hp (cid:2)Tr|Qι|2(cid:3)K.

Gi∼ι(G)

Since Qι = I ιQι (where here I denotes the n×n identity matrix), H¨older’s inequality implies
(50)

= E

E

Tr|Qι|2K .

Hi∼Hp (cid:0)Tr|Qι|2(cid:1)K

Gi∼ι(G)

Hi∼Hp (cid:0)Tr|I ιQι|2(cid:1)K ≤ nK−1 E

Gi∼ι(G)

Gi∼ι(G)
Hi∼Hp

25

(48)

(49)

Combining (49) and (50) and applying Theorem 3.1,

PGi∼ι(G)

Hi∼Hp(cid:0)Tr|Qι|2 > t(cid:1) ≤ t−KnK−1(2K − 1)dKcd
≤ t−Kn2K−1(2K − 1)dKcd
K.

K(cid:16) E

Gi∼ι(G)
Hi∼Hp

Tr|Qι|2(cid:17)K

(51)

Let s > 0 be a constant to be ﬁxed later. Deﬁne ψ : [0,∞) → [0,∞) so that ψ(t) = tK for
any 0 ≤ t ≤ s, and ψ is linear with slope KsK−1 on (s + 1,∞). It is possible to construct
such a ψ with all three derivatives bounded, and in particular the third derivative bounded
by some a3 ≤ K 3sK−3 on the interval [s, s + 1]. From Theorem 3.5,

1
n

Trψ |Qι{GiHi}|2

E

Gi∼ι(G)
Hi∼Hp

1
n

Trψ |Q{bi}|2 + K 3sK−3n3/2(53c2c3)dτ 1/2 + On,m(p−1/2K 2sK−2).

(52)

≤ E
bi∼B

Finally, deﬁning a = K 3sK−3n3/2(53c2c3)dτ 1/2 + On,m(p−1/2K 2sK−2) and letting D be the

event that Trψ |Qι{GiHi}|2 ≤ s, we have

1
n

E

Gi∼ι(G)
Hi∼Hp

Tr|Qι{GiHi}|2K = E
Gi∼ι(G)
Hi∼Hp

= E

Gi∼ι(G)
Hi∼Hp

1
n

1
n

Tr|Qι{GiHi}|2K · 1D + E
Gi∼ι(G)
Hi∼Hp

Trψ |Qι{GiHi}|2 + E
Gi∼ι(G)
Hi∼Hp

1
n

Tr|Qι{GiHi}|2K · 1Dc

1
n

Tr|Qι{GiHi}|2K · 1Dc.

Using (52) to bound the ﬁrst term and Cauchy-Schwarz for the second, the above can be
bounded as

1
n

E

Gi∼ι(G)
Hi∼Hp

Tr|Qι{GiHi}|2K

Gi∼ι(G)
Hi∼Hp

Trψ |Q{bi}|2 + a +(cid:16) E
Trψ |Q{bi}|2 + a +(cid:16) E
Trψ |Q{bi}|2 + a + (2K − 1)dKcd

Gi∼ι(G)
Hi∼Hp

1
n

1
n

1
n

≤ E
bi∼B

≤ E
bi∼B

≤ E
bi∼B

n

Tr|Qι{GiHi}|2K(cid:17)2(cid:17)1/2(cid:16) E
(cid:16) 1
Tr|Qι{GiHi}|4K(cid:17)1/2(cid:16)PGi∼ι(G)

Gi∼ι(G)
Hi∼Hp

Hi∼Hp(cid:0)Tr|Qι|2 > s(cid:1)(cid:17)1/2

1Dc(cid:17)1/2

K(cid:0) E

Gi∼ι(G)
Hi∼Hp

Tr|Qι{GiHi}|2(cid:1)Ks−K/2nK−1,

using Theorem 3.1 to bound the ﬁrst term inside a square root, and (51) for the second.
Tr|Qι{GiHi}|2 ≤ 1.
Finally, using Lemma 2.6 and kQ(σ)k ≤ 1 for all σ ∈ {−1, 1}m, EGi∼ι(G)
Hi∼Hp
Choosing s = τ−1/(4K) ﬁnishes the proof.

(cid:3)

26

4. Dictatorship testing

Fix integers m, n, N ∈ N, M ∈ Mn×n(C) and V ∈ Un(CN ). Given f : {−1, 1}m → Mn(C),
f =PS⊆{1,...,m} bf (S)WS, deﬁne B(f ) : {−1, 1}m → Mn(C) by
where the partial trace Tr2 is deﬁned for any X = Pi Ai ⊗ Bi ∈ Mn×n(C) as Tr2(X) =
Pi AiTr(Bi) and for any U, V ∈ Mn(CN ), U ⊙ V is as deﬁned in Remark 1.11. For any
f, h : {−1, 1}m → Mn(C) let

Tr2(cid:16)(V ⊙ V )M(cid:0)I ⊗ bf (S)(cid:1)(cid:17)WS,

B(f ) := XS⊆{1,...,m} : |S|=1

(53)

OBJ(f ) := OBJ(f, f ).

(54)

OBJ(f, h) := 2−m Xσ∈{−1,1}m

Tr(cid:0)f (σ)B(h)(σ)(cid:1),

Using the identity

Tr(A1(C ⊗ C′)D1) = Tr(C Tr2[D1A1(I ⊗ C′)])
valid for any C, C′ ∈ Mn(C) and A1, D1 ∈ Mn×n(C), we can rewrite

OBJ(f, h) = XS⊆{1,...,m}: |S|=1

Tr(cid:0)(V ⊙ V )M( ˆf (S) ⊗ ˆh(S)(cid:1).

(55)

We interpret OBJ(f, h) as a “dictatorship test”, where dictators are fuctions f : {−1, 1}m →
Mn(C) such that there exists i ∈ {1, . . . , m} such that for all σ ∈ {−1, 1}m, f (σ) = σiIn.
The main lemmas of this section state the completeness and soundness properties of this
test.
Lemma 4.1 (Completeness). Let f : {−1, 1}m → Mn(C) be a dictator. Then

OBJ(f ) = Tr(cid:0)(V ⊙ V )M(cid:1).

Proof. Follows directly from (55).
Lemma 4.2 (Soundness). Assume M ∈ Mn×n(C) is PSD. For any ε > 0 there is τ =
(ε/n)O(ε−1) such that for any f : {−1, 1}m → Mn(C) such that kf (σ)k ≤ 1 for all σ ∈
{−1, 1}m and maxi=1,...,m Inf if ≤ τ it holds that
OBJ(f ) ≤ (1 + ε) sup
R,Z∈Un

Tr(cid:0)M · (R ⊗ Z)(cid:1).

(cid:3)

The proof of Lemma 4.2 involves the introduction of a Gaussian analogue of (54). For

any f, h : {−1, 1}m → Mn(C) and p ≥ n, deﬁne
Tr(cid:16)M ι ·(cid:0)PiQι

Cp(f, h) :=

mXi=1

E

Gj∼ι(V)
Hj,Jj∼Hp

f{GjHj} ⊗ PiQι

h{GjJj}(cid:1) · (H∗i ⊗ J∗i )(cid:17),

(56)

where the coordinate projection Pi is deﬁned in (15). For any 0 < τ < 1 and p ≥ n, deﬁne
(57)

sup

Cp(f, f ).

Cτ,p :=

f : {−1,1}m→Un
maxi=1,...,m Inf if≤τ

The following lemma equates the two quadratic forms OBJ(f, h) and Cp(f, h).

27

Lemma 4.3. For any f, h : {−1, 1}m → Mn(C) and p ≥ n,

OBJ(f, h) = Cp(f, h).

Proof. From the deﬁnition (56), Cp(f, h) equals

mXi=1

E

Gj∼ι(V)
Hj ,Jj∼Hp

f{GjHj} ⊗ PiQι

h{GjJj}(cid:1) · (H∗i ⊗ J∗i )(cid:17)

Tr(cid:16)M ι ·(cid:0)PiQι
= Tr(cid:16) XS⊆{1,...,m} : |S|=1
= Tr(cid:16) XS⊆{1,...,m} : |S|=1

= OBJ(f, h),

ι(M)(ι(cid:0)bf (S) ⊗ ι(bh(S))(cid:1)(ι(V ) ⊙ ι(V ))(cid:17)
M(cid:0)bf (S) ⊗bh(S))(cid:1)(V ⊙ V )(cid:17)

where the ﬁrst equality follows since the expectation over Hj, Jj itself projects onto the linear
terms of Qf , Qh, and the last follows from (55).
(cid:3)

The motivation for introducing Cp(f, h) is the following. On the one hand, Lemma 4.1
tells us the value of Cp(f, f ) when f is a dictatorship function. On the other hand, when f
has low inﬂuences and kf (σ)k ≤ 1 for all σ ∈ {−1, 1}m, we will show that Qf with random
matrix inputs typically has operator norm bounded by 1. This will let us relate Cp(f, h) to
the right-hand side of the inequality stated in Lemma 4.2.

Based on Lemma 4.3, in order to prove Lemma 4.2 it suﬃces to establish the following.
Lemma 4.4. Assume M ∈ Mn×n(C) is PSD. For any ε > 0, for suﬃciently small τ =
(ε/n)O(ε−1) and large enough p (depending on τ, ε, n, m),

Cτ,p ≤ (1 + ε) sup
R,Z∈Un

Tr(cid:0)M · (R ⊗ Z)(cid:1)

Proof. The proof is based on the majorization principle developed in Section 3. To apply
the principle, we introduce the following smoothed, truncated analogue of Cp(f, h), for any
1/2 < ρ < 1,

˜Cp,ρ(f, h) :=

mXi=1

E

Gj∼ι(V)
Hj ,Jj∼Hp

Tr(cid:16)M ι ·(cid:0)PiChopTρQι

f{GjHj}⊗PiChopTρQι

h{GjJj}(cid:1)· (H∗i ⊗ J∗i )(cid:17).

(58)

(59)

(60)

(61)

Using (1) from Lemma 2.3,

˜Cp,ρ(f, f ) ≤ E

Tr(cid:0)M ι ·(cid:0)ChopTρQι
Tr(cid:0)M · (R ⊗ Z)(cid:1),

Gj∼ι(V)
H,J∼Hp
≤ sup
R,Z∈Un

f{GjH} ⊗ ChopTρQι

f{GjH}(cid:1) · (H∗ ⊗ J∗)(cid:1)

where the second inequality follows since ChopTρQι
conclude the proof of the lemma it will suﬃce to show that, for appropriate τ, ρ and p,

f has operator norm at most 1. To

sup

maxi=1,...,m Inf if≤τ(cid:0)Cp(f, f ) − ˜Cp,ρ(f, f )(cid:1) ≤ ε sup

f : {−1,1}m→Un

28

Tr(cid:0)M · (R ⊗ Z)(cid:1).

R,Z∈Un

To alleviate notation write

Ai = PiTρQι

f{GjHj},

˜Ai = PiChopTρQι

f{GjHj}

and similarly

Then

Bi = PiTρQι

Cp(f, f ) =

mXi=1

E

Gj∼ι(V)
Hj ,Jj∼Hp

= ρ−2

E

Gj∼ι(V)
Hj ,Jj∼Hp

= ρ−2 ˜Cp,ρ(f, f ) + ρ−2

+ ρ−2

mXi=1

f{GjJj}(cid:1) · (H∗i ⊗ J∗i )(cid:17)

f{GjJj}.

˜Bi = PiChopTρQι

f{GjHj} ⊗ PiQι

f{GjJj},
Tr(cid:16)M ι ·(cid:0)PiQι
mXi=1
Tr(cid:0)M ι · (Ai ⊗ Bi) · (H∗i ⊗ J∗i )(cid:1)
mXi=1
Tr(cid:0)M ι · (Ai ⊗ (Bi − ˜Bi)) · (H∗i ⊗ J∗i )(cid:1),

Gj∼ι(V)
Hj ,Jj∼Hp

E

E

Gj∼ι(V)
Hj,Jj∼Hp

Tr(cid:0)M ι · ((Ai − ˜Ai) ⊗ ˜Bi) · (H∗i ⊗ J∗i )(cid:1)

(62)

where the second equality uses that Pi projects on the linear part of the multilinear poly-
f , on which Tρ amounts to multiplication by ρ−1. Interpreting (Ai − ˜Ai)i=1,...,m as
nomial Qι
vector-valued matrices and using (2) from Lemma 2.3,

mXi=1

(cid:13)(cid:13)(cid:13)

Ai − ˜Ai(cid:13)(cid:13)(cid:13)

2

L2,V ≤(cid:13)(cid:13)TρQι
≤ 10n3/2τ

f{GjHj} − ChopTρQι

1−ρ

500 + Om,n(τ−1/3p−1/2),

f{GjHj}(cid:13)(cid:13)2

L2,V

1−ρ

500 + Om,n(τ−1/3p−1/2)(cid:1)

where the second inequality follows from Corollary 3.8. A similar bound holds for the terms
involving Bi − ˜Bi, so that from (62) we get
Cp(f, f ) ≤ ρ−2 ˜Cp,ρ(f, f ) +(cid:0)20n3/2τ
Tr(M ι· (R⊙ Z)). (63)
First we take ρ close enough to 1 that ρ−2 ≤ (1 + ε/2). Next, bounding the supremum in
the right-hand side of (63) using the noncommutative Grothendieck inequality (4), choosing
τ small enough as a function of n, ρ and ε so that 20n3/2τ
500 ≤ ε/8, and ﬁnally p large
enough so that Om,n(τ−1/3p−1/2) ≤ ε/8 as well, using (60) the right-hand side of (63) is at
most ˜Cp,ρ(f, f ) + ε supR,Z∈Un Tr(M · (R ⊗ Z)). So, taking the supremum over suitable f in
(63) proves (61), proving the lemma.

R,Z∈Up(CN )

sup

1−ρ

(cid:3)

5. Unique games hardness

In this section we prove Theorem 1.12, implementing the dictatorship vs. low inﬂuences
machinery using the tools introduced in Section 4. Let L(G(S,W,E), m,{πvw}(v,w)∈E ) be
a Unique Games instance as in Deﬁnition 2.7. Let v ∈ S. Let F : W × {−1, 1}m → Un.

29

Deﬁne Fw(σ) := F (w, σ), for all w ∈ W and for all σ = (σ1, . . . , σm) ∈ {−1, 1}m. For each
(v, w) ∈ E, since πvw : {1, . . . , m} → {1, . . . , m} is a bijection we can deﬁne
∀ σ ∈ {−1, 1}m.

Fw ◦ πvw(σ) := Fw(σπvw(1), . . . , σπvw(m)),

For any v ∈ S deﬁne Fv : {−1, 1}m → Un by

Fv(σ) := E(v,w)∈E (Fw ◦ πvw(σ)),

∀ σ ∈ {−1, 1}m.

Unless otherwise stated, expectations and probabilities involving S,W and E will always be
taken with respect to the uniform distribution on these sets.

Let M ∈ Mn×n(C) be PSD, and V ∈ Un(CN ) such that

Tr(M · (V ⊙ V )) = λ1 :=

sup

W,Z∈Un(CN )

Tr(M · (W ⊙ Z)).

(64)

When M ∈ Mn×n(C) is PSD the NCGI becomes simpler in the following sense.

Lemma 5.1. Let M ∈ Mn×n(C) be PSD and let D be a set of matrices. Then

sup

Tr(M(X ⊗ X)).
Proof. Since M is PSD, by deﬁnition it can be written as M = PN
for some M1, . . . , MN ∈ Mn(C). Let X, Y ∈ D. Using the Cauchy-Schwarz inequality,

Tr(M(X ⊗ Y )) = sup
X∈D

X,Y ∈D×D

i=1 Mi ⊗ Mi ∈ Mn×n(C)

Tr(M(X ⊗ Y )) =

Tr(MiX)Tr(MiY )

NXi=1
≤(cid:16) NXi=1

|Tr(MiX)|2(cid:17)1/2(cid:16) NXi=1

|Tr(MiY )|2(cid:17)1/2

,

with equality if X = Y .

Let 0 < ρ < 1. We investigate the quantity
sup

OPTρ,L(M) :=

F : W×{−1,1}m→Un

Ev∈S [OBJ(TρFv)],

(cid:3)

(65)

where OBJ(f ) is deﬁned in (54).
Lemma 5.2 (Completeness). Suppose the Unique Games instance L has an almost satis-
fying labeling, i.e. (1 − ε) fraction of the labels are satisﬁed. Then ∀ 0 < ρ < 1,

OPTρ,L(M) ≥ (1 − ε)ρλ1.

Proof. Let η : S ∪ W → {1, . . . , m} be a labeling such that, for at least a 1 − ε fraction of
edges (v, w) ∈ E,
For each i ∈ {1, . . . , m}, let f dict,i(σ) := σiI, for all σ ∈ {−1, 1}m. Deﬁne F : W×{−1, 1}m →
Un so that, for every w ∈ W, Fw : {−1, 1}m → Un satisﬁes Fw := f dict,η(w). Note that
f dict,j ◦ π = f dict,π(j) for any j ∈ {1, . . . , m}. To see this, assume that σπ(j) = ℓ. Then
f dict,π(j)(σ) = ℓ1n×n since σπ(j) = ℓ. Also, (f dict,j ◦ π)(σ) = f dict,j(σπ(1), . . . , σπ(n)) = ℓ1n×n,
since σπ(j) = ℓ. Therefore, if πvw(η(w)) = η(v), then Fw ◦ πvw = f dict,πvwη(w) = f dict,η(v), and
by Lemma 4.1,

πvw(η(v)) = η(w).

OBJ(TρFw ◦ πvw) = ρλ1.

30

If πvw(η(w)) 6= η(v), then since M is PSD, |OBJ(TρFw ◦ πvw)| ≥ 0. Therefore, OPTρ,L(M)
is at least (1 − ε)ρλ1.
Lemma 5.3. Let 0 < τ, ρ < 1 and suppose that there exists ε > 0 such that OPTρ,L(M) >
Cτ,p + 2ε. Then the Unique Games instance L has a labeling satisfying at least an ετ 2(1 −
ρ)n−2λ−1
Proof. Assume that OPTρ,L(M) > Cτ,p + 2ε, and let F be such that2

1 /4 fraction of its edges.

(cid:3)

Ev∈S [OBJ(TρFv)] = OPTρ,L(M).

Using OBJ(TρFv) ≤ λ1,

Pv∈S(OBJ(TρFv) > Cτ,p + ε) > ε/λ1.

(66)
If v ∈ S satisﬁes OBJ(TρFv) > Cτ,p + ε, then by deﬁnition of Cτ,p in (57) and Lemma 4.3,
there exists an i0 ∈ {1, . . . , m} such that Inf i0TρFv > τ . Using convexity of the function
A 7→ Tr(AA∗) for A ∈ Mn(C), we see from the deﬁnition (7) of the inﬂuence that Inf i0(f ) is
a convex function of f . Therefore,

τ < Inf i0TρFv = Inf i0(E(v,w)∈E TρFw ◦ πvw) ≤ E(v,w)∈E (Inf i0TρFw ◦ πvw) .

Using Inf i0TρFw ◦ πvw ≤ n, we deduce that if v ∈ S satisﬁes OBJ(TρFv) > Cτ,p + ε then for
this ﬁxed v,
(67)

For any w ∈ W, deﬁne a set of labels L(w) ⊆ {1, . . . , m} by

P(v,w)∈E (Inf i0TρFw ◦ πvw > τ /2) > τ /(2n).
L(w) :=(cid:8)i ∈ {1, . . . , m} : Inf iTρFw > τ /2(cid:9),

and note that if v, w and i are such that Inf iTρFw ◦ πvw > τ /2 then Inf π−1
vw iTρFw > τ /2,
i.e. π−1
vw i ∈ L(w). We now deﬁne a labeling η : S ∪ W → {1, . . . , m}. If v ∈ S satisﬁes
OBJ(TρFv) > Cτ,p + ε, then as shown above there exists an i0 ∈ {1, . . . , m} such that
Inf i0TρFv > τ , and we let η(v) := i0; otherwise we deﬁne η(v) arbitrarily. For w ∈ W, let
η(w) be chosen uniformly at random in L(w) in case L(w) 6= ∅, and arbitrarily otherwise.
Since kFw(σ)k ≤ 1 for all σ ∈ {−1, 1}m we have kFwkL2,G ≤ n for all w ∈ W . Therefore,
as noted after (11),

mXi=1

Inf iTρFw = XS⊆{1,...,m}

|S| ρ|S|Tr(|cFw(S)|2) ≤ n(max

t≥0

tρt) ≤ n(1 − ρ)−1,

and |L(w)| ≤ 2τ−1n(1− ρ)−1. By deﬁnition of η any (v, w) ∈ E such that both events in (66)
and (67) hold leads to a satisﬁed edge with probability at least |L(w)|−1, hence we have
shown that the unique games instance has value at least (ετ /(2n))λ−1
1 (τ /2)(1 − ρ)n−1. (cid:3)
Proof of Theorem 1.14. Let K = KC = 2 be the inﬁmum of all constants KC such that (4)
holds for all PSD M. By deﬁnition of K and λ1 there exists an n and a PSD matrix
M ∈ Mn×n(C) such that λ1/OPT(M) > K − δ/2.
For an instance L of Unique Games and appropriately chosen ρ and c, s, consider the
problem of deciding whether OPTρ,L(M) > s or OPTρ,L(M) < c, where OPTρ,L is deﬁned
in (65).

2The supremum in (65) is always achieved.

31

that

If L has an (1 − α)-satisfying labeling for some 0 < α < 1, then by Lemma 5.2 it holds
(68)
Conversely, assume that no assignment satisﬁes more than a fraction β of edges in L. By
the contrapositive of Lemma 5.3, as long as

OPTρ,L(M) ≥ (1 − α)ρλ1.

(69)
it must be that OPTρ,L(M) ≤ Cτ,p + 2ε. Choosing τ small enough (depending on ε and n)
so that Lemma 4.4 applies, we deduce that

ετ 2(1 − ρ)n−2λ−1

1 /4 > β

OPTρ,L(M) ≤ (1 + ε)OPT(M) + 2ε.

(70)
In summary, deciding whether OPTρ,L(M) > (1− α)ρλ1 or OPTρ,L(M) < (1 + ε)OPT(M) +
2ε could disprove the Unique Games Conjecture (UGC). That is, approximating the quantity
OPT(M) within a multiplicative factor smaller than (1− α)ρλ1/((1 + ε)OPT(M) + 2ε) is as
computationally hard as disproving UGC.
Now we choose parameters. First select ε small enough so that (1 + ε)OPT(M) + 2ε ≤
OPT(M)(1 + δ/8), and then select α small enough and ρ close enough to 1 so that (1 −
α)ρ(K − δ/2) > K − 3δ/4. Finally, choose τ , and p, so that the application of Lemma 4.4 in
(70) is justiﬁed, and β small enough so that (69) holds. In summary for any δ > 0 assuming
UGC is it is always possible to ﬁnd a family of instances L with parameters α and β, hence
a family of instances of NCGI with parameters ρ and c, s such that all constraints above
are satisﬁed, i.e. c/s > K − δ and it is NP-hard to decide whether OPTρ,L(M) > s or
OPTρ,L(M) < c for any δ > 0.
Proof of Theorem 1.12. Theorem 1.12 follows immediately from Theorem 1.14, since [BRS15,
Theorem 1.2] implies that K = KC = 2 in Theorem 1.14.
(cid:3)
Proof of Theorem 1.16. For each i ∈ {1, . . . , n}, let fi, hi : {−1, 1}m → Mn(C) and let Ui ∈
Mn(CN ). Theorem 1.16 is proven in the same way as Theorem 1.14, using slightly diﬀerent
deﬁnitions for the main quantities used in the proof. Speciﬁcally, for any (f1, . . . , fn), deﬁne

(cid:3)

Bij(cid:18) XS⊆{1,...,m}

and for 0 < ρ < 1 and p > n deﬁne

OBJ((f1, . . . , fn), (h1, . . . , hn)) :=

Finally, deﬁne

Cτ,p(M) := sup(cid:26)E

nXi,j

nXi,j=1

ι(bfi(S))ι(Mij)WS,

ι(bfi(S))WS(cid:19) := XS⊆{1,...,m} : |S|=1
2−m Xσ∈{−1,1}m
fi(hg1, ι(Ui)iH1, . . . ,hgm, ι(Ui)iHm)(cid:3)
fj (hg1, ι(Ui)iH1, . . . ,hgm, ι(Ui)iHm)i∗(cid:1) :
Inf ifj ≤ τ}.

Tr(cid:0)ι(Mij)(cid:2)ChopTρQι
·hChopTρQι

i (σ)BijTρhι

Tr[Tρf ι

j(σ)].

(71)

(72)

(73)

(cid:3)

fi : {−1, 1}m → Un(C),

max

i,j=1,...,m

32

6. Maximizing Noncommutative Noise Stability

By adapting the proof of the Majority is Stablest Theorem from [MOO10], we can get the

following consequence of Corollary 3.8.

30(c2 c3) + Om,n(τ−1/3p−1/2).
Corollary 6.1. Let 0 ≤ ρ < 1 and let ε > 0. Let δ = 20n1/2τ
Then there exists τ > 0 such that, if f : {−1, 1}m → Mn(C) satisﬁes kf (σ)k ≤ 1 for all
σ ∈ {−1, 1}m, Ebi∼Bf{bi} = 0, and maxi=1,...,m Inf i(f ) < τ , then

1−ρ

1
n

Tr|TρQf{bi}|2 ≤

E
bi∼B

1
n

Tr(cid:12)(cid:12)ChopTρQι

f{GiHi}(cid:12)(cid:12)2 + O(ε + δ).

E

Gi∼ι(G)
Hi∼Hp

(74)

n

Tr ChopTρQι

EGi∼ι(G)
Hi∼Hp

f{GiHi}| ≤ δ.

Moreover, | 1
Remark 6.2. In the case n = p = 1, it is known from [MOO10] that the right-hand side
of (74) is 2
π arcsin ρ + ε. For larger n, the left side of (74) can be interpreted as the noise
stability of Qf with discrete inputs, and the right side as the noise stability of a function with
operator norm pointwise bounded by 1 under random Gaussian matrix inputs. Eq. (74) can
thus be thought of as a matrix-valued version of one of the two main steps in the proof of the
Majority is Stablest Theorem. However, for larger n, there seems to be no version of Borell’s
isoperimetric inequality that describes what the right-hand side of (74) should be. (Recall
that Borell’s isoperimetric inequality states that the noise stability of a subset of Euclidean
space of ﬁxed Gaussian measure is maximized when the set is a half space.)
Proof. Since Ebi∼B f{bi} = 0, we have Ebi∼B Qf{bi} = 0. Using the Cauchy-Schwarz inequal-
ity and Corollary 3.8,

f{GiHi}(cid:12)(cid:12)2(cid:12)(cid:12)(cid:12)

E

Gi∼ι(G)
Hi∼Hp
1
n

E

E

1
n

1
n

Gi∼ι(G)
Hi∼Hp
1
n

(cid:12)(cid:12)(cid:12)
=(cid:12)(cid:12)(cid:12)
+(cid:12)(cid:12)(cid:12)
≤(cid:16)(cid:13)(cid:13)TρQι

Tr(cid:12)(cid:12)TρQι
f{GiHi}(cid:12)(cid:12)2 −
f{GiHi}(cid:12)(cid:12)2 −
Tr(cid:12)(cid:12)TρQι
Tr(cid:16)ChopTρQι
f{GiHi}(cid:13)(cid:13)2,V

Gi∼ι(G)
Hi∼Hp
1
n

Gi∼ι(G)
Hi∼Hp

1−ρ

E

+(cid:13)(cid:13)ChopTρQι

30(c2c3) + Om,n(τ−1/3p−1/2).

E

Tr(cid:12)(cid:12)ChopTρQι
Tr(cid:16)ChopTρQι
f{GiHi}]∗(cid:17) −
f{GiHi}(cid:13)(cid:13)2,V(cid:17) ·(cid:13)(cid:13)TρQι

1
n

Gi∼ι(G)
Hi∼Hp
f{GiHi}[TρQι

f{GiHi}[TρQι

f{GiHi}]∗(cid:17)(cid:12)(cid:12)(cid:12)
Tr(cid:12)(cid:12)ChopTρQι
Gi∼ι(G)
Hi∼Hp
f{GiHi} − ChopTρQι

E

f{GiHi}(cid:12)(cid:12)2(cid:12)(cid:12)(cid:12)
f{GiHi}(cid:13)(cid:13)2,V

≤ 20n1/2τ
Therefore,

1
n

E
bi∼B

Tr|TρQf{bi}|2 (17)∧(20)

=

1
n

E

Gi∼ι(G)
Hi∼Hp

Tr(cid:12)(cid:12)TρQι
Tr(cid:12)(cid:12)ChopTρQι

f{GiHi}(cid:12)(cid:12)2
f{GiHi}(cid:12)(cid:12)2 + 20n1/2τ

1
n

≤

E

Gi∼ι(G)
Hi∼Hp

proving (74).

33

1−ρ

30(c2c3) + Om,n(τ−1/3p−1/2),

Using Ebi∼B Qι

f{bi} = EGi∼ι(G)
Hi∼Hp

TρQι

f{GiHi} = 0 and the Cauchy-Schwarz inequality,

1

n(cid:12)(cid:12)(cid:12) E

Gi∼ι(G)
Hi∼Hp

Tr ChopTρQι

f{GiHi}(cid:12)(cid:12)(cid:12) =

1

n(cid:12)(cid:12)(cid:12) E

Gi∼ι(G)
Hi∼Hp
≤ 20n1/2τ

Tr ChopTρQι

f{GiHi} − E
Gi∼ι(G)
Hi∼Hp

1−ρ

30(c2c3) + Om,n(τ−1/3p−1/2),

Tr TρQι

f{GiHi}(cid:12)(cid:12)(cid:12)

(cid:3)

using Corollary 3.8 again.

7. An Anti-Concentration Inequality

As in [MOO10], we can use our invariance principle to prove anti-concentration estimates

of polynomials.

Corollary 7.1 (An Anti-Concentration Estimate). There exists a constant C > 0
such that the following holds. Let Q : (Mn(C))m → Mn(C) be a noncommutative multilinear
n Tr|Q{bi}|2 ≤ 1. Let τ = max1≤j≤m Inf j(Q). Deﬁne
polynomial of degree d. Assume Ebi∼B
Var(Q) = Ebi∼BTr|Q{bi} − (Ebj∼BQ{bj})|2. Then, for any t ∈ R,

1

1
n

P

Gi∼ι(G)
Hi∼H

(kQι{GiHi}k > t) ≤ P
bi∼B

(kQ{bi}k > t) + O(n3cd

3τ 1/100)

+ Cd(4τ 1/100n/[Var(Q)]1/2)1/d + Om,n(τ−1/100p−1/2).

Remark 7.2. The 1
n term on the left side of the inequality seems to be an artifact of our
proof method. It comes from (77) below, where we bound the normalized trace of a matrix
by its operator norm.

1−x2(cid:1) if |x| < 1 and φ(x) = 0 for all other x ∈ R.
Proof. Deﬁne φ : R → R by φ(x) = c · exp(cid:0) −1
The constant 1/2 < c < 4 is chosen so that RR φ(x)dx = 1. It is well-known that φ is an
Fix r, s > 0. Deﬁne ψ : R → R by

inﬁnitely diﬀerentiable function with bounded derivatives.

ψ(x) =

Deﬁne ψλ(x) = ψ ∗ φλ(x) = RR ψ(y)φλ(x − y)dy. Then ψλ(x) = ψ(x) for any x ∈ R with
x > r + s + λ or x < r − s − λ. So,

if x ≤ r − s
if r − s ≤ x ≤ r + s
if x > r + s.

0
s−r+x
1

(75)

2s

E

Gi∼ι(G)
Hi∼Hp

Trψλ |Qι{GiHi}|2 ≥ P
Gi∼ι(G)
Hi∼H

(kQι{GiHi}k > r + s + λ).

1
n

E
bi∼B

Trψλ |Q{bi}|2 ≤ P
bi∼B

34

(kQ{bi}k > r − s − λ).

(76)

(77)

Note that |ψ′′′λ (x)| ≤ 1010λ−2s. Applying Theorem 3.5,

1
n

Trψλ |Qι{GiHi}|2

E

Gi∼ι(G)
Hi∼Hp

1
n

Trψλ |Q{bi}|2 + sλ−2n3/21010(53c3)dτ 1/4 + Om,n(λ−2sp−1/2).

≤ E
bi∼B

Combining (76), (77) and (78),

1
n

P

Gi∼ι(G)
Hi∼H
≤ P
bi∼B
= P
bi∼B

(kQι{GiHi}k > r + s + λ)

(kQι{bi}k > r − s − λ) + sλ−21010(53c3)dτ 1/4 + Om,n(sλ−2p−1/2)
(kQ{bi}k > r + s + λ) + P
(r + s + λ > kQ{bi}k > r − s − λ)
bi∼B

+ sλ−2n3/21010(53c3)dτ 1/4 + Om,n(sλ−2p−1/2).

(78)

(79)

It remains to show that the second term in (79) is small. To this end we apply the anti-
concentration result of [CW01, Theorem 8] (with q = 2d in their notation) to get: there
exists an absolute constant C′ > 0 such that, if g1, . . . , gm are i.i.d. standard real Gaussian
random variables, and if Q is any noncommutative multilinear polynomial, then for all ε > 0,

Pg1,...,gm(kQ{gi}k < ε) ≤ C′d(ε/[Eg1,...,gm kQ{gi}k2]1/2)1/d.

Since Eg1,...,gm kQ{gi}k2 ≥ Eg1,...,gm
that, for any r ∈ R, we have the following “small ball” probability estimate.

n Tr|Q{gi} − bQ(∅)|2, we conclude

(80)
Now, applying the invariance principle [IM12, Theorem 3.6] with the function Ψ : Mn(C) →
R deﬁned by

Pg1,...,gm(|kQ{gi}k − r| < ε) ≤ C′d(εn/[Eg1,...,gmTr|Q{gi} − bQ(∅)|2]1/2)1/d.

1

n Tr|Q{gi}|2 ≥ Eg1,...,gm

1

|Ebi∼BΨ(Q{bi}) − Eg1,...,gmΨ(Q{gi})| ≤

n3C′′τ 1/50.

(81)

So, applying the deﬁnition of Ψ to (81), we get

2
s
(80)

Pbi∼B(|kQ{bi}k − r| < s) ≤
≤
=

C′′n3τ 1/50 + Pg1,...,gm(|kQ{gi}k − r| < 2s)
2
C′′n3τ 1/50 + C′d(2sn/[Eg1,...,gmTr|Q{gi} − bQ(∅)|2]1/2)1/d
s
C′′n3τ 1/50 + C′d(2sn/[Ebi∼BTr|Q{bi} − bQ(∅)|2]1/2)1/d.

2
s

35

Finally, substitute the last inequality into (79) and set s = λ = τ 1/100.

(cid:3)

Ψ(A) =

We get

s

0
2s−r+kAk
1
2s+r−kAk
0

s




if kAk ≤ r − 2s
if r − 2s ≤ kAk ≤ r − s
if r − s ≤ kAk ≤ r + s
if r + s ≤ kAk ≤ r + 2s
if kAk > r + 2s.
2
s

Remark 7.3. The theorem [IM12, Theorem 3.6] used in (81) provides an extra multiplicative
factor of 2n2 in (81). However, this constant can be removed in the following way. Using
their notation, they deﬁne a function φ : Rk → R so that φ(x) = exp( −1
) if kxk2 < 1
1−kxk2
(In the present paper, we set k = n2.) This is the function
and φ(x) = 0 otherwise.
they use in their convolution formula. If we instead use a function φ which is a product of
one-dimensional functions, each of which is supported in the interval [−1, 1], e.g. φ(x) =
Qk

i , then the factor 2k no longer appears in their proof.

Remark 7.4. The stronger, though more restrictive anti-concentration inequality

i=1 e

−1
1−x2

2

sup

t∈R(cid:12)(cid:12)(cid:12) P

g1,...,gm

(kQ{gi}k > t) − P
bi∼B
3τ 1/100) + Cd(2τ 1/100n/[Var(Q)]1/2)1/d,

(kQ{bi}k > t)(cid:12)(cid:12)(cid:12)

≤ O(n3cd

(82)

follows more directly from [CW01, Theorem 8] and [IM12, Theorem 3.6] by repeating the
argument above. For example, if we use ψ deﬁned in (75), then [IM12, Theorem 3.6] implies
that

|Ebi∼Bψ(Q{bi}) − Eg1,...,gmψ(Q{gi})| ≤

2
s

n3C′′τ 1/50.

Applying the deﬁnition of ψ to this inequality, we get

(kQ{gi}k > r + s + λ) − P
bi∼B

Therefore,

(kQ{gi}k > r + s + λ) − P
bi∼B

n3C′′τ 1/50.

(83)

2
s

(kQ{bi}k > r − s − λ)(cid:12)(cid:12)(cid:12) ≤
(kQ{bi}k > r + s + λ)(cid:12)(cid:12)(cid:12)

g1,...,gm

g1,...,gm

(cid:12)(cid:12)(cid:12) P
(cid:12)(cid:12)(cid:12) P
=(cid:12)(cid:12)(cid:12) − P
≤(cid:12)(cid:12)(cid:12) P
+(cid:12)(cid:12)(cid:12) P

+ P

g1,...,gm

g1,...,gm

g1,...,gm

(r + s + λ > kQ{gi}k > r − s − λ)
(kQ{gi}k > r + s + λ) − P
bi∼B

(r + s + λ > kQ{gi}k > r − s − λ)(cid:12)(cid:12)(cid:12)

(kQ{gi}k > r + s + λ) − P
bi∼B

g1,...,gm

(kQ{bi}k > r − s − λ)(cid:12)(cid:12)(cid:12)
(kQ{bi}k > r − s − λ)(cid:12)(cid:12)(cid:12).

The second term is bounded by (83) and the ﬁrst term is bounded by (80), setting s = λ =
τ 1/100.

Remark 7.5. It would be desirable to upgrade Corollary 7.1 and (82) to the stronger
inequalities presented in [MNV15]. We leave this research direction to future investigations.

Acknowledgements. Thanks to Todd Kemp, Elchanan Mossel, Assaf Naor, Krzysztof
Oleszkiewicz, and Dimitri Shlyakhtenko for helpful discussions.

References

[AN06]

[Bha97]

Noga Alon and Assaf Naor, Approximating the cut-norm via Grothendieck’s inequality, SIAM J.
Comput. 35 (2006), no. 4, 787–803 (electronic). MR 2203567 (2006k:68176)
R. Bhatia, Matrix analysis, Graduate Texts in Mathematics, Springer New York, 1997.

36

[BKS16]

Afonso S. Bandeira, Christopher Kennedy, and Amit Singer, Approximating the little
Grothendieck problem over the orthogonal and unitary groups, Mathematical Programming
(2016), to appear.

[Bon70]

[BR15]

[CL93]

[Cha06]

[BRS15]

[BMMN13] Mark Braverman, Konstantin Makarychev, Yury Makarychev, and Assaf Naor, The Grothendieck
constant is strictly smaller than Krivine’s bound, Forum Math. Pi 1 (2013), e4, 42. MR 3141414
Aline Bonami, ´Etude des coeﬃcients de Fourier des fonctions de Lp(G), Ann. Inst. Fourier
(Grenoble) 20 (1970), no. fasc. 2, 335–402 (1971). MR 0283496 (44 #727)
Jonah Brown-Cohen and Prasad Raghavendra, Combinatorial optimization algorithms via poly-
morphisms, Electronic Colloquium on Computational Complexity (ECCC) 22 (2015), 7.
Jop Bri¨et, Oded Regev, and Rishi Saket, Tight hardness of the non-commutative grothendieck
problem, IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015,
Berkeley, CA, USA, 17-20 October, 2015, 2015, pp. 1108–1122.
Sourav Chatterjee, A generalization of the Lindeberg principle, Ann. Probab. 34 (2006), no. 6,
2061–2076. MR 2294976 (2008c:60028)
Eric A. Carlen and Elliott H. Lieb, Optimal hypercontractivity for Fermi ﬁelds and related non-
commutative integration inequalities, Comm. Math. Phys. 155 (1993), no. 1, 27–46. MR 1228524
(94h:46101)
Anthony Carbery and James Wright, Distributional and Lq norm inequalities for polynomials
over convex bodies in Rn, Math. Res. Lett. 8 (2001), no. 3, 233–248. MR 1839474
Aur´elien Deya and Ivan Nourdin, Invariance principles for homogeneous sums of free random
variables, Bernoulli 20 (2014), no. 2, 586–603. MR 3178510
Sjoerd Dirksen and ´Eric Ricard, Some remarks on noncommutative Khintchine inequalities, Bull.
Lond. Math. Soc. 45 (2013), no. 3, 618–624. MR 3065031
Kenneth R. Davidson and Stanislaw J. Szarek, Local operator theory, random matrices and
Banach spaces, Handbook of the geometry of Banach spaces, Vol. I, North-Holland, Amsterdam,
2001, pp. 317–366. MR 1863696
A. Grothendieck, R´esum´e de la th´eorie m´etrique des produits tensoriels topologiques, Bol. Soc.
Mat. S˜ao Paulo 8 (1953), 1–79. MR 0094682 (20 #1194)
Leonard Gross, Existence and uniqueness of physical ground states, J. Functional Analysis 10
(1972), 52–109. MR 0339722 (49 #4479)

[DR13]

[DS01]

[Gro53]

[Gro72]

[CW01]

[DN14]

[Gro75]

[Haa85]

[HT99]

[IM12]

[Kai83]

[Kal02]

[Kan14]

[Kho02]

[KKL88]

, Logarithmic Sobolev inequalities, Amer. J. Math. 97 (1975), no. 4, 1061–1083.

MR 0420249 (54 #8263)
Uﬀe Haagerup, The Grothendieck inequality for bilinear forms on C ∗-algebras, Adv. in Math.
56 (1985), no. 2, 93–116. MR 788936 (86j:46061)
U. Haagerup and S. Thorbjørnsen, Random matrices and K-theory for exact C ∗-algebras, Doc.
Math. 4 (1999), 341–450 (electronic). MR 1710376 (2000g:46092)
Marcus Isaksson and Elchanan Mossel, Maximally stable Gaussian partitions with discrete ap-
plications, Israel J. Math. 189 (2012), 347–396. MR 2931402
Sten Kaijser, A simple-minded proof of the Pisier-Grothendieck inequality, Banach spaces, har-
monic analysis, and probability theory (Storrs, Conn., 1980/1981), Lecture Notes in Math., vol.
995, Springer, Berlin, 1983, pp. 33–44. MR 717227 (85d:46097)
Gil Kalai, A Fourier-theoretic perspective on the Condorcet paradox and Arrow’s theorem, Adv.
in Appl. Math. 29 (2002), no. 3, 412–426. MR 1942631
Daniel M. Kane, The correct exponent for the Gotsman-Linial conjecture, Comput. Complexity
23 (2014), no. 2, 151–175. MR 3212596
Subhash Khot, On the power of unique 2-prover 1-round games, Proceedings of the Thirty-
Fourth Annual ACM Symposium on Theory of Computing (New York), ACM, 2002, pp. 767–775
(electronic). MR MR2121525
Jeﬀ Kahn, Gil Kalai, and Nathan Linial, The inﬂuence of variables on boolean functions, Proc.
of 29th Annual IEEE Symposium on Foundations of Computer Science, 1988, pp. 68–80.

[KKMO07] Subhash Khot, Guy Kindler, Elchanan Mossel, and Ryan O’Donnell, Optimal inapproximability
results for MAX-CUT and other 2-variable CSPs?, SIAM J. Comput. 37 (2007), no. 1, 319–357.
MR 2306295 (2008d:68035)

37

[KN09]

[KN12]

[KN13]

Subhash Khot and Assaf Naor, Approximate kernel clustering, Mathematika 55 (2009), no. 1-2,
129–165. MR 2573605 (2011c:68166)

, Grothendieck-type inequalities in combinatorial optimization, Comm. Pure Appl. Math.

65 (2012), no. 7, 992–1035. MR 2922372

, Sharp kernel clustering algorithms and their associated grothendieck inequalities, Ran-

dom Structures & Algorithms 42 (2013), no. 3, 269–300.

[KVV14] Dmitry S. Kaliuzhnyi-Verbovetskyi and Victor Vinnikov, Foundations of free noncommutative
function theory, Mathematical Surveys and Monographs, vol. 199, American Mathematical So-
ciety, Providence, RI, 2014. MR 3244229
J. Lindenstrauss and A. Pe lczy´nski, Absolutely summing operators in Lp-spaces and their appli-
cations, Studia Math. 29 (1968), 275–326. MR 0231188 (37 #6743)

[LP68]

[MJC+14] Lester Mackey, Michael I. Jordan, Richard Y. Chen, Brendan Farrell, and Joel A. Tropp, Matrix
concentration inequalities via the method of exchangeable pairs, Ann. Probab. 42 (2014), no. 3,
906–945. MR 3189061
Elchanan Mossel and Joe Neeman, Robust optimality of Gaussian noise stability, J. Eur. Math.
Soc. (JEMS) 17 (2015), no. 2, 433–482. MR 3317748

[MN15]

[MNV15] Raghu Meka, Oanh Nguyen, and Van Vu, Anti-concentration for polynomials of independent

random variables, Preprint, arXiv:1507.00829, 2015.

[Mos10]

[MOO10] Elchanan Mossel, Ryan O’Donnell, and Krzysztof Oleszkiewicz, Noise stability of functions
with low inﬂuences: invariance and optimality, Ann. of Math. (2) 171 (2010), no. 1, 295–341.
MR 2630040 (2012a:60091)
Elchanan Mossel, Gaussian bounds for noise correlation of functions, Geom. Funct. Anal. 19
(2010), no. 6, 1713–1756. MR 2594620 (2011b:60080)
Shahar Mendelson and Grigoris Paouris, On the singular values of random matrices, J. Eur.
Math. Soc. (JEMS) 16 (2014), no. 4, 823–834. MR 3191978
Mark W. Meckes and Stanis law J. Szarek, Concentration for noncommutative polynomials in ran-
dom matrices, Proc. Amer. Math. Soc. 140 (2012), no. 5, 1803–1813. MR 2869165 (2012j:60048)
Edward Nelson, The free Markoﬀ ﬁeld, J. Functional Analysis 12 (1973), 211–227. MR 0343816
(49 #8556)
Ivan Nourdin, Giovanni Peccati, and Gesine Reinert, Invariance principles for homogeneous
sums: universality of Gaussian Wiener chaos, Ann. Probab. 38 (2010), no. 5, 1947–1985.
MR 2722791 (2011g:60043)
Assaf Naor and Oded Regev, Krivine schemes are optimal, Proc. Amer. Math. Soc. 142 (2014),
no. 12, 4315–4320. MR 3266999
Assaf Naor, Oded Regev, and Thomas Vidick, Eﬃcient rounding for the noncommutative
Grothendieck inequality, Theory Comput. 10 (2014), 257–295. MR 3267842

[NPR10]

[NRV14]

[MP14]

[MS12]

[Nel73]

[NR14]

[O’D14a] Ryan O’Donnell, Analysis of Boolean functions, Cambridge University Press, 2014.
[O’D14b]

, Social choice, computational complexity, gaussian geometry, and boolean functions, Pro-

[Pis78]

[Pis12]

[Rot79]

[RS09]

[Tal94]

[TV11]

ceedings of the ICM, 2014.
Gilles Pisier, Grothendieck’s theorem for noncommutative C ∗-algebras, with an appendix on
Grothendieck’s constants, J. Funct. Anal. 29 (1978), no. 3, 397–415. MR 512252 (80j:47027)

, Grothendieck’s theorem, past and present, Bull. Amer. Math. Soc. (N.S.) 49 (2012),

no. 2, 237–323. MR 2888168
V. I. Rotar′, Limit theorems for polylinear forms, J. Multivariate Anal. 9 (1979), no. 4, 511–530.
MR 556909 (81m:60039)
Prasad Raghavendra and David Steurer, Towards computing the Grothendieck constant, Proceed-
ings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms (Philadelphia,
PA), SIAM, 2009, pp. 525–534. MR 2809257 (2012i:90109)
Michel Talagrand, On Russo’s approximate zero-one law, Ann. Probab. 22 (1994), no. 3, 1576–
1587. MR 1303654 (96g:28009)
Terence Tao and Van Vu, Random matrices: universality of local eigenvalue statistics, Acta
Math. 206 (2011), no. 1, 127–204. MR 2784665 (2012d:60016)

38

[Ver12]

[Voi91]

Roman Vershynin, Introduction to the non-asymptotic analysis of random matrices, Compressed
sensing, Cambridge Univ. Press, Cambridge, 2012, pp. 210–268. MR 2963170
Dan Voiculescu, Limit laws for random matrices and free products, Invent. Math. 104 (1991),
no. 1, 201–220. MR 1094052 (92d:46163)

Department of Mathematics, UCLA, Los Angeles, CA 90095-1555
E-mail address: heilman@math.ucla.edu

Department of Computing and Mathematical Sciences, California Institute of Technol-

ogy, Pasadena, CA 91125-2100

E-mail address: vidick@cms.caltech.edu

39

