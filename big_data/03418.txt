Multivariate tests of association based on univariate tests

Ruth Heller1 and Yair Heller

Abstract. For testing two random vectors for independence, we consider testing whether

the distance of one vector from a center point is independent from the distance of the

other vector from a center point by a univariate test. In this paper we provide conditions

under which it is enough to have a consistent univariate test of independence on the

distances to guarantee that the power to detect dependence between the random vectors

increases to one, as the sample size increases. These conditions turn out to be minimal. If

the univariate test is distribution-free, the multivariate test will also be distribution-free. If

we consider multiple center points and aggregate the center-speciﬁc univariate tests, the

power may be further improved, and the resulting multivariate test may be distribution-free

for speciﬁc aggregation methods (if the univariate test is distribution-free). We show that

several multivariate tests recently proposed in the literature can be viewed as instances of

this general approach.

Keywords: High-dimensional response; Independence test; Multivariate data; Random

vector; Two-sample problem.

6
1
0
2

 
r
a

 

M
0
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
8
1
4
3
0

.

3
0
6
1
:
v
i
X
r
a

1Address for correspondence: Department of Statistics and Operations Research, Tel-Aviv uni-

versity, Tel-Aviv, Israel. E-mail: ruheller@post.tau.ac.il.

1

1

Introduction

Let X ∈ (cid:60)p and Y ∈ (cid:60)q be random vectors, where p and q are positive integers. The

null hypothesis of independence is H0 : FXY = FXFY , where the joint distribution

of (X, Y ) is denoted by FXY , and the distributions of X and Y , respectively, by FX

and FY . If X is a categorical variable with K categories, then the null hypothesis of

independence is the null hypothesis in the K-sample problem, H0 : F1 = . . . = FK,
where Fk, k ∈ {1, . . . , K} is the distribution of Y in category k.

The problem of testing for independence of random vectors, as well as the K-sample
problem on a multivariate Y , against the general alternative H1 : FXY (cid:54)= FXFY ,

has received increased attention in recent years. The most common approach is

based on pairwise distances or similarity measures. See Sz´ekely et al. (2007), Gretton

et al. (2008), Sejdinovic et al. (2013), and Heller et al. (2013) for consistent tests of

independence, and Hall and Tajvidi (2002), Sz´ekely and Rizzo (2004), Baringhaus and

Franz (2004), Rosenbaum (2005), Gretton et al. (2007), and Gretton et al. (2012a)

for recent K-sample test. Earlier tests based on nearest neighbours include Schilling

(1986), and Henze (1988). Another approach is to ﬁrst reduce the multivariate data

to a lower dimensional sub-space by (random) projections, see Cuesta-Albertos et al.

(2006) and Wei et al. (2015).

We suggest the following approach: ﬁrst compute the distances from a ﬁxed cen-

ter point, then apply any univariate test on the distances. If the univariate test is

distribution-free, then so is the multivariate test. In Section 2 we show that a result

by Rawat and Sitaram (2000) in integral geometry implies that if H0 is false, then ap-

plying a univariate consistent test on distances from a single center point will result,

for almost every center point, in a multivariate test with power increasing to one as

the sample size increases.

2

In Section 3 we show that considering the distances from M > 1 points and aggre-

gating the resulting statistics can also result in consistent tests, which may be more

powerful than tests that consider a single center point. Both distribution-free and

permutation-based tests can be generated, depending on the choice of aggregation

method and univariate test.

In section 4 we draw the connection between these results and some known tests

mentioned above. The tests of Hall and Tajvidi (2002) and of Heller et al. (2013) can

be viewed as instances of this approach, where the center point is a sample point,

and all sample points are considered each in turn as a center point, for a particular

univariate test.

2 Main results

We use the following result by Rawat and Sitaram (2000). Let Bd(x, r) = {y ∈ (cid:60)d :
(cid:107)x − y(cid:107) ≤ r} be a ball centered at x with radius r. A complex Radon measure µ
on (cid:60)d is said to be of at most exponential-quadratic growth if there exist positive
constants A and α such that |µ|(Bd(0, r)) ≤ Aeαr2.

Proposition 2.1 (Rawat and Sitaram (2000)). Let Γ ⊂ (cid:60)d be such that the only real

analytic function (deﬁned on an open set containing Γ) that vanishes on Γ, is the zero
function. Let C = {Bd(x, r) : x ∈ Γ, r > 0}. Then for any complex Radon measure
µ on (cid:60)d of at most exponential-quadratic growth, if µ(C) = 0 for all C ∈ C, then it

necessarily follows that µ = 0.

For the two-sample problem, let Y ∈ (cid:60)q be a random variable with cumulative
distribution F1 in category X = 1, and F2 in category X = 2. For z ∈ (cid:60)q, let F (cid:48)
iz be
the cumulative distribution function of (cid:107)Y − z(cid:107) when Y has cumulative distribution

3

Fi, i ∈ {1, 2}. We show that if the distribution of Y diﬀers across categories, then so

does the distribution of the distance of Y from almost every point z. Therefore, any

univariate consistent two-sample test on the distances from z results in a consistent

test of the equality of the multivariate distributions F1 and F2, for almost every z. It

is straightforward to generalize these results to K > 2 categories.
Theorem 2.1. If H0 : F1 = F2 is false, then for every z ∈ (cid:60)q, apart from at most a
set of Lebesgue measure 0, there exists an r > 0 such that F (cid:48)

1z(r) (cid:54)= F (cid:48)

2z(r).

(cid:82)

y∈Bq(z,r) dF1(y) −(cid:82)

Proof. Suppose by contradiction, that there is a set Γ ⊆ (cid:60)q with positive Lebesgue
measure, such that for all z ∈ Γ, F (cid:48)

It follows that

2z(r) for all r > 0.

1z(r) = F (cid:48)

y∈Bq(z,r) dF2(y) = 0 for all r > 0 and z ∈ Γ. Since |F1 − F2| ≤
1, clearly F1 − F2 is of at most exponential-quadratic growth. Moreover, the only

real analytic function that vanishes on Γ is the zero function, since Γ has positive
Lebesgue measure. Therefore, it follows from Proposition 2.1 that F1 − F2 = 0, thus

contradicting the fact that H0 is false.
Corollary 2.1. For every z ∈ (cid:60)q, apart from at most a set of Lebesgue measure 0, a
consistent two-sample univariate test of the null hypothesis H(cid:48)

1z = F (cid:48)
H0 : F1 = F2 with a power increasing to one as the sample size increases.

2z will reject

0 : F (cid:48)

Proof. If H0 : F1 = F2 is false, then Theorem 2.1 guarantees that for every z, apart
from at most a set of Lebesgue measure 0, the null univariate hypothesis, H(cid:48)
F (cid:48)
2z, is false. Since for such a z the asymptotic power of a false null univariate

0 : F (cid:48)

1z =

hypothesis will be one for any consistent two-sample univariate test, the power of the

multivariate test will be one.

For the multivariate test of independence, let X ∈ Rp and Y ∈ (cid:60)q be two random

vectors with marginal distributions FX and FY , respectively, and with joint distri-
bution FXY . For z = (zx, zy), zx ∈ (cid:60)p, zy ∈ (cid:60)q, let F (cid:48)

XY z be the joint distribution of

4

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

((cid:107)X − zx(cid:107),(cid:107)Y − zy(cid:107)). Let F (cid:48)
(cid:107)Y − zy(cid:107), respectively.

Xz and F (cid:48)

Y z be the marginal distribution of (cid:107)X − zx(cid:107) and

Theorem 2.2. If H0 : FXY = FXFY is false, then for every zx ∈ (cid:60)p, zy ∈ (cid:60)q, apart

from at most a set of Lebesgue measure 0, there exists rx > 0, ry > 0, such that
XY z(rx, ry) (cid:54)= F (cid:48)
F (cid:48)

Xz(rx)F (cid:48)

Y z(ry).

Proof. Suppose by contradiction, that there is a set Γ ⊆ (cid:60)p+q with positive Lebesgue
measure, such that for all z ∈ Γ, F (cid:48)
It follows that for all z ∈ Γ and rx > 0, ry > 0,

Y z(ry) for all rx > 0, ry > 0.

XY z(rx, ry) = F (cid:48)

Xz(rx)F (cid:48)

(cid:90)

((cid:107)x−zx(cid:107),(cid:107)y−zy(cid:107))≤(rx,ry)

dFXY (x, y) =

dFX(x)

dFY (y).

(cid:107)x−zx(cid:107)≤rx

(cid:107)y−zy(cid:107)≤ry

It thus follows that for all z ∈ Γ and any r > 0,

((cid:107)(x,y)−(zx,zy)(cid:107))≤r

(cid:107)(x,y)−(zx,zy)(cid:107)≤r

dFXY (x, y) =

dFX(x)dFY (y).

(2.1)

However, from Theorem 2.1, with F1 = FXY and F2 = FXFY , it follows that for all
z ∈ Γ, apart from a set of Lebesgue measure 0, there exists an r > 0 such that

dFXY (x, y) (cid:54)=

(cid:107)(x,y)−(zx,zy)(cid:107)≤r

dFX(x)dFY (y),

((cid:107)(x,y)−(zx,zy)(cid:107))≤r

thus contradicting (2.1).

Corollary 2.2. For every z ∈ (cid:60)p+q, apart from at most a set of Lebesgue measure 0, a
consistent univariate test of inependence of the null hypothesis H(cid:48)
XzF (cid:48)
will reject H0 : FXY = FXFY with a power increasing to one as the sample size

XY z = F (cid:48)

0 : F (cid:48)

Y z

increases.

Proof. If H0 : FXY = FXFY is false, then Theorem 2.2 guarantees that for every

5

z, apart from at most a set of Lebesgue measure 0, the null univariate hypothesis,
H(cid:48)
0 : F (cid:48)
univariate hypothesis will be one for any consistent univariate test of independence,

Y z, is false. Since for such a z the asymptotic power of a false null

XY z = F (cid:48)

XzF (cid:48)

the power of the multivariate test will be one.

We have N independent copies (xi, yi) (i = 1, . . . , N ) from the joint distribution FXY .

The above results motivate the following two-step procedure for the multivariate test.
For the K-sample test, xi ∈ {1, . . . , K} determines the category and yi ∈ (cid:60)q is the
observation in category xi, so the two-step procedure is to ﬁrst choose z ∈ (cid:60)q and
then to apply a univariate K-sample consistent test on (x1,(cid:107)y1 − z(cid:107)), . . . , (xN ,(cid:107)yN −
z(cid:107)). Examples of such univariate tests include the classic Kolmogorov-Smirnov and

Cramer-von Mises tests. For the test of independence, the two-step procedure is to
ﬁrst choose zx ∈ (cid:60)p and zy ∈ (cid:60)q, and then to apply a univariate consistent test of
independence on ((cid:107)x1 − zx(cid:107),(cid:107)y1 − zy(cid:107)), . . . , ((cid:107)xN − zx(cid:107),(cid:107)yN − zy(cid:107)). An example of

such a univariate test is the classic test of Hoeﬀding (1948a). See Heller et al. (2016)

for novel K-sample and independence tests and a review of existing distribution-

free univariate tests. Note that the consistency of a univariate test may be satisﬁed

only under some assumptions on the distribution of the distances. For example, the
consistency of Hoeﬀding (1948a) follows if the densities of (cid:107)X − zx(cid:107) and (cid:107)Y − zy(cid:107) are

continuous.

A great advantage of the two-step procedure is the fact that it has the same computa-

tional complexity as the univariate test. For example, if one chooses to use Hoeﬀding’s

univariate independence permutation test (Hoeﬀding, 1948a) , then the total complex-

ity is only O(N log N ), which is the cost of computing the test statistic. The p-value

can be extracted from a look-up table since Hoeﬀding’s test is distribution-free. For

comparison, note that the computational complexity of the multivariate permutation

tests of Sz´ekely et al. (2007) and Heller et al. (2013) is O(BN 2), and O(BN 2 log N ),

6

respectively, where B is the number of permutations. For many univariate tests the

asymptotic null distribution is known, thus it can be used to compute the signiﬁ-

cance eﬃciently without resorting to permutations, which are typically required for

assessing the multivariate signiﬁcance.

Another advantage of the two-step procedure is the fact that the test statistic may

be estimating an easily interpretable population value. The univariate test statistics

often converge to easily interpretable population values, which are often between 0

and 1. These values carry over to provide meaning to the new multivariate statistics.

In practice, the choice of the center value from which the distances are measured can

be important. In the next Section 3, we suggest generalizations of the above two-step

approach that use multiple center points.

3 Pooling univariate tests together

We need not rely on a single z ∈ (cid:60)p+q (or a single z ∈ (cid:60)q for the K-sample problem).

If we apply a consistent univariate test using many points zi for i = 1, . . . , M as our

center points, where the test is applied on the distances of the N sample points from

the center point, we obtain M test-statistics and corresponding p-values, p1, . . . , pM .

We can use the p-values or the test statistics of the univariate tests to design consistent

multivariate tests. We suggest three useful approaches. The ﬁrst approach is to com-
bine the p-values, using a combining function f : [0, 1]M → [0, 1]. Common combining

functions include f (p1, . . . , pM ) = mini=1,...,M pi, and f (p1, . . . , pM ) = −2(cid:80)M

i=1 log pi.

The test statistic f (p1, . . . , pM ) = maxi=1,...,M pi may also have excellent power in

applications where the univariate test on the distances from almost all points has

power. The second approach is to combine the univariate test statistics, by taking

7

the average, maximum, or minimum statistic. These aggregation methods can result

in test statistics which converge to meaningful population values, see equation (3.3)

below for multivariate tests based on the univariate Kolmogorov-Smirnov two sample

test (Kolmogorov, 1941). We note that if the univariate tests are distribution-free

then taking the maximum (minimum) p-value is equivalent to taking the minimum

(maximum) test statistic (when the test rejects for large values of the test statis-

tic). The signiﬁcance of the combined p-value or the combined test statistic can be

computed by a permutation test.

A drawback of the two approaches above is that the distribution-free property of the

univariate test does not carry over to the multivariate test. In our third approach,

we consider the set of M p-values as coming from the family of M null hypotheses,

and then apply a valid test of the global null hypothesis that all M null hypotheses
are true. Let p(1) ≤ . . . ≤ p(M ) be the sorted p-values. The simplest valid test
for any type of dependence is the Bonferroni test, which will reject the global null
if M p(1) ≤ α. Another valid test is the test of Hommel (1983), which rejects if
l=1 1/l)p(j)/j} ≤ α. This test statistic was suggested independently in
a multiple testing procedure for false discovery rate control under general dependence

minj≥1{M ((cid:80)M

in Benjamini and Yekutieli (2001). This approach is computationally much more

eﬃcient than the ﬁrst two approaches, since no permutation test is required after the

computation of the univariate p-values, but it may be less powerful. Clearly, if the

univariate test is distribution free, the resulting multivariate test is also distribution

free.

As an example we shall prove that when using the Kolmogorov-Smirnov two sample

test as the univariate test, all of the pooling methods above result in consistent
multivariate two-sample tests. Let KS(z) = supd∈(cid:60) |F (cid:48)
2z(d)| be the population
value of the univariate Kolmogorov-Smirnov two sample test statistic comparing the

1z(d)−F (cid:48)

8

distribution of the distances. Let N be the total number of independent observations,

where we assume for simplicity an equal number of observations from F1 and from

F2.

Theorem 3.1. Let z1, . . . , zM be a sample of center points from an absolutely contin-

uous distribution with probability measure ν, whose support S has a positive Lebesgue
measure in (cid:60)q. Let KSN (zi) be the empirical value of KS(zi) with corresponding
p-value pi, i = 1, . . . , M . Let p(1) ≤ . . . ≤ p(M ) be the sorted p-values. Assume that
the distribution functions F1 and F2 are continuous. For M = o(eN ), if H0 : F1 = F2
is false, then ν-almost surely, as N → ∞, the power will increase to one for the

following level α tests:

1. the permutation test using the test statistics S1 = maxi=1,...,M{KSN (zi)} or

S2 = p(1).

2. the test based on Bonferroni, which rejects H0 if M p(1) ≤ α.

3. for M log M = o(eN ), the test based on Hommel’s global null p-value, which

(cid:111) ≤ α.
i=1 KSN (zi) or T 2 = −2(cid:80)M
4. the permutation tests using the statistics T 1 =(cid:80)M

(cid:110)
M ((cid:80)M

i=1 log pi.

rejects H0 if minj=1,...,M

l=1 1/l)p(j)/j

Proof. Proving item 2 will prove item 1 since if the Bonferonni adjusted p-value is

consistent then so is the permutation test based on the minimum p-value (or maximum

statistic), which has necessarily a smaller p-value than M p(1). We need to show that

the probability of rejection goes to one when H0 is false. According to Corollary

2.1 when H0 is false, ν-almost surely any point zi oﬀers a consistent univariate test.
Therefore, ν-almost surely, supi=1,...,M KS(zi) ≥ KS(z1) > 0. Let d0 be a distance
such that |F (cid:48)

2z1(d0)| = c > 0.

1z1(d0) − F (cid:48)

9

Let F (cid:48)
distances from F (cid:48)

iz1N be the empirical cumulative distribution function based on N/2 sampled

iz1, i ∈ {1, 2}. The test statistic is bounded away from zero:

KSN (zi) > c/2)} ≥ pr{KSN (z1) > c/2} = pr{sup

1z1N (d) − F (cid:48)
|F (cid:48)

2z1N (d)| > c/2}

d

i=1,...,M

pr{ sup
≥ pr{|F (cid:48)
≥ pr{|F (cid:48)

1z1N (d0) − F (cid:48)
1z1N (d0) − F (cid:48)

2z1N (d0)| > c/2}
1z1(d0)| < c/4}pr{|F (cid:48)

2z1N (d0) − F (cid:48)

2z1(d0)| < c/4} ≥ (1 − 2e−N c2/8)2

2z1N (d0) − F (cid:48)
1z1N (d0) − F (cid:48)

where in the last row, the ﬁrst inequality follows since if |F (cid:48)
and |F (cid:48)
1z1(d0) − F (cid:48)
that |F (cid:48)

1z1(d0)| < c/4
1z1N (d0) − F (cid:48)
2z1(d0)| = c, it implies
2z1(d0)| < c/4, given that |F (cid:48)
2z1N (d0)| > c/2, and the last inequality is the Dvoretzky-Kiefer-
Wolfowitz inequality (Dvoretzky et al., 1956). Therefore, when H0 is false, the prob-
ability that the statistic is greater than c/2 goes to 1 as N → ∞.

When H0 is true, let F (cid:48)
each z ∈ {z1, . . . , zM},

z denote the common distribution function of (cid:107)Y − z(cid:107). For

pr{KSN (z) > c/2} = pr{sup
≤ pr{sup
≤ pr{sup

|F (cid:48)
1zN (d) − F (cid:48)
|F (cid:48)
1zN (d) − F (cid:48)

|F (cid:48)
1zN (d) − F (cid:48)
z(d)| + sup
z(d)| > c/4} + pr{sup

z(d) − F (cid:48)
z(d)| > c/2}
|F (cid:48)
2zN (d) − F (cid:48)
|F (cid:48)
2zN (d) − F (cid:48)

z(d) + F (cid:48)

d

d

d

d

d

2zN (d)| > c/2}

z(d)| > c/4} ≤ 4e−N c2/8,
(3.1)

where the last inequality follows from the Dvoretzky-Kiefer-Wolfowitz inequality.

It follows from (3.1) that the Bonferonni adjusted p-value is bounded above by
4M e−N c2/8, and therefore goes to zero as N → ∞ for M = o(eN ), proving con-

sistency.

M ((cid:80)M

For item 3, the proof is very similar. Hommel’s global null p-value is at most

l=1 1/l)p(1), and as in the proof for item 2 it is bounded above by 4M ((cid:80)M

l=1 1/l)e−N c2/8,

which goes to zero as N → ∞ for M log M = o(eN ).

10

For item 4, let z0 ∈ (cid:60)q be a center point sampled from ν. When H0 is false, ν-almost

surely KS(z0) = c > 0. By Lebesgue’s density theorem ν-almost surely there exists

an  such that if r <  then at least half of the ball Bq(z0, r) is within the support S.

Since F1 and F2 are continuous, KS(z) is a continuous function of z. Therefore, there
exists an (cid:48) <  such that KS(z) > c/2 for all z ∈ Bq(z0, (cid:48)) ∩ S. Similar arguments
to those for item 2 show that ν-almost surely for any zi ∈ S ∩ Bq(z0, (cid:48)),

P r(KSN (zi) < c/4) < 4e−N c2/32.

(3.2)

Therefore, P r(∪zi∈S∩Bq(z0,(cid:48))KSN (zi) < c/4) < 4M e−N c2/32. Since ν-almost surely
pr{Z ∈ S ∩ Bq(z0, (cid:48))} > 0, then ν-almost surely with probability going to one T 1 is
O(M ), as long as M = o(eN ). On the other hand when H0 is true, E(KSN (z)) =

√
√
N ), see for example Marsaglia et al. (1983). Therefore, E(T 1) = O(M/
O(1/

N ),

and by Markov’s inequality the permutation test based on T 1 will have ν-almost

surely power increasing to one as the sample size increases. For the test based on
T2, from equations (3.2) and (3.1) it follows that for N large enough pi < 4e−N c2/32

i=1 log pi is
greater than O(N M )pr{Z ∈ S ∩ Bq(z0, (cid:48))}. On the other hand, when H0 is true Pi

for zi ∈ S ∩ Bq(z0, (cid:48)), i = 1, . . . , M . Therefore, for N large enough −2(cid:80)M
is uniformly distributed, so E(−2(cid:80)M
permutation test based on −2(cid:80)M

i=1 log Pi) is O(M ). By Markov’s inequality the

i=1 log pi will have ν-almost surely power increasing

to one as the sample size increases.

The test statistics S1 and T 1/M converge to meaningful population quantities,

lim

N,M→∞ S1 = lim

M→∞ max

z1,...,zM

KS(z) = sup
z∈S

KS(z),

lim

N,M→∞ T1/M = lim
M→∞

KS(zi)/M = E{KS(Z)},

(3.3)

M(cid:88)

i=1

where the expectation is over the distribution of the center point Z.

11

Arguably, the most natural choice of center points is the sample points themselves.

Interestingly, if the univariate test is a U -statistic (Hoeﬀding, 1948b) of order m,

then the resulting multivariate test is a U -statistic of order m + 1, if each sample

point acts as a center point, and the univariate test statistics are averaged. The

proof is as follows. Denote B(a, b) for the binomial coeﬃcient a choose b.

If the

(cid:80)
univariate test statistic TN−1 is a U -statistic, then it can be written as TN−1 =
h{(uj1, vj1), . . . , (ujm, vjm)}/B(N − 1, m), where h is a symmetric function,
(uj1, vj1), . . . , (ujm, vjm) is a subset of size m from a sample of size N − 1, and CN−1,m
is the set of all such subsets of size m. The multivariate test statistic is therefore

CN−1,m

(cid:88)

f{(xj1, yj1), . . . , (xjm+1, yjm+1)}/B(N, m + 1),

CN,m+1

where f{(x1, y1), . . . , (xm+1, ym+1)} is the symmetric function

1

m + 1

[h{((cid:107)xk−x1(cid:107),(cid:107)yk−y1(cid:107)), k = 2, . . . , m+1}+. . .+h{((cid:107)xk−xm+1(cid:107),(cid:107)yk−ym+1(cid:107)), k = 1, . . . , m}].

4 Connection to existing methods

We are aware of two multivariate test statistics of the above-mentioned form: ag-

gregation of the univariate test statistics on the distances from center points. The

tests are the two sample test of Hall and Tajvidi (2002) and the independence test of

Heller et al. (2013). Both these tests use the second pooling method mentioned above

by summing up the univariate test statistics. Furthermore, both these tests use the

N sample points as the center points (or z’s) and perform a univariate test on the
remaining N − 1 points. Indeed, Hall and Tajvidi (2002) recognized that their test

can be viewed as summing up univariate Cramer von-Mises tests on the distances

from each sample point. We shall show that the test statistic of Heller et al. (2013)

12

can be viewed as aggregation by summation of the univariate weighted Hoeﬀding

independence test suggested in Thas & Ottoy (2004).

Heller et al. (2013) presented a permutation test based on the test statistic(cid:80)N

i=1

(cid:80)N

j=1,j(cid:54)=i S(i, j),

where S(i, j) is the Pearson test score for the 2 × 2 contingency table for the random
variables I((cid:107)X − xi(cid:107) ≤ (cid:107)xj − xi(cid:107)) and I((cid:107)Y − yi(cid:107) ≤ (cid:107)yj − yi(cid:107)), where I(·) is the
indicator function. Since (cid:107)X − xi(cid:107) and (cid:107)Y − yi(cid:107) are univariate random variables,

S(i, j) can also be viewed as the test statistic for the test of independence between
(cid:107)X − xi(cid:107) and (cid:107)Y − yi(cid:107), based on the 2 × 2 contingency table induced by the 2 × 2
partition of (cid:60)2 about the point ((cid:107)xj − xi(cid:107),(cid:107)yj − yi(cid:107)) using the N − 2 sample points
((cid:107)xk − xi(cid:107),(cid:107)yk − yi(cid:107)), k = 1, . . . , N, k (cid:54)= i, k (cid:54)= j. Thas & Ottoy (2004) showed
that the statistic that sums the Pearson test statistics over all 2 × 2 partitions of (cid:60)2

based on the observations, results in a consistent test of independence for univariate

random variables. The test statistic of Thas & Ottoy (2004) on the sample points

((cid:107)xk − xi(cid:107),(cid:107)yk − yi(cid:107)), k = 1, . . . , N, k (cid:54)= i, is therefore (cid:80)

j=1,j(cid:54)=i S(i, j). The multi-

variate test statistic of Heller et al. (2013) aggregates by summation the univariate

test statistics of Thas & Ottoy (2004), where the ith univariate test statistic is based
on the N − 1 distances of xk from xi, and the N − 1 distances of yk from yi, for
k = 1, . . . , N, k (cid:54)= i.

Of course, not all known consistent multivariate tests belong to the framework deﬁned

above. As an interesting example we discuss the energy test of Sz´ekely and Rizzo

(2004) and Baringhaus and Franz (2004) for the two-sample problem. Without loss

of generality, let y1, . . . , yN1 be the observations from F1, and yN 1+1, . . . , yN be the

(cid:32)

N1(cid:88)

N(cid:88)

N1(cid:88)

N1(cid:88)

l=1

m=1

(cid:107)yl − ym(cid:107) − 1
N 2
2

N(cid:88)

N(cid:88)

l=N1+1

m=N1+1

(cid:33)

,

(cid:107)yl − ym(cid:107)

observations from F2. The test statistic is

E =

N1N2

2

N1 + N2

N1N2

l=1

m=N1+1

(cid:107)yl − ym(cid:107) − 1
N 2
1

13

where (cid:107) · (cid:107) is the Euclidean norm.

It is easy to see that E = (cid:80)N
m=N1+1 (cid:107)yi − ym(cid:107)(cid:111)
(cid:80)N2

(cid:110) 1
(cid:80)N1
m=1 (cid:107)yi − ym(cid:107) − 1
N if i > N1, for i ∈ {1, . . . , N}. The statistic Si is
N if i ≤ N1 and w(i) = N1
not an omnibus consistent test statistic, since a test based on Si will have no power

univariate score is Si =
− N2

N1

i=1 Si, where the

w(i), w(i) =

N2

to detect diﬀerence in distributions with the same expected distance from yi across

groups. However, the energy test is omnibus consistent.

Acknowledgement

We thank Elchanan Mossel for useful discussions of the main results.

References

Baringhaus, L. & Franz, C. (2004). On a new multivariate two-sample test.

Journal of Multivariate Analysis, 88:190–206.

Benjamini, Y. & Yekutieli, D. (2001). The control of the false discovery rate in

multiple testing under dependency. The Annals of Statistics, 29 (4):1165–1188.

Cuesta-Albertos, J. A., Freiman, R. & Ransford, T. (2006). Random pro-

jections and goodness-of-ﬁt tests in inﬁnite-dimensional spaces. Bull. Braz. Math.

Soc. 37(4), 1–25.

Dvoretzky, A., Kiefer, J. & Wolfowitz, J. (1956). Asymptotic Minimax

Character of the Sample Distribution Function and of the Classical Multinomial

Estimator. Annals of Mathematical Statistics, 27 (3):642–669.

Gretton, A., Bogwardt, K.M., Rasch, M.J., Scholkopf, B & Smola, A.

14

(2007). A kernel method for the two-sample problem. Advances in Neural Infor-

mation Processing Systems (NIPS), 19.

Gretton, A., Fukumizu, K., TEO, C.H., Song, L., Scholkopf, B. & Smola,

A. (2008). A kernel statistical test of independence. Advances in Neural Informa-

tion Processing Systems, 20:585–592.

Gretton, A. & Gyorfi, L. (2010). Consistent nonparametric tests of indepen-

dence. Journal of Machine Learning Research, 11:1391–1423.

Gretton, A., Borgwardt, K.M., Rasch, M.J. Scholkopf, B. & Smola, A.

(2012). A kernel two-sample test. The Journal of Machine Learning Research, 13:

723–773.

Hall, P. & Tajvidi, N. (2002). Permutation tests for equality of distributions in

high-dimensional settings. Biometrika, 89 (2):359–374.

Heller, R., Heller, Y., Kaufman, S., Brill, B. & Gorfine, M. (2016).

Consistent distribution-free K-sample and independence tests for univariate random

variables Journal of Machine Learning, accepted. arXiv:1410.6758.

Heller, R., Heller, Y. & Gorfine, M. (2013). A consistent multivariate test of

association based on ranks of distances. Biometrika, 100(2):503–510.

Henze, N.(1988). A multivariate two-sample test based on the number of nearest

neighbor type coincidences. The Annals of Statistics, 16(2): 772–783.

Hoeffding, W.(1948a). A non-parametric test of independence. Ann. Math. Stat.,

19 (4), 546–557.

Hoeffding, W.(1948b). A class of statistics with asymptotically normal distribu-

tions. Annals of Statistics, 19, 293-325.

15

Hommel, G. (1983). Tests of the overall hypothesis for arbitrary dependence struc-

tures Biom. J. 25:423–430

Kolmogorov, A. N.(1941). Conﬁdence limits for an unknown distribution function.

Ann. Math. Stat. 12, 461–463.

Marsaglia, G., Tsang, W. & Wang, J. (2003). Evaluating Komogorov’s distri-

bution Journal of Statistical Software, 8(18), 1–4

Rawat, R. & Sitaram, A. (2000). Injectivity sets for spherical means on Rn and

on symmetric spaces Journal of Fourier Analysis and Applications, 6(3):343–348.

Rosenbaum, R. (2005). An exact distribution-free test comparing two multivari-

ate distributions based on adjacency. Journal of the Royal Statitistical Society B,

67:515–530.

Schilling, M. F. (1986). Multivariate two-sample tests based on nearest neighbors.

J. Am. Statist. Assoc. 81, 799–806.

Sejdinovic, D., Sriperumbudur, B., Gretton, A. & Fukumizu, K. (2013).

Equivalence of distance-based and rkhs-based statistics in hypothesis testing. An-

nals of Statistics, 41 (5):2263–2291.

Sz´ekely, G. & Rizzo, M. (2004). Testing for equal distributions in high dimensions.

InterStat.

Sz´ekely, G., Rizzo, M. & Bakirov, N. (2007). Measuring and testing dependence

by correlation of distances. The Annals of Statistics, 35:2769–2794.

Thas, O. & Ottoy, J.P. (2004). A nonparamteric test for independence based on

sample space partitions.. Communcations in Statistics - Simulation and Computa-

tion 33 (3), 711–728.

16

Wei, S., Lee, C., Wichers, L. & Marron, J. S. (2015). Direction-Projection-

Permutation for High Dimensional Hypothesis Tests. Journal of Computational

and Graphical Statisitcs, doi: 10.1080/10618600.2015.1027773.

17

