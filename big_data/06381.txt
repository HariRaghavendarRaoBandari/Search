6
1
0
2

 
r
a

 

M
1
2

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
1
8
3
6
0

.

3
0
6
1
:
v
i
X
r
a

Forward and Inverse Uncertainty Quantiﬁcation using

Multilevel Monte Carlo Algorithms for an Elliptic Nonlocal

Equation

A. Jasra, ∗

K.J.H. Law, †

Y. Zhou ‡

March 22, 2016

Abstract

This paper considers uncertainty quantiﬁcation for an elliptic nonlocal equation. In par-

ticular, it is assumed that the parameters which deﬁne the kernel in the nonlocal operator

are uncertain and a priori distributed according to a probability measure. It is shown that

the induced probability measure on some quantities of interest arising from functionals of the

solution to the equation with random inputs is well-deﬁned; as is the posterior distribution

on parameters given observations. As the elliptic nonlocal equation cannot be solved ap-

proximate posteriors are constructed. The multilevel Monte Carlo (MLMC) and multilevel

sequential Monte Carlo (MLSMC) sampling algorithms are used for a priori and a posteriori

estimation, respectively, of quantities of interest. These algorithms reduce the amount of

work to estimate posterior expectations, for a given level of error, relative to Monte Carlo

and i.i.d. sampling from the posterior at a given level of approximation of the solution of

the elliptic nonlocal equation.

Key words: Uncertainty quantiﬁcation, multilevel Monte Carlo, sequential Monte Carlo,

nonlocal equations, Bayesian inverse problem

AMS subject classiﬁcation: 82C80, 60K35.

∗Department of Statistics & Applied Probability National University of Singapore Singapore, Singapore
†Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA, 37831
‡Department of Statistics & Applied Probability National University of Singapore Singapore, Singapore

1

1

Introduction

Anomalous diﬀusion, where the associated underlying stochastic process is not Brownian motion,

has recently attracted considerable attention [2]. This case is interesting because there may be

long-range correlations, among other reasons. Anomalous superdiﬀusion can be be related to

fractional Laplacian operators and/or so-called nonlocal operators [10], deﬁned point-wise by
their operation on a function u : Rd → R as

L(u)(x) :=

B(x, x(cid:48))[u(x(cid:48)) − u(x)]dx(cid:48).

(1.1)

(cid:90)

Rd

The fractional Laplacian is actually a special case of this equation. The present work will focus on

these operators and in particular the associated stationary equation, analogous to the local elliptic

equation. It should be noted that these nonlocal operators and the associated equations can be

applied not only to problems of anomalous diﬀusion, but to a wide range of phenomena, including

peridynamic models of continuum mechanics which allow crack nucleation and propagation [17,

10].

In this work it will be assumed that the kernel B appearing above is parametrized by a (pos-
sibly inﬁnite-dimensional) parameter λ ∈ E; λ is assumed random. Furthermore, some partial

noisy observations of the solution u will be available. In a probabilistic framework, a prior distri-
bution is placed on λ ∼ µ and the posterior distribution λ|y ∼ η results from conditioning on the

observations. The joint distribution can often trivially be derived and evaluated in closed form
for a given pair (λ, y), as P(λ, y) = P(y|λ)µ(λ). Hence, the posterior for a given observed value of

y can be evaluated, up to a normalizing constant. One aims to approximate quantities of interest

E Q(λ)ν(dλ) for some Q : E → R, where ν = µ for the forward problem and ν = η for
the inverse problem. The likelihood P(y|λ) is often concentrated in a small, possibly nonlinear,

q = (cid:82)
dard forward approximation algorithms independently to the numerator, (cid:82)
and denominator,(cid:82)

E Q(λ)P(y|λ)µ(dλ),
P(y|λ)µ(dλ). It is noted that, typically, (1.1) will have to be approximated

subspace of E. This posterior concentration generically precludes the naive application of stan-

E

numerically, and this will lead to an approximate posterior density.

Monte Carlo (MC) and Sequential Monte Carlo (SMC) methods are amongst the most widely

this N times and approximates (1/N )(cid:80)N

used computational techniques in statistics, engineering, physics, ﬁnance and many other disci-
plines. In particular, if i.i.d. samples λi ∼ µ may be obtained, the MC sampler simply iterates
E Q(λ)µ(dλ). SMC samplers [8] are de-
signed to approximate a sequence {ηl}l≥0 of probability distributions on a common space, whose
densities are only known up-to a normalising constant. The method uses N ≥ 1 samples (or

i=1 Q(λi) ≈ (cid:82)

2

particles) that are generated in parallel, and are propagated with importance sampling (often)

via Markov chain Monte Carlo (MCMC) and resampling methods. Several convergence results,

as N grows, have been proved (see e.g. [7]).

For problems which must ﬁrst be approximated at ﬁnite resolution, as is the case in this article,

and must subsequently be sampled from using MC-based methods, a multilevel (ML) framework

may be used. This can potentially reduce the cost to obtain a given level of mean square error

(MSE) [14, 11, 12], relative to performing i.i.d. sampling from the approximate posterior at a

given (high) resolution. A telescopic sum of successively reﬁned approximation increments are

estimated instead of a single highly resolved approximation. The convergence of the reﬁned

approximation increments allows one to balance the cost between levels, and optimally obtain a
cost O(ε−2) for MSE O(ε2). This is shown in the context of the models in this article. Speciﬁcally,
it is shown that the cost of MLMC, to provide a MSE of O(ε2), is less than i.i.d. sampling from

the most accurate prior approximation.

SMC within the ML framework has been primarily developed in [3, 9, 16]. These methodolo-

gies, some of which consider a similar context to this article, have been introduced where the ML

approach is thought to be beneﬁcial, but i.i.d. sampling is not possible. Indeed, one often has to

resort to advanced MCMC or SMC methods to implement the ML identity; see for instance [15]

for MCMC. SMC for nonlocal problems however, is a very sensible framework to implement the

ML identity, as it will approximate a sequence of related probabilities of increasing complexity; in

some cases, exact simulation from couples of the probabilities is not possible. As a result, SMC is

the computational framework which we will primarily pursue in this article. It is shown that the
cost of MLSMC, to provide a MSE of O(ε2), is less than i.i.d. sampling from the most accurate

posterior approximation.

It is noted, however, that some developments both with regards to

theoretical analysis and implentation of the algorithm, must be carefully considered in order to

successfully use MLSMC for nonlocal models.

This article is structured as follows. In section 2 the nonlocal elliptic equations are given, along

with the Bayesian inverse problem, and well-posedness of both are established. In particular, it

is shown that the posterior has a well-deﬁned Radon-Nikodym derivative w.r.t. prior measure. In

section 3 we consider how one can approximate expectations w.r.t. the prior and the posterior,

speciﬁcally using MLMC and MLSMC methods. Our complexity results are given also. Section

4 provides some numerical implementations of MLMC and MLSMC for the prior, and posterior,

respectively.

3

2 Nonlocal elliptic equations

2.1 Setup

Consider the following equation

L(u)(x) = b(x),

for x ∈ Ω ⊂ Rd

u(x) = 0,

for x ∈ Γ,

(2.1)

(2.2)

where L is given by (1.1), the domain Ω is simply connected, and its “boundary” Γ is suﬃciently
regular and nonlocal, in the sense that it has non-zero volume in Rd,
Under appropriate conditions on B, L−1 : H−s(Ω ∪ Γ) → H s

c (Ω ∪ Γ), for s ∈ [0, 1), with
s = 1 only for the limiting local version in which B(x, x(cid:48)) = (I − ∂2/∂x2)δ(x(cid:48) − x) (or a similar
Rd u(x)dx = 0} denotes the
uniformly elliptic form) [10]. Following [10] H s
volume constrained space of functions, and the fractional Sobolev space H s is deﬁned as follows,
for s ∈ (0, 1),

c = {u ∈ H s; u|Γ = 0,(cid:82)

H s := {u ∈ L2(Ω ∪ Γ) : (cid:107)u(cid:107)L2(Ω∪Γ) + |u|H s(Ω∪Γ) < ∞},

(2.3)

where

|u|H s(Ω∪Γ) :=

(cid:90)

(cid:90)

(u(x) − u(y))2
|x − y|d+2s dydx.

Ω∪Γ

Ω∪Γ

We deﬁne H 0

c := L2

c for ease of notation.

For b ∈ V ∗(Ω∪ Γ), the dual with respect to L2 of some space V (Ω∪ Γ), the weak formulation

of (2.2) can be deﬁned as follows.
v ∈ V (Ω ∪ Γ). Now, ﬁnd u ∈ V (Ω ∪ Γ) (satisfying the boundary conditions) such that

Integrate the equation against an arbitrary test function

B(u, v) = F (v),

for all v ∈ V (Ω ∪ Γ),

(2.4)

where

(cid:90)
(cid:90)

Rd

Rd

B(u, v) :=

F (v) :=

(cid:90)

Rd

[u(x(cid:48)) − u(x)]B(x, x(cid:48))dx(cid:48)v(x)dx,

b(x)v(x)dx.

2.2 Numerical methods for forward solution
Let h(cid:96) denote the maximum diameter of an element of V(cid:96) ⊂ V(cid:96)+1 ⊂ ··· ⊂ V . The ﬁnite
approximation of (2.4) is stated as follows. Identify some u(cid:96) ∈ V(cid:96) such that

B(u(cid:96), v(cid:96)) = F (v(cid:96)),

for all v(cid:96) ∈ V(cid:96).

(2.5)

4

u(cid:96) =(cid:80)M(cid:96)

M(cid:96)(cid:88)

Assuming the spaces V(cid:96) are spanned by elements {φk

(cid:96)}M(cid:96)

k=1, then one can substitute the ansatz

k=1 uk

(cid:96) φk

(cid:96) into the above equation, resulting in the ﬁnite linear system

B(φj

(cid:96), φk

(cid:96) )uk = F (φj

(cid:96)),

for j = 1, . . . , M(cid:96).

(2.6)

k=1

In particular, the spaces {V(cid:96)} will be comprised of discontinuous elements, so that the method
described is a discontinuous Galerkin ﬁnite element method (FEM) [4]. Piecewise polynomial
c for s ∈ [0, 1/2), and are therefore conforming when
c = V for s ∈ [0, 1/2), so there is no need to impose penalty terms at the boundaries as

discontinuous element spaces are dense in H s
u ∈ H s

one must do for smoother problems in which the discontinuous elements are non-conforming.

2.3 Forward UQ

The following assumption will be made for simplicity
Assumption 2.1. V = L2(Ω∪Γ), with norm (cid:107)·(cid:107) and inner product (cid:104)·,·(cid:105). B(x, x(cid:48)) = B(|x(cid:48)−x|) ≥
0 is continuous with B(0) ≥ c(cid:48) > 0, and there exist K1 > 0 such that for all x ∈ Ω

B(x, x(cid:48))dx(cid:48) ≤ K1.

(2.7)

(cid:90)

Ω∪Γ

As shown in Section 6 of [13], this implies that for all u, v, b ∈ V
• F is continuous: |F (v)| ≤ (cid:107)b(cid:107)(cid:107)v(cid:107);

• B is continuous: |B(u, v)| ≤ 4K1(cid:107)u(cid:107)(cid:107)v(cid:107);
• B is coercive: |B(u, u)| ≥ K2(cid:107)u(cid:107)2 for some K2(B, Ω) > 0. This actually follows from the

Poincar´e-type inequality derived in Proposition 2.5 of [1].

Hence, Lax-Milgram lemma ([5], Thm. 1.1.3) can be invoked, guaranteeing existence of a

unique solution u ∈ L2(Ω) such that

In other words, the map b (cid:55)→ u is continuous. The system (2.6) inherits solvability since the

(cid:107)u(cid:107) ≤ K−1

2 (cid:107)b(cid:107).

bilinear is a fortiori coercive on V(cid:96), so that

(cid:107)u(cid:96)(cid:107) ≤ K−1

2 (cid:107)b(cid:107).

Let Bλ be a parametrization of B, where λ ∼ µ0 and λ ∈ E, and either E = Rp or E ⊂ Rp

compact. Then the following theorem holds.

5

Theorem 2.2 (Well-posedness of forward UQ). If Assumption 2.1 holds almost surely for λ ∼ µ
and the map u (cid:55)→ Q is continuous from L2 to R, then the map λ (cid:55)→ Q is almost surely continuous.
Hence Q(λ) ∈ L∞ ⊃ Lp for all p ≥ 1, i.e. all moments exist. Lp here denotes the space of random

variables X such that(cid:82)

E |X|pµ(dλ) < ∞.

Proof. Since Assumption 2.1 holds almost surely for λ ∼ µ, then (cid:107)u(cid:107) ≤ u∗ := K−1
2 (cid:107)b(cid:107) uniformly.
So, the quantity of interest Q(λ) = (cid:107)u(λ)(cid:107) ∈ L∞ ⊃ Lp for all p ≥ 1. Therefore, for any Q :
L2(Ω) → R such that Q(λ) := Q(u(λ)) ≤ K(cid:107)u(cid:107), the result follows, since then Q(λ) ≤ Q∗ := Ku∗

for all λ.

Corollary 2.3 (Well-posedness of ﬁnite approximation). If Assumption 2.1 holds almost surely
for λ ∼ µ and the map u(cid:96) (cid:55)→ Q(cid:96) is continuous from L2 to R, then the map through the discrete
system λ (cid:55)→ Q(cid:96) is almost surely continuous and Q(cid:96) ∈ L∞.

2.4 Inverse UQ

For the inverse problem, let us assume that some data is given in the form

y = G(λ) + ξ,

λ ⊥ ξ ∼ N (0, Σ),

where

G(λ) := [(cid:104)g1, u(λ)(cid:105), . . . ,(cid:104)gM , u(λ)(cid:105)](cid:62),

gi ∈ V.

Then the following theorem holds.

(2.8)

(2.9)

Theorem 2.4 (Well-posedness of inverse UQ). The posterior distribution of λ|y is well-deﬁned

and takes the form

with Z =(cid:82)

E exp{− 1

(λ) =

dηy
dµ

exp{− 1
2
2|Σ−1/2(G(λ) − y)|2}µ(dλ).

1
Z

|Σ−1/2(G(λ) − y)|2},

(2.10)

Proof. The form of the posterior is obtained by a change of variables to {λ, ξ = y −G(λ)}, which

are independent by assumption. Note the change of variables has Jacobian 1. So, changing

variables back, this also gives the joint density. The posterior is obtained by normalizing for the
observed value of y. The form of the observation operator guarantees Z > exp{−|Σ−1|(|G∗|2 +
i is deﬁned as in the proof of Theorem 2.2, and |Σ−1|
|y|2)}, where G∗ = (G∗

M ) and G∗

2 , . . . ,G∗

1 ,G∗

is the matrix norm.

Now deﬁne G(cid:96)(λ) := [(cid:104)g1, u(cid:96)(λ)(cid:105), . . . ,(cid:104)gM , u(cid:96)(λ)(cid:105)],

gi ∈ L2(Ω).

6

Corollary 2.5 (Well-posedness of inverse UQ for ﬁnite problem). The ﬁnite approximation of
the posterior distribution of λ|y is well-deﬁned and takes the form

with Z(cid:96) =(cid:82)

dηy
(cid:96)
dµ

(λ) =

1
Z

exp{− 1
2

|Σ−1/2(G(cid:96)(λ) − y)|2},

(2.11)

E exp{− 1

2|Σ−1/2(G(cid:96)(λ) − y)|2}µ(dλ).

Remark 2.6. The forcing may also be taken as uncertain, although the uniformity will require it

to be deﬁned on a compact space. The probability space E will be taken as compact for simplicity,

as it is then easy to verify Assumption 2.1.

3 Approximation of expectations

The objective here is to approximate expectations of some functional ϕ : E → R with respect
to a probability measure η (µ or ηy), denoted Eη(ϕ). The solution u of (2.2) above must be

approximated by some u(cid:96), with a degree of accuracy which depends on (cid:96). Indeed there exists

a hierarchy of levels (cid:96) = 0, . . . , L (where L may be arbitrarily large) of increasing accuracy

(cid:96)=0 via the approximation of (2.10). For the forward problem η(cid:96) = η for all (cid:96), but

and increasing cost. For the inverse problem this manifests in a hierarchy of target probability
measures {η(cid:96)}L
it will be assumed that the function ϕ requires evaluation of the solution of (2.2), as in Theorem
2.2, and the corresponding approximations will be denoted by {ϕ(cid:96)}L
the estimator ˆY N
ηL ≡ η for all L. The mean square error (MSE) is given by

(cid:96)=0. One may then compute
L ∼ ηL, where for the forward problem it may be that

n=1 ϕL(λn

L), λn

L = 1
N

(cid:80)N
E(cid:12)(cid:12)(cid:12)Eη[ϕ(λ)] − ˆY N
(cid:12)(cid:12)(cid:12)2

L

= E(cid:110)EηL [ϕL(λ)] − ˆY N
(cid:124)

(cid:123)(cid:122)

L

variance

(cid:111)2
(cid:125)

(cid:124)
(cid:125)
+{EηL[ϕL(λ)] − Eη[ϕ(λ)]}2

(cid:123)(cid:122)

bias

.

(3.1)

Now assume there is some discretization level, say of diameter hL, which gives rise to an error
estimate on the output of size O(hα

L), for example arising from the deterministic numerical

L , where d is the spatio-temporal dimension. Now the complexity of

approximation of a spatio-temporal problem. This also translates to a number of degrees of
freedom proportional to h−d
typical forward solves will range from a dot product O(h−d
L ) (linear) to a full Gaussian elimination
L = O(ε), so one would ﬁnd the cost controlled
O(h−3d
by O(ε−ζ/α), for ζ ∈ (d, 3d). If one can obtain independent, identically distributed samples un
L,
then the necessary number of samples to obtain a variance of size O(ε2) is given by N = O(ε−2).
The total cost to obtain a mean-square error tolerance of O(ε2) is therefore O(ε−2−ζ/α).

L ) (cubic). In this problem one aims to ﬁnd hα

7

3.1 Multilevel Monte Carlo

For the forward UQ problem, in which one can sample directly from η(cid:96), one very popular method-

ology for improving the eﬃciency of solution to such problems is the multilevel Monte Carlo

(MLMC) method [14, 11]. Indeed there has been an explosion of recent activity [12] since its

introduction in [11].

In this methodology the simple estimator ˆY N

L above for a given desired
(cid:96) ) −
L is replaced by a telescopic sum of unbiased increment estimators Y N(cid:96)
(cid:96) } are i.i.d. samples, with marginal laws η(cid:96)−1, η(cid:96), respectively,
ϕ(cid:96)−1(λ(i)
carefully constructed on a joint probability space. This is repeated independently for 0 ≤ (cid:96) ≤ L.

(cid:96) where {λ(i)

i=1{ϕ(cid:96)(λ(i)

(cid:96)−1)}N−1

(cid:96)−1, λ(i)

(cid:96) = (cid:80)N(cid:96)

The overall multilevel estimator will be

ˆYL,Multi =

L(cid:88)

(cid:96)=0

Y N(cid:96)
(cid:96)

,

(3.2)

under the convention that g(λ(i)−1) = 0. A simple error analysis shows that the mean squared
error (MSE) in this case is given by

E{ ˆYL,Multi − Eη[ϕ(λ)]}2 =

L(cid:88)
(cid:124)

(cid:96)=0

E(cid:110)

(cid:111)2
(cid:96) − [Eη(cid:96)ϕ(cid:96)(λ) − Eη(cid:96)−1ϕ(cid:96)−1(λ)]
(cid:125)
Y N(cid:96)
(cid:125)
(cid:124)
+{EηL[ϕL(λ)] − Eη[ϕ(λ)]}2

(cid:123)(cid:122)
(cid:123)(cid:122)

variance

.

bias

(3.3)

Notice that the bias is given by the ﬁnest level, whilst the variance is decomposed into a sum of
variances of the increments. The variance of the (cid:96)th increment estimator has the form V(cid:96)N−1
,
where the terms V(cid:96) = E|ϕ(cid:96)(λ(cid:96))−ϕ(cid:96)−1(λ(cid:96)−1)|2 decay, following from reﬁnement of the approxima-
tion of η and/or ϕ. One can therefore balance the variance at a given level V(cid:96) with the number of
samples N(cid:96). As the level increases, the corresponding cost increases, but the variance decreases,

(cid:96)

allowing fewer samples to achieve a given variance. This can be optimized, and results in a total
cost of O(ε− max{2,ζ/α}), in the optimal case.

To be explicit, denote by Q(cid:96) := Q(u(cid:96)(λ(cid:96)), λ(cid:96)) the level (cid:96) approximation of the quantity of

interest Q. Introduce the following assumptions

(A1) There exist α, β, ζ > 0, and a C > 0 such that



|E(QL − Q∞)| ≤ Chα
L;
E|Q(cid:96) − Q(cid:96)−1|2 ≤ Chβ
(cid:96) ;
−ζ
≤ Ch
(cid:96)

C(Q(cid:96))

,

(3.4)

where C(Q(cid:96)) denotes the cost to evaluate Q(cid:96).

8

We have the following classical MLMC Theorem [12]

Theorem 3.1. Assume (A1) and max{β, ζ} ≤ 2α. Then for any ε > 0, there exist L,{N(cid:96)}L
and C > 0 such that

(cid:96)=0

E(cid:104)(cid:16) ˆYL,Multi − Eη[Q]

(cid:17)2(cid:105) ≤ Cε2,

(3.5)

(3.6)

for the following cost

COST ≤ C



ε−2,
ε−2| log(ε)|2,
ε−(2+ ζ−β

α ),

if β > ζ,

if β = ζ,

if β < ζ.

3.2 Multilevel sequential Monte Carlo sampler
Now the inverse problem will be considered. There is a sequence of probability measures {η(cid:96)}(cid:96)≥0
on a common measurable space (E,E), and for each l there is a measure γ(cid:96) : E → R+, such that

where the normalizing constant Z(cid:96) =(cid:82)

η(cid:96)(dλ) =

γ(cid:96)(dλ)

Z(cid:96)

(3.7)

E γ(cid:96)(dλ) is unknown. The objective is to compute:

(cid:90)

E

Eη∞[ϕ(λ)] :=

ϕ(λ)η∞(dλ)

for potentially many measurable η∞−integrable functions ϕ : E → R.

3.2.1 Notations
Let (E,E) be a measurable space. The notation Bb(E) denotes the class of bounded and measur-
able real-valued functions, and the supremum norm is written as (cid:107)f(cid:107)∞ = supλ∈E |f (λ)|. Consider
non-negative operators K : E × E → R+ such that for each λ ∈ E the mapping A (cid:55)→ K(λ, A) is
a ﬁnite non-negative measure on E and for each A ∈ E the function λ (cid:55)→ K(λ, A) is measurable;
the kernel K is Markovian if K(λ, dv) is a probability measure for every λ ∈ E. For a ﬁnite
measure µ on (E,E), and a real-valued, measurable f : E → R, we deﬁne the operations:

µK : A (cid:55)→

We also write µ(f ) =(cid:82) f (λ)µ(dλ). In addition (cid:107) · (cid:107)r, r ≥ 1, denotes the Lr−norm, where the

f (v) K(λ, dv).

K(λ, A) µ(dλ) ; Kf : λ (cid:55)→

(cid:90)

(cid:90)

expectation is w.r.t. the law of the appropriate simulated algorithm.

9

3.2.2 Algorithm
As described in Section 1, the context of interest is when a sequence of densities {η(cid:96)}(cid:96)≥0, as
in (3.7), are associated to an ‘accuracy’ parameter hl, with h(cid:96) → 0 as (cid:96) → ∞, such that
∞ > h0 > h1 ··· > h∞ = 0. In practice one cannot treat h∞ = 0 and so must consider these
distributions with h(cid:96) > 0. The laws with large h(cid:96) are easy to sample from with low computational

cost, but are very diﬀerent from η∞, whereas, those distributions with small h(cid:96) are hard to sample

with relatively high computational cost, but are closer to η∞. Thus, we choose a maximum level
L ≥ 1 and we will estimate

By the standard telescoping identity used in MLMC, one has

EηL [ϕ(λ)] :=

EηL[ϕ(λ)] = Eη0 [ϕ(λ)] +

= Eη0 [ϕ(λ)] +

E

ϕ(λ)ηL(λ)dλ .

(cid:90)
(cid:110)Eη(cid:96)[ϕ(λ)] − Eη(cid:96)−1[ϕ(λ)]
(cid:111)
L(cid:88)
(cid:17)
L(cid:88)

(cid:104)(cid:16) γ(cid:96)(λ)Z(cid:96)−1

(cid:96)=1

Eη(cid:96)−1

γ(cid:96)−1(λ)Z(cid:96)

(cid:96)=1

− 1

ϕ(λ)

(cid:105)

.

(3.8)

Suppose now that one applies an SMC sampler [8] to obtain a collection of samples (par-

ticles) that sequentially approximate η0, η1, . . . , ηL. We consider the case when one initial-

izes the population of particles by sampling i.i.d. from η0, then at every step resamples and

, . . . , λ1:NL−1
applies a MCMC kernel to mutate the particles. We denote by (λ1:N0
L−1
+∞ > N0 ≥ N1 ≥ ··· NL−1 ≥ 1, the samples after mutation; one resamples λ1:Nl
accord-
(cid:96)), for indices l ∈ {0, . . . , L − 1}. We will denote by
ing to the weights G(cid:96)(λi
{M(cid:96)}1≤(cid:96)≤L−1 the sequence of MCMC kernels used at stages 1, . . . , L − 1, such that η(cid:96)M(cid:96) = η(cid:96).
For ϕ : E → R, (cid:96) ∈ {1, . . . , L}, we have the following estimator of Eη(cid:96)−1 [ϕ(λ)]:

(cid:96)) = (γ(cid:96)+1/γ(cid:96))(λi

), with

0

l

ηN(cid:96)−1
(cid:96)−1 (ϕ) =

1

N(cid:96)−1

ϕ(λi

(cid:96)−1) .

N(cid:96)−1(cid:88)
N(cid:96)−1(cid:88)

i=1

i=1

We deﬁne

ηN(cid:96)−1
(cid:96)−1 (G(cid:96)−1M(cid:96)(dλ(cid:96))) =

1

N(cid:96)−1

G(cid:96)−1(λi

(cid:96)−1)M(cid:96)(λi

(cid:96)−1, dλ(cid:96)) .

The joint probability distribution for the SMC algorithm is

N0(cid:89)

L−1(cid:89)

N(cid:96)(cid:89)

η0(dλi
0)

i=1

(cid:96)=1

i=1

ηN(cid:96)−1
(cid:96)−1 (G(cid:96)−1M(cid:96)(dλi

(cid:96)))

ηN(cid:96)−1
(cid:96)−1 (G(cid:96)−1)

.

If one considers one more step in the above procedure, that would deliver samples {λi
standard SMC sampler estimate of the quantity of interest in (3.8) is ηN

L}NL

i=1, a

L (g); the earlier samples

are discarded. Within a multilevel context, a consistent SMC estimate of (3.8) is

(cid:111)

(cid:96)−1 (ϕG(cid:96)−1)
ηN(cid:96)−1
(cid:96)−1 (G(cid:96)−1)

− ηN(cid:96)−1

(cid:96)−1 (ϕ)

,

(3.9)

(cid:98)Y = ηN0

0 (ϕ) +

L(cid:88)

(cid:110) ηN(cid:96)−1

(cid:96)=1

10

and this will be proven to be superior than the standard one, under assumptions.

The relevant MSE error decomposition here is:

E(cid:2){(cid:98)Y − Eη∞ [ϕ(λ)]}2(cid:3) ≤ 2 E(cid:2){(cid:98)Y − EηL[ϕ(λ)]}2(cid:3) + 2{EηL[ϕ(λ)] − Eη∞ [ϕ(λ)]}2 .

(3.10)

3.2.3 Multilevel SMC

We will now restate an analytical result from [3] that controls the error term E[{(cid:98)Y −EηL[ϕ(λ)]}2]
in expression (3.10). For any (cid:96) ∈ {0, . . . , L} and ϕ ∈ Bb(E) we write: η(cid:96)(ϕ) :=(cid:82)

E ϕ(λ)η(cid:96)(λ)dλ.

The following standard assumptions will be made ; see [3, 7].

(A2) There exist 0 < C < C < +∞ such that

G(cid:96)(λ) ≤ C ;
G(cid:96)(λ) ≥ C .

sup
(cid:96)≥1
inf
(cid:96)≥1

sup
λ∈E
inf
λ∈E

(A3) There exists a ρ ∈ (0, 1) such that for any (cid:96) ≥ 1, (λ, v) ∈ E2, A ∈ E:

(cid:90)

(cid:90)

M(cid:96)(λ, dλ(cid:48)) ≥ ρ

M(cid:96)(v, dλ(cid:48)) .

Under these assumptions the following Theorem is proven in [3]

A

A

Theorem 3.2. Assume (A2-3). There exist C < +∞ such that for any ϕ ∈ Bb(E), with
(cid:107)ϕ(cid:107)∞ = 1,

E(cid:2){(cid:98)Y − EηL [ϕ(λ)]}2(cid:3) ≤ C

(cid:18) 1

+

N0

(cid:18) Vl

Nl−1

L(cid:88)

(cid:96)=1

+

V 1/2
(cid:96)
N 1/2
(cid:96)−1

(cid:19)(cid:19)

,

V 1/2
Nq−1

q

L(cid:88)

q=(cid:96)+1

where V(cid:96) := (cid:107) Z(cid:96)−1

Z(cid:96)

G(cid:96)−1 − 1(cid:107)2∞.

The following additional assumption will now be made

(A4) There exist α, β, ζ > 0, and a C > 0 such that



V(cid:96)
≤ Chβ
(cid:96) ;
|(ηL − η∞)(1)| ≤ Chα
L;
−ζ
≤ Ch
(cid:96)

C(G(cid:96))

,

(3.11)

where C(G(cid:96)) denotes the cost to evaluate G(cid:96).

The following Theorem may now be proven

11

Theorem 3.3. Assume (A2-4) and max{β, ζ} ≤ 2α. Then for any ε > 0, there exist L,{N(cid:96)}L
and C > 0 such that

(cid:96)=0

E(cid:104)(cid:16)(cid:98)Y − Eη[ϕ(λ)]

(cid:17)2(cid:105) ≤ Cε2,

for the following cost

COST ≤ C



ε−2,
ε−2| log(ε)|2,
ε−(2+ ζ−β

α ),

if β > ζ,

if β = ζ,

if β < ζ.

(3.12)

(3.13)

that hα
L

Proof. The MSE can be bounded using (3.10). Following from (A4)(ii), the second term requires
(cid:104) ε, and assuming hL = M−L for some M ≥ 2, this translates to L (cid:104) log ε. As in
[3], the additional error term is dealt with by ﬁrst ignoring it and optimizing COST(N), for a

given V(N) = ε2, where N := (N1, . . . , NL). This requires that N(cid:96) ∝(cid:112)Vl/Cl (cid:104) h(β+ζ)/2

. The

(cid:96)

, where KL =(cid:80)L−1

l=1 h(β−ζ)/2

l

, so one has

constraint then requires that N(cid:96) = ε−2KLh(β+ζ)/2

(cid:96)

L(cid:88)

COST(N) =

N(cid:96)C(cid:96) = ε−2K 2
L.

It was shown in [3] that the result follows, provided ζ < 2α. In fact, re-examining the additional

(cid:96)=0

L(cid:88)

L(cid:88)

term as in Section 3.3 of [3], one has

L−1(cid:88)
where the second line follows from the inequality ((cid:80)L
(cid:96)=0 a(cid:96))2 ≤ L(cid:80)L

= O(ε2ε1−ζ/2αL1/2K−1
L ),

= O(ε2ε1−ζ/2α

h(β−ζ)/4

V 1/2
(cid:96)
N 1/2
(cid:96)−1

V 1/2
Nq−1

q

q=(cid:96)+1

(cid:96)=1

(cid:96)=1

(cid:96)

−3/2
L

)

K

(cid:96) and the deﬁnition
of KL. Now substituting KL = O(1),O(L),O(ε−(ζ−β)/2α) for the 3 cases and recalling the
assumption max{β, ζ} ≤ 2α gives V(N) = ε2 for the costs in (3.13).

(cid:96)=0 a2

3.2.4 Veriﬁcation of assumptions

Assume a uniform prior µ. Following from (2.8), the unnormalized measures will be given by

γ(cid:96) = exp{−Φ(G(cid:96)(λ))},

(3.14)

where Φ(Gl(λ)) = 1

2|Σ−1/2(Gl(λ) − y)|2, and

G(cid:96)(λ) := [(cid:104)g1, u(cid:96)(λ)(cid:105), . . . ,(cid:104)gM , u(cid:96)(λ)(cid:105)](cid:62),

gi ∈ L2(Ω),

and ul is the solution of the numerical approximation of (2.2). Notice that these are uniformly
bounded, over both λ ∈ E and over (cid:96), following from Corollary 2.5 for ﬁnite (cid:96), and Theorem 2.4

in the limit.

12

It is shown in [3] Section 4 that

|Φ(G(cid:96)(λ)) − Φ(G(cid:96)−1(λ))| ≤ C(|G(cid:96)|,|G(cid:96)−1|)|G(cid:96)(λ) − G(cid:96)−1(λ)|.

One proceeds similarly to that paper, and ﬁnds that

|G(cid:96)(λ) − G(cid:96)−1(λ)| ≤ C(cid:107)u(cid:96)(λ) − u(cid:96)−1(λ)(cid:107)V .

(3.15)

(3.16)

Note that G(cid:96) = γ(cid:96)+1/γ(cid:96), so inserting the bound (3.15) into (3.14), and observing the bound-
edness of the {G(cid:96)} noted above, one can see that Assumption (A2) holds. Now inserting (3.15)
and (3.16) into (3.14), one can see that in order to establish Assumption (A4), it suﬃces to
establish rates of convergence for (cid:107)u(cid:96)(λ) − u(cid:96)−1(λ)(cid:107)V . In particular, Assumption (A2) and the
C2 inequality (Minkowski plus Young’s) provide that

V(cid:96) = (cid:107) Z(cid:96)−1

Z(cid:96)

G(cid:96)−1 − 1(cid:107)2∞ =

(cid:13)(cid:13)(cid:13)(cid:13) G(cid:96)−1

η(cid:96)−1(G(cid:96)−1)

(cid:13)(cid:13)(cid:13)(cid:13)2

∞

− 1

≤ C((cid:107)G(cid:96)−1 − 1(cid:107)2∞ + (cid:107)η(cid:96)−1(G(cid:96)−1) − 1(cid:107)2∞) ≤ 2C(cid:107)G(cid:96)−1 − 1(cid:107)2∞
≤ C(cid:48)(cid:107)u(cid:96)(λ) − u(cid:96)−1(λ)(cid:107)2
V .

Therefore, the rate of convergence of (cid:107)u(cid:96)(λ) − u(cid:96)−1(λ)(cid:107)V is the required quantity for both
the forward (for Lipschitz Q) and the inverse multilevel estimation problems. By the triangle
inequality it suﬃces to consider the approximation of the truth (cid:107)u(cid:96)(λ) − u(λ)(cid:107)V , and by C´ea’s
lemma ([5], Thm. 2.4.1), Assumption 2.1 guarantees the existence of a C > 0 such that

(cid:107)u(cid:96)(λ) − u(λ)(cid:107)V ≤ C inf
v(cid:96)∈V(cid:96)

(cid:107)v(cid:96)(λ) − u(λ)(cid:107)V

(3.17)

Theorem 6.2 of [10] provides rates of convergence of the best approximation above, in the case

that V = H s and the FEM triangulation is shape regular and quasi-uniform. In particular, their
Case 1 corresponds to the case of a singular kernel, i.e. not even integrable, and L : H s
c → H−s,
for s ∈ (0, 1), so the solution operator is smoothing. Their Case 2 corresponds to a slightly more
regular kernel than ours, where in fact B(x,·) ∈ L2, rather than L1 as given by Assumption
2.1 and consequently L : L2
c → L2. It is shown that in Case 2, if the solution u ∈ H m+t, for
polynomial elements of order m and some t ∈ [0, 1], then convergence with respect to the L2
norm is in fact O(hm+t). So, for linear elements, second order convergence can be obtained,

leading to β = 4 in (3.11).

Recall that 2.1 ensures well-posedness of the solution from L2 (cid:51) b (cid:55)→ u ∈ L2, and so discon-

tinuities are allowed. This is actually a strength of nonlocal models and one impetus for their

use. It is reasonable to expect that if the the nodes of the discontinuous elements match up with

the discontinuities, i.e. there is a node at each point of discontinuity for d = 1 or there are nodes

13

all along a curve/surface of discontinuity for d > 1, and if the solution is suﬃciently smooth in

the subdomains, then the rates of convergence should match those of the smooth subproblems.

For example, if there is a point discontinuity such that the domain can be separated at that
point into Ω1 ∪ Ω2 = Ω and one has u|Ω1 ∈ H m+t(Ω1) and u|Ω2 ∈ H m+t(Ω2), where u|Ωi is the
restriction of u to the set Ωi, then one expects the convergence rate of m+t to be preserved. This

is postulated and veriﬁed numerically in [4]. It is also illustrated numerically in that work that

even with discontinuous elements, if there is no node at the discontinuity then the convergence

rate reverts to 1/2, so β = 1 in (3.11).

4 Numerical examples

The particular nonlocal model which will be of interest here is that in which the kernel is given

by

Bλ(x, x(cid:48)) = f (x, x(cid:48), θ)

1

(4.1)
where λ := (θ, α, δ), and 1A is the characteristic function which takes the value 1 if (x, x(cid:48)) ∈ A
and zero otherwise, and f (x, x(cid:48)) = f (x(cid:48), x). Notice that α ∈ (0, d) is scalar, but (θ, δ) may

δ2|x − x(cid:48)|α 1{|x−x(cid:48)|<δ},

be deﬁned on either a ﬁnite-dimensional subspace of function-space, or in principle in the full

inﬁnite dimensional space. This may be of interest to incorporate spatial dependence of material

properties.

Following [6] we consider the following model. Let Ω = [0, 1] and Γ = [−δ, 0) ∪ (1, 1 + δ]. For

(x, x(cid:48)) ∈ Ω2, let z = (x + x(cid:48))/2. The kernel is deﬁned by,

0.2 + (z − 0.625)2

z + θ
14.4 + (z − 0.75) + 2

if z ∈ [0, 0.625)
if z ∈ [0.625, 0.75)
if z ∈ [0.75, 1)



f (x, x(cid:48), θ) =

b(x) = 5

The following prior is used for the parameters,

θ ∼ Uniform(1, 2)
α ∼ Beta(2, 2)
δ ∼ Gamma(1, 1) truncated on (0.125, 1)

This is an extension of an example used in [6], which is identical to this model with θ = 1.25 and

α = 1 (various values of δ are used there). To solve the equation with FEM, a uniform partition

14

Figure 1: Cost vs. MSE for MLMC

on Ω is used with h(cid:96) = 2−(k+(cid:96)), k = 3. Thus for each level (cid:96), h < δ. The same discontinuous
Galerkin FEM as in [6] is applied to solve the system.

Similar methods to those in [3] are used to estimate the convergence rates for both the forward

and inverse problems. The estimated rates are ˆα = 2.005 and ˆβ = 4.271. The rates α = 2 and

β = 4 are used for the simulations.

For the inverse problem, data is generated with θ = 2, α = 0.5 and δ = 0.2. The observations
are y|u ∼ N (m(u), σ2), where σ2 = 0.01 and m(u) = [u(a), u(b)](cid:62). The quantity of interest for

both the forward and inverse problem is u(0.5).

The forward problem is solved in the standard multilevel fashion by generating coupled par-

ticles from the prior at each level. The main result, the cost vs. MSE of the estimates is shown

in Figure 1. The Bayesian inverse problem is also solved for this model, and the main result is

shown in Figure 2.

5 Summary

This is the ﬁrst systematic treatment of UQ for nonlocal models, to the knowledge of the au-

thors. Natural extensions include obtaining rigorous convergence results for piecewise smooth

15

20252102152202252302−202−152−102−520MSE(𝜀2)Runtimecost∝∑𝐿𝑙=0𝑁𝑙ℎ−1𝑙AlgorithmMLMCMCFigure 2: Cost vs. MSE for MLSMC

solutions, exploring higher-dimensional examples, spatial parameters, time-dependent models,

and parameters deﬁned on non-compact spaces.

Acknowledgements. KJHL was supported by the DARPA FORMULATE project. AJ &

YZ were supported by Ministry of Education AcRF tier 2 grant, R-155-000-161-112. We express

our gratitude to Marta D’Elia, Pablo Seleson, and Max Gunzburger for useful discussions.

References

[1] Andreu, F., Mazon, J. M., Jose, M. Rossi, J. & Toledo, J. (2009). A nonlocal p-Laplacian

evolution equation with nonhomogeneous Dirichlet boundary conditions. SIAM Journal on

Mathematical Analysis, 40, 1815–1851.

[2] Bakunin, O. G. (2008). Turbulence and diﬀusion: scaling versus equations. Springer Science

& Business Media.

[3] Beskos, A., Jasra, A., Law, K. J. H, Tempone, R. & Zhou, Y. (2015). Multilevel sequential

Monte Carlo samplers. arXiv preprint arXiv:1503.07259.

16

20252102152202252302−202−152−102−520MSE(𝜀2)Runtimecost∝∑𝐿𝑙=0𝑁𝑙ℎ−1𝑙AlgorithmMLSMCSMC[4] Chen, X. & Gunzburger, M. (2011). Continuous and discontinuous ﬁnite element meth-

ods for a peridynamics model of mechanics. Computer Methods in Applied Mechanics and

Engineering, 200, 1237–1250.

[5] Ciarlet, P. G. (2002). The ﬁnite element method for elliptic problems. SIAM.

[6] D’Elia, M. & Gunzburger, M. (2014). Optimal distributed control of nonlocal steady

diﬀusion problems. SIAM Journal on Control and Optimization. 52, 243–273.

[7] Del Moral, P. (2004). Feynman-Kac Formulae: Genealogical and Interacting Particle Sys-

tems with Applications. Springer: New York.

[8] Del Moral, P., Doucet, A. & Jasra, A. (2006). Sequential Monte Carlo samplers.

J. R. Statist. Soc. B, 68, 411–436.

[9] Del Moral, P., Jasra, A., Law, K. J. H, & Zhou, Y. (2016). Multilevel sequential Monte

Carlo samplers for normalizing constants. arXiv preprint arXiv:1603.01136.

[10] Du, A., Gunzburger, M., Lehoucq, R. & Zhou, K. (2012). Analysis and approximation

of nonlocal diﬀusion problems with volume constraints. SIAM review, 54, 667–696.

[11] Giles, M. B. (2008). Multilevel Monte Carlo path simulation. Op. Res. 56, 607-617.

[12] Giles, M. B (2015). Multilevel Monte Carlo methods. Acta Numerica, 24, 259-328.

[13] Gunzburger, M. & Lehoucq, R. B. (2010). A nonlocal vector calculus with application

to nonlocal boundary value problems. Multiscale Modeling & Simulation, 8, 1581–1598.

[14] Heinrich, S. (2001). Multilevel Monte Carlo methods. Large-scale scientiﬁc computing.

Springer Berlin Heidelberg, 2001. 58-67.

[15] Hoang, V., Schwab, C. & Stuart, A. (2013). Complexity analysis of accelerated MCMC

methods for Bayesian inversion. Inverse Prob., 29, 085010.

[16] Jasra, A., Kamatani, K., Law, K. J., & Zhou, Y. (2015). Multilevel particle ﬁlter. arXiv

preprint arXiv:1510.04977.

[17] Silling, S. A., Zimmermann, M & Abeyaratne, R. (2003). Deformation of a peridynamic

bar. Journal of Elasticity. 73, 173–190.

17

