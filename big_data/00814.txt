6
1
0
2

 
r
a

M
2

 

 
 
]

Y
S
.
s
c
[
 
 

1
v
4
1
8
0
0

.

3
0
6
1
:
v
i
X
r
a

Active Requirement Mining of Bounded-Time
Temporal Properties of Cyber-Physical Systems

Gang Chen

Zachary Sabato

Zhaodan Kong

March 3, 2016

Abstract

This paper uses active learning to solve the problem of mining bounded-time
signal temporal requirements of cyber-physical systems or simply the requirement
mining problem. By utilizing robustness degree, we formulates the requirement
mining problem into two optimization problems, a parameter synthesis problem
and a falsiﬁcation problem. We then propose a new active learning algorithm
called Gaussian Process Adaptive Conﬁdence Bound (GP-ACB) to help solving
the falsiﬁcation problem. We show theoretically that the GP-ACB algorithm has a
lower regret bound thus a larger convergence rate than some existing active learn-
ing algorithms, such as GP-UCB. We ﬁnally illustrate and apply our requirement
mining algorithm on two case studies, the Ackley’s function and a real world auto-
matic transmission model. The case studies show that our mining algorithm with
GP-ACB outperforms others, such as those based on Nelder-Mead, by an average
of 30% to 40%. Our results demonstrate that there is a principled and efﬁcient way
of extracting requirements for complex cyber-physical systems.

1

Introduction

In this paper, we propose the use of active learning for mining bounded-time signal
temporal logic (STL) requirements of cyber-physical systems (CPSs). CPSs are char-
acterized by the tight interaction of a collection of digital computing devices (the Cy-
ber part) and a continuous-time dynamical system (the Physical part) [3, 22]. They
are used for modeling in many safety-critical domains, such as automotive, medical,
and aerospace industries, where the correctness of the end product is of signiﬁcant
importance [16, 17, 24]. However, in many industrial settings, the requirements in-
tended to enforce the correctness guarantee are vague and in many cases expressed
in natural languages, such as “smooth steering” and “good fuel efﬁciency.” This pre-
vents the application of formal veriﬁcation tools, such as Statistical Model Checking
(SMC) [31]. Further, due to the complex nature of many CPSs, writing down the appro-
priate requirements to reﬂect the desirable system properties can be challenging even
for experienced designers.

Given a system S, e.g., a Stateﬂow/Simulink model of a steering system, and a
requirement template ϕ with a set of unknown parameters θ, the goal of this paper is

1

to develop an improved method which can automatically infer a requirement ϕθ (often
called a speciﬁcation in formal methods literature) written in signal temporal logic,
i.e., the system S satisﬁes the requirement ϕθ . Such a problem can be called either
a requirement mining problem or a temporal logic inference problem. In the past few
years with the introduction of the concept of robustness degree [11,13], the problem has
received increased attention and has achieved signiﬁcant progress. Robustness degree
quantiﬁes how strongly a given sample trajectory s of a system S exhibits a temporal
logic property ϕθ as a real number rather than just providing a yes or no answer. A
robustness degree of r(s,ϕθ ) means that the trajectory s can tolerate perturbations up
to the size of |r(s,ϕθ )| and still maintain its current Boolean truth value with respect to
ϕθ .

With the help of robustness degree, the requirement mining problem of a CPS
can be converted to an optimization problem for the expected robustness. Various
techniques, such as particle swarm optimization [15], simulated annealing [21], and
stochastic gradient descent algorithm [20] can then be used to solve the optimization
problem1. Based on the nature of the search space, the requirement mining problems
can be classiﬁed into two categories, parameter estimation problems and structural in-
ference problems. In the former case, the structure of the requirement is given but with
some unknown parameter values, for example, in a vehicular application, a require-
ment structure may be known as F[0,τ](v > π), which means that “eventually between
time 0 and some unspeciﬁed time τ, the speed v is greater than some unspeciﬁed value
π.” The goal of the parameter estimation problem is to identify appropriate values
for the parameters [8, 18].
In the latter example, even the structure of the require-
ment is unspeciﬁed. The structural inference problem is inherently hard. Candidate
formulas have to be restricted to a certain template with which a partial order is well
deﬁned [19–21]. This paper focuses on the parameter estimation problem, but there are
signiﬁcant implications for structural inference as well.

Due to the hybrid nature of CPSs, the robustness degree function can be highly
nonlinear. Furthermore, many CPSs are stochastic because of various uncertainties in-
herent to the system and its mathematical model. These complexities added together
makes uniform sampling method inappropriate to solve the optimization problem (there
are even cases in which the exact probability distribution over the parameter space is
unknown a priori). Monte-Carlo techniques have been shown to be an effective sam-
pling method to tackle the issue [2, 18]. It is worth pointing out that these techniques
may suffer from slow convergence, meaning that the inference procedure may take a
long time. In many applications, a quick verdict is needed, consider for instance an
online diagnosis of a faulty safety-critical system.

In this paper, we use active learning to partly mitigate the need for a large num-
ber of iterations during optimization. The idea behind active learning is to accelerate
convergence by actively selecting potentially “informative” samples, in contrast with
random sampling from a predeﬁned distribution [25, 28]. Active learning can be ex-
plained with a supervised video classiﬁcation problem. With passive learning, a large
amount of labeled data is needed for classiﬁcation, e.g., cat videos vs. non cat videos.
1Other optimization algorithms that have been used to solve similar optimization problems related to
veriﬁcation/falsiﬁcation include rapidly exploring random tree (RRT) [12], cross-entropy method [27], Tabu
search [1], and Ant Colony Optimization (ACO) [4].

2

Labeling itself involves an oracle, e.g., a human, and it might be time consuming. Fur-
ther, much of the data may be redundant, not contributing much to the classiﬁcation.
With active learning, based on the current information gathered about the data distri-
bution, the learning algorithm only asks for the label of the most informative data. In
our setting, the CPS model together with a stochastic model checker will serve as an
oracle, providing the learner an approximate expected robustness degree based on the
current most informative parameter.

Contributions The main contribution of this paper is an active learning based scheme
for requirement mining of cyber-physical systems. It uniﬁes two complementary camps
of requirement mining and veriﬁcation philosophies: one is model based [2,4,6,18,27],
and the other is data driven [8,19,21]. In our method, models are used as oracles, gen-
erating data which enables our method to gain knowledge of the system. This helps
focusing ongoing searches in promising parameter ranges, and thus eliminating unnec-
essary samples. The philosophy of our paper is similar to simulation based optimiza-
tion [14] but with a particular focus on utilizing machine learning techniques. Second,
instead of using existing learning algorithms, we develop a new active learning al-
gorithm called Gaussian Process Adaptive Conﬁdence Bound (GP-ACB). We prove
that our GP-ACB algorithm converges faster than Gaussian Process Upper Conﬁdence
Bound (GP-UCP) algorithm [30], a state-of-the-art active learning algorithm. Third, we
integrate our GP-ACB algorithm with an existing veriﬁcation tool, called Breach [10].
Using an automatic transmission controller as an example system, we show that the
efﬁciency of Breach can be improved by as much as 40%.

Organization This paper is subdivided into the following sections. Section 2 dis-
cusses the relevant background on signal temporal logic and Gaussian processes. Sec-
tion 3 formally deﬁnes the requirement mining problem and shows how to transform
the problem into an optimization problem with the help of robustness degree. Section 4
discusses our GP-ACB algorithm. Section 5 provides two case studies to demonstrate
our algorithm, an academic example with Ackley’s function as the target function and
an automotive example. Section 6 concludes the paper.

2 Preliminaries
2.1 Signal Temporal Logic
Given two sets A and B, F (A,B) denotes the set of all functions from A to B. Given
a time domain R+ := [0,∞), a continuous-time, continuous-valued signal is a function
s ∈ F (R+,Rn). We use s(t) to denote the value of signal s at time t, and s[t] to denote
the sufﬁx of signal s from time t, i.e., s[t] = {s(τ)|τ ≥ t}.

Signal temporal logic (STL) [23] is a temporal logic deﬁned over signals. STL is a
predicate logic with interval-based temporal semantics. The syntax of STL is deﬁned
as

ϕ := f (s) ∼ d|¬ϕ|ϕ1 ∧ ϕ2|ϕ1 ∨ ϕ2|F[a,b)ϕ|G[a,b)ϕ,

(1)

3

where a and b are non-negative ﬁnite real numbers, and f (s) ∼ d is a predicate where
s is a signal, f ∈ F (Rn,R) is a function, ∼∈ {<,≥}, and d ∈ R is a constant. The
Boolean operators ¬ and ∧ are negation (“not”) and conjunction (“and”), respectively.
The other Boolean operators are deﬁned as usual. The temporal operators F and G
stand for “Finally (eventually)” and “Globally (always)”, respectively.

The semantics of STL is recursively deﬁned as
f (s(t)) ∼ d
s[t] |= ϕ1 and s[t] |= ϕ2
s[t] |= ϕ1 or s[t] |= ϕ2
∀t(cid:48) ∈ [t + a,t + b),s[t(cid:48)] |= ϕ
∃t(cid:48) ∈ [t + a,t + b),s.t. s[t(cid:48)] |= ϕ.

s[t] |= ( f (s) ∼ d)
s[t] |= ϕ1 ∧ ϕ2
s[t] |= ϕ1 ∨ ϕ2
s[t] |= G[a,b)ϕ
s[t] |= F[a,b)ϕ

iff
iff
iff
iff
iff

In plain English, F[a,b)ϕ means “within a and b time units in the future, ϕ is true”, and
G[a,b)ϕ means “for all times between a and b time units in the future ϕ is true”.

STL is equipped with a robustness degree [11, 13] (also called “degree of satis-
faction”) that quantiﬁes how well a given signal s satisﬁes a given formula ϕ. The
robustness is calculated recursively as follows

r(s, ( f (s) < d),t) = d − f (s(t))
r(s, ( f (s) ≥ d),t) = f (s(t))− d

r(s,ϕ1 ∧ ϕ2,t) = min(cid:0)r(s,ϕ1,t),r(s,ϕ2,t)(cid:1)
r(s,ϕ1 ∨ ϕ2,t) = max(cid:0)r(s,ϕ1,t),r(s,ϕ2,t)(cid:1)

r(s,ϕ,t(cid:48))
r(s,ϕ,t(cid:48)).

r(s,G[a,b)ϕ,t) = min
r(s,F[a,b)ϕ,t) = max

t(cid:48)∈[t+a,t+b)
t(cid:48)∈[t+a,t+b)

We use r(s,ϕ) to denote r(s,ϕ,0). If r(s,ϕ) is large and positive, then s would have to
change by a large deviation in order to violate ϕ.

Parametric signal temporal logic (PSTL) is an extension of STL where the bound d
and the endpoints of the time intervals [a,b) are parameters instead of constants [6]. We
denote them as scale parameters π = [π1, ...,πnπ ] and time parameters τ = [τ1, ...,τnτ ],
respectively. A full parameterization is given as [π,τ]. The syntax and semantics of
PSTL are the same as those of STL. A valuation θ is a mapping that assigns real values
to the parameters appearing in an PSTL formula. A valuation θ of an PSTL formula ϕ
induces an STL formula ϕθ . For example, if ϕ = F[τ1,τ2)(x < π1) and θ ([π1,τ1,τ2]) =
[0,0,3], then ϕθ = F[0,3)(x < 0).

2.2 Gaussian Processes
Formally, a Gaussian process (GP) is deﬁned as a collection of random variables, any
ﬁnite linear combination of which have a joint Gaussian distribution [26]. A simple
example of a Gaussian process is a linear regression model f ((cid:126)x) = φ ((cid:126)x)T (cid:126)w, where (cid:126)x ∈
Rn, φ ∈ F (Rn,RN), which maps an n-dimensional(cid:126)x to an N-dimensional feature space
(a feature is an individual, measurable property of a function space), and (cid:126)w ∼ N ((cid:126)0,Σ),
a zero mean Gaussian with covariance matrix Σ. Any GP is completely speciﬁed by its

4

mean function m((cid:126)x) and its covariance function or kernel k((cid:126)x,(cid:126)x(cid:48))

m((cid:126)x) = E[ f ((cid:126)x)],
k((cid:126)x,(cid:126)x(cid:48)) = E[( f ((cid:126)x)− m((cid:126)x))( f ((cid:126)x(cid:48))− m((cid:126)x(cid:48)))].

As an example, for the linear regression model,

E[ f ((cid:126)x)] = φ ((cid:126)x)T E[(cid:126)w],
E[ f ((cid:126)x) f ((cid:126)x(cid:48))] = φ ((cid:126)x)T E[(cid:126)w(cid:126)wT ]φ ((cid:126)x(cid:48)) = φ ((cid:126)x)T Σφ ((cid:126)x(cid:48)).

A ﬂat (or even zero) mean function m((cid:126)x) is chosen in the majority of cases in the
literature. Such a choice does not cause many issues since the mean of the posterior
process in not conﬁned to zero. There is a large set of available kernels k((cid:126)x,(cid:126)x(cid:48)). Two
common ones, which are also used in this paper, are [5]

• Gaussian kernel with length-scale l > 0,k((cid:126)x1,(cid:126)x2) = exp(−|(cid:126)x1−(cid:126)x2|2/(2l2)), where

|.| is the Euclidean length;

• Mat´ern kernel with length-scale l > 0

(cid:32)√

k((cid:126)x1,(cid:126)x2) =

21−ν
Γ(ν)

(cid:33)ν

Kν

(cid:32)√

2ν|(cid:126)x1 −(cid:126)x2|

l

(cid:33)

,

2ν|(cid:126)x1 −(cid:126)x2|

l

where Kν is the modiﬁed Bessel function and ν is a positive parameter.

3 Requirement Mining Problem Formulation

In this section, we ﬁrst provide some background on our cyber-physical system models.
We then formally deﬁne the requirement mining problem. Finally, we show how to
formulate the requirement mining problem as an optimization problem.

3.1 Cyber-Physical Systems
In this paper, we study autonomous (closed-loop) cyber-physical systems. Notations
from [2, 27] are adopted here. A system S maps an initial condition (or uncontrolled
environmental conditions, e.g., road conditions)(cid:126)x0 ∈ X0 ⊂ Rnx to a discrete-time output
signal (cid:126)y ∈ F ([0,T ],Y ) with Y ⊂ Rny and T as the ﬁnite maximal simulation time.
We assume both X0 and Y can be represented as the Cartesian product of intervals
[a1,b1]× [a2,b2]× . . . [an,bn], where ai,bi ∈ R.

3.2 Problem Statement
Formally, in this paper, we solve the following problem.

Problem 1. Given a system S with an initial condition set (or uncontrolled environ-
mental condition set) X0 ⊂ Rnx and a parametric signal temporal logic formula ϕθ

5

with unknown parameters θ ∈ Θ ⊂ Rnθ , where Θ is the set of feasible valuation, ﬁnd a
valuation θ such that

(2)
That is to say, the output signal (cid:126)y of the system S, starting from any initial condition
(cid:126)x0 ∈ X0, satisﬁes ϕθ at time 0. We write S((cid:126)x0) |= ϕθ as shorthand for S((cid:126)x0)[0] |= ϕθ .

∀(cid:126)x0 ∈ X0 : S((cid:126)x0)[0] |= ϕθ .

3.3 Requirement Mining as Optimization
Problem 1 is subject to the curse of dimensionality. If we solve Problem 1 directly,
the dimension of the search space is nx + nθ . In this paper, we use the following three
methods to mitigate the computational complexity.

• To reduce the search space’s dimension, we follow the idea described in [18].
We divide Problem 1 into two sub-problems: a parameter synthesis problem
(Problem 2) having a dimension of nθ and a falsiﬁcation problem (Problem 3)
having a dimension of nx.

• We then develop an active learning algorithm to progressively reduce the volume
of the search space. The active learning algorithm focuses only on “informative”
initial states, meaning it does not need to search the entire initial condition set
X0.

• Finally, we take advantage of the monotonicity property of PSTL [18, 20, 21]. If
a parameter has been found that solves Problem 2, then there is no need to check
for parameters that make the requirement more restrictive, as these parameters
surely do not constitute an optimal solution.

The ﬂowchart to solve Problem 1 is shown in Fig. 1.

Figure 1: Flowchart for solving Problem 1. Our algorithm is based on the Breach
toolbox [10, 18]. Our new active learning algorithm is used in the falsiﬁcation step
(shown in blue).

6

Problem 2. Parameter Synthesis Given a system S with a set of inputs ¯X0 = {(cid:126)xi
0 ∈
X0,i = 1,2,··· ,ns}, where ns is the number of counter-example traces obtained from
Problem 3, and a PSTL formula ϕθ with unknown parameters θ ∈ Θ ⊂ Rnθ , ﬁnd a
valuation θ to solve

(3)

max

θ

(0,ε − min
(cid:126)x0∈ ¯X0

(r(S((cid:126)x0),ϕθ ))),

where ε > 0 us a user-speciﬁed bound.
Problem 3. Falsiﬁcation Given a system S with an initial condition set (or uncon-
trolled environmental condition set) X0 ⊂ Rnx and a PSTL formula ϕθ with particular
parameters θ ∈ Θ, ﬁnd an initial condition (cid:126)x0 ∈ X0 to solve

min
(cid:126)x0∈X0

(r(S((cid:126)x0),ϕθ )).

(4)
The max function max(0,ε −·) in Eqn. (3) is a modiﬁed hinge loss function. As
the minimum of the robustness, min(r(S((cid:126)x0),ϕθ )), is positive, the loss function rewards
values that are close to the bound 0 and at the same time positive. It is utilized here to
tackle the issue related to the non-uniqueness of solutions to the requirement mining
problem, as pointed out in [18]. For a particular θ, the min functions in Eqn. (3) and
Eqn. (4) reward initial states that lead to negative robustness degrees. Their goals are
to ﬁnd an initial state which leads to a trace that does not meet the requirement ϕθ . The
max-min function, Eqn. (3), then ﬁnds a parameter θ, and, in turn, a requirement ϕθ
such that for any initial state x0 ∈ X0, the output of the system has a positive robustness
degree that is smaller than ε. This means that the system satisﬁes the requirement ϕθ ,
but only barely. Formally,
Lemma 1. A formula obtained by solving Problem 2 and Problem 3 together is a
solution to Problem 1. I.e., if a formula ϕθ∗, where

θ∗ = argmax

(0,ε − min
(cid:126)x0

θ

(r(S((cid:126)x0),ϕθ )))

and furthermore ∀(cid:126)x0 ∈ X0,r(S((cid:126)x0),ϕθ∗) > 0, the statement (2) hold.
Proof. Assume there is a parameter θ(cid:48) obtained by solving Problem 2 and Problem
(cid:48)
(cid:48)
0 ∈ X0 that meets the
0)[0] (cid:50) ϕθ(cid:48) . According to Problem 3, a (cid:126)x
3, and (cid:126)x
requirements cannot be found. Thus, Lemma 1 has been proven.

(cid:48)
0 ∈ X0 : S((cid:126)x

The most time-consuming part of solving Problem 1 is the generation of a trace
given an initial state (cid:126)x0. For the parameter synthesis problem, there is no need to gen-
erate traces. Moreover, according to [18], since PSTL satisﬁes the important property
of monotonicity, the solution is quite efﬁcient, via a binary search algorithm [12], for
example. The falsiﬁcation problem, on the other hand, does need a large number of
traces being generated. Therefore, ﬁnding a way to minimize the number of traces
needed in the falsiﬁcation step will beneﬁt the requirement mining process greatly.
In this paper, we solve the falsiﬁcation problem with a new active learning algorithm
called Gaussian Process Adaptive Conﬁdence Bound (GP-ACB). We will show theo-
retically (in Section 4) and empirically (in Section 5) the effectiveness of our algorithm
in reducing the computational time.

7

4 Active Learning Algorithm

In the section, we propose an active learning algorithm, called Gaussian Process Adap-
tive Conﬁdence Bound (GP-ACB) algorithm, to solve the falsiﬁcation problem men-
tioned in Section 3. It is inspired by Gaussian process upper conﬁdence bound (GP-
UCB) [30]. We analyze the regret bound of GP-ACB and show that it can achieve the
same regret bound with a higher probability than GP-UCB.

4.1 Gaussian Process Adaptive Conﬁdence Bound Active Learning

Algorithm

Active learning algorithms are originally developed to solve classiﬁcation problems
when an oracle is needed to provide labels [28]. The process of obtaining labels from
the oracle can be expensive in terms of both time and money. Thus, the goal of any
active learning algorithm is to achieve high classiﬁcation or regression accuracy by
using the fewest labeled instances. The requirement mining problem suffers from a
similar issue as elaborated at the end of Section 3. Our oracle is a simulator, e.g., a
Stateﬂow/Simulink model. Given the complexity of many CPS models, to obtain a
trace from the simulator can be costly in time. Thus, we need to decrease the number
of simulations needed to learn a formula.

One active learning algorithm is called Gaussian process upper conﬁdence bound

(GP-UCB) [30]. At each step t, it solves the following problem

(cid:126)xt = argmax

(cid:126)x∈D

mt−1((cid:126)x) + β

1
t σt−1((cid:126)x),
2

(5)

where D is the search space (D = X0 for the requirement mining problem), βt is a
function of t and independent of (cid:126)x (an example of βt will be given later), mt−1(.) and
σt−1(.) are the mean and covariance function of the Gaussian process, respectively,
and (cid:126)xt is the instance that will be inquired at step t, meaning the label of (cid:126)xt will be
obtained from the oracle. The GP-UCB algorithm balances the classical exploitation-
exploration trade-off by combining two strategies: the ﬁrst term tends to pick those
points that are expected to achieve high rewards (exploitation); and the second terms
tend to pick those points that are uncertain (exploration).

The second term of Eqn. (5) only depends on the covariance function σ ((cid:126)x), which
can potentially make the exploration somewhat random and inefﬁcient. To address
this problem, we propose an algorithm called Gaussian Process Adaptive Conﬁdence
Bound (GP-ACB) by adding a normalization term ηm(x) to Eqn. (5) as follows:

(cid:126)xt = argmax

(cid:126)x∈D

mt−1((cid:126)x) + ηm((cid:126)x)

1
2 β

1
t σt−1((cid:126)x),
2

(6)

where ηm((cid:126)x) normalizes the mean mt−1((cid:126)x) and can be written explicitly as

ηm((cid:126)x) =

mt−1((cid:126)x)− min(mt−1((cid:126)x))

max(mt−1((cid:126)x))− min(mt−1((cid:126)x))

.

8

It is quite obvious that 0 ≤ ηm((cid:126)x) ≤ 1. ηm((cid:126)x) serves two main purpose here. First, it
acts as an adaptive factor to uncertainty and favors exploration directions associated
with increasing rewards. Thus, it can improve the exploration efﬁciency. Second, it
provides an adaptive upper quantile of the marginal posterior P( f ((cid:126)x) |(cid:126)yt−1), where(cid:126)yt−1
is a vector containing all the observations until time t −1. In this paper, we assume that
the observation at time t, yt ∈ R, is yt = f ((cid:126)x) + εt with εt ∼ N (0,σ 2) and σ 2 known.
Pseudocode for the GP-ACB algorithm is provided in Algorithm 1.

Algorithm 1: GP-ACB Algorithm

Input:
Search space D (X0 for the requirement mining problem);
GP priors m((cid:126)x)0 = 0 and σ0;
Kernel function k;
Maximal simulation time T
1: for i = 1 to T do
2:
3:

Perform Bayesian update to obtain mt−1((cid:126)x) and σt−1((cid:126)x);
Calculate the normalization factor ηm((cid:126)x);
1
Choose (cid:126)xt = argmax(cid:126)x∈D mt−1((cid:126)x) + ηm((cid:126)x)
2 β
Calculate (cid:126)yt = f ((cid:126)xt ) + εt with εt ∼ N (0,σ 2).

1
t σt−1((cid:126)x);
2

4:
5:

4.2 Regret Bound of GP-ACB
The goal of any learning algorithm can be stated as follows: given an unknown reward
function f ∈ F (D,R), maximize the sum of rewards ∑T
t=1 f ((cid:126)xt ), which is equivalent
to ﬁnding a (cid:126)x∗ such that (cid:126)x∗ = argmax(cid:126)x∈D f ((cid:126)x). A concept called regret bound can be
used to quantify the convergence rate of a learning algorithm [7, 28, 29]. First, the
instantaneous regret at time t is deﬁned as rt = f ((cid:126)x∗)− f ((cid:126)xt ). Then, the cumulative
regret RT after T rounds is the sum of instantaneous regrets RT = ∑T
t=1 rt. A desired
property of the learning algorithm is then to guarantee limT→∞ RT /T = 0, implying
the convergence to the global maximum (cid:126)x∗. Finally, the bounds on the average regret
RT /T are directly related to the convergence rate of the learning algorithm. The lower
the bound is, the faster the algorithm converges. This section investigates the regret
bound of the GP-ACB algorithm.
cases when the search space is ﬁnite, i.e., |D| < ∞.
Lemma 2. Pick δ ∈ (0,1) and set βt = 2log(|D|πt /δ ), where ∑t≤1 π−1
Then,
t σt−1((cid:126)x),∀(cid:126)x ∈ D and ∀t ≥ 1

Our proofs on the regret bound follow those in [29]. Here we only consider the

| f ((cid:126)x)− mt−1((cid:126)x)| ≤ ηm((cid:126)x)1/2β 1/2

t = 1, πt > 0.

holds with probability ≥ 1− δ ηm((cid:126)x).
Proof. For (cid:126)x ∈ D and t ≥ 1.
{(cid:126)x1,··· ,(cid:126)xt−1} are deterministic. Further, f ((cid:126)x) (cid:118) N (mt−1((cid:126)x),σ 2

It is known that conditioned on (cid:126)yt−1 = (y1,··· ,yt−1),
t−1((cid:126)x)). Now if r (cid:118)

9

N(0,1), then

Pr{r > c} = e−c2/2(2π)−1/2(cid:82) e−(l−c)2/2−c(l−c)dr

≤ e−c2/2Pr{r > 0} = (1/2)e−c2/2.

for c > 0, as e−c(r−c) ≤ 1 for r ≥ c. We have Pr{| f ((cid:126)x)−mt−1((cid:126)x)| > ηm((cid:126)x)β 1/2
e−ηm((cid:126)x)βt /2. Set r = ( f ((cid:126)x)− mt−1((cid:126)x))/σt−1((cid:126)x) and c = ηm((cid:126)x)1/2β 1/2
the adaptive bound, we have

t σt−1((cid:126)x)}≤
. After applying

t

| f ((cid:126)x)− mt−1((cid:126)x)| ≤ ηm((cid:126)x)1/2β 1/2

t σt−1((cid:126)x) ∀(cid:126)x ∈ D

holds with probability ≥ 1−|D|e−ηm((cid:126)x)βt /2. Choosing |D|e−ηm((cid:126)x)βt /2 = δ /πt, e.g., with
πt = π2t2/6, and using the adaptive bound for t ∈ N, the statement holds.
Lemma 3. Fix t ≥ 1, if | f ((cid:126)x) − mt−1((cid:126)x)| ≤ ηm((cid:126)x)1/2β 1/2
regret rt is bounded by 2β 1/2
Proof. According to the deﬁnition of (cid:126)x∗, mt−1((cid:126)xt ) + ηm((cid:126)xt )1/2β 1/2
≥ mt−1((cid:126)x∗) + ηm((cid:126)x∗)1/2β 1/2

t σt−1((cid:126)x∗) ≥ f ((cid:126)x∗). Therefore, the instantaneous regret

t σt−1((cid:126)x), ∀(cid:126)x ∈ D, then the

t σt−1((cid:126)xt ).

t σt−1((cid:126)xt )

rt = f ((cid:126)x∗)− f ((cid:126)xt )
≤ ηm((cid:126)xt )1/2β 1/2
≤ 2ηm((cid:126)xt )1/2β 1/2

t σt−1((cid:126)xt ) + mt−1((cid:126)xt )− f ((cid:126)xt )
t σt−1((cid:126)xt ) ≤ 2β 1/2
t σt−1((cid:126)xt )

Lemma 4. Pick δ ∈ (0,1) and set βt = 2log(πt /δ ), where ∑t≥1 π−1
t σt−1((cid:126)x) ∀t ≥ 1

| f ((cid:126)x)− mt−1((cid:126)x)| ≤ ηm((cid:126)x)1/2β 1/2

t = 1,πt > 0. Then,

holds with probability ≤ 1− δ ηm((cid:126)xt ).
Proof. For (cid:126)x ∈ D and t ≥ 1. Conditioned on yt−1 = {y1,··· ,yt−1},{(cid:126)x1,··· ,(cid:126)xt−1}
t−1((cid:126)x)). According to Lemma 2,
are deterministic. Further,
Pr{| f ((cid:126)xt )− mt−1((cid:126)xt )| > ηm((cid:126)xt )1/2β 1/2
t σt−1((cid:126)xt )} ≤ e−ηm((cid:126)xt )βt /2. Since e−βt /2 = δ /πt,
and with the adaptive bound for t ∈ N, the statement holds.
Lemma 5. Set Lt = max(mt ((cid:126)x)) − min(mt ((cid:126)x)), ∀(cid:126)x ∈ D, and let βt be deﬁned as in
Lemma 4, then

f ((cid:126)xt ) ∼ N (mt−1((cid:126)x),σ 2

1− ηm((cid:126)xt )1/2 ≤ β 1/2

t σt−1((cid:126)xt )/Lt ∀t ≥ 1

Proof. Set mt−1((cid:126)xm) = max(mt−1((cid:126)x)), ∀(cid:126)x∈ D, according to GP-ACB, ηm((cid:126)xt )1/2β 1/2
mt−1((cid:126)xt ) ≥ ηm((cid:126)xm)1/2β 1/2

t σt−1((cid:126)xm) + mt−1((cid:126)xm), then

t σt−1((cid:126)xt )+

mt−1((cid:126)xm)− mt−1((cid:126)xt−1) ≤ ηm((cid:126)xt )1/2β 1/2
⇒ 1− ηm((cid:126)xt )1/2 ≤ β 1/2
t
Lt
(ηm((cid:126)xt )1/2σt−1((cid:126)xt ) ≤ β 1/2

t σt−1((cid:126)xt )− ηm((cid:126)xm)1/2β 1/2
(ηm((cid:126)xt )1/2σt−1((cid:126)xt )− ηm((cid:126)xm)1/2σt−1((cid:126)xm))

t σt−1((cid:126)xt )/Lt

≤ β 1/2
t
Lt

t σt−1((cid:126)xm)

10

Remark 1. Lemma 5 shows that when the scale of the function Lt is large, ηm((cid:126)x)
will be close to 1, meaning the GP-ACB algorithm degrades to GP-UCB. Conversely,
when the scale of function is large, mt ((cid:126)x) will play a more important role in the search
process, driving the algorithm to be very greedy. To investigate the effects of the scaling
function on the optimization performance, a numerical experiment has been conducted
and the results are shown in Section 5.1.

Deﬁne γT as the maximum information gain after T rounds as follows [29]:

γT = max
T(cid:48)≤T

T(cid:48)
∑

t=1

1
2

log(1 + σ−2σ 2

t−1((cid:126)xt ))

According to [29], if the search space D ∈ Rd is compact and convex, where d is the
dimension of the search space, with the assumption that the kernel function satisﬁes
k((cid:126)x,(cid:126)x

(cid:48)
) (cid:54) 1, we have
• γT = O((logT )d+1) for Gaussian kernel;

T d(d+1)/(2ν+d(d+1))(logT )

for Mat´ern kernels with ν > 1.

• γT = O(cid:16)

(cid:17)

We can ﬁnally obtain the following bound for the GP-ACB algorithm.
Theorem 1. Let δ ∈ (0,1), βt = 2log(|D|t2π2/6δ ) , m = mint=(1,··· ,T )(ηm((cid:126)xt )) and
n = maxt=(1,··· ,T )(ηm((cid:126)xt )). Running GP-ACB results in a regret bound as follows

Pr{RT ≤(cid:112)nC1TβT γT ,∀T ≥ 1} ≥ 1− δ m,

(7)

where C1 = 8/log(1 + σ−2).
Proof. Deﬁne the information gain I as follows:

I(yT ,fT ) =

1
2

T

∑

t=1

log(1 + σ−2σ 2

t−1((cid:126)xt )),

where f T = ( f ((cid:126)x1), . . . , f ((cid:126)xT ))(cid:48) ∈ RT . According to Lemma 2 and Lemma 4, the regret
bound {r2
t−1((cid:126)xt ), ∀t ≥ 1} holds with probability ≥ 1−δ ηm((cid:126)xt ) ≥ 1−δ m.
As βt is non-decreasing, we have

t ≤ 4ηm((cid:126)xt )βtσ 2

4ηm((cid:126)x)βtσ 2

t−1((cid:126)xt ) ≤ 4nβT σ 2(σ−2σ 2

where S = σ−2/log(1 + σ−2), since σ−2σ 2
C1 = 8/log(1 + σ−2) ≥ 8σ 2 and h2 ≤ Slog(1 + h2) f or
for T ≥ 1 we have
t ≤ ∑T
≤ nC1βT γT .

t−1((cid:126)xt ) ≤ n∑T

t=1 4ηm((cid:126)x)βtσ 2

∑T
t=1 r2

t=1

t−1((cid:126)xt ) ≤ 4nβT σ 2Slog(1 + σ−2σ 2
t−1((cid:126)xt ) ≤ σ−2k((cid:126)xt ,(cid:126)xt ) ≤ σ−2,

t−1((cid:126)xt ))

(8)

h ∈ [0,σ−2]. As C1 = 8σ 2S,

1

2βTC1 log(1 + σ−2σ 2

t−1((cid:126)xt ))

According to Cauchy-Schwarz inequality, R2

T ≤ T ∑T

t=1 r2

t . Theorem 1 has been proven.

11

Remark 2. The regret bound of the GP-UCB algorithm is [29]

Pr{RT ≤(cid:112)C1TβT γT ,∀T ≥ 1} ≥ 1− δ .

With the same parameter setting, the regret bound of our GP-ACB algorithm is shown
as Eqn. (7). Based on the bound of η((cid:126)x) shown in Lemma 5 and the deﬁnition of
the information gain γT , we know that σt−1((cid:126)xt ) is close to 0 when T is large, which
means m and n in Theorem 1 are close to one when iteration T is large and are close
to zero when iteration T is small. This indicates that GP-ACB algorithm is greedy at
the beginning and degrades to GP-UCB algorithm when iteration T is large. Since
0 < m,n (cid:54) 1, we can get the conclusion that the GP-ACB algorithm can get the same
regret bound more efﬁcient than that of the GP-UCB algorithm. The regret bound can
be easily translated into convergence rate. The maximum maxt≤T f ((cid:126)xt ) in the ﬁrst T
iterations is no further from f ((cid:126)x∗), where (cid:126)x∗ is the global optimum, than the average
regret RT /T . Thus, compared with GP-UCB, on average, the GP-ACB algorithm has
a higher or equal convergence rate.

5 Case Studies

To demonstrate the capability of GP-ACB, in this section, we ﬁrst use it to solve a
global optimization problem with Ackley’s function as the target function. The per-
formance of GP-ACP is compared with those of other active learning algorithms. We
then use GP-ACB to solve a full scale requirement mining problem of an automatic
transmission system, a benchmark used widely in CPS community [17,18,27]. In both
cases, GP-ACB outperforms other active learning algorithms as well as some other
state-of-art optimization algorithms such as Nelder-Mead.

5.1 Global Optimization of Ackley’s Function
To verify the performance of the proposed GP-ACB algorithm, we compare the GP-
ACB algorithm with four types of Gaussian-Process-based strategy: (i) GP-UCB active
learning, (ii) Batch-greedy UCB active learning [9], (iii) pure exploration, i.e., choos-
ing points of maximum variance at each step, and (iv) pure exploitation or greedy, i.e.,
choosing points of maximum mean at each step. We use Ackley’s function (shown in
Fig. 2.(a)) as the target function. Its formula is as follows,

√
f (x,y) = −20e−0.2
0.5(x2+y2) − e0.5(cos(2πx)+cos(2πy)) + e + 20,

where e is the observation noise with zero mean and variance σ 2 at 0.025. The search
space D = [−5,5]2 is randomly discretized into 1000 points. We run each algorithm
for T = 58 iterations with sampling time δ = 0.1. Since the global minimum of the
Ackley’s function (x∗,y∗) is known (unknown to the learning algorithms though), for
the i-th trial, if (xi
t ) is the solution obtained by running the algorithm for t iterations,
t )− f (x∗,y∗)]/Nt, where
then mean regret for the algorithm at time t is ¯Rt = ∑Nt
Nt is the number of trials. In this case study, we set Nt = 1000. Each trial is initialized
randomly.

i=0[ f (xi

t ,yi

t ,yi

12

(a)

(b)

(c)

Figure 2: (a) Ackley’s function. (b) and (c) compares the performance of GP-ACB with
t )−
those of other strategies. The mean regret over 1000 trails ¯R(t) := ¯Rt = ∑1000
f (x∗,y∗)]/1000 is chosen as the performance metric. (b) shows the comparison results
with Gaussian kernel. (c) shows the comparison results with Mat´ern kernel.

i=0 [ f (xi

t ,yi

Fig. 2(b) and Fig. 2(c) show the comparison of the mean regret ¯Rt incurred by the
different Gaussian Process based algorithms with Gaussian kernel and Mat´ern kernel,
respectively. With both kernels, GP-ACB outperforms the others. For instance, GP-
UCB arrives at its minimum regret in an average of 58 iterations; while GP-ACB arrives
at its minimum regret in an average of 45 iterations. Another interesting observation is
that pure exploitation (greedy) strategy on average converges quicker than others. But
the regret it converges to is on average higher than others, implying the convergence to
local minimums. As for kernels, for this particular case, Mat´ern kernel outperforms the
Gaussian kernel. This is not supervising, given that the Ackley’s function (as shown
in Fig. 2(a)) is quite “non-smooth” and Mat´ern kernel is designed to capture non-
smoothness.

Remark 1 shows that the range of the mean function m((cid:126)x) at time t,

Lt = max(mt ((cid:126)x))− min(mt ((cid:126)x)),

affects the performance of the GP-ACB algorithm. When the range Lt is large, the

13

Iterations01020304050Mean Regret0123456GP-ACBGP-UCBBatch greedy UCBPure explorationPure exploitationIterations01020304050Mean Regret0123456GP-ACBGP-UCBBatch greedy UCBPure explorationPure exploitation(a)

(b)

t ,yi

Figure 3:
(a) The performance of the GP-ACB’s algorithm with respect to the
t ) −
scaling factor ξ . The mean regret over 1000 trails ¯R(t) := ¯Rt = ∑1000
f (x∗,y∗)]/1000 is chosen as the performance metric.
(b) The effects of the scal-
ing factor ξ on different algorithms. The mean regret at time T over 1000 trails
T )− f (x∗,y∗)]/1000 is chosen as the performance metric. Here T = 58.
∑1000
i=0 [ f (xi

i=0 [ f (xi

T ,yi

GP-ACB algorithm degrades to the GP-UCB algorithm. In this paper, we use a scale
factor ξ to change the range of the reward or cost function.
Instead of optimizing
f ((cid:126)x) directly, we scale it by ξ ﬁrst and then solve the optimization problem with a
new reward or cost function ξ f ((cid:126)x). To demonstrate the effects, Fig. 3(a) shows the
performance of GP-ACB with different scaling factors. It shows that when the scaling
factor ξ is large, the algorithm prefers exploitation over exploration and has the dan-
ger of trapping in a local optimum; when the scaling factor ξ is small, the algorithm
prefers exploration over exploitation and it may take more than necessary time to ex-
plore irrelevant regions. Thus, a proper ξ needs to be selected to balance the trade-off
between exploration and exploitation. Fig. 3(b) shows the effects of the scaling factor
ξ on different algorithms. It shows that when the scaling factor ξ is large, the GP-UCB
and GP-ACB algorithms have the same mean regret after 58 iterations, implying GP-
ACB has degraded to GP-UCB. The pure exploration and pure exploitation algorithms’
performance are not effected by the scaling factor ξ .

5.2 Requirement Mining on an Automatic Transmission Model
The model used in the following study is the same as the one used in [18], a closed-loop
model of a four-speed automatic transmission, shown in Fig 4. The model contains all
necessary mechanical components, including engine, transmission and the longitudinal
chassis dynamics. In the model, the throttle position and brake torque are the input
signals. With the current gear selection, the transmission ratio (Ti) can be computed
through the transmission block, and the output torque can be obtained with the engine
speed (Ne), gear status and transmission RPM.

We tested three different template requirements which are the same with [18], thus
a comparison study can be conducted. The three template requirements are as follows:

14

Iterations01020304050Mean Regret0123456Iterations051015Mean Regret After T Iterations0123456GP-ACBGP-UCBBatch greedy UCBPure explorationPure exploitationFigure 4: Closed-loop Simulink model of an automatic transmission and controller,
whose exogenous inputs are the throttle position and brake torque.

1. Requirement ϕsp rpm(π1,π2), which has a strong correlation with safety require-
ments characterizing the operating region for the engine parameters speed and
RPM, specifying that always the speed is below π1 and RPM is below π2.

G((speed < π1)∧ (RPM < π2)).

2. Requirement ϕrpm100(π,τ), which measures the performance of the closed loop
system. The value for τ speciﬁes how fast the vehicle can reach a certain speed,
and the value for π can specify the lowest RPM needed to reach the above speed.
The formula speciﬁes that the vehicle cannot reach the speed of 100 mph in τ
seconds with RPM always below π:

¬(F[0,τ](speed > 100)∧ G(RPM < π)).

3. Requirement ϕstay(τ), which encodes undesirable transient shifting of gears,
specifying that whenever the system shifts to gear 2, it dwells in gear 2 for at
least τ seconds:

G(cid:0)(cid:0)gear (cid:54)= 2∧ F[0,ε]gear = 2(cid:1) ⇒ G[0,τ]gear = 2(cid:1) .

Our requirement mining algorithm as shown in Fig. 1 and elaborated in Section
3.3 is implemented to mind requirements for the automatic transmission model. Our
algorithm is based on the Breach toolbox [10] with the falsiﬁcation problem solved
by GP-ACB. The Breach toolbox uses the Nelder-Mead algorithm for falsiﬁcation [10,
18]. Section 5.1 shows that the scaling factor affects the performance of the GP-ACB
algorithm. We ﬁrst run a few test trials in order to ﬁnd the optimal scaling factor, the
results of which are shown in Table 1. It can be seen that when scaling factor is set
to 0.5, the time spent on falsiﬁcation, the time spent on parameter synthesis and the
number of simulations are all at their lowest values. Thus, the scaling factors are set to
0.5 for all the following experiments.

15

Table 1: Requirement mining results for GP-ACB with different scaling factors. In this
table and Table 2, “Parameter Values” are the mined parameter values. For instance,
parameter values (4849,155) together with the formula template ϕsp rpm(π1,π2) means
that the mined formula is G((speed < 4849)∧ (RPM < 155)). “Fals.(s)” is the CPU
time spent on solving the falsiﬁcation problem. “Synth.(s)” is the CPU time spent
on solving the parameter synthesis problem.
is the number of simula-
tions.“Robustness” is the robustness degree corresponding to the mined parameters.

“#Sim.”

Scaling Factor
ϕsp rpm(π1,π2)

Requirement Mining Results

Parameter Values

Fals.(s)

Synth.(s)

#Sim. Robustness
393
385
255
276
367

0.1541
0.3510
0.7108
0.8197
1.5098

0.1
0.25
0.5
0.75

1

(4849, 155)
(4846, 155)
(4849, 155)
(4846, 155)
(4847, 155)

112
109
67
79
107

16.1
17.4
15.4
15.6
16.6

The comparison results, averaged over 20 randomly started trails, for GP-ACB with
Gaussian kernel, GP-ACB with Mat´ern kernel, GP-UCB and Nelder-Mead are shwon
in Table 2. The results show GP-ACB with Mat´ern kernel outperforms the others in
terms of the times spent on falsiﬁcation, the times spent on parameter synthesis and the
numbers of simulations. For instance, in mining the formula ϕsp rpm(π1,π2), GP-UCB
saves on average 10% of simulation number compared with Nelder-Mead, while GP-
ACB with Mat´ern kernel saves up to about 40%. The same level of improvement is
observed for the other two requirements, ϕrpm100(π,τ) with a 40% improvement and
ϕstay(π) with a 30% improvement.

Parameter Values

Fals.(s)

Synth.(s)

Requirement Mining Results

(4845, 155)
(4849, 155)
(4844, 155)
(4857, 155)

Table 2: Requirement mining results for GP-ACB with Gaussian kernel, GP-ACB with
Mat´ern kernel, GP-UCB with Mat´ern kernel, and Nelder-Mead algorithm.
Template
ϕsp rpm(π1,π2)
GP-ACB(Gaussian)
GP-ACB(Mat´ern)
GP-UCB(Mat´ern)
Nelder-Mead
ϕrpm100(π,τ)
GP-ACB(Gaussian)
GP-ACB(Mat´ern)
GP-UCB(Mat´ern)
Nelder-Mead
ϕstay(τ)
GP-ACB(Gaussian)
GP-ACB(Mat´ern)
GP-UCB(Mat´ern)
Nelder-Mead

#Sim. Robustness
330
255
334
380
#Sim. Robustness
207
179
190
340
#Sim. Robustness
941
940
1056
1246

(5997, 12.20)
(5997, 12.20)
(5997, 12.20)
(5997, 12.20)

0.1556
0.0802
0.1116
0.06453

Parameter Values

Fals.(s)

Parameter Values

Fals.(s)

0.0586
0.0586
0.0586
0.0586

1.2812
0.7108
0.7425
0.6738

0.05
0.05
0.05
0.1

10.6
10.8
10.6
9.3

100
67
97
112

64
59
63
104

296
294
363
422

15.6
15.4
15.4
13.3

2.0
1.9
2.0
1.4

Synth.(s)

Synth.(s)

16

6 Conclusions and Future Work

In this paper, we introduced an active learning method, called Gaussian Process adap-
tive conﬁdence bound (GP-ACB), for mining requirements of bounded-time temporal
properties of cyber-physical systems. The theoretical analysis of the proposed algo-
rithm showed that it had a lower regret bound thus a higher convergence rate than
other Gaussian-Process-based active learning algorithms, such as GP-UCB. By using
two case studies, one of which was an automatic transmission model, we showed that
our requirement mining algorithm outperformed other existing algorithms, e.g., those
based on GP-UCB or Nelder-Mead, by an average of 30% to 40%. Our results have
signiﬁcant implications for not only the requirement mining but also the validation
and veriﬁcation of cyber-physical systems. We are currently exploring the possibility
of utilizing active learning to solve the structural inference problem, i.e., to mind a
requirement without any given template.

References

[1] H. Abbas and G. Fainekos. Linear hybrid system falsiﬁcation through local
search. In Automated Technology for Veriﬁcation and Analysis, pages 503–510.
Springer, 2011.

[2] H. Abbas, G. Fainekos, S. Sankaranarayanan, F. Ivanˇci´c, and A. Gupta. Proba-
bilistic temporal logic falsiﬁcation of cyber-physical systems. ACM Transactions
on Embedded Computing Systems (TECS), 12(2s):95, 2013.

[3] R. Alur. Principles of cyber-physical systems. MIT Press, 2015.

[4] Y. Annpureddy, C. Liu, G. Fainekos, and S. Sankaranarayanan. S-taliro: A tool

for temporal logic falsiﬁcation for hybrid systems. Springer, 2011.

[5] H. K. Anwar S and S. W. Fixed point optimization of deep convolutional neural

networks for object recognitio. pages 1131–1135. IEEE, 2015.

[6] E. Asarin, A. Donz´e, O. Maler, and D. Nickovic. Parametric identiﬁcation of

temporal properties. In Runtime Veriﬁcation, pages 147–160. Springer, 2012.

[7] P. Auer, T. Jaksch, and R. Ortner. Near-optimal regret bounds for reinforcement
learning. In Advances in neural information processing systems, pages 89–96,
2009.

[8] E. Bartocci, L. Bortolussi, and G. Sanguinetti. Data-driven statistical learning of
temporal logic properties. In Formal Modeling and Analysis of Timed Systems,
pages 23–37. Springer, 2014.

[9] K. A. Desautels T and B. J. W. Parallelizing exploration-exploitation tradeoffs
in gaussian process bandit optimization. The Journal of Machine Learning Re-
search, pages 3873–3923, 2014.

17

[10] A. Donz´e. Breach, a toolbox for veriﬁcation and parameter synthesis of hybrid

systems. pages 167–170. Springer Berlin Heidelberg, 2010.

[11] A. Donz´e and O. Maler. Robust satisfaction of temporal logic over real-valued

signals. Formal Modeling and Analysis of Timed Systems, pages 92–106, 2010.

[12] T. Dreossi, T. Dang, A. Donz´e, J. Kapinski, X. Jin, and J. V. Deshmukh. Efﬁcient
guiding strategies for testing of temporal properties of hybrid systems. In NASA
Formal Methods, pages 127–142. Springer, 2015.

[13] G. E. Fainekos and G. J. Pappas. Robustness of temporal logic speciﬁcations
for continuous-time signals. Theoretical Computer Science, 410(42):4262–4291,
2009.

[14] Gosavi and Abhijit. Simulation-based optimization: An overview. pages 29–35.

Springer US, 2015.

[15] I. Haghighi, A. Jones, Z. Kong, E. Bartocci, R. Gros, and C. Belta. Spatel: a
novel spatial-temporal logic and its applications to networked systems. In Pro-
ceedings of the 18th International Conference on Hybrid Systems: Computation
and Control, pages 189–198. ACM, 2015.

[16] Z. Jiang, M. Pajic, and R. Mangharam. Cyber-physical modeling of implantable

cardiac medical devices. Proceedings of the IEEE, 100(1):122–137, 2012.

[17] X. Jin, J. V. Deshmukh, J. Kapinski, K. Ueda, and K. Butts. Powertrain control
veriﬁcation benchmark. In Proceedings of the 17th international conference on
Hybrid systems: computation and control, pages 253–262. ACM, 2014.

[18] X. Jin, A. Donz´e, J. V. Deshmukh, and S. A. Seshia. Mining requirements from
closed-loop control models. In Proceedings of the 16th international conference
on Hybrid systems: computation and control, pages 43–52. ACM, 2013.

[19] A. Jones, Z. Kong, and C. Belta. Anomaly detection in cyber-physical systems:
A formal methods approach. In Decision and Control (CDC), 2014 IEEE 53rd
Annual Conference on, pages 848–853. IEEE, 2014.

[20] Z. Kong, A. Jones, and C. Belta. Temporal logics for learning and detection of

anomalous behavior. IEEE Transactions on Automatic Control, accepted.

[21] Z. Kong, A. Jones, A. Medina Ayala, E. Aydin Gol, and C. Belta. Temporal
logic inference for classiﬁcation and prediction from data. In Proceedings of the
17th international conference on Hybrid systems: computation and control, pages
273–282. ACM, 2014.

[22] E. A. Lee and S. A. Seshia. Introduction to embedded systems: A cyber-physical

systems approach. Lee & Seshia, 2011.

[23] O. Maler and D. Nickovic. Monitoring temporal properties of continuous signals.
Formal Techniques, Modelling and Analysis of Timed and Fault-Tolerant Systems,
pages 71–76, 2004.

18

[24] R. R. Rajkumar, I. Lee, L. Sha, and J. Stankovic. Cyber-physical systems: the next
computing revolution. In Proceedings of the 47th Design Automation Conference,
pages 731–736. ACM, 2010.

[25] A. Ramdas and A. Singh. Algorithmic connections between active learning and
stochastic convex optimization. In Algorithmic Learning Theory, pages 339–353.
Springer, 2013.

[26] Rasmussen and C. Edward. Gaussian processes for machine learning. The MIT

Press, 2006.

[27] S. Sankaranarayanan and G. Fainekos. Falsiﬁcation of temporal properties of
hybrid systems using the cross-entropy method. In Proceedings of the 15th ACM
international conference on Hybrid Systems: Computation and Control, pages
125–134. ACM, 2012.

[28] B. Settles. Active learning literature survey. University of Wisconsin, Madison,

52(11):55–56, 2010.

[29] N. Srinivas, A. Krause, and S. Kakade. Gaussian process optimization in the
bandit setting: No regret and experimental design. arXiv preprint arXiv, pages
0912–3995, 2009.

[30] N. Srinivas, A. Krause, S. M. Kakade, and M. W. Seeger. Information-theoretic
regret bounds for gaussian process optimization in the bandit setting. Information
Theory, IEEE Transactions on, 58(5):3250–3265, 2012.

[31] P. Zuliani, A. Platzer, and E. M. Clarke. Bayesian statistical model checking with
application to simulink/stateﬂow veriﬁcation. In Proceedings of the 13th ACM in-
ternational conference on Hybrid systems: computation and control, pages 243–
252. ACM, 2010.

19

