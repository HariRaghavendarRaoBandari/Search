6
1
0
2

 
r
a

M
4

 

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
9
4
6
1
0

.

3
0
6
1
:
v
i
X
r
a

Functional linear instrumental regression under

second order stationarity.

Jan Johannes∗

Ruprecht-Karls-Universit¨at Heidelberg

Abstract

We consider the problem of estimating the slope parameter in functional linear in-
strumental regression, where in the presence of an instrument W , i.e., an exogenous ran-
dom function, a scalar response Y is modeled in dependence of an endogenous random
function X. Assuming second order stationarity jointly for X and W a nonparametric
estimator of the functional slope parameter and its derivatives is proposed based on
an n-sample of (Y, X, W ). In this paper the minimax optimal rate of convergence of
the estimator is derived assuming that the slope parameter belongs to the well-known
Sobolev space of periodic functions. We discuss the cases that the cross-covariance
operator associated to the random functions X and W is ﬁnitely, inﬁnitely or in some
general form smoothing.

Keywords: Functional linear model, Instrument, Orthogonal series estimation,

Spectral cut-oﬀ, Optimal rate of convergence, Sobolev space.

JEL classiﬁcations: Primary C14; secondary C30.

The author gratefully acknowledges support from the research program ”New Challenges
for New Data” of LCL and Genes.

1 Introduction

The analysis of functional data is becoming very important in a diverse range of disciplines,
including medicine, linguistics, chemometrics as well as econometrics (see for instance Ram-
say and Silverman [2005] and Ferraty and Vieu [2006], for several case studies). In particu-
lar, there is a wide diversity of applications in economics. Forni and Reichlin [1998] study
business cycle dynamics and Preda and Saporta [2005] consider shares at the Paris stock
exchange, to name but a few. Roughly speaking, in all these applications the dependence
of a response variable Y on the variation of an explanatory random function X is modeled
by a functional linear regression model, that is,

(cid:90) 1

Y =

β(t)X(t)dt + σU,

σ > 0,

(1.1)

0

∗Institut f¨ur Angewandte Mathematik, Im Neuenheimer Feld, 294, D-69120 Heidelberg, Germany, e-mail:

johannes@math.uni-heidelberg.de

1

for some error term U . The important point to note here is that often in economical appli-
cations the commonly used hypothesis, that the regressor X is exogenous, can be rejected
using, for example, a test proposed by Blundell and Horowitz [2007]. Thus analyzing the
inﬂuence of endogeneity is of particular interest in econometrics. One objective is then to
estimate nonparametrically in the presence of an instrument W the slope function β or its
derivatives based on an n-sample of (Y, X, W ).

Background. Suppose ﬁrst the regressor X is exogenous, i.e., E[U X(s)] = 0, s ∈ [0, 1].
In this case the estimation of the slope function β has been considered, for example, in
Cardot et al. [2003], M¨uller and Stadtm¨uller [2005], Hall and Horowitz [2007] or Crambes
et al. [2009]. Assuming the random function X to be centered the most popular approach
is to multiply both sides in (1.1) by X(s). Then taking the expectation leads to

(cid:90) 1

0

E[Y X(s)] =

β(t) Cov(X(t), X(s))dt,

s ∈ [0, 1].

(1.2)

to be square integrable, then their generalized Fourier coeﬃcients Xi :=(cid:82) 1
βi :=(cid:82) 1

The normal equation (1.2) is the continuous equivalent of a normal equation in a classical
linear model. To be more precise, suppose the random function X and the slope function β
0 X(s)ψ(s)ds and
0 β(s)ψ(s)ds, i ∈ N, with respect to some orthonormal basis {ψi} are well-deﬁned.
∞(cid:88)

The functional linear model (1.1) (FLM for short) and hence the normal equation (1.2) can
be rewritten as

∞(cid:88)

Y =

βiXi + σU,

and E[Y Xj] =

βi · Cov(Xi, Xj),

j = 1, 2, . . . ,

(1.3)

i=1

i=1

(cid:80)k
respectively. Therefore, the FLM (1.1) extends the linear model (LM for short) Y =
i=1 βiXi + σU , k ∈ N, to an inﬁnite number of regressors. Since in analogy to the
estimation in the LM recovering from (1.3) the coeﬃcients (βi)i∈N necessitates the inver-
sion of the inﬁnite dimensional covariance matrix Σ∞ := (Cov(Xi, Xj))i,j∈N, the estimation
of β is called an inverse problem. It is well-known that in both, the linear and the functional
linear model identiﬁcation as well as the accuracy of any estimator depends strongly on the
i,j=1 and Σ∞ respectively. That
properties of the covariance matrix Σk := (Cov(Xi, Xj))k
is, in both cases the coeﬃcients can be identiﬁed as long as the covariance matrix Σk and
Σ∞ respectively, is not singular. Moreover, in the LM a high degree of multicolinearity
between the regressors X1, . . . , Xk, that is, the smallest eigenvalue of Σk is close to zero,
produces unacceptable uncertainty in the coeﬃcient estimates. However, as long as the co-
variance matrix Σk is not singular an ordinary least squares estimator (LSE for short) will
be consistent and leads under fairly weak assumptions to a minimal asymptotic variance.
In general the situation in the FLM is diﬀerent. Since under very mild assumptions zero
is an accumulation point of the eigenvalues of Σ∞ we always have to face a classical multi-
colinearity problem in the presence of many regressors. Therefore, although the covariance
matrix Σ∞ is not singular the LSE will not longer be consistent. This corresponds to the
setup of ill-posed inverse problems.

In the literature several approaches are proposed in order to circumvent in the FLM
the instability issue. Essentially, all of them replace the covariance matrix Σ∞ in equation

2

j=1{E[Y Xj]−(cid:80)∞

i=1 βi·Cov(Xi, Xj)}2 +α(cid:80)∞

Tikhonov functional Fα(β) =(cid:80)∞

(1.3) by a regularized version. A popular example is based on the functional principal
components regression (c.f. Bosq [2000], M¨uller and Stadtm¨uller [2005] or Cardot et al.
[2007]), which corresponds to a method called spectral cut-oﬀ in the literature of numerical
analysis (c.f. Tautenhahn [1996]). Another example is the Tikhonov regularization (c.f. Hall
and Horowitz [2007]), where the regularized solution βα is deﬁned as unique minimizer of the
j for some
strictly positive α. Regularization using a penalized least squares approach after projection
onto some basis (such as splines) is also considered in Ramsay and Dalzell [1991], Eilers and
Marx [1996] or Cardot et al. [2003]. The common aspect of all these regularization schemes
is the introduction of an additional regularization parameter α (for example, the parameter
determining the weight of the penalty in the Tikhonov functional). The risk of the resulting
regularized estimator can then be decomposed, roughly speaking, into a function of the risk
of the estimators of E[Y Xj] and Cov(Xi, Xj), i, j ∈ N, plus an additional bias term which
is a function of the regularization parameter α. The optimal value of α is then obtained
by balancing these two terms. However, in order to obtain a rate of convergence additional
regularity assumptions on the slope function β and the inﬁnite dimensional covariance
matrix Σ∞ := (Cov(Xi, Xj))i,j∈N are necessary (a detailed discussion in the context of
inverse problems in econometrics can be found in Carrasco et al. [2006] or Johannes et al.
[2011]).

j=1 β2

The objective of this paper is to study the estimation of the slope function β when the
regressor X is endogenous, which to the best of our knowledge has not yet been considered
in the literature. In the following the approach of this paper is described in more details.

and square integrable, we consider its generalized Fourier coeﬃcients Wi :=(cid:82) W (s)ψ(cid:48)

Methodology. To treat the endogeneity problem, we assume that an instrument W , i.e.,
an exogenous random function, is given. Assuming the random function W to be centered
i(s)ds,
i} not necessarily the same as {ψi} used
i ∈ N, with respect to some orthonormal basis {ψ(cid:48)
above in the decomposition of X and β. Then multiplying the equation (1.1) by Wj and
taking the expectation leads to the normal equation

E[Y Wj] =

βi · Cov(Xi, Wj),

j = 1, 2, . . . .,

(1.4)

∞(cid:88)

i=1

Y =(cid:80)k

which provides a natural extension of the linear instrumental regression (LIR for short)
i=1 βiXi + σU with E[WjU ] = 0, j = 1, . . . , q, to an inﬁnite number of regressors
and instruments. Therefore, in the presence of an instrument W we call (1.1) functional
linear instrumental regression (FLIR for short). The estimation of the coeﬃcients in both,
linear and functional linear instrumental regression is then again an inverse problem, since
it involves now the inversion of the cross-covariance matrix Σkq := (Cov(Xi, Wj))k,q
i,j=1 and
Σ∞∞ := (Cov(Xi, Wj))i,j∈N respectively. Furthermore, in both cases the coeﬃcients are
identiﬁable as long as Σkq and Σ∞∞ respectively, is not singular and moreover, the obtain-
able accuracy of any estimator depends now on the properties Σkq and Σ∞∞ respectively.
It is worth to pointing out that the FLIR parallels developments in econometric theory
such as nonparametric instrumental regression (c.f. Darolles et al. [2011], Newey and Pow-
ell [2003], Hall and Horowitz [2005] or Florens et al. [2011]), nonparametric instrumental

3

quantile regression of Horowitz and Lee [2007] or semi-nonparametric estimation of Engel
curve with shape-invariant speciﬁcation of Blundell et al. [2007].

The estimator of the slope function in FLIR considered in this paper is based on a two
stage least squares approach. To be more precise, consider ﬁrst the LIR. Then as long as
the cross-covariance matrix Σkq is not singular a two stage least squares procedure (2SLS
for short) will lead to a consistent estimator. That is, in a ﬁrst step a linear regression of the
endogenous vector X = (X1, . . . , Xk)t onto the vector of instruments W = (W1, . . . , Wq)t

is performed, resulting into an estimator (cid:99)W of the optimal linear instrument (cid:102)W , i.e., the
best linear predictor (cid:102)W := ΣkqΣ−1
obtain considering a linear regression of Y onto (cid:99)W . Applying a 2SLS approach in FLIR

i,j=1. Note that
the optimal linear instrument is well-deﬁned as long as the covariance matrix Σq of W
has full rank. Then in the second step an estimator of the k-vector of coeﬃcients (βj) is

q W of X with Σq := (Cov(Wi, Wj))q

we have to face additional technical diﬃculties given through the facts that the optimal
linear instrument, i.e., the best linear predictor of the random function X given the random
function W , is not always well-deﬁned and that both stages of the estimation procedure
necessitate the solution of an ill-posed inverse problem (see the discussion above in case of
an exogenous regressor). Therefore, assuming the optimal linear instrument is well-deﬁned,
we apply in each stage a regularization scheme in order to circumvent the instability issue.
Although the estimation in the ﬁrst step has to be stabilized, it has only a minor inﬂuence

on the obtainable accuracy of the ﬁnal estimator. Particularly, the proposed estimator of(cid:102)W
in the second stage. To be more precise, if the optimal linear instrument (cid:102)W is given,
exogenous regressor(cid:102)W . Thereby, the relationship between the regularity assumption on the
slope function β and the inﬁnite dimensional covariance matrix (cid:101)Σ∞ := (Cov((cid:102)Wi,(cid:102)Wj))i,j∈N
associated to the instrument (cid:102)W determines the obtainable accuracy of any estimator of
instrument (cid:102)W is not given and thus has to be estimated. However, the estimation in the

β (see also the discussion above in case of an exogenous regressor). Nevertheless, the

will in general not be optimal. The main complexity of the estimation problem is contained

then the second step in fact only consists of the estimation in a FLM (1.1) given now with

ﬁrst step is possible without changing the optimal rate of the estimator of β, where only
higher moment conditions are the price to pay.

rate of convergence of any estimator. We now describe two examples. First consider the

Suppose the slope function β belongs to the Sobolev space of periodic functions Wp
(deﬁned below). Given an n-sample of (Y, X, W ) our objective is not only the estimation
of the slope function β itself but also of its derivatives. We show that the relationship

between the Sobolev spaces and the covariance matrix (cid:101)Σ∞ associated to the optimal linear
instrument(cid:102)W , i.e., the “smoothing” property of (cid:101)Σ∞, is essentially determining the optimal
covariance matrix(cid:101)Σ∞ to be ﬁnitely smoothing, that is, the range of(cid:101)Σ∞ equals Wa for some
Hall and Horowitz [2007] and Crambes et al. [2009]). However, assuming (cid:101)Σ∞ to be ﬁnitely
smoothing excludes several interesting situations, such as our second example. Suppose(cid:101)Σ∞
to be inﬁnitely smoothing, that is, the range of | log((cid:101)Σ∞)|−1 equals Wa for some a > 0.

a > 0. Then the optimal rate is a polynomial of the sample size n. It is worth to note
that all published results in the FLM with exogenous regressor consider only this case (c.f.

Then the optimal rate is a logarithm of the sample size n. The important point to note

4

here is the theory behind these cases can be generalized by using an index function κ (c.f.

Nair et al. [2005]), which ‘links’ the range of (cid:101)Σ∞ and the Sobolev spaces. Then (cid:101)Σ∞ is
role as the covariance matrix (cid:101)Σ∞, can be found in Chen and Reiß [2011] or Johannes et al.

called in some general form smoothing and moreover the index function κ determines the
functional form of the optimal rate of convergence. A similar approach in the context of
nonparametric instrumental regression, where the conditional expectation plays the same

[2011].

In this paper we deal with the estimation of the slope function when the regressor X
and the instrument W are jointly second order stationary (deﬁned below). We derive a
lower bound of the rate of convergence for any estimator of β or its derivatives assuming

some general form of smoothing of (cid:101)Σ∞. Assuming second order stationarity we propose an

orthogonal series estimator of β and its derivatives based on a spectral cut-oﬀ (thresholding
in the Frequency domain). Then we show that the rate of the lower bound provides also an
upper bound for the risk of the orthogonal series estimator. Therefore, the rate is optimal
and hence the proposed estimator is minimax-optimal. The results for general smoothing

(cid:101)Σ∞ imply then as propositions the minimax optimal rate of convergence in estimating β
and its derivatives respectively in case of ﬁnitely as well as inﬁnitely smoothing (cid:101)Σ∞.

Organization of the paper. We summarize in Section 2 the model assumptions and
deﬁne the estimator of β and its derivatives. In Section 3 we provide minimal conditions to
ensure consistency of the estimator. Furthermore, we derive a lower and an upper bound

for the risk in the Sobolev norm when (cid:101)Σ∞ is in some general form smoothing. This results
are illustrated in Section 4 assuming (cid:101)Σ∞ to be ﬁnitely or inﬁnitely smoothing and Section

5 concludes. All proofs can be found in the Appendix.

2 Formalization of the model and deﬁnition of the estimator

Model. The setting of this paper can be summarized through the model

(cid:90) 1

Y =

β(t)X(t)dt + σU,

σ > 0,

(2.1a)

0

where Y ∈ R is a response variable, the endogenous random function X is deﬁned on the
interval [0, 1] and U is a centered error term with variance one such that

E[U W (t)] = 0,

t ∈ [0, 1]

(2.1b)

for some instrument W , i.e., an exogenous random function deﬁned also on [0, 1]. The
objective is the nonparametric estimation of the slope function β and its derivatives based
on a n-sample of (Y, X, W ). We assume throughout the paper that the random functions
X and W are deﬁned on the interval [0, 1] that (technically) simpliﬁes the notations. Of
course, it does not touch the applicability of the model and suggested estimator in a general
setting when X and W are deﬁned on some compact intervals I1 and I2, respectively.
Moreover, we suppose that the random functions X and W have a ﬁnite second moment,
E|W (t)|2dt < ∞. In order to simplify the presentation we

E|X(t)|2dt < ∞ and(cid:82) 1

i.e.,(cid:82) 1

0

0

5

(cid:90) 1

0

assume that the mean function of X and W are zero. Then multiplying both sides in (2.1a)
by W (s), s ∈ [0, 1], and taking the expectation leads to

s ∈ [0, 1],

E[Y W (s)] =

β(t) Cov[X(t), W (s)]dt =: [TW X β](s),

matrix Σ∞∞ considered in the introduction satisﬁes Σ∞∞ = ((cid:82) 1

(2.2)
where the function E[Y W (·)] is square integrable and TW X denotes the cross-covariance
operator associated to the random functions X and W . Note that the cross-covariance
j(s)ds)i,j∈N.
Estimation of β is thus linked with the inversion of the cross-covariance operator TW X
of (X, W ), and, hence called an inverse problem. Throughout the paper we require the
following assumption, which provides a necessary and suﬃcient condition for the existence
of a unique solution of equation (2.2).
Assumption 2.1. The cross-covariance operator TW X associated to the random functions
X and W is injective and the function E[Y W (·)] belongs to the range R(TW X ) of TW X .

0 [TW X ψi](s)ψ(cid:48)

In case a solution of the normal equation (2.2) does not exist all the results below can also
straightforward be obtained for the unique least-square solution with minimal norm, which
exists if only if E[Y W (·)] is contained in the direct sum of R(TW X ) and its orthogonal
complement R(TW X )⊥ (for a deﬁnition and detailed discussion in the context of inverse
problems c.f. Engl et al. [2000] or Carrasco et al. [2006]).

Notations and basic assumptions.
In this paper we suppose that the random function
(X, W ) is second order stationary and, hence there exists a function cW X : [−1, 1] → R
such that Cov[X(t), W (s)] = cW X (s − t), t, s ∈ [0, 1]. Notice that due to the ﬁnite second
moment of X and W the cross-covariance function cW X (·) is square integrable. Therefore,
its Fourier coeﬃcients with respect to the Fourier complex exponentials, i.e.,

ck :=

cW X (t) exp(−2πkit)dt,

for all k ∈ Z,

(cid:90) 1

−1

(2.3)

(2.4)

are well-deﬁned and by applying the well-known convolution theorem we have
for all k ∈ Z.

TW X ϕk = ck · ϕk with ϕk(t) := exp(2πkit), t ∈ [0, 1],

which is endowed with inner product (cid:104)f, g(cid:105) = (cid:82) 1

Thereby, it is convenient to consider the real-valued random functions X and W as elements
of the Hilbert space L2[0, 1] of square integrable complex valued functions deﬁned on [0, 1],
0 f (t)g(t)dt and associated norm (cid:107)f(cid:107) =
(cid:104)f, f(cid:105)1/2, f, g ∈ L2[0, 1]. Here and subsequently, g(t) denotes the complex conjugate of g(t).
Furthermore, the cross-covariance operator TW X is a well-deﬁned mapping from L2[0, 1] into
itself. Consider the centered complex valued random variables (cid:104)ϕk, X(cid:105) and (cid:104)W, ϕk(cid:105), k ∈ Z,
which due to the identity (2.4) satisfy

ck = E[(cid:104)ϕk, X(cid:105)(cid:104)W, ϕk(cid:105)]

and

0 = E[(cid:104)ϕk, X(cid:105)(cid:104)W, ϕj(cid:105)]

for all j (cid:54)= k.

Now an equivalent formulation of Assumption 2.1 is given by
|E[Y (cid:104)W, ϕk(cid:105)]|2

|ck|2 > 0,

for all k ∈ Z

and

|ck|2

< ∞.

(cid:88)

k∈Z

6

(2.5)

(2.6)

k∈Z

ck
wk

· (cid:104)W, ϕk(cid:105) · ϕk

Optimal linear instrument. Let xk := Var(cid:104)X, ϕk(cid:105), wk := Var(cid:104)W, ϕk(cid:105) and deﬁne λk :=
c2
k∈Z xk,
the sequences (λk)k∈Z is summable. If we further assume that supk∈Z |λk/wk| < ∞, then
the complex valued random function

k/wk (cid:54) xk, k ∈ Z, where due to the ﬁnite second moment of X, i.e., E(cid:107)X(cid:107)2 =(cid:80)
(cid:88)
(cid:102)W := (cid:96)(W ) :=
is well-deﬁned, i.e., (cid:107)(cid:102)W(cid:107) < ∞. Note that (cid:96) is a linear operator mapping L2[0, 1] into itself.
If in addition (cid:80)
k∈Z λk/wk < ∞, then (cid:96) is a Hilbert-Schmidt operator and (cid:102)W = (cid:96)(W ) is
E(cid:107)X−(cid:96)(cid:48)(W )(cid:107)2 over all Hilbert-Schmidt operator (cid:96)(cid:48) (c.f. Bosq [2000]). Therefore, we call(cid:102)W
optimal linear instrument. Throughout the paper we suppose the linear predictor(cid:102)W is well-
deﬁned, i.e., supk∈Z |λk/wk| < ∞, which implies an additional restriction on the behavior
of the sequences (λk)k∈Z and (wk)k∈Z as |k| → ∞. In particular the sequence of variances
(wk)k∈Z associated to the instrument W has to tend slowlier to zero than the sequence
optimal linear instrument (cid:102)W exist, in general it is not known to the econometrician.
(ck)k∈Z of cross-covariances associated to X and W . Note that although we suppose the

the best linear predictor of X based on W . That is, (cid:96) minimizes the mean prediction error

(2.7)

Moment assumptions. The results derived below involve additional conditions on the
moments of the random functions X and W and the error term U , which we formalize now.
Let F be the set of all centered second order stationary random functions (X, W ). Here and
η,τ , m ∈ N, η, τ (cid:62) 1, denotes the subset of F containing all random functions
subsequently, F m
(X, W ) such that the m-th moment of the corresponding random variables {(cid:104)X, ϕk(cid:105)/
xk}
√
wk} are uniformly bounded and such that the linear predictor of X based
and {(cid:104)W, ϕk(cid:105)/
on W is well-deﬁned, that is

√

(cid:110)

F m
η,τ :=

(X, W ) ∈ F with sup
k∈Z

E

(cid:12)(cid:12)(cid:12)(cid:104)X, ϕk(cid:105)√

xk

(cid:12)(cid:12)(cid:12)m (cid:54) η and sup

k∈Z

E

(cid:12)(cid:12)(cid:12)(cid:104)W, ϕk(cid:105)√

(cid:12)(cid:12)(cid:12)m (cid:54) η

wk
|λk/wk| (cid:54) τ

(cid:111)

.

(2.8)

and associated values (λk)k∈Z such that 1 ∨ sup
k∈Z

In what follows, E m
ﬁnite m-th moment, i.e., E|U|m (cid:54) η.

η stands for the set of all centered error terms U with variance one and

(cid:88)

gk
λk

β =

Estimation of β as an ill-posed inverse problem. Consider the optimal linear instru-

ment (cid:102)W deﬁned in (2.7), then due to Assumption 2.1 the normal equation (2.2) implies

· ϕk with gk := (cid:104)g, ϕk(cid:105), k ∈ Z, and g := E[Y(cid:102)W (·)].

k∈Z

Moreover, λk = E|(cid:104)(cid:102)W , ϕk(cid:105)|2, k ∈ Z, are the eigenvalues of the covariance operator T(cid:102)W
associated to (cid:102)W .
covariance operator T(cid:102)W . Accordingly, replacing in (2.9) the unknown function g by a
consistent estimator(cid:98)g does in general not lead to a consistent estimator of β even in case of
known values {λk}. To be more precise, since the sequence (λk)k∈Z tends to zero as |k| → ∞,

In other words, the estimation of β necessitates the inversion of the

(2.9)

7

E(cid:107)(cid:98)g−g(cid:107)2 = o(1) does generally not imply(cid:80)∞

k∈Z |λk|−2·E|(cid:104)(cid:98)g−g, ϕk(cid:105)|2 = o(1). Consequently,

the estimation in FLIR is called ill-posed and additional regularity assumptions on the slope
function β are necessary in order to obtain a uniform rate of convergence (c.f. Engl et al.
[2000]).

In this paper we assume that the slope function β belongs to the well-known Sobolev

space Wp, p > 0, of periodic functions, which can be deﬁned for ν ∈ R by

Wν :=

f ∈ L2[0, 1] : (cid:107)f(cid:107)2

ν :=

(2.10)

k|(cid:104)f, ϕk(cid:105)|2 < ∞(cid:111)

,

γν

(cid:88)

k∈Z

(cid:110)

(cid:110)

where {ϕk} are the complex exponentials given in (2.4) and the weights {γk} satisfy

γk = 1 + |2πk|2,
ν := {f ∈ Wν : (cid:107)f(cid:107)2

k ∈ Z.
(2.11)
(cid:54) ρ} for ρ > 0. Notice that for integer ν ∈ N the Sobolev space

Let W ρ
of periodic functions Wν is equivalently given by

ν

Wν =

f ∈ Hν : f (j)(0) = f (j)(1),

j = 0, 1, . . . , ν − 1

,

(cid:111)

where Hν := {f ∈ L2[0, 1] : f (ν−1) absolutely continuous , f (ν) ∈ L2[0, 1]} is a Sobolev space
(c.f. Neubauer [1988a,b], Mair and Ruymgaart [1996] or Tsybakov [2004]).

In the literature several approaches are proposed in order to circumvent an instability
issue due to the inversion of the covariance operator (for a detailed discussion in the context
of inverse problems in econometrics we refer e.g. to Carrasco et al. [2006] and Johannes
et al. [2011]). Essentially, all of them replace equation (2.9) by a regularized version which
avoids that the denominator becomes too small. For example, Hall and Horowitz [2007]
use in a functional linear model with exogenous regressor a Tikhonov regularization. There
is a large number of alternative regularization schemes in the numerical analysis literature
available like the iterative Tikhonov regularization, Landweber iteration or the ν-Method
to name but a few (c.f. Engl et al. [2000]). However, in this paper we regularize equation
(2.9) by introducing a threshold α > 0 and weights {γk} deﬁned in (2.11). For ν (cid:62) 0 we
consider the regularized version βν given by

βν :=

· 1{λk/γν

k

(cid:62) α} · ϕk,

gk
λk

(2.12)

(cid:88)

k∈Z

which obviously belongs to the Sobolev space Wν. Thresholding in the Fourier domain
in this situation is new, however has been used, for example, in a deconvolution problem
in Mair and Ruymgaart [1996], Neumann [1997] or Johannes [2009] and coincides with
an approach called spectral cut-oﬀ in the numerical analysis literature (c.f. Tautenhahn
[1996]).

Deﬁnition of the estimator. Let (Y1, X1, W1), . . . , (Yn, Xn, Wn) be an i.i.d. sample of

(Y, X, W ), which we use in a ﬁrst step to construct an estimator (cid:99)Wi of the optimal linear
instrument (cid:102)Wi, i = 1, . . . , n, exploiting the identity (2.7). Consider the unbiased estimator

of ck = E(cid:104)ϕk, X(cid:105)(cid:104)W, ϕk(cid:105) and wk = E|(cid:104)W, ϕk(cid:105)|2 given by

n(cid:88)

(cid:98)ck :=

1
n

(cid:98)wk :=

1
n

n(cid:88)

i=1

(cid:104)ϕk, Xi(cid:105)(cid:104)Wi, ϕk(cid:105)

and

i=1

8

|(cid:104)Wi, ϕk(cid:105)|2,

k ∈ Z,

(2.13)

introducing a threshold α > 0, that is

· 1{(cid:98)wk (cid:62) α} · (cid:104)Wi, ϕk(cid:105) · ϕk,

(cid:99)Wi :=

(cid:88)

k∈Z

(cid:98)ck(cid:98)wk

respectively. Then we deﬁne the estimator of (cid:102)Wi by regularizing equation (2.7), i.e., by

i = 1,··· , n,

(2.14)

1
n

i=1

i=1

1
n

k ∈ Z.

n(cid:88)

which motivate the estimators deﬁned by

and (cid:98)gk :=

where the threshold α = α(n) has to tend to zero as the sample size n increases. In a second
step we use the estimated optimal linear instrument to construct an estimator of β based on

the decomposition (2.9). Consider the identities λk = E|(cid:104)(cid:102)W , ϕk(cid:105)|2 and gk = E[Y (cid:104)(cid:102)W , ϕk(cid:105)],

Yi · (cid:104)(cid:99)Wi, ϕk(cid:105),

(cid:98)λk :=
(cid:98)βν :=

Finally, the estimator (cid:98)βν of β is based on the regularized version (2.12). That is,

n(cid:88)
|(cid:104)(cid:99)Wi, ϕk(cid:105)|2
(cid:88)
(cid:98)gk(cid:98)λk
· 1{(cid:98)λk/γν
Parseval’s formula(cid:80)
(cid:80)n
k∈Z (cid:98)wk = 1
which obviously belongs also to the Sobolev space Wν. It is worth pointing out that due to
i=1(cid:107)Wi(cid:107)2 is ﬁnite. Thereby, the sum in (2.14) contains
values (cid:98)λk are nonzero. Consequently, the sum in (2.16) contains only a ﬁnite number of
only a ﬁnite but random number of nonzero summands, and hence only a ﬁnite number of
nonzero summands. We emphasize that the same threshold is used in the deﬁnition of (cid:99)Wi
and (cid:98)βν given in (2.14) and (2.16) respectively. In general this will not lead to an optimal
estimator of(cid:102)Wi, however as we will see below, it is suﬃcient to ensure the optimality of (cid:98)βν.

(cid:62) α} · ϕk,

(2.16)

(2.15)

k∈Z

k

n

3 Optimal estimation of slope function and its derivatives

We shall measure the performance of the estimator (cid:98)βν deﬁned in (2.16) by the Wν-risk,
that is E(cid:107)(cid:98)βν − β(cid:107)2
ν, provided that β ∈ Wp for some p (cid:62) ν (cid:62) 0. For an integer ν the Sobolev
weak sense. Consequently, the Wν-risk reﬂects the performance of (cid:98)βν and (cid:98)β(ν)
norm (cid:107)g(cid:107)ν is equivalent to (cid:107)g(cid:107) + (cid:107)g(ν)(cid:107), where g(ν) denotes the ν-th derivative of g in a
as estimator

ν

of β and β(ν) respectively.

k

k∈Z

ν :=

(cid:88)

The Wν-risk is essentially determined by the deviation of the estimators of gk and λk,

(cid:62) α} · ϕk with βk := (cid:104)β, ϕk(cid:105), k ∈ Z,

(cid:101)βα
E(cid:107)(cid:98)βν − β(cid:107)2

k ∈ Z, and by the regularization error due to the spectral cut-oﬀ. In fact, if

βk · 1{(cid:98)λk/γν
then by assuming β ∈ Wp for some p (cid:62) ν (cid:62) 0 we bound the Wν-risk of (cid:98)βν by
(cid:54) 2{E(cid:107)(cid:98)βν −(cid:101)βα
ν + E(cid:107)(cid:101)βα
ν (cid:107)2
ν − β(cid:107)2
ν}.
proof of the next proposition that E(cid:107)(cid:98)βν − (cid:101)βα
η,τ deﬁned in (2.8) and U ∈ E 4
ν (cid:107)2
E(cid:107)(cid:101)βα
ν − β(cid:107)2

η we show in the
ν is bounded up to a universal constant
by (α2 · n)−1 · {σ2 + (cid:107)β(cid:107)2 · E(cid:107)X(cid:107)2} · E(cid:107)W(cid:107)2 · η and that the regularization error satisﬁes
ν = o(1) provided α = o(1) and (α · n)−1 = o(1) as n → ∞. The next assertion

Under the moment condition (X, W ) ∈ F 8

(3.1)

(3.2)

ν

summarizes the minimal conditions to ensure consistency of the proposed estimator.

9

Proposition 3.1 (Consistency). Let β ∈ Wp, p (cid:62) 0. Consider for 0 (cid:54) ν (cid:54) p the estimator
addition (X, W ) ∈ F 8

(cid:98)βν given in (2.16) with threshold satisfying α = o(1) and (α2 · n)−1 = o(1) as n → ∞. If in
η , then we have E(cid:107)(cid:98)βν − β(cid:107)2
without an additional smoothness assumption on β. However, (cid:98)β(cid:48)

ν = o(1) as n → ∞.

Remark 3.1. The last result covers the case 0 = ν = p, i.e., the estimator of β is consistent
1 is a consistent estimator
(cid:3)

of β(cid:48), only if β is diﬀerentiable, i.e., β ∈ Wp, p (cid:62) 1.

η and U ∈ E 4

Link condition.
In order to obtain a rate of convergence of the regularization error and
hence the Wν-risk we link the smoothness condition on β, i.e., the Sobolev space Wp, and the
values {λk} associated to the cross-covariance function of (X, W ). In fact, the obtainable
rate of convergence is essentially determined by the decay of (λk)k∈Z as |k| → ∞, which
(cid:80)
we ﬁrst allow to a have a general form. Notice that due to the ﬁnite second moment
of X the sequence (λk)k∈Z belongs to the set (cid:96)+
1 of nonnegative summable sequences, i.e.,
k∈Z λk < ∞. Thereby, λ+ := 1∨maxk∈Z λk is ﬁnite and the rescaled sequence (λk/λ+)k∈Z
is taking only values in (0, 1]. It is convenient to choose an index function κ : (0, 1] → R+
(c.f. Nair et al. [2005]), which we always assume here to be a continuous and strictly
increasing function with κ(0+) = 0. Then, we require that the sequence (λk/λ+)k∈Z is an
element of the subset Sκ,d of (cid:96)+

1 deﬁned for d (cid:62) 1 by

Sκ,d :=

(λk) ∈ (cid:96)+

1 : κ

(cid:16) λk

d γν

k λ+

(cid:17) (cid:54) γν−p

k

(cid:16) d λk

γν
k λ+

(cid:17)

, k ∈ Z(cid:111)

,

(cid:54) κ

where the weights {γk} are given in (2.11). First we consider this general class of values
{λk}. However, we illustrate condition (3.3) in Section 4 by assuming a “regular decay”.
The lower bound as well as the upper bound of the Wν-risk derived below involve
additional conditions on the moments of the random function (X, W ), which are formalized
by using the set F m
η,τ deﬁned in (2.8). We suppose in what follows that for some index
function κ(·) the random function (X, W ) belongs to the subset F m

κ of F m

η,τ given by

(cid:110)

(cid:110)

(3.3)

(cid:111)

F m
κ :=

(X, W ) ∈ F m

η,τ with associated values (λk)k∈Z ∈ Sκ,d

and such that E(cid:107)X(cid:107)2 (cid:54) Λ, E(cid:107)W(cid:107)2 (cid:54) Λ

(3.4)

for some constants d, η, τ, Λ (cid:62) 1 and m ∈ N.

The lower bound.
It is well-known that in general the hardest one-dimensional subprob-
lem does not capture the full diﬃculty in estimating the solution of an inverse problem even
in case of a known operator (for details see e.g. the proof in Mair and Ruymgaart [1996]).
In other words, there does not exist two sequences of slope functions β1,n, β2,n ∈ W ρ
p , which
are statistically not consistently distinguishable and which satisfy (cid:107)β1,n − β2,n(cid:107)2
(cid:62) Cψn,
where ψn is the optimal rate of convergence. Therefore we need to consider subsets of W ρ
p
with growing number of elements in order to get the optimal lower bound. More speciﬁc, we
obtain the following lower bound by applying Assouad’s cube technique (see e.g. Korostolev
and Tsybakov [1993] or Chen and Reiß [2011]).

ν

10

p , p, ρ > 0, as set of slope functions, U ∈ E l

Theorem 3.2. Assume an n-sample of (Y, X, W ) satisfying (2.1a) and (2.1b) with σ > 0.
Consider W ρ
η, l ∈ N, as set of error terms
and F m
κ , m ∈ N, as class of regressors deﬁned in (3.4) for an arbitrary index function
κ, constants d, η, τ, Λ (cid:62) 1 and 0 (cid:54) ν < p. Denote by ϕ the inverse function of κ. Let
k∗ := k∗(n) ∈ N and δ∗ := δ∗(n) ∈ (0, 1] for some (cid:52) (cid:62) 1 be chosen such that

γp−ν
k∗

n · ϕ(γν−p

k

(cid:54) (cid:52) and

δ∗ := ϕ(γν−p
k∗ ).

)

(3.5)

(cid:52)−1 (cid:54) (cid:88)

|k|(cid:54)k∗

If we assume in addition that η is suﬃciently large, then

inf(cid:101)β

sup

β∈W ρ

p ,(X,W )∈F m

κ ,U∈E l

η

(cid:110)
E(cid:107)(cid:101)β − β(cid:107)2

ν

(cid:111) (cid:62) 1

(cid:110) σ2

d(cid:52) ,

(cid:111) · κ(δ∗)

Λ

.

ρ
2

· min

4

κ and the class of error terms E l

Remark 3.2. The lower bound in the last result is obtained under the assumption that the
class of regressors F m
η provide a certain complexity, i.e., the
uniform bound η allows the moments of (X, W ) to be suﬃciently large. In fact, we ensure
that for certain slope functions β ∈ W ρ
p , the conditional distribution of the linear prediction

error Y −(cid:104)β,(cid:102)W(cid:105) given the optimal linear instrument(cid:102)W can be chosen to be Gaussian. This

assumption is only needed to simplify the calculation of the distance between distributions
(cid:3)
corresponding to diﬀerent slope functions.

The upper bound.

(cid:98)βν deﬁned in (2.16) assuming an index function κ with the additional property that

In the following theorem we provide an upper bound for the estimator

for all c (cid:62) 1 :

κ(c · t)
κ(t)

= O(1)

and

κ(t)
κ(t/c)

= O(1)

as t → 0.

(3.6)

The next theorem states that the rate κ(δ∗) of the lower bound given in Theorem 3.2

provides also an upper bound of the proposed estimator (cid:98)βν. We have thus proved that the
rate κ(δ∗) is optimal and hence the estimator (cid:98)βν is minimax optimal.

p , p, ρ > 0, as set of slope functions, U ∈ E l

some constants d, η, τ, Λ (cid:62) 1 and 0 (cid:54) ν < p. Let (cid:98)βν be the estimator deﬁned in (2.16). If

Theorem 3.3. Assume an n-sample of (Y, X, W ) satisfying (2.1a) and (2.1b) with σ > 0.
Consider W ρ
η, l (cid:62) 16, as set of error terms and
F m
κ , m (cid:62) 32, as class of regressors deﬁned in (3.4) for an index function κ satisfying (3.6),
in addition the threshold α := α(n) satisﬁes α = 8 d Λ δ∗, where δ∗ ∈ (0, 1] is given in (3.5)
for some (cid:52) (cid:62) 1, then we have

(cid:54) C η d(cid:52) · [σ2 + ρ Λ] · [(cid:52) Λ κ(δ∗) + 1]4 · κ(δ∗),

E(cid:107)(cid:98)βν − β(cid:107)2

ν

sup

β∈W ρ

p ,(X,W )∈F m

κ ,U∈E l

η

where the constant C > 0 does only depend on the index function κ and the constants d, τ, Λ.

Remark 3.3. We would like to stress, that for integer ν < p the Theorem 3.2 and 3.3
show together that κ(δ∗) is the optimal rate of convergence for the estimation of the ν-th
of the in (2.16) proposed estimator
(cid:3)

derivative β(ν) of β. Moreover the ν-th derivative (cid:98)β(ν)
(cid:98)βν attains this optimal rate, i.e, is minimax.

ν

11

4 Optimality in case of a “regular decay”
In this section we consider two special cases describing a “regular decay” of the values {λk}
associated to the cross-covariance operator TW X of the random function (X, W ). In the
ﬁrst example we suppose the values {λk} descend polynomial, which in case of a linear
functional model with exogenous regressor is considered e.g. in Cardot et al. [2003] or Hall
and Horowitz [2007]. The second example concerns values {λk} with exponential decay.
The ﬁnitely smoothing case. Assume now the values {λk} associated to the random
function (X, W ) have a polynomial decay, that is1

λk (cid:16) |k|−2a

for some a > 0.

(4.1)

covariance operator associated to the optimal linear instrument (cid:102)W (see the identity (2.9)
Then straightforward calculus shows the identity R(T(cid:102)W

) = W2a, where T(cid:102)W

and its discussion in Section 3).
(2a)-times and, hence it is called ﬁnitely smoothing. Furthermore, it is easily seen that

In other words, the operator T(cid:102)W

acts like integrating

denotes the

p ,(X,W )∈F m

∀ 0 (cid:54) ν < p :

(λk)k∈Z ∈ Sκ,d with κ(t) := t(p−ν)/(a+ν) and some d (cid:62) 1.

(cid:8)E(cid:107)(cid:101)β − β(cid:107)2

(4.2)
In the proof of the next proposition we shown that the condition (3.5) implies δ∗ (cid:16)
n−2(a+ν)/[2(p+a)+1]. Thereby, we have κ(δ∗) (cid:16) n−2(p−ν)/[2(p+a)+1] and hence the lower bound
(cid:9) (cid:62) C · n−2(p−ν)/[2(p+a)+1] for some C > 0.
in the next assertion follows from Theorem 3.2.
Proposition 4.1. Let the assumptions of Theorem 3.2 be satisﬁed with κ(t) = t(p−ν)/(a+ν).
Then inf(cid:101)β supβ∈W ρ
On the other hand, if the threshold α in the deﬁnition of the estimator (cid:98)βν given in (2.16)
is chosen such that α (cid:16) n−2(a+ν)/[2(p+a)+1]. Then by applying Theorem 3.3 the rate
estimator (cid:98)βν, which is summarized in the next proposition. We have thus proved that the
n−2(p−ν)/[2(p+a)+1] provides up to a constant also the upper bound of the Wν-risk of the
rate n−2(p−ν)/[2(p+a)+1] is optimal and the proposed estimator (cid:98)βν is minimax optimal. Note
Consider the estimator (cid:98)βν deﬁned in (2.16) with threshold α = c· n−2(a+ν)/[2(p+a)+1], c > 0.

that the index function κ(t) = t(p−ν)/(a+ν) satisﬁes the additional condition (3.6).
Proposition 4.2. Let the assumptions of Theorem 3.3 be satisﬁed with κ(t) = t(p−ν)/(a+ν).

κ ,U∈E l

ν

η

Then we have supβ∈W ρ

p ,(X,W )∈F m

κ ,U∈E l

η

(cid:8)E(cid:107)(cid:98)βν − β(cid:107)2

ν

(cid:9) = O(n−2(p−ν)/[2(p+a)+1]).

Remark 4.1. We shall emphasize the interesting inﬂuence of the parameters p and a char-
acterizing the smoothness of β and the smoothing property of T(cid:102)W
respectively. As we see
from Proposition 4.1 and 4.2, if the value of a increases the obtainable optimal rate of con-
vergence decreases. Therefore, the parameter a is often called degree of ill-posedness (c.f.
Natterer [1984]). On the other hand, an increasing of the value p leads to a faster optimal
rate. In other words, as we expect, a smoother slope function can be faster estimated. Fi-
nally, the estimation of higher derivatives of the slope function, i.e., increasing of the value
(cid:3)
of ν, is as usual only with a slower optimal rate possible.

1We write ak (cid:16) bk if there exists a ﬁnite positive constant c such that c−1ak (cid:54) bk (cid:54) cak for all k ∈ Z.

12

Remark 4.2. There is an interesting issue hidden in the parametrization we have chosen.
associated to the optimal instrument(cid:102)W , i.e., Y = [T(cid:102)W
Consider classical indirect regression with known operator given by the covariance operator
T(cid:102)W
β](Z) + ε where Z has a uniform
distribution on [0, 1] and ε is white noise (for details see e.g. Mair and Ruymgaart [1996]).
Then given a n-sample of Y the optimal rate of convergence of the Wν-risk of any estimator
of β is of order n−2(p−ν)/[2(p+2a)+1], since R(T(cid:102)W
) = W2a (c.f. Mair and Ruymgaart [1996]
or Chen and Reiß [2011]). However, we have shown in Proposition 4.1 and 4.2 that in FLIR
the rate n−2(p−ν)/[2(p+a)+1] is optimal. Thus comparing both rates we see that in FLIR the
covariance operator T(cid:102)W
has the degree of ill-posedness a while the same operator has in
indirect regression a degree of ill-posedness (2a). In other words in FLIR we do not face the
may be seen as a multiplication of the stochastic equation Y(cid:102)W = (cid:104)β,(cid:102)W(cid:105)(cid:102)W + ε(cid:102)W by the
complexity of an inversion of T(cid:102)W
. This, roughly speaking,
ε(cid:102)W . Thus multiplying to the stochastic equation the inverse of T 1/2(cid:102)W
. Notice that T(cid:102)W
inverse of T 1/2(cid:102)W

leads, roughly speaking,
to an additive white noise and hence it is then comparable with an indirect regression model
is unknown and thus it has to be
(cid:3)

but only of its square root T 1/2(cid:102)W

. However, the operator T 1/2(cid:102)W

is also the covariance operator associated to the error term

with operator given by T 1/2(cid:102)W

estimated from the data.

The inﬁnitely smoothing case. Suppose now the values {λk} associated to the regres-
sors X and W have an exponential decay, that is

λk (cid:16) exp(−|k|2a)

for some a > 0.

(4.3)

Then it is easy to check that R(T(cid:102)W
is called inﬁnitely
smoothing. In fact, the transformed values {| log λk|−1} satisfy the polynomial condition
(4.1). Consequently, by applying the functional calculus we have R(| log(T(cid:102)W
)|−1) = W2a.
In other words, | log(T(cid:102)W

)|−1 acts like integrating (2a)-times. Moreover, it follows that

) ⊂ Wν for all ν > 0, therefore T(cid:102)W

∀ 0 (cid:54) ν < p :

(λk)k∈Z ∈ Sκ,d with κ(t) := | log t|−(p−ν)/a and some d (cid:62) 1.

(4.4)
Let ω be the inverse function of ω−1(t) := t · ϕ(t), where ϕ denotes the inverse function
of κ. We show in the proof of the next proposition that in an inﬁnitely smoothing case
the condition (3.5) implies 1/n (cid:16) δ∗ κ(δ∗). Then it is straightforward to see that δ∗ (cid:16)
1/(n ω(1/n)) and κ(δ∗) (cid:16) ω(1/n). Furthermore, it is shown in Mair [1994] that ω(t) =
| log t|−(p−ν)/a(1 + o(1)) as t → 0. Consequently, the lower bound in the next assertion
(cid:9) (cid:62) C·(log n)−(p−ν)/a for some C > 0.
follows again from Theorem 3.2.
Proposition 4.3. Let the assumptions of Theorem 3.2 be satisﬁed with κ(t) = | log t|−(p−ν)/a.
Then we have inf(cid:101)β supβ∈W ρ
4.3 provides up to a constant also the upper bound of the Wν-risk of the estimator (cid:98)βν. We
The next proposition states that the rate (log n)−(p−ν)/a of the lower bound in Proposition
have thus proved that the rate (log n)−(p−ν)/a is optimal and (cid:98)βν is minimax-optimal.
Consider the estimator (cid:98)βν deﬁned in (2.16) with threshold α = c · n−1/4, c > 0. Then we

Proposition 4.4. Let the assumptions of Theorem 3.3 be satisﬁed with κ(t) = | log t|−(p−ν)/a.

(cid:8)E(cid:107)(cid:101)β−β(cid:107)2

p ,(X,W )∈F m

κ ,U∈E l

ν

η

have supβ∈W ρ

p ,(X,W )∈F m

κ ,U∈E l

η

(cid:8)E(cid:107)(cid:98)βν − β(cid:107)2

ν

(cid:9) = O((log n)−(p−ν)/a).

13

Remark 4.3. It seems rather surprising that in opposite to Proposition 4.2 in the last asser-
tion the threshold α does not depend on the values of p, ν or a. This, however, is due to the

fact that for α = cn−1/4, c > 0, the Wν-risk of(cid:98)βν is of order O(n−1/2+| log n−1/4|−(p−ν)/a) =

O((log n)−(p−ν)/a). Note, that the parameter a specifying in condition (4.3) the decay of the
values {λk} describes also in this situation the degree of ill-posedness. Finally, a comparison
(cid:3)
with an indirect regression model as in Remark 4.2 leads to the same ﬁndings.

5 Conclusion and perspectives

Assuming joint second order stationarity of the regressor X and the instrument W we
derive in this paper the minimax optimal rate of convergence of an estimator of the slope
function β and its derivatives provided the covariance operator associated to the optimal

linear instrument(cid:102)W is in some general form smoothing. This results in its generality cover

in particular the case of ﬁnitely or inﬁnitely smoothing covariance operators. It is worth
pointing out that for establishing the lower bound it is not necessary to assume that the
regressor and the instrument are jointly second order stationary. Moreover, the lower bound
is derived by assuming a certain complexity of the class of distributions of X and W , in
particular, it contains a Gaussian model. Therefore, we claim that replacing the optimal
linear by the optimal instrument, i.e., the conditional expectation of X given W , will not
improve the optimal rate of convergence. Indeed, in a Gaussian model both instruments, if
they exist, coincide.

Many ideas in this paper can be adapted to the general case without the assumption
of joint second order stationarity of the regressor X and the instrument W . However, the
estimation procedure itself may be diﬀerent, since a projection onto the in general unknown
eigenfunctions of the covariance operator of the optimal linear instrument is not possible.
This is subject of ongoing research.

Once this will be established, the open problem of how to choose the threshold α adap-
tively from the data will remain in case not knowing the true smoothness of the slope
function or not knowing the true link between the covariance operator of the optimal linear
instrument and the Sobolev spaces.

A Appendix: Proofs

A.1 Proofs of Section 3

We begin by deﬁning and recalling notations to be used in the proofs:

n(cid:88)

i=1

(YiWik − (cid:98)ck(cid:98)wk

βk|Wik|2),

Xik := (cid:104)Xi, ϕk(cid:105), Wik := (cid:104)Wi, ϕk(cid:105), Tn,k :=

1
n

ck = E[X ikWik], wk = Var(Wik),

xk = Var(Xik), and λk = c2

k/wk.

(A.1)

We shall prove in the end of this section four technical Lemma (A.1 - A.4) which are used
in the following proofs.

14

Proof of the consistency.

Proof of Proposition 3.1. The proof is based on the decomposition (3.2). We show
below for some universal constant C > 0 the following bound

ν

E(cid:107)(cid:98)βν −(cid:101)βα
ν (cid:107)2
E(cid:107)(cid:101)βα
ν − β(cid:107)2

(A.2)
while in case of (cid:107)β(cid:107)ν < ∞ we conclude from Lebesgue’s dominated convergence theorem

· E(cid:107)W(cid:107)2 · {σ2 + (cid:107)β(cid:107)2E(cid:107)X(cid:107)2},

(cid:54) C η
α2 n

ν = o(1) provided α = o(1) and (α n)−1 = o(1) as n → ∞.

(A.3)

Consequently, the conditions on α ensure the convergence to zero of the two terms on the
right hand side in (3.2) as n tends to ∞, which gives the result.

Proof of (A.2). By making use of the notations given in (A.1) it follows that

E(cid:107)(cid:98)βν −(cid:101)βα
ν (cid:107)2

ν

(cid:88)

k∈Z

E

|(cid:98)gk − βk(cid:98)λk|2

(cid:98)λk

(cid:54) 1
α

1{(cid:98)λk (cid:62) α γν

(cid:88)

k∈Z

k} (cid:54) 1
α2

E|Tn,k|2

and hence by using (A.8) in Lemma A.1 we obtain (A.2).

Proof of (A.3). If β ∈ Wp, p (cid:62) ν (cid:62) 0, then by making use of the relation
(cid:54) (cid:107)β(cid:107)2

k < α} (cid:54)(cid:88)

k = (cid:107)β(cid:107)2

(cid:88)

ν =

k · E1{(cid:98)λk/γν
k · γν
β2

E(cid:107)(cid:101)βα
ν − β(cid:107)2
claim E1{(cid:98)λk/γν
(A.14) in Lemma A.2 we bound E1{(cid:98)λk/γν

the result follows from Lebesgue’s dominated convergence theorem since for each k ∈ Z we
k < α} = o(1) provided α = o(1) and (α n)−1 = o(1) as n → ∞. Indeed,
k and hence by using
k < α} up to a constant by (α n)−1E(cid:107)X(cid:107)2{1 +
(α n)−1E(cid:107)W(cid:107)2}. Thereby, the conditions on α imply (A.3) which completes the proof. (cid:3)

there exists αk > 0 such that for all α (cid:54) αk it holds λk (cid:62) 4τ αγν

p < ∞

k · γν
β2

k∈Z

k∈Z

ν

Proof of the lower bound.

Proof of Theorem 3.2. Assuming η to be suﬃciently large we can pick an i.i.d. sam-
ple (Xi, Wi) ∈ F m
η,τ , i = 1, . . . , n, of Gaussian random functions such that the associated
sequence of values (λk)k∈Z is an element of Sκ,d, i.e., {(Xi, Wi)} ⊂ F m
κ . Consider indepen-
dent error terms εi ∼ N (0, 1), i = 1,··· , n, which are independent of the random functions

{(Xi, Wi)}. For i = 1, . . . , n let (cid:102)Wi be the optimal instrument given in (2.7), and denote
(cid:102)Wik := (cid:104)(cid:102)Wi, ϕk(cid:105), k ∈ Z. Note, that (cid:102)Wik is a centered random variable with variance λk.

Let θ = (θk) ∈ {−1, 1}2k∗+1, where k∗ := k∗(n) ∈ N satisﬁes (3.5) for some (cid:52) (cid:62) 1. Deﬁne a
(2k∗ + 1)-vector of coeﬃcients (bj) such that (b2
j ) satisﬁes (A.24) in Lemma A.4. For each
θ we deﬁne a function βθ which by (A.26) in Lemma A.4 yields:

(cid:88)

βθ :=

θkbkϕk ∈ W ρ
p .

|k|(cid:54)k∗

Deﬁne for each θ an error term Uθi = εi/2 + τθ(cid:104)βθ, Xi −(cid:102)Wi(cid:105). Then {Uθi} are independent
centered Gaussian random variables with variance one for an appropriate chosen τθ, and
hence {Uθi} ⊂ E m
η . Moreover, we have E[UθiWi(t)] = 0, for all t ∈ [0, 1] and i = 1, . . . , n.

15

j

σ2
θ = σ2

θ
θ(k) it is easily seen that the log-likelihood of Pθ(k) w.r.t. Pθ is given by

:= (cid:104)βθ, Xi(cid:105) + σUθi,
Consequently, for each θ the random variables (Yi, Xi, Wi) with Yi
i = 1, . . . , n, form a sample of the model (2.1a)-(2.1b) and we denote its joint distribution
by Pθ. Furthermore, for |k| (cid:54) k∗ and each θ we introduce θ(k) = (θ(k)
) ∈ {−1, 1}2k∗+1 by
k = −θk. As in case of Pθ the conditional distribution of Yi
j = θj for k (cid:54)= j and θ(k)
θ(k)
(cid:62) σ2/4 with
n(cid:88)

given (cid:102)Wi is Gaussian with mean (cid:104)βθ,(cid:102)Wi(cid:105) =(cid:80)|k|(cid:54)k∗ θkbk(cid:102)W ik and variance σ2
n(cid:88)
(cid:102)W 2

n(cid:88)
U θi(cid:102)W ik +
k · λk/(2σ2) since λk = Var(cid:102)Wik and
Its expectation satisﬁes EPθ [log(dPθ(k)/dPθ)] (cid:62) −n · b2
In terms of Kullback-Leibler divergence this means KL(Pθ(k), Pθ) (cid:54) n · b2
k ·
σ2
θ
λk/(2σ2). Since the Hellinger distance H(Pθ(k), Pθ) satisﬁes H 2(Pθ(k), Pθ) (cid:54) KL(Pθ(k), Pθ)
it follows from (A.25) in Lemma A.4 that

Uθi(cid:102)Wik − 1

(cid:16) dPθ(k)

= − 1
σ2
θ

(cid:62) σ2/4.

2
σ2
θ

(cid:17)

θkbk

θkbk

dPθ

σ2
θ

log

ik.

b2
k

i=1

i=1

i=1

H 2(Pθ(k), Pθ) (cid:54) n

Consider the Hellinger aﬃnity ρ(Pθ(jk , Pθ) =(cid:82)(cid:112)dPθ(k)dPθ, then we obtain for any estima-
tor (cid:101)β that

|k| (cid:54) k∗.

(A.4)

k · λk (cid:54) 1,

2σ2 · b2
(cid:90) |(cid:104)(cid:101)β − βθ(k), ϕk(cid:105)|
(cid:54)(cid:16)(cid:90) |(cid:104)(cid:101)β − βθ(k), ϕk(cid:105)|2

|(cid:104)βθ − βθ(k), ϕk(cid:105)|

(cid:90)
(cid:16)(cid:90)

E

+

(cid:110)

ρ(Pθ(k), Pθ) (cid:54)

|(cid:104)βθ − βθ(k), ϕk(cid:105)|2 dPθ(k)

Due to the identity ρ2(Pθ(k), Pθ) = 1 − 1
From this we conclude for each estimator (cid:101)β that

(cid:112)dPθ(k)dPθ +
(cid:17)1/2
θ(k)|(cid:104)(cid:101)β − βθ(k), ϕj(cid:105)|2 + Eθ|(cid:104)(cid:101)β − βθ, ϕk(cid:105)|2(cid:111) (cid:62) 1
Eθ(cid:107)(cid:101)β − βθ(cid:107)2
(cid:88)
k · Eθ|(cid:104)(cid:101)β − βθ, ϕk(cid:105)|2
·(cid:110)
(cid:88)
Eθ|(cid:104)(cid:101)β − βθ, ϕk(cid:105)|2 + E
(cid:111) · κ(δ∗)
(cid:110) σ2

E(cid:107)(cid:101)β − β(cid:107)2

sup
β∈Wρ
p ,U∈El
η ,
(X,W )∈Fm
κ

(cid:88)
(cid:88)

θ∈{−1,1}2k∗+1

θ∈{−1,1}2k∗+1

θ∈{−1,1}2k∗+1

|k| (cid:54) k∗.

(cid:88)

22k∗+1

22k∗+1

(cid:62) 1

· min

|k|(cid:54)k∗

|k|(cid:54)k∗

γν
k
2

b2
k,

sup

γν

(cid:62)

=

1

ν

4

ν

(cid:62) 1
8

|k|(cid:54)k∗

γν
k b2
k

(cid:62) 1
4

d(cid:52) ,

ρ
2

,

Λ

|(cid:104)(cid:101)β − βθ, ϕk(cid:105)|
|(cid:104)(cid:101)β − βθ, ϕk(cid:105)|2
|(cid:104)βθ − βθ(k), ϕk(cid:105)|
|(cid:104)βθ − βθ(k), ϕk(cid:105)| dPθ

(cid:112)dPθ(k)dPθ
(cid:17)1/2

.

(A.5)

2 H 2(Pθ(k), Pθ) combining (A.4) with (A.5) yields

θ(k)|(cid:104)(cid:101)β − βθ(k), ϕk(cid:105)|2(cid:111)

where the last inequality follows from (A.27) in Lemma A.4 together with λ+ (cid:54) Λ which
(cid:3)
completes the proof.

16

Proof of the upper bound.

Proof of Theorem 3.3. The proof is based on the decomposition (3.2), where we show
below under the condition (X, W ) ∈ F 32
η and (λk)k∈Z ∈ Sκ,d for some universal
constant C > 0 and I := {k ∈ Z : 8 λk > α γν

k} the following two bounds

η,τ , U ∈ E 16

E(cid:107)(cid:98)βν −(cid:101)βα
ν (cid:107)2

ν

(cid:54) C · η ·(cid:110) 1

1

α n

k∈I

n · ϕ(γν−p

+ d ·(cid:88)

(cid:111)·
· {(cid:107)β(cid:107)2 · E(cid:107)X(cid:107)2 + σ2} ·(cid:110) E(cid:107)X(cid:107)2
p ·(cid:16)

· E(cid:107)X(cid:107)2 · (cid:107)β(cid:107)2

p +

α n

)

k

C η
α n

+ 1

p and (X, W ) ∈ F 32

(cid:111)3

+ 1

,

(A.6)

(cid:111) ·(cid:110) E(cid:107)W(cid:107)2
(cid:17)

E(cid:107)W(cid:107)2

α n

.

α n

1 +

(A.7)
κ , i.e., E(cid:107)X(cid:107)2 (cid:54) Λ, E(cid:107)W(cid:107)2 (cid:54) Λ, and hence
(cid:105)4

(cid:111)· [σ2 + ρ· Λ]·(cid:104) Λ

+ κ(d 4 τ α)

+ 1

.

1

α n

+ d·(cid:88)

n · ϕ(γν−p

)

k

k∈I

E(cid:107)(cid:101)βα
ν − β(cid:107)2

ν

(cid:54) κ(d 4 τ α) · (cid:107)β(cid:107)2

E(cid:107)(cid:98)βν − β(cid:107)2

Consequently, for all β ∈ W ρ
λ+ (cid:54) Λ, follows

(cid:54) C · η ·(cid:110) 1
that 1/[α · n] (cid:54) (cid:52) · κ(δ∗) and(cid:80)

α n

ν

Let k∗ := k∗(n) ∈ N and δ∗ := δ∗(n) ∈ (0, 1] be given by (3.5) for some (cid:52) (cid:62) 1 then the
condition on α, that is α = 8 d Λ δ∗, implies I ⊂ {k ∈ Z : |k| (cid:54) k∗}. We conclude from (3.5)

E(cid:107)(cid:98)βν − β(cid:107)2

ν

k∈I 1/[n · ϕ(γν−p
(cid:54) C η d(cid:52) · κ(d 4 τ α)
κ(α/(8 d Λ))

k

)] (cid:54) (cid:52) · κ(δ∗), hence that

· [σ2 + ρ Λ] ·(cid:104)(cid:52) Λ κ(δ∗) + 1

(cid:105)4 · κ(δ∗).

Thereby, the condition (3.6), that is κ(d 4 τ α)/κ(α) = O(1) and κ(α)/κ(α/(8 d Λ)) = O(1),
as α tends to zero, implies the result.

Proof of (A.6). By using Tn,k introduced in (A.1) we obtain the identity

(cid:104)|Tn,k|2|(cid:98)ck/(cid:98)wk|21{(cid:98)wk (cid:62) α}

k}(cid:105)
1{(cid:98)λk (cid:62) α γν

.

k · E
γν

E(cid:107)(cid:98)βν −(cid:101)βα
ν (cid:107)2

ν =

(cid:88)

k∈Z

(cid:98)λ2

k

sum over I c := {k ∈ Z : 8 λk (cid:54) α γν

We partition the sum into two parts which we estimate separately using the bounds in
k together with (A.22) in Lemma A.3 we bound the

Lemma A.3. First by using (cid:98)λk (cid:62) αγν
·{(cid:107)β(cid:107)2·E(cid:107)X(cid:107)2+σ2}·(cid:110) E(cid:107)W(cid:107)2
k} by
(cid:88)
1{(cid:98)wk (cid:62) α}1{(cid:98)λk (cid:62) α γν
While due to the identity(cid:98)λk = |(cid:98)ck|2/(cid:98)wk1{(cid:98)wk (cid:62) α} together with (A.21) in Lemma A.3 the

k}(cid:105) (cid:54) C η E(cid:107)X(cid:107)2

(cid:104)|Tn,k|2|(cid:98)ck|2
α2 (cid:98)w2

n2 α2

k∈I c

α n

+1

E

k

(cid:111)3

.

sum over I := {k ∈ Z : 8 λk > α γν

(cid:88)

k∈I

k · E
γν

(cid:104)|Tn,m|2
(cid:98)wk ·(cid:98)λk
(cid:54)(cid:88)

k∈I

k} is bounded by

k}(cid:105)
1{(cid:98)wk (cid:62) α}1{(cid:98)λk (cid:62) α γν

· {(cid:107)β(cid:107)2 · E(cid:107)X(cid:107)2 + σ2} ·(cid:110) E(cid:107)X(cid:107)2

α n

(cid:111) ·(cid:110) E(cid:107)W(cid:107)2

α n

(cid:111)

.

+ 1

+ 1

C η γν
k
n λk

17

From λ+ (cid:62) 1 it follows that by combining the two parts of the sum we have

E(cid:107)(cid:98)βν −(cid:101)βα
ν (cid:107)2

ν

(cid:54) C · η ·(cid:110) 1

α n

+

(cid:111)·

k∈I c

γν
k λ+
λk n

(cid:88)
· {(cid:107)β(cid:107)2 · E(cid:107)X(cid:107)2 + σ2} ·(cid:110) E(cid:107)X(cid:107)2
ν = (cid:80)

ν − β(cid:107)2

α n

+ 1

(cid:111) ·(cid:110) E(cid:107)W(cid:107)2

(cid:111)3
kP ((cid:98)λk < α γν

+ 1

α n

k β2

.

k∈Z γν

Now the link condition (λk)k∈Z ∈ Sκ,d implies (A.6).

The proof of (A.7) is based on the identity E(cid:107)(cid:101)βα

k ),
where we partition the sum again into two parts which we estimate separately. First we
sum over I := {k ∈ Z : λk < 4 τ α γk}. Since λ+ (cid:62) 1, the link condition (λk)k ∈ Sκ,d
together with the monotonicity of κ shows that

(cid:88)

k∈I

(cid:88)

k∈I c

kP ((cid:98)λk < α γν

γν
k β2

k ) (cid:54)(cid:88)

k∈I

kP ((cid:98)λk < α γν

γν
k β2

k ) (cid:54) C η
α n

γp
kβ2

kκ(d

λk
γν
k λ+

· E(cid:107)X(cid:107)2 ·(cid:110)

1 +

) (cid:54) κ(d 4 τ α) ·(cid:88)
(cid:111)(cid:88)

E(cid:107)W(cid:107)2

k∈I

α n

k∈I c

γp
kβ2
k.

k β2
γν
k.

The sum over I c := {k ∈ Z : λk (cid:62) 4 τ α γk} we bound using (A.14) in Lemma A.2, that is

Combining the two parts of the sum we obtain (A.7), which completes the proof.

(cid:3)

Technical assertions.

η

wm
k

sup
k∈Z

η,τ and U ∈ E 4m

(cid:110) 1
(cid:110) λm

, m ∈ N. Then for some constant C > 0

The following four lemma gather technical results used in the proof of Proposition 3.1,
Theorem 3.2 and Theorem 3.3.
Lemma A.1. Suppose (X, W ) ∈ F 4m
only depending on m we have

· E|Tn,k|2m(cid:111) (cid:54) C · 1
nm · {(cid:107)β(cid:107)2m · (E(cid:107)X(cid:107)2)m + σ2m} · η,
(cid:12)(cid:12)(cid:12)(cid:98)wk − wk
(cid:12)(cid:12)(cid:12)2m (cid:54) C · 1
nm · η,
(cid:12)(cid:12)(cid:12)(cid:98)ck − ck
(cid:12)(cid:12)(cid:12)2m(cid:111) (cid:54) C · 1
nm · η.
Proof. Let ηik :=(cid:80)
l(cid:54)=k βlX il, ζik := βk{X ik − W ikck/wk}, τik := βkW ik{ck/wk −(cid:98)ck/(cid:98)wk},
n(cid:88)

i = 1, . . . , n and k ∈ Z. Then we have

{ηik + ζik + τik + σUi}Wik =: T1 + T2 + T3 + T4,

Tn,k =

(A.10)

sup
k∈Z

sup
k∈Z

k
xm
k

(A.9)

(A.8)

· E

wk

ck

E

1
n

i=1

where we bound below each summand separately, that is

k

E|Tj|2m (cid:54) C · wm
E|T4|2m (cid:54) C · wm

nm · (cid:107)β(cid:107)2m · (E(cid:107)X(cid:107)2)m · η,
nm · σ2m · η

k

j = 1, 2, 3,

(A.11)

(A.12)

18

for some C > 0 only depending on m. Consequently, the inequality (A.8) follows from (A.11)
and (A.12). Consider T1. For each k ∈ Z the random variables (ηik · Wik), i = 1, . . . , n,
are independent and identically distributed with mean zero. From Theorem 2.10 in Petrov
[1995] we conclude E|T1|2m (cid:54) Cn−mE|η1kW1k|2m for some constant C > 0 only depending
on m. Then we claim that (A.11) follows in case of T1 from the Cauchy-Schwarz inequality
together with (X, W ) ∈ F 4m
wk|4m (cid:54) η.
Indeed, we have

η,τ , i.e., supi,k E|Xik/

√

√
xk|4m (cid:54) η and supi,k E|Wik/
(cid:88)
m(cid:89)
k · (

|X1jl|2 (cid:54) (cid:107)β(cid:107)2m · wm

xj)m · η.

j(cid:54)=k

E|η1kW1k|2m (cid:54) (

(cid:88)

j(cid:54)=k

j )m(cid:88)

β2

··· (cid:88)

j1(cid:54)=k

jm(cid:54)=k

E|Wik|2m

l=1

Consider T2. (A.11) follows in analogy to the case of T1 since {ζik · Wik} are independent
and identically distributed with mean zero respectively, and E|ζ1k · W1k|2m (cid:54) C · β2m
k {xm
k ·
k {|ck|2mE|(cid:98)wk/wk − 1|2m + E|(cid:98)ck − ck|2m} for some
k · η, where C > 0 does only depend on m.
k · η + |ck|2mη} (cid:54) C · (cid:107)β(cid:107)2m · (E(cid:107)X(cid:107)2)m · wm
wm
C > 0 only depending on m, by the identity T3 = βk{(cid:98)wkck/wk −(cid:98)ck}. Therefore (A.11) in
Consider T3. We have E|T3|2m (cid:54) Cβ2m

case of T3 follows from (A.9) and (A.10). Consider T4. (A.12) follows in analogy to the
case of T1, because {σUi · Wik} are independent and identically distributed with mean zero
k · η, where C > 0 does only depend on m.
respectively, and E|σ · U1 · W1k|2m (cid:54) C · σ2m · wm
Proof of (A.9) and (A.10). Since {(|Wik|2/wk − 1)} and {(X ikWik − ck)} are indepen-
dent and identically distributed with mean zero respectively, where E||W1k|2/wk|2m (cid:54) η
k · η, for some C > 0 only depending on m, the result
and E|X 1kW1k − ck|2m (cid:54) C · xm
(cid:3)
follows by applying Theorem 2.10 in Petrov [1995], which proves the lemma.

k · wm

η,τ , m ∈ N, then for all 0 < d < 1 and some constant C > 0

Lemma A.2. Let (X, W ) ∈ F 4m
only depending on m we have

P ((cid:98)wk/wk < d) (cid:54) C ·

sup
k∈Z

nm · η.

1

(1 − d)2m · 1
· E(cid:107)X(cid:107)2 ·(cid:110)
(cid:111) (cid:54) C η
n2 ·(cid:110)

1 +

E(cid:107)W(cid:107)2

1 +

k ) (cid:54) C η
α n

α n
η,τ and I2 := {k ∈ Z : 8 λk (cid:54) α γν
k}. Then
(cid:111)2
E(cid:107)W(cid:107)2

· P ((cid:98)λk (cid:62) α γν

k )

Suppose (X, W ) ∈ F 8
constant C > 0 we have

sup
k∈I1

P ((cid:98)λk < α γν
(cid:110) λ2

sup
k∈I2

k
x2
k

While if (X, W ) ∈ F 16

(cid:111)

.

.

Proof. Since P ((cid:98)wk/wk < d) (cid:54) P (|(cid:98)wk/wk − 1| (cid:62) 1 − d) by applying Markov’s inequality

α n

the estimate (A.13) follows from (A.9) in Lemma A.1.

The proof of (A.14) is based on the elementary inequality

P ((cid:98)λk < α γν

k ) (cid:54) P ((cid:98)wk < α) + P ((cid:98)λk < α γν

k ∧ (cid:98)wk (cid:62) α),

19

η,τ and let I1 := {k ∈ Z : λk (cid:62) 4 τ α γν

k}. Then for some universal

(A.13)

(A.14)

(A.15)

where we show below for some universal constant C > 0 the following two bounds

which imply together (A.14). Since α/wk (cid:54) λk/wk · α γν
for all k ∈ I1, the estimate (A.16) follows from (A.13). The proof of (A.17) is based on

P ((cid:98)wk < α) (cid:54) C · 1
P ((cid:98)λk < α γν

n

k ∧ (cid:98)wk (cid:62) α) (cid:54) Cη

αn

· η,

sup
k∈I1

sup
k∈I1

· E(cid:107)X(cid:107)2 ·(cid:110)

1 +

which implies for all k ∈ I1 that

1 − 2

λk

(cid:98)λk

(cid:54)(cid:110)
(cid:111) ·(cid:110)|(cid:98)wk/wk − 1|2
4|(cid:98)ck/ck − 1|2 + 1
(cid:98)wk
1/4 (cid:54) 4|(cid:98)ck/ck − 1|2(cid:17)
(cid:16)
P ((cid:98)λk < α γν
k ∧ (cid:98)wk (cid:62) α) (cid:54) P
1/4 (cid:54)(cid:110)
4|(cid:98)ck/ck − 1|2 + 1

(cid:16)

+ P

E(cid:107)W(cid:107)2

(cid:111)
α n
k /λk (cid:54) τ α γν

+ |(cid:98)wk/wk − 1|(cid:111)

(A.16)

(A.17)

k /λk (cid:54) 1/4 holds true
+ 4|(cid:98)ck/ck − 1|2 (A.18)

(cid:111) ·(cid:110)|(cid:98)wk/wk − 1|2

α

+ |(cid:98)wk/wk − 1|(cid:111)(cid:17)

.

Therefore, by applying Markov’s inequality together with (A.9) and (A.10) in Lemma A.1
we obtain the estimate (A.17).

The proof of (A.15) is based on the decomposition
(cid:62) αγν
k
4λk

k ) (cid:54) P

which implies for all k ∈ I2 together with Markov’s inequality that

P ((cid:98)λk (cid:62) α γν
P ((cid:98)λk (cid:62) α γν
P ((cid:98)λk (cid:62) α γν

k ) (cid:54) 1
4

(cid:16)|(cid:98)ck/ck − 1|2
1{(cid:98)wk (cid:62) α}
(cid:98)wk/wk
(cid:104)|(cid:98)ck/ck − 1|4
(cid:105)
1{(cid:98)wk (cid:62) α}
|(cid:98)wk/wk|2
(cid:104)|(cid:98)ck/ck − 1|4{|(cid:98)wk/wk − 1|4

E

α2/w2
k

+ P

(cid:17)
(cid:16) 1{(cid:98)wk (cid:62) α}
(cid:98)wk/wk
(cid:16)(cid:98)wk/wk (cid:54) 1/2
(cid:17)
+ |(cid:98)wk/wk − 1|2 + 1}(cid:105)

.

+ P

Therefore, by using 1 (cid:54) 23{|(cid:98)wk/wk − 1|4 + |(cid:98)wk/wk|2|(cid:98)wk/wk − 1|2 + |(cid:98)wk/wk|2} we obtain
(cid:16)(cid:98)wk/wk (cid:54) 1/2

k ) (cid:54) 4E

+ P

(cid:17)

.

(cid:17)

(cid:62) αγν
k
4λk

Now (A.9) and (A.10) in Lemma A.1 and (A.13) gives (A.15), which completes the proof. (cid:3)

Lemma A.3. Suppose (X, W ) ∈ F 32
some universal constant C > 0 we have

(cid:110)

(cid:110)

sup
k∈Z

sup
k∈Z

λk · E

k · E
w2

η,τ and U ∈ E 16
η . Let I := {k ∈ Z : 8 λk (cid:54) α γν}, then for
(cid:105)(cid:111)

(cid:54) C η
n

(cid:104)|Tn,k|2|(cid:98)ck/ck − 1|21{(cid:98)wk (cid:62) α}

(cid:98)wk
· {(cid:107)β(cid:107)2 · E(cid:107)X(cid:107)2 + σ2} · E(cid:107)X(cid:107)2
(cid:104)|Tn,k|41{(cid:98)wk (cid:62) α}
(cid:105)(cid:111)
n2 · {(cid:107)β(cid:107)2 · E(cid:107)X(cid:107)2 + σ2}2 ·(cid:110) E(cid:107)W(cid:107)2

·(cid:110) E(cid:107)W(cid:107)2

(cid:98)w4

(cid:54) C η

α n

α n

n

k

(cid:111)4

+ 1

,

(A.20)

(cid:111)

+ 1

,

(A.19)

20

(cid:110)

λk · E

sup
k∈Z

(cid:110) 1

sup
k∈I

xk

· E

(cid:104) |Tn,k|2
(cid:98)wk ·(cid:98)λk
(cid:104)|Tn,k|2 |(cid:98)ck|2
(cid:98)w2

k}(cid:105)(cid:111)
1{(cid:98)wk (cid:62) α}1{(cid:98)λk (cid:62) α γν
· {(cid:107)β(cid:107)2 · E(cid:107)X(cid:107)2 + σ2} ·(cid:110) E(cid:107)X(cid:107)2
k}(cid:105)(cid:111)
1{(cid:98)wk (cid:62) α}1{(cid:98)λk (cid:62) α γν

(cid:54) C η
n

α n

k

n2 · {(cid:107)β(cid:107)2 · E(cid:107)X(cid:107)2 + σ2} ·(cid:110) E(cid:107)W(cid:107)2

α n

(cid:54) C η

(cid:111) ·(cid:110) E(cid:107)W(cid:107)2

α n

(cid:111)

,

+ 1

+ 1

(A.21)

(cid:111)3

+ 1

.

(A.22)

(A.23)

Proof. Consider the elementary inequality

By applying the Cauchy-Schwarz inequality it follows that

1 (cid:54) 2

(cid:110)|(cid:98)wk/wk − 1|2 + |(cid:98)wk/wk||(cid:98)wk/wk − 1| + |(cid:98)wk/wk|(cid:111)
(cid:104)|Tn,k|2|(cid:98)ck/ck − 1|21{(cid:98)wk (cid:62) α}
(cid:98)wk
E|Tn,k|4(cid:17)1/2(cid:16)
(cid:16)

E|(cid:98)ck/ck−1|8(cid:17)1/4(cid:110) (E|(cid:98)wk/wk − 1|8)1/2

(cid:105)

.

E

+

α2

w2
k

(cid:54) 2

+

1
w2
k

and, hence (A.8), (A.9) and (A.10) in Lemma A.1 imply (A.19).

(E|(cid:98)wk/wk − 1|4)1/2

(cid:111)1/2
The proof of (A.20) is similar to the proof of (A.19), but uses 1 (cid:54) 27{|(cid:98)wk/wk − 1|8 +
|(cid:98)wk/wk|4|(cid:98)wk/wk − 1|4 + |(cid:98)wk/wk|4} rather than (A.23) and we omit the details.
Proof of (A.21). Due to the elementary inequality 1 (cid:54) 2|(cid:98)ck/ck − 1|2 + 2|(cid:98)ck/ck|2 we have
(cid:105)
(cid:104)|Tn,k|2|(cid:98)ck/ck − 1|21{(cid:98)wk (cid:62) α}
(cid:104) |Tn,k|2
(cid:98)wk ·(cid:98)λk
(cid:104)(cid:12)(cid:12)(cid:12) Tn,k(cid:98)ck(cid:98)wk

1{(cid:98)wk (cid:62) α}1{(cid:98)λk (cid:62) α γν
(cid:12)(cid:12)(cid:12)2
k}(cid:105)
1{(cid:98)wk (cid:62) α}1{(cid:98)λk (cid:62) α γν
(cid:17)1/2 ·(cid:110)
(cid:16)
1{(cid:98)wk (cid:62) α}
(cid:98)w4

(E|(cid:98)ck/ck − 1|4)1/2 + |P ((cid:98)λk (cid:62) α γν

Proof of (A.22). By using the Cauchy-Schwarz inequality we obtain the decomposition

Consequently, (A.19) and (A.8) in Lemma A.1 lead to (A.21).

k}(cid:105) (cid:54) 2E

k )|1/2(cid:111)

|Tn,k|4w2

(cid:98)wk α γν

k

E|Tn,k|2

(cid:54) 2λk

E

k

k

+2

.

c2
k

E

E

.

Now (A.10) in Lemma A.1, (A.15) in Lemma A.2 and (A.20) imply (A.22), which completes
(cid:3)
the proof.

Lemma A.4. Let (λk)k∈Z be an element of Sκ,d deﬁned in (3.3) with λ+ := 1 ∨ maxk∈Z λk.
Consider k∗ ∈ N and δ∗ ∈ (0, 1] given in (3.5) for some (cid:52) (cid:62) 1. If we deﬁne

b2
k :=

ζ

n · λk

,

k ∈ Z,

with

ζ := min(cid:8)2σ2, ρ/(d(cid:52))(cid:9) ,

(A.24)

21

then we have

k ∈ Z,

(cid:88)
n
kλk (cid:54) 1,
2σ2 b2
(cid:54) ρ,
kγp
b2
(cid:88)

|k|(cid:54)k∗

k

b2
kγν
k

(cid:62) min

|k|(cid:54)k∗

(cid:26) 2σ2

d(cid:52) ,

ρ

(d(cid:52))2

(cid:27)

· κ(δ∗)

λ+

.

(A.25)

(A.26)

(A.27)

Proof. The inequality (A.25) follows trivially by using the deﬁnition of ζ.

Proof of (A.26). If ϕ denotes the inverse function of κ, then the link condition (λk) ∈

Sκ,d, can be rewritten as

d−1 (cid:54) |ϕ(γν−p

k

)|−1 λk
γν
k λ+

(cid:54) d.

(A.28)

Thereby, the monotonicity of (γν

k ) together with λ+ (cid:62) 1 implies

kγp
b2

k

(cid:54) ζ
n

γν
k λ+
λk

γp−v

k

|k|(cid:54)k∗

|k|(cid:54)k∗

(cid:54) ζ d(cid:52) (cid:54) ρ.
Proof of (A.27). By using the condition (A.28) together with the deﬁnition of δ∗ we

Thus (A.26) follows from the deﬁnition of k∗ given in (3.5), i.e.,(cid:80)|k|(cid:54)k∗ b2
have (cid:88)

|k|(cid:54)k∗

kγp

· κ(δ∗) · (cid:88)

)

k

k

.

kγν
b2
k

(cid:62) ζ
d λ+

|k|(cid:54)k∗

γp−v
k∗
nϕ(γν−p

k

)

|k|(cid:54)k∗

Consequently, the deﬁnition of k∗ given in (3.5) implies (A.27), which proves the lemma.(cid:3)

A.2 Proofs of Section 4

(cid:88)

· (cid:88)

(cid:54) ζ d · (cid:88)

γp−v
k∗
nϕ(γν−p

.

The ﬁnitely smoothing case.
Proof of Proposition 4.1. Consider the inverse function ϕ(t) = t(a+ν)/(p−ν) of κ. Then
k=1 kr (cid:16) mr+1 for r > 0 together with the deﬁnition of γk
. It follows that the condition on k∗

the well-known approximation(cid:80)m
given in (2.11) implies(cid:80)|k|(cid:54)k∗ 1/ϕ(γν−p

) (cid:16) γ(a+ν)+1/2

k∗

given in (3.5) of Theorem 3.2 can be rewritten as

1/n (cid:16) γν−p
k∗

1/ϕ(γν−p

)

k

= |ϕ(γν−p

k∗ )|[2(p+a)+1]/[2(a+ν)].

|k|(cid:54)k∗
From this δ∗ := ϕ(γν−p
k∗ ) implies δ∗ (cid:16) n−2(a+ν)/[2(p+a)+1] and κ(δ∗) (cid:16) n−2(p−ν)/[2(p+a)+1].
(cid:3)
Consequently, the lower bound in Proposition 4.1 follows by applying Theorem 3.2.
Proof of Proposition 4.2. Since the condition on α ensures α (cid:16) n−2(a+ν)/[2(p+a)+1] (cid:16) δ∗
(cid:3)
(see the proof of Proposition 4.1) the result follows from Theorem 3.3.

(cid:12)(cid:12)(cid:12) (cid:88)

k

(cid:12)(cid:12)(cid:12)−1 (cid:16) γp+a+1/2

k∗

22

The inﬁnitely smoothing case.
Proof of Proposition 4.3. Consider the inverse function ϕ(t) = exp{−ta/(ν−p)} of κ.
By applying Laplace’s Method (c.f. chapter 3.7 in Olver [1974]) the deﬁnition of γk given
k∗ ). It follows that by using δ∗ := ϕ(γν−p
k∗ )

in (2.11) implies (cid:80)|k|(cid:54)k∗ 1/ϕ(γν−p

) (cid:16) 1/ϕ(γν−p

the condition on k∗ given in (3.5) of Theorem 3.2 can be rewritten as

k

(cid:12)(cid:12)(cid:12) (cid:88)

|k|(cid:54)k∗

(cid:12)(cid:12)(cid:12)−1 (cid:16) γν−p

1/n (cid:16) γν−p
k∗

1/ϕ(γν−p

k

)

k∗ ϕ(γν−p

k∗ ) = δ∗κ(δ∗),

which implies κ(δ∗) (cid:16) ω(1/n), where ω denotes the inverse function of ω−1(t) = t · ϕ(t).
Therefore, the lower bound given in Proposition 4.3 follows from Theorem 3.2 together with
ω(t) = | log t|−(p−ν)/a(1 + o(1)) as t → 0 (c.f. Mair [1994]), which proofs the result
(cid:3)
Proof of Proposition 4.4. The proof is based on the decomposition (3.2), where we
bound the two right hand side terms by (A.2) derived in the proof of Proposition 3.1 and
(A.7) shown in the proof of Theorem 3.3 respectively. It follows that,

(cid:54) C η
α2 n

· Λ · {σ2 + ρΛ} +

C η
α n

1 +

Λ
α n

+ C · κ(d 4 τ α) · ρ

E(cid:107)(cid:98)βν − β(cid:107)2

for some positive constant C. Consequently, the condition α = c · n−1/4, c > 0 implies
ν = O(n−1/2) + O(n−3/4) + O(| log n−1/4|−(p−ν)/a) = O((log n)−(p−ν)/a)), which
(cid:3)

completes the proof.

· Λ · ρ ·(cid:16)

(cid:17)

E(cid:107)(cid:98)βν − β(cid:107)2

ν

References

R. Blundell and J. Horowitz. A nonparametric test of exogeneity. Review of Economic

Studies, 74(4):1035–1058, 2007.

R. Blundell, X. Chen, and D. Kristensen. Semi-nonparametric iv estimation of shape-

invariant engel curves. Econometrica, 75:1613–1670, 2007.

D. Bosq. Linear Processes in Function Spaces., volume 149 of Lecture Notes in Statistics.

Springer-Verlag, 2000.

H. Cardot, F. Ferraty, and P. Sarda. Spline estimators for the functional linear model.

Statistica Sinica, 13:571–591, 2003.

H. Cardot, A. Mas, and P. Sarda. Clt in functional linear regression models. Probability

Theory and Related Fields, 138:325–361, 2007.

M. Carrasco, J.-P. Florens, and E. Renault. Linear inverse problems in structural econo-
metrics: Estimation based on spectral decomposition and regularization. In J. Heckman
and E. Leamer, editors, Handbook of Econometrics, volume 6. North Holland, 2006.

X. Chen and M. Reiß. On rate optimality for ill-posed inverse problems in econometrics.

Econometric Theory, 27:497–521, 2011.

23

C. Crambes, A. Kneip, and P. Sarda. Smoothing splines estimators for functional linear

regression. The Annals of Statistics, 37(1):35–72, 2009.

S. Darolles, Y. Fan, J.-P. Florens, and E. Renault. Nonparametric instrumental regression.

Econometrica, 79(5):1541–1565, 2011.

P. H. Eilers and B. D. Marx. Flexible smoothing with b-splines and penalties. Statistical

Science, 11:89–102, 1996.

H. W. Engl, M. Hanke, and A. Neubauer. Regularization of inverse problems. Kluwer

Academic, Dordrecht, 2000.

F. Ferraty and P. Vieu. Nonparametric Functional Data Analysis: Methods, Theory, Ap-

plications and Implementations. Springer-Verlag, London, 2006.

J.-P. Florens, J. Johannes, and S. Van Bellegem. Identiﬁcation and estimation by penal-
ization in nonparametric instrumental regression. Econometric Theory, 27(3):472–496,
2011.

M. Forni and L. Reichlin. Let’s get real: A factor analytical approach to disaggregated

business cycle dynamics. Review of Economic Studies, 65:453–473, 1998.

P. Hall and J. L. Horowitz. Nonparametric methods for inference in the presence of instru-

mental variables. Annals of Statistics, 33:2904–2929, 2005.

P. Hall and J. L. Horowitz. Methodology and convergence rates for functional linear regres-

sion. Annals of Statistics, 35(1):70–91, 2007.

J. L. Horowitz and S. Lee. Nonparametric instrumental variables estimation of a quantile

regression model. Econometrica, 75:1191–1208, 2007.

J. Johannes. Deconvolution with unknown error distribution. Annals of Statistics, 37(5A):

2301–2323, 2009.

J. Johannes, S. Van Bellegem, and A. Vanhems. Convergence rates for ill-posed inverse

problems with an unknown operator. Econometric Theory, 27(3):522–545, 2011.

A. P. Korostolev and A. B. Tsybakov. Minimax Theory for Image Reconstruction., vol-

ume 82 of Lecture Notes in Statistics. Springer-Verlag, 1993.

B. A. Mair. Tikhonov regularization for ﬁnitely and inﬁnitely smoothing operators. SIAM

Journal on Mathematical Analysis, 25:135–147, 1994.

B. A. Mair and F. H. Ruymgaart. Statistical inverse estimation in Hilbert scales. SIAM

Journal on Applied Mathematics, 56(5):1424–1444, 1996.

H.-G. M¨uller and U. Stadtm¨uller. Generalized functional linear models. Annals of Statistics,

33:774–805, 2005.

M. Nair, S. V. Pereverzev, and U. Tautenhahn. Regularization in Hilbert scales under

general smoothing conditions. Inverse Problems, 21:1851–1869, 2005.

24

F. Natterer. Error bounds for Tikhonov regularization in Hilbert scales. Applicable Analysis,

18:29–37, 1984.

A. Neubauer. When do Sobolev spaces form a Hilbert scale? Procedings of the American

Mathematical Society, 103(2):557–562, 1988a.

A. Neubauer. An a posteriori parameter choice for tikhonov regularization in hilbert scales
leading to optimal convergence rates. SIAM Journal on Numerical Analysis, 25(6):1313–
1326, 1988b.

M. H. Neumann. On the eﬀect of estimating the error density in nonparametric deconvolu-

tion. Journal of Nonparametric Statistics, 7:307–330, 1997.

W. K. Newey and J. L. Powell. Instrumental variable estimation of nonparametric models.

Econometrica, 71:1565–1578, 2003.

F. Olver. Asymptotics and special functions. Academic Press, 1974.

V. V. Petrov. Limit theorems of probability theory. Sequences of independent random vari-

ables. Oxford Studies in Probability. Clarendon Press., Oxford, 4. edition, 1995.

C. Preda and G. Saporta. Pls regression on a stochastic process. Computational Statistics

& Data Analysis, 48:149 –158, 2005.

J. Ramsay and B. Silverman. Functional Data Analysis. Springer, New York, second ed.

edition, 2005.

J. O. Ramsay and C. J. Dalzell. Some tools for functional data analysis. Journal of the

Royal Statistical Society, Series B, 53:539–572, 1991.

U. Tautenhahn. Error estimates for regularization methods in Hilbert scales. SIAM Journal

on Numerical Analysis, 33(6):2120–2130, 1996.

A. B. Tsybakov. Introduction `a l’estimation non-param´etrique (Introduction to nonpara-

metric estimation). Math´ematiques & Applications (Paris). 41. Springer: Paris, 2004.

25

