6
1
0
2

 
r
a

 

M
4
1

 
 
]

G
C
.
s
c
[
 
 

1
v
9
0
1
4
0

.

3
0
6
1
:
v
i
X
r
a

Combinatorial rigidity of Incidence systems and

Application to Dictionary learning∗

Meera Sitharam

Mohamad Tariﬁ

Menghan Wang

March 15, 2016

Abstract

Given a hypergraph H with m hyperedges and a set Q of m pinning subspaces,
i.e. globally ﬁxed subspaces in Euclidean space Rd, a pinned subspace-incidence
system is the pair (H, Q), with the constraint that each pinning subspace in Q is
contained in the subspace spanned by the point realizations in Rd of vertices of
the corresponding hyperedge of H. This paper provides a combinatorial charac-
terization of pinned subspace-incidence systems that are minimally rigid, i.e. those
systems that are guaranteed to generically yield a locally unique realization.

Pinned subspace-incidence systems have applications in the Dictionary Learn-
ing (aka sparse coding) problem, i.e. the problem of obtaining a sparse represen-
tation of a given set of data vectors by learning dictionary vectors upon which
the data vectors can be written as sparse linear combinations. Viewing the dic-
tionary vectors from a geometry perspective as the spanning set of a subspace
arrangement, the result gives a tight bound on the number of dictionary vectors
for sufﬁciently randomly chosen data vectors, and gives a way of constructing a
dictionary that meets the bound. For less stringent restrictions on data, but a nat-
ural modiﬁcation of the dictionary learning problem, a further dictionary learning
algorithm is provided. Although there are recent rigidity based approaches for
low rank matrix completion, we are unaware of prior application of combinato-
rial rigidity techniques in the setting of Dictionary Learning. We also provide a
systematic classiﬁcation of problems related to dictionary learning together with
various algorithms, their assumptions and performance.

1 Introduction
A pinned subspace-incidence system (H, Q) is an incidence constraint system speci-
ﬁed as a hypergraph H together with a set Q of pinning subspaces in Rd, each speciﬁed
by a collection of basis vectors called pins. The pinning subspaces are in one-to-one
∗This research was supported in part by the research grant NSF CCF-1117695 and a research gift from
SolidWorks. Portions of this paper have appeared on Proceedings of 2014 Canadian Conference on Com-
putational Geometry and Post-Proceedings of 2014 International Workshop on Automated Deduction in
Geometry.

1

2

Figure 1: A pinned subspace-incidence framework with d = 3, projectivized in P2(R).

correspondence with the hyperedges of H. A realization of (H, Q) is a subspace ar-
rangement that assigns vectors in Rd to the vertices of H. Each hyperedge of H is
assigned the subspace spanned by its vertex vectors and contains the associated pin-
ning subspace in Q.

Pinned subspace-incidence systems naturally arise in ﬁnding bounds for the dictio-
nary learning (aka sparse coding) problem [35] for highly general data, i.e. the prob-
lem of obtaining a sparse representation of data vectors by learning dictionary vectors
upon which the data vectors can be written as sparse linear combinations. This set
of dictionary vectors can be viewed from a geometry perspective as the spanning set
of a subspace arrangement, where the subspaces contain the data vectors. The subset
of data vectors on a given subspace spans a pinning subspace. Thus the solution of
dictionary learning problem corresponds to a pinned subspace-incidence system.

The special case of 2-dimensional pinned line-incidence systems was also used in
modeling microﬁbirils in biomaterials such as cellulose and collagen [7]. Each such
microﬁbril is attached to some ﬁxed larger organelle/membrane at one site and cross-
linked at two sites with other ﬁbrils, where the cross-linking is like an incidence con-
straint that the crosslinked ﬁbrils can slide against each other while remaining incident.
Consequently, they can be modeled using a pinned line-incidence system with H being
a graph, where each ﬁbril is modeled as an edge of H with the two cross-linkings as its
two vertices, and the attachment is modeled as the corresponding pin.

Previous works on related types of frameworks include pin-collinear body-pin frame-
works [23], direction networks [50], slider-pinning rigidity [43], body-cad constraint
system [17], k-frames [47, 48], and afﬁne rigidity [15]. All of which involve some
form of incidence constraints. However, we are not aware of any previous results on
systems that are similar to pinned subspace-incidence systems.

2 Contributions

In this paper, we follow the combinatorial rigidity approaches [6, 47] to give a complete
combinatorial characterization for pinned subspace-incidence systems. Speciﬁcally,

• We formulate the pinned subspace-incidence systems as a nonlinear algebraic

3

system (H, Q)(p) and apply classic method of Asimow and Roth [6] by lineariz-
ing (H, Q)(p).

• We then apply another well-known method of White and Whiteley [47] to com-
binatorially characterize the rigidity of the underlying hypergraph H, using the
Laplace decomposition of the rigidity matrix, which corresponds to a map-decomposition [42]
of the underlying hypergraph. The polynomial resulting from the Laplace de-
composition is called the pure condition, which characterizes the conditions that
the framework has to avoid for the combinatorial characterization to hold.

To our best knowledge, the only known results with a similar ﬂavor are [17, 30]
which characterize the rigidity of Body-and-Cad frameworks. However, these results
are dedicated to speciﬁc frameworks in 3D instead of arbitrary dimension subspace
arrangements and hypergraphs, and their formulation process start directly with the
linearized Jacobian (omitting the ﬁrst bullet alone).

We then apply the combinatorial rigidity result to dictionary learning problems.
• As a corollary of the main result, we give a tight bound on the number of dic-
tionary vectors for sufﬁciently randomly chosen data vectors, and give a way of
constructing a dictionary that meets the bound (see Corollary 14 and 15).

• On the other hand, more common types of data can be handled using a standard
preprocessing step such as generalized PCA [46] that converts the dictionary
learning problem to a so-called ﬁtted dictionary learning problem that yields a
speciﬁc pinned subspace incidence system where a realization yields a dictio-
nary, followed by recursive decomposition of the underlying pinned subspace-
incidence system (referred to as DR-planning) to obtain the realization.

• We also provide a systematic classiﬁcation of problems related to Dictionary
Learning together with various approaches, assumptions required and perfor-
mance (see Section 6.1).

There are some recent applications of rigidity in machine learning [24, 27, 38, 31],
speciﬁcally for low rank matrix completability. These use the graph of entries of dis-
tance matrices and gram matrices, i.e., rigidity with respect to distance and inner prod-
uct constraints. We are however unaware of applications of rigidity with respect to
incidence constraints, or to dictionary learning.

3 Preliminaries

In this section, we introduce the formal deﬁnition of pinned subspace-incidence sys-
tems and basic concepts in combinatorial rigidity.

A hypergraph H = (V, E) is a set V of vertices and a set E of hyperedges, where
each hyperedge is a subset of V . The rank r(H) of a hypergraph H is the maximum
cardinality of any edge in E, i.e. r(H) = maxek∈E s(ek), where s(ek) denotes the
cardinality of the hyperedge ek. A hypergraph is s-uniform if all edges in E have the
same cardinality s, where 2-uniform hypergraph is called a graph G. A conﬁguration

4

or realization of a hypergraph H = (V, E) in Rd is a mapping from the vertices of H
to the vectors in Rd, i.e. p : V → Rd. When there is no ambiguity, we simply use pi to
denote the vector p(vi), p(ek) to denote the set of vectors {p(vi)|vi ∈ ek}, and sk to
denote the cardinality s(ek).
In the following, we use (cid:104)P(cid:105) to denote the subspace spanned by a set P of points
in Rd.
Deﬁnition 1 (Pinned Subspace-Incidence System). A pinned subspace-incidence sys-
tem in Rd is a pair (H, Q), where H = (V, E, m) is a weighted hypergraph of rank
r(H) < d, and Q = {q1, q2, . . . , q|E|} is a set of pinning subspaces (subspaces of Rd)
in one-to-one correspondence with the hyperedges of H. Here the weight assignment is
a function m : E → Z+, where m(ek) denotes the dimension of the pinning subspace
qk associated with the hyperedge ek. We may write m(qk) or simply mk in substitute
of m(ek). Often we ignore the weight m and just refer to the hypergraph (V, E) as H.
A pinned subspace-incidence framework realizing the pinned subspace-incidence
system (H, Q) is a triple (H, Q, p), where p is a realization of H, such that for all
pinning subspaces qk ∈ Q, qk is contained in (cid:104)p(ek)(cid:105), the subspace spanned by the set
of vectors realizing the vertices of the hyperedge ei corresponding to qk.

Since we only care about incidence relations, we projectivize the Euclidean space
Rd to treat the pinned subspace-incidence system in the real projective space Pd−1(R),
and refer to the vectors pi realizing the vertices of H as points in Pd−1(R) using the
same notation when the meaning is clear from the context. Figure 1 gives an example
of a pinned subspace-incidence framework in the real projective space Pd−1(R) with
d = 3, where the crosses denote the projectivized pins. As each pinning subspace is
spanned by two pins, m(e) = 2 for any hyperedge e.
Note: as the pinning subspaces in Q are treated as globally ﬁxed, the trivial motion
group of a pinned subspace-incidence system (H, Q) reduces to the identity.

Deﬁnition 2. A pinned subspace-incidence system (H, Q) is independent if none of the
polynomial constraints is in the real ideal generated by others, implying existence of a
realization. It is rigid if there exist at most ﬁnitely many realizations. It is minimally
rigid if it is both rigid and independent. It is globally rigid if there exists at most one
realization.

4 Algebraic Representation and Linearization

In the following, we use A[R, C] to denote a submatrix of a matrix A, where R and
C are respectively index sets of the rows and columns contained in the submatrix. In
addition, A[R, · ] represents the submatrix containing row set R and all columns, and
A[· , C] represents the submatrix containing column set C and all rows.

4.1 Representation Using Polynomials
For any hyperedge ek = {vk
set {pk

2 , . . . , vk|ek|}, the subspace (cid:104)p(ek)(cid:105) spanned by the point
2, . . . , pk|ek|} is constrained to contain the pinning subspace qk associated with

1 , vk

1, pk

5

1, xk

. . . pk

. . . xk

ek. Recall that qk is a subspace of dimension mk−1 in Pd−1(R) spanned by a set of mk
pins {xk
}, and the incidence constraint is equivalent to requiring (cid:104)p(e)(cid:105)
2, . . . , xk
mk
l , for 1 ≤ l ≤ mk.
to contain each pin xk
Using homogeneous coordinates pk
l,1 xk
l,2

pk
i,2
[ xk
ting all the |ek| × |ek| minors of the |ek| × (d − 1) matrix

l,d−1 ], we write this incidence constraint for each pin xk

l =
l by let-

i,d−1 ] and xk

i = [ pk
i,1

1 − xk
pk
2 − xk
pk


(cid:1) equations. Note that any d − |ek| of
(cid:1) equations are independent and span the rest. So we can write the incidence


(cid:1) minors, giving(cid:0)d−1|ek|
be zero. There are(cid:0)d−1|ek|
these(cid:0)d−1|ek|
l [· , C(t)](cid:1) = 0,
det(cid:0)Ek

constraint as (d − |ek|) independent equations:

1 ≤ t ≤ d − |ek|

(1)

pk|ek| − xk

Ek

l =

...

l

l

l

where C(t) denote the following index sets of columns in E:

C(t) = {1, 2, . . .|ek| − 1} ∪ {|ek| − 1 + t},

1 ≤ t ≤ d − |ek|

In other words, C(t) contains the ﬁrst |ek|−1 columns together with column |ek|−1+t.
Now the incidence constraint for the pinning subspace qk is represented as mk(d−
|ek|) equations for all the mk pins {xk
}. Consequently, the pinned subspace-
k=1 mk(d − |ek|) equations, each

incidence problem reduces to solving a system of(cid:80)|E|

2, . . . xk

1, xk

mk

of form (1). We denote this algebraic system by (H, Q)(p) = 0.

4.2 Linearization and Genericity
We are interested in characterizing minimal rigidity of pinned subspace-incidence sys-
tems. However, checking independence relative to the ideal generated by the variety is
computationally hard and best known algorithms, such as computing Gr¨obner basis, are
exponential in time and space [33]. So we deﬁne the notion of rigidity for frameworks
(H, Q, p), which is equivalent to maximal rank of the Jacobian of (H, Q)(p).

Deﬁnition 3. A pinned subspace-incidence framework (H, Q, p) is rigid if there exists
a neighborhood N (p) such that (H, Q, p) is the only framework realizing (H, Q) in
N (p). A rigid pinned subspace-incidence framework (H, Q, p) is minimally rigid if it
is no longer rigid after removing any pin.

A generic framework with respect to a property P, when viewed as a point in an ap-
propriate real space, avoids a measure-zero set NP of the ambient space of frameworks
that depends only on the underlying weighted hypergraph. This implies the following
notion of genericity:

6

Deﬁnition 4. A pinned subspace-incidence framework (H, Q, p) is generic w.r.t. a
property P if and only if there exists a neighborhood N (Q, p) such that for all frame-
works (H, Q(cid:48), p(cid:48)) with (Q(cid:48), p(cid:48)) ∈ N (Q, p), (H, Q(cid:48), p(cid:48)) satisﬁes P if and only if (H, Q, p)
satisﬁes P.

Furthermore we can deﬁne when a property is generic, i.e. becomes a property of

the hypergraph underlying a geometric constraint system.
Deﬁnition 5. A property P is generic (i.e, becomes a property of the underlying
weighted hypergraph alone) if for any weighted hypergraph H = (V, E, m), either
all generic (w.r.t. P) frameworks (H, Q, p) satisfy P, or all generic (w.r.t. P) frame-
works (H, Q, p) do not satisfy P.

The primary activity of the area of combinatorial rigidity is to give purely com-
binatorial characterizations of generic properties P. In practice, the set NP deﬁning
genericity is usually not speciﬁed, as long as it is of measure zero in the ambient space
of frameworks. The measure-zero set NP may include zero-sets of some polynomi-
als called pure conditions that appear in the process of drawing such combinatorial
characterizations (we will see this in the proof of Theorem 12).

We deﬁne the generic rigidity of a pinned subspace-incidence system to be the

rigidity of a generic framework.

Deﬁnition 6. A pinned subspace-incidence system (H, Q) is generically (minimally)
rigid if some generic framework (H, Q, p) realizing (H, Q) is (minimally) rigid.

4.2.1 Linearization as Rigidity Matrix

Next we follow the approach taken by traditional combinatorial rigidity theory [6, 16]
to show that rigidity and independence (based on nonlinear polynomials) of pinned
subspace-incidence systems are generically properties of the underlying weighted hy-
pergraph H, and can furthermore be captured by linear conditions in an inﬁnitesi-
mal setting. Speciﬁcally, we give a lemma showing that generic rigidity of a pinned
subspace-incidence system is equivalent to existence of a full rank rigidity matrix, ob-
tained by taking the Jacobian of the algebraic system (H, Q)(p) = 0 at a generic
framework (H, Q, p) realizing (H, Q).

A rigidity matrix of a framework (H, Q, p) is the whose kernel is the inﬁnitesimal
motions (ﬂexes) of (H, Q, p). A framework is inﬁnitesimally rigid if the rigidity matrix
has full rank. To deﬁne a rigidity matrix for a pinned subspace-incidence framework
(H, Q, p), we take the Jacobian of the algebraic system (H, Q)(p) = 0 by taking
partial derivatives with respect to the coordinates of pi’s. In the Jacobian, each vertex
vi has d − 1 corresponding columns, and each pinning subspace qk associated with the
2 , . . . , vk|ek|} has mk(d − |ek|) corresponding rows, where each
hyperedge ek = {vk
l , gives the following

l [· , C(t)](cid:1) = 0 (1), i.e. equation t of the pin xk

equation det(cid:0)Ek

1 , vk

row (the columns corresponding to vertices not in ek are all zero):

(cid:20)

0, . . . , 0,

. . . , 0,

. . . . . .

. . . , 0,

l [· , C(t)]
l [· , C(t)]

1,1

∂ det Ek
∂pk

∂ det Ek
∂pk

2,1

l [· , C(t)]
l [· , C(t)]

1,2

∂ det Ek
∂pk

∂ det Ek
∂pk

2,2

, . . . ,

, . . . ,

∂ det Ek
∂pk

∂ det Ek
∂pk

∂ det Ek

l [· , C(t)]
∂pk|ek|,1

∂ det Ek

l [· , C(t)]
∂pk|ek|,2

, . . . ,

∂ det Ek

∂pk|ek|,d−1

7

, 0, . . .

, 0, . . .

(cid:21)

(2)

, 0, . . . , 0

l [· , C(t)]
1,d−1
l [· , C(t)]
2,d−1
l [· , C(t)]

Let V k be the matrix whose rows are coordinates of pk

1, pk

2, . . . , pk|ek|:

p1,1
p2,1
...

p1,2
p2,2
...

p|ek|,1

p|ek|,2

p1,d−1
p2,d−1

. . .
. . .
...
. . . p|ek|,d−1

...



,

,

,



i , i.e. xl =(cid:80)|ek|
(cid:104)

rk
t,l =

0, . . . , 0, 0, Dk

. . . , 0, 0, Dk

. . . . . .

t,j (1 ≤ j ≤ d − 1) be the matrices obtained from V k

t be the V k[· , C(t)], i.e. the |ek|×|ek| submatrix of V k containing only columns
Let V k
t by replacing
in C(t). Let V k
the column corresponding to coordinate j with the all-ones vector (1, 1, . . . , 1) for
j ∈ C(t), and the zero matrix for j /∈ C(t). Let Dk
t,j. Let
(1 ≤ i ≤ |ek|) be the barycentric coordinates of the pin xl with respect to the
bk,l
i
points pk
i . Now (2) can be rewritten in the following simpliﬁed
form:

t,j be the determinant of V k

i=1 bk,l

i pk

t,1bk,l
t,1bk,l

1 , Dk
2 , Dk

t,2bk,l
t,2bk,l

1 , . . . , Dk
2 , . . . , Dk

t,d−1bk,l
t,d−1bk,l

1 , 0, 0,

2 , 0, 0, . . .

(cid:105)

. . . , 0, 0, Dk

Each vertex vk

t,1bk,l|ek|, Dk

i has the entries Dk

t,d−1bk,l|ek|, 0, 0, . . . , 0

t,2bk,l|ek|, . . . , Dk
, 1 ≤ j ≤ d − 1 in its d − 1 columns, among
which exactly |ek| entries with j ∈ C(t), i.e. the ﬁrst |ek| − 1 columns together with
column |ek|− 1 + t, are generically non-zero. Note that the terms Dk
t,|ek|−1+t are equal
for all t, so we may just use Dk to denote it.

For each 1 ≤ t ≤ d−|ek|, there are mk rows as (3), where each pin xk

l corresponds
t,l for 1 ≤ l ≤ mk. These mk rows have exactly the same row pattern

to the row rk

t,jbk,l

(3)

i

8

v|ek|,2
t,2bk,1|ek|
Dk
t,2bk,2|ek|

Dk

. . .
. . . Dk

. . . Dk

v|ek|,d−1
t,d−1bk,1|ek|
t,d−1bk,2|ek|

. . .

. . .

v|ek|,1
t,1bk,1|ek|
t,1bk,2|ek|

. . . Dk

. . . Dk

. . .

. . .

...



. . . Dk

t,1bk,mk

1

Dk

t,2bk,mk

1

. . . Dk

t,d−1bk,mk

1

. . .

. . . Dk

t,1bk,mk
|ek|

Dk

t,2bk,mk
|ek|

. . . Dk

t,d−1bk,mk
|ek|

. . .

except for different bk,l

’s:

i

v1,1
t,1bk,1
t,1bk,2

1

1

. . . Dk

. . . Dk

v1,2
t,2bk,1
t,2bk,2

1

1

Dk

Dk

. . .
. . . Dk

. . . Dk

v1,d−1
t,d−1bk,1
t,d−1bk,2

1

1



Example 1. For d = 4, consider a pinning subspace q with m(q) = 2 associated with
the hyperedge e = {v1, v2}. The pinning subspace has the following m(q)·(d−|e|) = 4
rows in the simpliﬁed Jacobian (the index k is omitted):



t = 1, l = 1
t = 1, l = 2
t = 2, l = 1
t = 2, l = 2

v1,1

v1,2
1 Db1
. . . D1,1b1
1
1 Db2
. . . D1,1b2
1
. . . D2,1b1
0
1
. . . D2,1b2
0
1

v1,3
0
0

Db1
1
Db2
1

v2,1

v2,2
2 Db1
. . . D1,1b1
2
2 Db2
. . . D1,1b2
2
. . . D2,1b1
0
2
. . . D2,1b2
0
2

v2,3
0
0

Db1
2
Db2
2

. . .
. . .
. . .
. . .



where each row has form (3). It is a matrix of size(cid:80)

We deﬁne the rigidity matrix M (H, Q, p) or simply M (p) for a pinned subspace-
incidence framework (H, Q, p) to be the simpliﬁed Jacobian matrix obtained above,
k mk(d − |ek|) by n(d − 1). In
general, we use M to denote the rigidity matrix of a generic framework (H, Q, p) with
respect to inﬁnitesimal rigidity.
Lemma 7. Inﬁnitesimal rigidity of a generic subspace-incidence framework (H, Q, p)
is equivalent to rigidity of (H, Q, p), thus generic rigidity of the system (H, Q).

Proof. The proof of Lemma 7 follows the approach taken by traditional combinatorial
rigidity [6].

First we show that if a framework (H, Q, p) is generic, inﬁnitesimal rigidity im-
plies rigidity. Consider the polynomial system (H, Q)(p) of equations. The Implicit
Function Theorem states that there exists a function g, such that p = g(Q) on some
open interval, if and only if the rigidity matrix M has full rank. Therefore, if the frame-
work is inﬁnitesimally rigid, the solutions to the algebraic system are isolated points
(otherwise g could not be explicit). Since the algebraic system contains ﬁnitely many
components, there are only ﬁnitely many such solution and each solution is a 0 di-
mensional point. This implies that the total number of solutions is ﬁnite, which is the
deﬁnition of rigidity.

To show that generic rigidity implies generic inﬁnitesimal rigidity, we take the con-
trapositive: if a generic framework is not inﬁnitesimally rigid, we show that there is a
ﬁnite ﬂex. If (H, Q, p) is not inﬁnitesimally rigid, then the rank r of the rigidity matrix
M is less than (d − 1)|V |. Let E∗ be a set of edges in H such that |E∗| = r and the
corresponding rows in M are all independent. In M [E∗, · ], we can ﬁnd r independent
columns. Let p∗ be the components of p corresponding to those r independent columns

9

and p∗⊥ be the remaining components. The r-by-r submatrix M [E∗, p∗], made up of
the corresponding independent rows and columns, is invertible. Then, by the Implicit
Function Theorem, in a neighborhood of p there exists a continuous and differentiable
function g such that p∗ = g(p∗⊥). This identiﬁes p(cid:48), whose components are p∗ and the
level set of g corresponding to p∗, such that (H, Q)(p(cid:48)) = 0. The level set deﬁnes the
ﬁnite ﬂexing of the framework. Therefore the system is not rigid.

5 Combinatorial Rigidity Characterization
5.1 Required Hypergraph Properties
This section introduces pure hypergraph properties and deﬁnitions that will be used in
stating and proving our main theorem.
Deﬁnition 8. A hypergraph H = (V, E) is (k, 0)-sparse if for any V (cid:48) ⊂ V , the
induced subgraph H(cid:48) = (V (cid:48), E(cid:48)) satisﬁes |E(cid:48)| ≤ k|V (cid:48)|. A hypergraph H is (k, 0)-
tight if H is (k, 0)-sparse and |E| = k|V |.

This is a special case of the (k, l)-sparsity condition that was widely studied in the
geometric constraint solving and combinatorial rigidity literature. A relevant concept
from graph matroids is map-graph, deﬁned as follows.

Deﬁnition 9. An orientation of a hypergraph is given by identifying as the tail of each
edge one of its endpoints. The out-degree of a vertex is the number of edges which
identify it as the tail and connect v to V − v. A map-graph is a hypergraph that admits
an orientation such that the out degree of every vertex is exactly one.

The following lemma from [42] follows Tutte-Nash Williams [34, 44] to give a

useful characterization of (k, 0)-tight graphs in terms of maps.

Lemma 10. A hypergraph H is composed of k edge-disjoint map-graphs if and only
if H is (k, 0)-tight.

Our characterization of rigidity of a weighted hypergraph H is based on map-

decomposition of a multi-hypergraph (cid:98)H obtained from H.
hypergraph (cid:98)H = (V,(cid:98)E) is obtained by replacing each hyperedge ek in E with a set
A labeling of a multi-hypergraph (cid:98)H gives a one-to-one correspondence between

Deﬁnition 11. Given a weighted hypergraph H = (V, E, m), the associated multi-
Ek of mk(d − |ek|) copies of multi-hyperedges.

Ek and the set Rk of mk(d−|ek|) rows for the hyperedge ek in the rigidity matrix M,
where the multi-hyperedge corresponding to the row rk
Note: an alternative representation commonly adopted in geometric constraint solv-
ing [4, 13, 20] is to represent H as a bipartite graph B(H), with d − 1 copies of each
vertex in V as one of its vertex set, and mk(d − |ek|) copies of each hyperedge in E
as the other vertex set. A combinatorial rigidity characterization can be equivalently
stated using either ﬂow based conditions on B(H) or hypergraph sparsity conditions

t,l is labeled ek
t,l.

on (cid:98)H [21, 22, 25]. However, such an equivalence of the two combinatorial properties

has no bearing on the proof of equivalence of either combinatorial property to generic
rigidity, an algebraic property. Showing that the combinatorial property generically
implies the algebraic property is the substance of the proof of the theorem. This is
generally called the “Laman direction” and it is in fact where the hardness of every
combinatorial characterization of rigidity lies.

10

5.2 Characterizing Rigidity
In this section, we apply [47] to give combinatorial characterization for minimal rigid-
ity of pinned subspace-incidence systems.

Theorem 12 (main theorem). A pinned subspace-incidence system with pins being in
general position is generically minimally rigid if and only if:

(1) The underlying weighted hypergraph H = (V, E, m) satisﬁes(cid:80)|E|
|ek|) = (d− 1)|V |, and(cid:80)
k=1 mk(d −
ek∈E(cid:48) mk(d−|ek|) ≤ (d− 1)|V (cid:48)| for every vertex in-
(cid:98)H = (V,(cid:98)E) has a decomposition into (d − 1) maps.
duced subgraph H(cid:48) = (V (cid:48), E(cid:48)). In other words, the associated multi-hypergraph
(2) There exists a labeling of (cid:98)H compatible with the map-decomposition (deﬁned

with l1 = l2 are not contained in the

t1,l1

t2,l2

with t1 = t2 do not have the same

later) such that in each set Ek of multi-hyperedges,
(2a) two multi-hyperedges ek
same map in the map-decomposition,
(2b) two multi-hyperedges ek
and ek
vertex as tail in the map-decomposition.

and ek

t1,l1

t2,l2

matrix M, which corresponds to decomposing the (d− 1, 0)-tight multi-hypergraph (cid:98)H

To prove Theorem 12, we apply Laplace expansion to the determinant of the rigidity
as a union of d − 1 maps. We then prove det(M ) is not identically zero by showing
that the minors corresponding to each map are not identically zero, as long as a certain
polynomial called pure condition is avoided by the framework.

A Laplace expansion rewrites the determinant of the rigidity matrix M as a sum
of products of determinants (brackets) representing each of the coordinates taken sep-
arately. In order to see the relationship between the Laplace expansion and the map-
decomposition, we ﬁrst group the columns of M into d−1 column groups Cj according
to the coordinates, where columns for the ﬁrst coordinate of each vertex belong to C1,
columns for the second coordinate of each vertex belong to C2, etc.
Example 2. For d = 4, consider a pinning subspace q with m(q) = 2 associated with
the hyperedge e = {v1, v2}. The regrouped rigidity matrix has d − 1 = 3 column

groups, where q has the following 4 rows (the index k is omitted):



t = 1, l = 1
t = 1, l = 2
t = 2, l = 1
t = 2, l = 2

v1,1

. . . D1,1b1
. . . D1,1b2
. . . D2,1b1
. . . D2,1b2

v2,1
1 . . . D1,1b1
1 . . . D1,1b2
1 . . . D2,1b1
1 . . . D2,1b2

2 . . .
2 . . .
2 . . .
2 . . .

v1,2
. . . Db1
. . . Db2

v2,2
1 . . . Db1
1 . . . Db2

2 . . .
2 . . .

11

v1,3

v2,3

. . . Db1
. . . Db2

1 . . . Db1
1 . . . Db2

2 . . .
2 . . .



We have the following observation on the pattern of the regrouped rigidity matrix.
Observation 1. In the rigidity matrix M with columns grouped into column groups,
in (cid:98)H. In a column group j where j ≤ |ek| − 1, each row associated with ek contains
a hyperedge ek has mk(d − |ek|) rows, each associated with a multi-hyperedge of ek
|ek| nonzero entries at the columns corresponding to vertices of ek. In a column group
t,l with |ek| − 1 + t = j, each containing |ek|
j where j ≥ |ek|, there are mk rows rk
nonzero entries at the columns corresponding to vertices of ek, while the remaining
A labeling of (cid:98)H compatible with a given map-decomposition can be obtained as
rows are all zero.

following. We start from the last column group of M and associate each column group
j with a map in the map-decomposition. For each multi-hyperedge of the map that is
a copy of the hyperedge ek, we pick a row rk
t,j that is not all zero in column groups j
and label the multi-hyperedge as ek
t,j. By the above observation, this is always possible
if each map contains at most mk multi-hyperedges of the same hyperedge ek, which

must be true if there exists any labeling of (cid:98)H satisfying Theorem 12(2a).

In the Laplace expansion

±(cid:89)

j

(cid:88)

σ



det(M ) =

det M [Rσ

j , Cj]

(4)

the sum is taken over all partitions σ of the rows into d − 1 subsets Rσ
. . . , Rσ
rows Rσ
each row has a common coefﬁcient Dk

2 , . . . , Rσ
j ,
d−1, each of size |V |. In other words, each summation term of (4) contains |V |
j from each column group Cj. Observe that for any submatrix M [Rσ
j , Cj],

1 , Rσ

t,j, so

 (cid:89)

t,j∈Rσ
rk

j

 det(M(cid:48)[Rσ

det(M [Rσ

j , Cj]) =

Dk
t,j

j , Cj])

where each row of M(cid:48)[Rσ

j , Cj] is either all zero, or of the pattern

2 , 0, . . . , bk,l|ek|, 0, . . . , 0]
with non-zero entries only at the |ek| indices corresponding to vk

[0, . . . , 0, bk,l

1 , bk,l

For a ﬁxed σ, we refer to a submatrix M [Rσ

j , Cj] simply as Mj.

i ∈ ek.

(5)

12

(a)

(b)

Figure 2: (a) A minimally rigid pinned subspace-incidence system in d = 3. (b) A map-
decomposition of the multi-hypergraph of the system in (a), where differently drawn
multi-hyperedges are in different maps, and the tail vertex of each multi-hyperedge is
pointed to by an arrow.

Example 3. Figure 2a shows a pinned subspace-incidence system in d = 3 with
4 vertices and 5 hyperedges, where e1 = {v1}, e2 = {v2}, e3 = {v1, v3}, e4 =
{v2, v4}, e5 = {v3, v4}, and mk = 1 for all 1 ≤ k ≤ 5 except that m5 = 2. Fig-

ure 2b gives a map-decomposition of the multi-hypergraph (cid:98)H of (a). The labeling of
generically minimally rigid, as the map-decomposition and labeling of (cid:98)H satisﬁes the

multi-hyperedges is given in the regrouped rigidity matrix (6), where the shaded rows
inside the column groups constitute the submatrices M σ
in the summation term of
j
the Laplace decomposition corresponding to the map-decomposition. The system is

conditions of Theorem 12.

Proof of main theorem. First we show the only if direction. For a generically mini-
mally rigid pinned subspace-incidence framework, the rigidity matrix M is generically
full rank, so there exists at least one summation term σ in (4) where each submatrix
Mj is generically full rank. As the submatrices don’t have any overlapping rows with

(6)

v1v2v3v4e1e2e3e4e5v1v2v3v4E1E2E3E4E513

each other, we can perform row elimination on M to obtain a matrix N with the same
rank, where all submatrices Mj are simultaneously converted to a permuted reduced
row echelon form Nj, where each row in Nj has exactly one non-zero entry βj
i at a
unique column i, In other words, all Nj’s can be converted simultaneously to reduced
row echelon form by multiplying a permutation matrix on the left of N. Now we can

obtain a map-decomposition of (cid:98)H by letting each map j contain multi-hyperedges cor-

t1,l and ek

Condition (2a): assume two multi-hyperedges ek

responding to rows of the submatrix Nj, and assigning each multi-hyperedge in map j
the vertex i corresponding to the non-zero entry βj
i in the associated row in Nj as tail.
In addition, such a map-decomposition must satisfy the conditions of Theorem 12(2):
t2,l are in the same map j,
i.e. the rows corresponding to these two edges are included in the same submatrix Mj.
If j > s − 1, one of these rows must be all-zero in Mj by Observation 1, contradicting
the condition that Mj is full rank. If j ≤ s − 1, both of these rows in Mj will be a
multiple of the same row vector (5), contradicting the condition that Mj is full rank.
Condition (2b): note that the rows in M corresponding to multi-hyperedges ek
have the exactly the same pattern except for different values of b’s. If ek

and ek
has vertex i as tail, after the row elimination, the column containing bk,l1
the only non-zero entry of column i for Nj1, while the column containing bk,l2
become zero in all column groups, thus i cannot be assigned as tail for ek

t,l1
i will become
i will

t,l2

t,l1

.

t,l2

rigidity.

Next we show the if direction, that the conditions of Theorem 12 imply inﬁnitesimal

ditions of Theorem 12, we can obtain summation term σ in the Laplace decomposi-

Given labeled multi-hypergraph (cid:98)H with a map-decomposition satisfying the con-
tion (4) according to the labeling of (cid:98)H, where each submatrix Mj contain all rows
map-graph, the function τ : (cid:98)E → V assigning a tail vertex to each multi-hyperedge is a

First, it is not hard to show that each submatrix Mj is generically full rank [49].
For completeness, we give a short proof as following. According to the deﬁnition of a

corresponding to the map associated with column group j.

t,l), which is a polynomial in bk,l

one-to-one correspondence. We perform symbolic row elimination of the matrix M to
simultaneously convert each Mj to its permuted reduced row echelon form Nj, where
for each row of Nj, all entries are zero except for the entry βk
t,l corresponding to the
vertex τ (ek
’s in the submatrix Mj. Since Mj cannot
contain two rows with the same k and l by Condition (2a), the bk,l
’s in different rows
t,l (cid:54)= 0 under a generic specialization of
of a same map are independent of each other, βk
bk,l
. Since each row of Nj has exactly one nonzero entry and the nonzero entries from
different rows are on different columns, the |V | × |V | matrix Nj is clearly full rank.
i
Thus Mj must also be generically full rank.

i

i

We conclude that

det(M ) =

(cid:88)

±(cid:89)

(cid:32)(cid:16) (cid:89)

(cid:17)

where the sum is taken over all σ corresponding to a map-decomposition of (cid:98)H. Generi-

cally, the summation terms of the sum (7) do not cancel with each other, since det(M(cid:48)[Rσ

t,j∈Rσ
rk

σ

j

j

j , Cj])

Dk
t,j

det M(cid:48)[Rσ

j , Cj]

(7)

(cid:33)

14

(a)

(b)

Figure 3: (a) A pinned subspace-incidence system in d = 4. (b) A map-decomposition
of the multi-hypergraph of the system in (a), where multi-hyperedges with different
patterns are in different maps, and the tail vertex of each multi-hyperedge is pointed to
by an arrow.

are independent of the multi-linear coefﬁcients(cid:81)
Condition (2b). This implies that(cid:99)M is generically full rank.

t,j because of the require-
ment that the pins are in general position, and any two rows of M are independent by

t,j∈Rσ
rk

The polynomial (7) gives the pure condition characterizing non-generic frame-

works.

Dk

j

5.2.1 Pure condition

The pure condition (7) obtained in the proof of Theorem 12 vanishes at a measure-
zero subset of frameworks. For those frameworks, the above characterization fails.
However, the geometric meaning of the pure condition is not completely clear.

example, H with a subgraph (V (cid:48), E(cid:48)), |V (cid:48)| < d such that(cid:80)

Note that there exist combinatorial types of underlying weighted hypergraphs, for
ek∈E(cid:48) mk > |V (cid:48)|, that
force the pins to lie in a non-generic position no matter how the vertices are realized.
We rule out such systems by the requirement that pins being in general position in the
statement of Theorem 12.

As an example, Figure 3a shows a pinned subspace-incidence system in d = 4 with
4 vertices and 5 hyperedges, where mk is 2 for k = 5 and is 1 otherwise. A map-

decomposition of the multi-hypergraph (cid:98)H of the system is given in Figure 3b, and
we can easily ﬁnd a labeling of (cid:98)H satisfying Conditions (1) and (2) in Theorem 12.

However, the system is overconstrained, as generically the four pins associated with
e1, e4 and e5 will not fall on the same plane in P3(R). This situation is both captured
by the pure condition and is ruled by the requirement of Theorem 12 that the pins being
in general position.

Figure 4 shows a more standard non-generic example captured by the pure con-
dition. Frameworks (a) and (b) are two pinned subspace incidence frameworks with
d = 4, and they have the same underlying weighted hypergraph satisfying the combi-
natorial characterization of the main theorem. However, framework (a) is minimally
rigid but (b) is not, since the pins on hyperedges e1 and e2 in (b) lie on the same line.
Evaluating the pure condition at framework (b) shows that it is not generic.

v1v2v3v4e1e2e3e4e5v1v2v3v4E1E2E3E4E515

Figure 4: Two pinned subspace-incidence frameworks in d = 4 with the same under-
lying weighted hypergraph, where (a) is generic but (b) is not generic.

6 Application: Dictionary Learning

Dictionary Learning (aka Sparse Coding) is the problem of obtaining a sparse repre-
sentation of data points, by learning dictionary vectors upon which the data points can
be written as sparse linear combinations. The Dictionary Learning problem arises in
various context(s) such as signal processing and machine learning.
Problem 1 (Dictionary Learning). A point set X = [x1 . . . xm] in Rd is said to be
s-represented by a dictionary D = [v1 . . . vn] for a given sparsity s < d, if there exists
Θ = [θ1 . . . θm] such that xi = Dθi, with (cid:107)θi(cid:107)0 ≤ s (i.e. θi has at most s non-zero
entries). Given an X known to be s-represented by an unknown dictionary D of size
|D| = n, Dictionary Learning is the problem of ﬁnding any dictionary ´D satisfying the
properties of D, i.e. | ´D| ≤ n, and there exists ´θi such that xi = ´D ´θi for all xi ∈ X.

The dictionary under consideration is usually overcomplete, with n > d. However
we are interested in asymptotic performance with respect to all four variables n, m, d, s.
Typically, m (cid:29) n (cid:29) d > s. Both cases when s is large relative to d and when s is
small relative to d are interesting.
We understand the Dictionary Learning problem from an intrinsically geometric
point of view. Notice that each x ∈ X lies in an s-dimensional subspace SD(x), which
is the span of s vectors v ∈ D that form the support of x. The resulting s-subspace
arrangement SX,D = {SD(x) : x ∈ X} has an underlying labeled (multi)hypergraph
H(SX,D) = (I(D), I(SX,D)), where I(D) denotes the index set of the dictionary D
and I(SX,D) is the set of (multi)hyperedges over the indices I(D) corresponding to
the subspaces SD(x). The word “multi” appears because if SD(x1) = SD(x2) for
data points x1, x2 ∈ X with x1 (cid:54)= x2, then that subspace is multiply represented in
SX,D (resp. I(SX,D)) as SD(x1) and SD(x2).

Note that there could be many dictionaries D and for each D, many possible sub-

space arrangements SX,D that are solutions to the Dictionary Learning problem.

v1v2v3v4e1e2e3e4v1v2v3v4e1e2e3e4(a)(b)16

6.1 Systematic Classiﬁcation of Problems Closely Related to Dic-

tionary Learning and Previous Approaches

A closely related problem to Dictionary Learning is the Vector Selection (aka sparse
recovery) problem, which ﬁnds a representation of input data in a known dictionary D.
Problem 2 (Vector Selection). Given a dictionary D ∈ Rd×n and an input data point
x ∈ Rd, the Vector Selection problem asks for θ ∈ Rn such that x = Dθ with (cid:107)θ(cid:107)0
minimized.

That is, θ is a sparsest support vector that represents x as linear combinations of

the columns of D.

An optimization version of Dictionary Learning can be written as:

In practice, it is often relaxed to the Lagrangian min(cid:80)m

i=0((cid:107)xi − Dθi(cid:107)2 + λ(cid:107)θi(cid:107)1).
Several traditional Dictionary Learning algorithms work by alternating minimiza-

min(cid:107)θi(cid:107)0 : xi = Dθi.

D∈Rd×n

max

min

xi

tion, i.e. iterating the following two steps [41, 36, 35]:

1. Starting from an initial estimation of D, solving the Vector Selection problem
for all data points X to ﬁnd a corresponding Θ. This can be done using any vector
selection algorithm, such as basis pursuit from [10].

2. Given Θ, updating the dictionary estimation by solving the optimization prob-
lem is now convex in D. For an overcomplete dictionary, the general Vector Selection
problem is ill deﬁned, as there can be multiple solutions for a data point x. Overcom-
ing this by framing the problem as a minimization problem is exceedingly difﬁcult.
Indeed under generic assumptions, the Vector Selection problem has been shown to be
NP-hard by reduction to the Exact Cover by 3-set problem [37]. One is then tempted
to conclude that Dictionary Learning is also NP-hard. However, this cannot be directly
deduced in general, since even though adding a witness D turns the problem into an
NP-hard problem, it is possible that the Dictionary Learning solves to produce a differ-
ent dictionary ´D.
for all θ
such that (cid:107)θ(cid:107)0 ≤ s, there exists a δs such that (1 − δs) ≤ (cid:107)Dθ(cid:107)2
≤ (1 + δs), it is
(cid:107)θ(cid:107)2
guaranteed that the sparsest solution to the Vector Selection problem can be found via
L1 minimization [11, 9].

On the other hand, if D satisﬁes the condition of being a frame, i.e.

One popular alternating minimization method is the Method of Optimal Dictionary
(MOD) [12], which follows a two step iterative approach using a maximum likelihood
)−1.
formalism, and uses the pseudoinverse to compute D: D(i+1) = XΘ(i)T
The MOD can be extended to Maximum A-Posteriori probability setting with different
priors to take into account preferences in the recovered dictionary.

(ΘnΘiT

2

2

Similarly, k-SVD [3] uses a two step iterative process, with a Truncated Singular
Value Decomposition to update D. This is done by taking every atom in D and ap-
plying SVD to X and Θ restricted to only the columns that have contribution from
that atom. When D is restricted to be of the form D = [B1, B2 . . . BL] where Bi’s
are orthonormal matrices, a more efﬁcient pursuit algorithm is obtained for the sparse
coding stage using a block coordinate relaxation.

17

Though alternating minimization methods work well in practice, there is no theo-
retical guarantee that the their results will converge to a true dictionary. Several recent
works give provable algorithms under stronger constraints on X and D. Spielman
et. al [40] give an L1 minimization based approach which is provable to ﬁnd the ex-
act dictionary D, but requires D to be a basis. Arora et. al [5] and Agarwal et. al
[2] independently give provable non-iterative algorithms for learning approximation of
overcomplete dictionaries. Both of their methods are based on an overlapping clus-
tering approach to ﬁnd data points sharing a dictionary vector, and then estimate the
dictionary vectors from the clusters via SVD. The approximate dictionary found using
these two algorithms can be in turn used in iterative methods like k-SVD as the ini-
tial estimation of dictionary, leading to provable convergence rate [1]. However, these
overlapping clustering based methods require the dictionaries to have the pairwise in-
coherence property which is much stronger than the frame property.

By imposing a systematic series of increasingly stringent constraints on the input,
we classify previous approaches to Dictionary Learning as well as a whole set of in-
dependently interesting problems closely related to Dictionary Learning. A summary
of the input conditions and results of these different types of Dictionary Learning ap-
proaches can be found in Table 1.

A natural restriction of the general Dictionary Learning problem is the following.
We say that a set of data points X lies on a set S of s-dimensional subspaces if for all
xi ∈ X, there exists Si ∈ S such that xi ∈ Si.
Problem 3 (Subspace Arrangement Learning). Let X be a given set of data points
that are known to lie on a set S of s-dimensional subspaces of Rd, where |S| is at
most k. (Optionally assume that the subspaces in S have bases such that their union
is a frame). Subspace arrangement learning ﬁnds any subspace arrangement ´S of s-
dimensional subspaces of Rd satisfying these conditions, i.e. | ´S| ≤ k, X lies on ´S,
(and optionally the union of the bases of ´Si ∈ ´S is a frame).

There are several known algorithms for learning subspace arrangements. Random
Sample Consensus (RANSAC) [45] is an approach to learning subspace arrangements
that isolates, one subspace at a time, via random sampling. When dealing with an
arrangement of k s-dimensional subspaces, for instance, the method samples s + 1
points which is the minimum number of points required to ﬁt an s-dimensional sub-
space. The procedure then ﬁnds and discards inliers by computing the residual to each
data point relative to the subspace and selecting the points whose residual is below a
certain threshold. The process is iterated until we have k subspaces or all points are
ﬁtted. RANSAC is robust to models corrupted with outliers. Another method called
Generalized PCA (GPCA) [46] uses techniques from algebraic geometry for subspace
clustering, ﬁnding a union of k subspaces by factoring a homogeneous polynomial of
degree k that is ﬁtted to the points {x1 . . . xm}. Each factor of the polynomail repre-
sents the normal vector to a subspace. We note that GPCA can also determine k if it is
unknown.

The next problem is obtaining a minimally sized dictionary from a subspace ar-

rangement.

Problem 4 (Smallest Spanning Set for Subspace Arrangement). Let S be a given set of s-
dimensional subspaces of Rd speciﬁed by giving their bases. Assume their intersections
are known to be s-represented by a set I of vectors with |I| at most n. Find any set of
vectors ´I that satisﬁes these conditions.

18

The smallest spanning set is not necessarily unique in general, and is closely re-
lated to the intersection semilattice of subspace arrrangement [8, 14]. Furthermore,
under the condition that the subspace arrangement comes from a frame dictionary, the
smallest spanning set is the union of: (a) the smallest spanning set I of the pairwise
intersection of all the subspaces in S; (b) any points outside the pairwise intersections
that, together with I, completely s-span the subspaces in S. This directly leads to a
recursive algorithm for the smallest spanning set problem.

The ﬁtted dictionary learning problem is the version of dictionary learning where

the underlying hypergraph H(SX,D) is speciﬁed.
Problem 5 (Fitted Dictionary Learning). Let X be a given set of data points in Rd. For
an unknown dictionary D = [v1, . . . , vn] that s-represents X, we are given the hyper-
graph H(SX,D) of the underlying subspace arrangement SX,D. Find any dictionary
´D of size | ´D| ≤ n, that is consistent with the hypergraph H(SX,D).

When X contains sufﬁciently dense data to solve Problem 3, Dictionary Learning
reduces to problem 4, i.e. we can use the following two-step procedure to solve the
Dictionary Learning problem:

• Learn a Subspace Arrangement S for X (instance of Problem 4).
• Recover D by either ﬁnding the smallest Spanning Set of S (instance of Problem

3), or a ﬁtted dictionary learning (instance of Problem 5).

Note that it is not true that the decomposition strategy should always be applied for
the same sparsity s. The decomposition starts out with the minimum given value of s
and is reapplied with iteratively higher s if a solution has not be obtained.

A summary of the input conditions and results of these different types of Dictionary

Learning problems from this section can be found in Table 1.

6.2 New Bounds and Algorithms for Dictionary Learning for Ran-

dom Data via Pinned Subspace-Incidence Systems

The following corollary of Theorem 12 gives a tight bound on dictionary size for
generic data points, which leads to an algorithm for ﬁnding a dictionary provided the
bounds hold.

Corollary 13 (Dictionary size tight bound for generic data). Given a set of m points
X = {x1, .., xm} in Rd, generically there is a dictionary D of size n that s-represents
X only if (d − s)m ≤ (d − 1)n. Conversely, if (d − s)m = (d − 1)n and the supports
of xi (the nonzero entries of the θi’s) are known to form a (d − 1, 0)-tight hypergraph
H, then generically, there is at least one and at most ﬁnitely many such dictionaries.

19

Dictionary Learning via
subspace arrangement
and spanning set

Dictionary
Learning for
Segmented Data

Fitted Dictionary
Learning (this
paper)

X with promise that
each subspace /
dictionary support set is
shared by sufﬁciently
many data points in X

of
Minimum number
to guarantee a
points
unique
ar-
rangement that will give
a spanning set of size n

subspace

Subspace Arrangement
Learning
Algorithms
(Problem 3) and Span-
ning
Set
(
Problem 4)
Unknown

Finding

Partitioned /
segmented Data
X

X with underlying
hypergraph
speciﬁed

Question 1

1
Spanning
(

Question
and
Set
Problem 4)

Finding

d − s
n, where the
d − 1
underlying hyper-
graph satisfy Theo-
rem 12

Realization
DR-plan
rem 17)

using
(Theo-

Unknown

Unknown

(a)

(b)

(c)

Alternating
Minimiza-
tion Ap-
proaches
D satisﬁes
frame
property

Traditional Dictionary Learning

Spielman
et. al [40]

Arora et.
al
[5], Agarwal
et. al [2]

X generated from hidden
dictionary D and certain
distribution of Θ
D is a ba-
sis

D is pairwise
incoherent

Question 4 O(n log n) O(n2 log2 n)

MOD, k-
SVD, etc.

Algorithm
from [40]

Algorithms
from [5, 2]

Unknown

O(n2 log2 n)

For
generic
data (this
paper)
Generic
data
points X

d − s
n
d − 1
(Corol-
lary
14);
Unknown
for general
position
data
(Ques-
tion 2)
Straight-
forward
algorithm
(Corol-
lary 15)
Unknown

Input and
Conditions

Minimum m
guaranteeing
existence
of a locally
unique dic-
tionary of a
given size n

Dictionary
Learning
algorithms

Minimum m
guaranteeing
efﬁcient
dictionary
learning
Illustrative
example

Table 1: Classiﬁcation of Problems

bcdeaABCDE20

Quantifying the term “generically” in Corollary 13 yields Corollaries 14 and 15

below.
Corollary 14 (Lower bound for random data). Given a set of m points X = {x1, .., xm}
picked from a distribution ρ with respect to which nongenericity has measure zero, a
dictionary D that s-represents X has size at least ( d−s
d−1 )m with probability 1. In other
words, |D| = Ω(X) if s and d are constants.
Corollary 15 (Straightforward Dictionary Learning Algorithm). Given a set of m
points X = [x1 . . . xm] picked from a distribution ρ with respect to which nongenericity
has measure zero, there is a straightforward pebble-game [25, 29, 28] based algorithm
constructs a dictionary D = [v1 . . . vn] that s-represents X, where n =
m.
d − 1
The time complexity of the algorithm is O(m) when we treat d and s as constants.

(cid:18) d − s

(cid:19)

The key idea of the algorithm for Corollary 15 is that we can choose a convenient
underlying hypergraph H(SX,D). Thus the algorithm has two parts: (1) constructing
H(SX,D) and (2) constructing the s-subspace arrangement SX,D and the dictionary
D.
(1) Algorithm for constructing the underlying hypergraph H(SX,D) for a hypo-
thetical s-subspace arrangement SX,D:

The algorithm works in three stages to construct a expanded mutli-hypergraph

ˆH(SX,D):

1. We start by constructing a minimal minimally rigid hypergraph H0 = (V0, E0),
using the pebble game algorithm introduced below. Here |V0| = k(d−s), |E0| =

k(d − 1), where k is the smallest positive integer such that(cid:0)k(d−s)

(cid:1) ≥ k(d − 1),

so it is possible to construct E0 such that no more than one hyperedge in E0
containing the same set of vertices in V0. The values |V0| and |E0| are constants
when we think of d and s as constants.

s

2. We use the pebble game algorithm to append a set V1 of d − s vertices and a
set E1 of d − 1 hyperedges to H0, such that each hyperedge in E1 contains at
least one vertex from V1, and the obtained graph H1 is still minimally rigid. The
subgraph B1 induced by E1 has vertex set VB1 = V1
call the vertex set VB the base vertices of the construction.

(cid:83) VB, where VB ⊂ V0. We
(cid:83) VB, and Bi is isomorphic to B1. In other words, at each step, we directly

3. Each of the following construction step i appends a set Vi of d− s vertices and a
set Ei of d−1 hyperedges such that the subgraph Bi induced by Ei has vertex set
Vi
append a basic structure the same as (V1, E1) to the base vertices VB. It is not
hard to verify that the obtained graph is still minimally rigid.

The pebble game algorithm by [42] works on a ﬁxed ﬁnite set V of vertices and con-
structs a (k, l)-sparse hypergraph. Conversely, any (k, l)-sparse hypergraph on vertex
set V can be constructed by this algorithm. The algorithm initializes by putting k
pebbles on each vertex in V . There are two types of moves:

21

• Add-edge: adds a hyperedge e (vertices in e must contain at least l + 1 pebbles),

removes a pebble from a vertex v in e, and assign v as the tail of e;

• Pebble-shift: for a hyperedge e with tail v2, and a vertex v1 ∈ e which containing
at least one pebble, moves one pebble from v1 to v2, and change the tail of e to
v1.

At the end of the algorithm, if there are exactly l pebbles in the hypergraph, then the
hypergraph is (k, l)-tight.
Our algorithm runs a slightly modiﬁed pebble game algorithm to ﬁnd a (d − 1, 0)-
tight expanded mutli-hypergraph. We require that each add-edge move adding (d − s)
copies of a hyperedge e, so a total of d − s pebbles are removed from vertices in e.
Additionally, the multiplicity of a hyperedge, not counting the expanded copies, cannot
exceed 1. For constructing the basic structure of Stage 2, the algorithm initializes by
putting d− 1 pebbles on each vertex in V1. In addition, an add-edge move can only add
a hyperedge that contains at least one vertex in V1, and a pebble-shift move can only
shift a pebble inside V1.

(cid:16)

s2|V0|(cid:0)|V0|

s

(cid:1)(cid:17)

time in Step 1, and

(cid:16)

The pebble-game algorithm takes O

(cid:1)(cid:17)
s2 (|V0| + (d − s))(cid:0)|V0|+(d−s)

time in Step 2. Since the entire underlying hy-
O
pergraph H(SX,D) has m = |X| edges, Step 3 will be iterated O(m/(d − 1)) times,
and each iteration takes constant time. Therefore the overall time complexity for con-
structing H(SX,D) is

s

(cid:18)

s2 (|V0| + (d − s))

O

+ (m/(d − 1))

(cid:19)
(cid:18)|V0| + (d − s)

s

(cid:19)

which is O(m) when d and s are regarded as constants.
(2) Algorithm for constructing the s-subspace arrangement SX,D and the dictio-
nary D:

The construction of the s-subspace arrangement SX,D naturally follows from the
construction of the underlying hypergraph H(SX,D). For the initial hypergraph H0, we
get a pinned subspace-incidence system (H0, X0)(D0) by arbitrarily choose |X0| =
|E0| pins from X. Similarly, for Step 2 and each iteration of Step 3, we form a pinned
subspace-incidence system (Bi, Xi)(Di) by arbitrarily choosing |Xi| = d − 1 pins
from X.

Given X0, we know that the rigidity matrix – of the s-subspace framework H0(SX0,D0 )

– with indeterminates representing the coordinate positions of the points in D0 – gener-
ically has full rank (rows are maximally independent), under the pure conditions of
Theorem 12; in which case, the original algebraic subsystem (H0, X0)(D0) (whose
Jacobian is the rigidity matrix), with X0 plugged in, is guaranteed to have a (possibly
complex) solution and only ﬁnitely many solutions for D0. Since the pure conditions
fail only on a measure-zero subset of the space of pin-sets X0, where each pin is in
Sd−1, it follows that if the pins in X0 are picked uniformly at random from Sd−1
we know such a solution exists for D0 (and SX0,D0) and can be found by solving the
algebraic system H0(SX0,D0 ).

22

Once we have solved (H0, X0)(D0), for each following construction step i, Bi is
also rigid since coordintate positions of the vertices in VB have been ﬁxed So similarly,
we know a solution exists for Di (and SXi,Di) and can be found by solving the alge-
braic system Bi(SXi,Di), which is of constant size O(d). Although there can be more
than one choice of solution for each step, since every construction step is based on base
vertices VB, the solution of one step will not affect any other steps, so generically any
choice will result in a successful solution for the entire construction sequence, and we
obtain D by taking the union of all Di’s.
When we regard d and s as constants, the time complexity for Stage (2) is the con-
stant time for solving the size O(|V0|) algebraic system (H0, X0)(D0), plus O(m/(d−
1)) times the constant time for solving the size O(d) system (Bi, Xi)(Di), that is O(m)
in total.

Therefore the overall time complexity of the dictionary learning algorithm is O(m).

6.3 Dictionary Learning for Commonly Occurring Data Using DR-

Planning

In practical dictionary learning problems, the data set is usually overconstrained, mak-
ing it impossible to apply the algorithm given above.

As one solution for such dictionary learning problems, recall the two-step proce-
dure described in Section 6.1, where we ﬁrst use standard preprocessing algorithms
such as RANSAC and GPCA to learn a subspace arrangement from the data set (Prob-
lem 3), and then constructing a ﬁtted dictionary learning problem (Problem 5) based
on the subspace arrangement learnt. Now we can obtain a dictionary by realizing the
(possibly overconstrained) pinned subspace incidence-system corresponding to the ﬁt-
ted dictionary learning problem.

To ﬁnd a realization to a geometric constraint system, the straightforward method
is to directly ﬁnd the real solutions to the entire multivariate polynomial system. How-
ever, such an approach requires double exponential time in the number of variables.
Thus, it is crucial to use recursive Decomposition-Recombination (DR-) plans [19, 18,
26, 39] , which decompose the original system into locally rigid subsystems and recom-
bines solutions of subsystems to get a solution for the original system. For a pinned
subspace-incidence system (H, Q), a DR-plan is formally deﬁned for the underlying
multi-hypergraph ˆH (recall the deﬁnition from Section 5.1) as following:

Deﬁnition 16 (DR-plan). A decomposition-recombination (DR-)plan of the underlying
multi-hypergraph ˆH of a pinned subspace-incidence system (H, Q) in Rd is a forest
that has the following properties:

1. Each node represents a connected vertex-induced subgraph of ˆH that is either

rigid, or a single hyperedge.

2. A root node represents a maximal subgraph of ˆH.

3. A node represents the subgraph of ˆH induced by the union of the node’s children.

4. A leaf node represents a single hyperedge.

23

(a)

Figure 5: (a) A generically minimally rigid pinned subspace-incidence system with
d = 3. (b) The optimal DR-plan of (a) obtained using Theorem 17.

(b)

With the use of a DR-plan, the complexity of realizing a geometric constraint sys-
tem is determined by the maximum fan-in of the DR-plan, i.e. the maximum number
of children nodes of any node. Finding an optimal DR-plan i.e. one that minimizes the
maximum fan-in, however, is usually hard for general geometric constraint systems.

In the recent work [7], it is shown that an optimal DR-plan can be efﬁciently found
for a large class of minimally rigid geometric constraint systems with O(n3) time com-
plexity, leading to efﬁcient realization. Speciﬁcally, this result holds for minimally
rigid pinned subspace-incidence systems, as stated in the following theorem:
Theorem 17 ([7]). For generically minimally rigid pinned subspace-incidence, there
exists a DR-plan of the underlying multi-hypergraph satisfying the following two addi-
tional properties, and any such DR-plan is optimal:

1. Children are connected, generically rigid, vertex-maximal proper subgraphs of

the parent.

2.

If all pairs of rigid vertex-maximal proper subgraphs intersect trivially then
all of them are children, otherwise exactly two that intersect non-trivially are
children.

Figure 5 shows an example of the optimal DR-plan found by applying Theorem 17
to a generically minimally rigid pinned subspace-incidence system with 12 vertices and
24 pins, where d = 3. The DR-plan has the optimal maximum fan-in 12.

However, for overconstrained systems, the optimal DR-planning problem was shown
to be NP-hard even for bar-joint systems [32], and we strongly conjecture it is also NP-
hard for pinned subspace-incidence systems. One possible workaround is to ﬁrst apply

24

(a)

(b)

(c)

Figure 6: (a) A overconstrained pinned subspace-incidence system with d = 3. (b) A
maximal minimally rigid subgraph with maximum fan-in 11 (optimal). (c) A maximal
minimally rigid subgraph with maximum fan-in 11

the pebble-game algorithm from [25, 29] to an overconstrained system (H, Q) to ﬁnd a
maximal, generically minimally rigid subsystem, and then apply the algorithm from [7]
to ﬁnd an optimal DR-plan for the subsystem. However, by doing this we are forgoing
the advantages of smaller sized DR-plan which is usually provided by overconstrained
cases. We also note that different maximal, generically minimally rigid subsystems of
(H, Q) may still have different maximum fan-ins. As an example, Figure 6(a) shows
a overconstrained pinned subspace-incidence system with d = 3. Figure 6 (b) and (c)
are DR-plans of two maximal, generically minimally rigid subsystems of (a), where
(b) has the optimal maximum fan-in 11, while (c) has a maximum fan-in of 12.

7 Open Questions

A natural restriction of the two step problem in Section 6.1, which learns subspace
arrangement followed by spanning set is the following, where the data set X is given in
support-equivalence classes. For a given subspace t in the subspace arrangement SX,D
(respectively hyperedge h in the hypergraph’s edge-set I(SX,D)), let Xt = Xh ⊆ X
be the equivalence class of data points x such that SD(x) = t. We call the data points
x in a same Xh as support-equivalent.
Question 1 (Dictionary Learning for Partitioned Data). (1) What is the minimum size of
X and Xi’s (representing data X partitioned into X(cid:48)
is) guaranteeing that there exists
a locally unique dictionary D for a s-subspace arrangement SX,D satisfying |D| ≤ n,

…...…...25

and Xi represents the support-equivalence classes of X with respect to D?

(2) How to ﬁnd such a dictionary D?
With regard to the problem of minimizing |D|, very little is known for simple re-

strictions on X. For example the following question is open.

Question 2. Given a general position assumption on X, what is the best lower bound
on |D| for Dictionary Learning? Conversely, are smaller dictionaries possible than
indicated by Corollary 14 (see Section 6.2) under such an assumption?

Question 2 gives rise to the following pure combinatorics open question closely

related to the intersection semilattice of subspace arrrangement [8, 14].
Question 3. Given weights w(S) ∈ N assigned to size-s subsets S of [n]. For T ⊆ [n]
with |T| (cid:54)= s,

|T| < s
|T| > s

0

(cid:88)

w(T ) =

w(S)

S⊂T,|S|=s

Assume additionally the following constraint holds: for all subsets T of [n] with s ≤
|T| ≤ d, w(T ) ≤ |T| − 1. Can one give a nontrivial upper bound on w([n])?

The combinatorial characterization given by Theorem 12 leads to the following

question for general Dictionary Learning.

Question 4. What is the minimum size of a data set X such that the Dictionary Learn-
ing for X has a locally unique solution dictionary D of a given size? What are the
geometric characteristics of such an X?

8 Conclusion

In this paper, we studied the generic rigidity of pinned subspace-incidence systems,
and obtained a combinatorial characterization of generic minimal rigidity for pinned
subspace-incidence systems. We then showed the application of pinned subspace-
incidence systems in Dictionary Learning, giving a tight bound on dictionary size in
a speciﬁc setting as well as new dictionary learning algorithms in this and other gen-
eral settings. We provided a systematic classiﬁcation of problems related to dictionary
learning together with various algorithms, their assumptions and performance, and for-
malized related open questions from a geometry perspective.

References

[1] ALEKH AGARWAL, ANIMASHREE ANANDKUMAR, PRATEEK JAIN, PRA-
NEETH NETRAPALLI, AND RASHISH TANDON, Learning sparsely used
overcomplete dictionaries
arXiv preprint
arXiv:1310.7991, (2013).

via alternating minimization,

26

[2] ALEKH AGARWAL, ANIMASHREE ANANDKUMAR, AND PRANEETH NETRA-
PALLI, Exact recovery of sparsely used overcomplete dictionaries, arXiv preprint
arXiv:1309.1952, (2013).

[3] MICHAL AHARON, MICHAEL ELAD, AND ALFRED BRUCKSTEIN, k-svd: An
algorithm for designing overcomplete dictionaries for sparse representation, Sig-
nal Processing, IEEE Transactions on, 54 (2006), pp. 4311–4322.

[4] SAMY AIT-AOUDIA, ROLAND JEGOU, AND DOMINIQUE MICHELUCCI, Re-
duction of constraint systems, in Proceedings of Compugraphics, 1993, pp. 83–
92.

[5] SANJEEV ARORA, RONG GE, AND ANKUR MOITRA, New algorithms for learn-
ing incoherent and overcomplete dictionaries, arXiv preprint arXiv:1308.6273,
(2013).

[6] LEONARD ASIMOW AND BEN ROTH, The rigidity of graphs, Transactions of the

American Mathematical Society, 245 (1978), pp. 279–289.

[7] TROY BAKER, MEERA SITHARAM, MENGHAN WANG, AND JOEL
WILLOUGHBY, Optimal decomposition and recombination of isostatic geometric
constraint systems for designing layered materials, Computer Aided Geometric
Design, (2015).

[8] ANDERS BJ ¨ORNER, Subspace arrangements, in First European Congress of

Mathematics, Springer, 1994, pp. 321–370.

[9] EMMANUEL J CAND `ES, JUSTIN ROMBERG, AND TERENCE TAO, Robust uncer-
tainty principles: Exact signal reconstruction from highly incomplete frequency
information, Information Theory, IEEE Transactions on, 52 (2006), pp. 489–509.

[10] SCOTT SHAOBING CHEN, DAVID L DONOHO, AND MICHAEL A SAUNDERS,
Atomic decomposition by basis pursuit, SIAM journal on scientiﬁc computing,
20 (1998), pp. 33–61.

[11] DAVID L DONOHO, Compressed sensing, Information Theory, IEEE Transac-

tions on, 52 (2006), pp. 1289–1306.

[12] KJERSTI ENGAN, SVEN OLE AASE, AND J HAKON HUSOY, Method of optimal
directions for frame design, in Acoustics, Speech, and Signal Processing, 1999
IEEE International Conference on, vol. 5, IEEE, 1999, pp. 2443–2446.

[13] IOANNIS FUDOS AND CHRISTOPH M. HOFFMANN, A graph-constructive ap-
proach to solving systems of geometric constraints, ACM Trans. Graph., 16
(1997), pp. 179–216.

[14] MARK GORESKY AND ROBERT MACPHERSON, Stratiﬁed morse theory,

Springer, 1988.

27

[15] STEVEN J GORTLER, CRAIG GOTSMAN, LIGANG LIU, AND DYLAN P
THURSTON, On afﬁne rigidity, Journal of Computational Geometry, 4 (2013),
pp. 160–181.

[16] JACK E GRAVER, BRIGITTE SERVATIUS, AND HERMAN SERVATIUS, Combi-

natorial rigidity, vol. 2, AMS Bookstore, 1993.

[17] KIRK HALLER, AUDREY LEE-ST JOHN, MEERA SITHARAM,

ILEANA
STREINU, AND NEIL WHITE, Body-and-cad geometric constraint systems, Com-
putational Geometry, 45 (2012), pp. 385–405.

[18] CHRISTOPH M HOFFMAN, ANDREW LOMONOSOV, AND MEERA SITHARAM,
Decomposition plans for geometric constraint problems, part ii: new algorithms,
Journal of Symbolic Computation, 31 (2001), pp. 409–427.

[19]

, Decomposition plans for geometric constraint systems, part i: Perfor-
mance measures for cad, Journal of Symbolic Computation, 31 (2001), pp. 367–
408.

[20] CHRISTOPH M HOFFMANN AND ROBERT JOAN-ARINYO, Symbolic constraints
in constructive geometric constraint solving, Journal of Symbolic Computation,
23 (1997), pp. 287–299.

[21] CHRISTOPH M HOFFMANN, ANDREW LOMONOSOV,

SITHARAM, Finding solvable subsets of constraint graphs,
and Practice of Constraint Programming-CP97, Springer, 1997, pp. 463–477.

AND MEERA
in Principles

[22]

, Geometric constraint decomposition, in Geometric constraint solving and

applications, Springer, 1998, pp. 170–195.

[23] BILL JACKSON AND TIBOR JORD ´AN, Pin-collinear body-and-pin frameworks
and the molecular conjecture, Discrete & Computational Geometry, 40 (2008),
pp. 258–278.

[24] BILL JACKSON, TIBOR JORD ´AN, AND SHIN-ICHI TANIGAWA, Combinatorial
conditions for the unique completability of low-rank matrices, SIAM Journal on
Discrete Mathematics, 28 (2014), pp. 1797–1819.

[25] DONALD J JACOBS AND BRUCE HENDRICKSON, An algorithm for two-
dimensional rigidity percolation: the pebble game, Journal of Computational
Physics, 137 (1997), pp. 346–365.

[26] CHRISTOPHE JERMANN, GILLES TROMBETTONI, BERTRAND NEVEU, AND
PASCAL MATHIS, Decomposition of geometric constraint systems: a survey,
International Journal of Computational Geometry & Applications, 16 (2006),
pp. 379–414.

[27] FRANZ J KIR ´ALY, LOUIS THERAN, AND RYOTA TOMIOKA, The alge-
braic combinatorial approach for low-rank matrix completion, arXiv preprint
arXiv:1211.4116, (2012).

28

[28] AUDREY LEE AND ILEANA STREINU, Pebble game algorithms and sparse

graphs, Discrete Mathematics, 308 (2008), pp. 1425–1437.

[29] AUDREY LEE, ILEANA STREINU, AND LOUIS THERAN, Finding and maintain-

ing rigid components., in CCCG, 2005, pp. 219–222.

[30] AUDREY LEE-ST.JOHN AND JESSICA SIDMAN, Combinatorics and the rigidity
of CAD systems, Computer-Aided Design, 45 (2013), pp. 473 – 482. Solid and
Physical Modeling 2012.

[31] PERCY S LIANG, DANIEL J HSU, AND SHAM M KAKADE, Identiﬁability and
unmixing of latent parse trees, in Advances in neural information processing sys-
tems, 2012, pp. 1511–1519.

[32] ANDREW LOMONOSOV, Graph and combinatorial analysis for geometric con-

straint graphs, PhD thesis, University of Florida, 2004.

[33] JOHANNES MITTMANN, Gr¨obner bases: Computational algebraic geometry and

its complexity, (2007).

[34] C ST JA NASH-WILLIAMS, Edge-disjoint spanning trees of ﬁnite graphs, Journal

of the London Mathematical Society, 1 (1961), pp. 445–450.

[35] BRUNO A OLSHAUSEN AND DAVID J FIELD, Sparse coding with an overcom-
plete basis set: A strategy employed by V1?, Vision research, 37 (1997), pp. 3311–
3325.

[36] IGNACIO RAMIREZ, PABLO SPRECHMANN, AND GUILLERMO SAPIRO, Clas-
siﬁcation and clustering via dictionary learning with structured incoherence and
shared features, in Computer Vision and Pattern Recognition (CVPR), 2010 IEEE
Conference on, IEEE, 2010, pp. 3501–3508.

[37] GEO REY DAVIS, Adaptive nonlinear approximations, PhD thesis, Courant Insti-

tute of Mathematical Sciences New York, 1994.

[38] AMIT SINGER AND MIHAI CUCURINGU, Uniqueness of low-rank matrix com-
pletion by rigidity theory, SIAM Journal on Matrix Analysis and Applications, 31
(2010), pp. 1621–1641.

[39] MEERA SITHARAM, Combinatorial approaches to geometric constraint solving:
Problems, progress, and directions, DIMACS Series in Discrete Mathematics and
Theoretical Computer Science, 67 (2005), p. 117.

[40] DANIEL A SPIELMAN, HUAN WANG, AND JOHN WRIGHT, Exact recovery of
sparsely-used dictionaries, in Proceedings of the Twenty-Third international joint
conference on Artiﬁcial Intelligence, AAAI Press, 2013, pp. 3087–3090.

[41] PABLO SPRECHMANN AND GUILLERMO SAPIRO, Dictionary learning and
sparse coding for unsupervised clustering,
in Acoustics Speech and Signal
Processing (ICASSP), 2010 IEEE International Conference on, IEEE, 2010,
pp. 2042–2045.

29

[42] ILEANA STREINU AND LOUIS THERAN, Sparse hypergraphs and pebble game

algorithms, European Journal of Combinatorics, 30 (2009), pp. 1944–1964.

[43]

, Slider-pinning rigidity: a maxwell–laman-type theorem, Discrete and

Computational Geometry, 44 (2010), pp. 812–837.

[44] WILLIAM T TUTTE, On the problem of decomposing a graph into n connected

factors, Journal of the London Mathematical Society, 1 (1961), pp. 221–230.

[45] REN ´E VIDAL, Subspace clustering, Signal Processing Magazine, IEEE, 28

(2011), pp. 52–68.

[46] RENE VIDAL, YI MA, AND SHANKAR SASTRY, Generalized principal compo-
nent analysis (gpca), Pattern Analysis and Machine Intelligence, IEEE Transac-
tions on, 27 (2005), pp. 1945–1959.

[47] NEIL WHITE AND WALTER WHITELEY, The algebraic geometry of motions
of bar-and-body frameworks, SIAM Journal on Algebraic Discrete Methods, 8
(1987), pp. 1–32.

[48] NEIL L WHITE AND WALTER WHITELEY, The algebraic geometry of stresses
in frameworks, SIAM Journal on Algebraic Discrete Methods, 4 (1983), pp. 481–
511.

[49] WALTER WHITELEY, A matroid on hypergraphs, with applications in scene anal-

ysis and geometry, Discrete & Computational Geometry, 4 (1989), pp. 75–95.

[50]

, Some matroids from discrete applied geometry, Contemporary Mathemat-

ics, 197 (1996), pp. 171–312.

