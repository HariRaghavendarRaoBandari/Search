6
1
0
2

 
r
a

M
9

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
3
4
7
2
0

.

3
0
6
1
:
v
i
X
r
a

Computing AIC for black-box models using Generalised
Degrees of Freedom: a comparison with cross-validation

Severin Hauenstein1, Carsten F. Dormann∗1, and Simon N. Wood2

1Department of Biometry and Environmental System Analysis, University of Freiburg, 79106 Freiburg, Germany

2School of Mathematics, University of Bristol, Bristol BS8 1TW, U.K.

10 March 2016

Abstract

Generalised Degrees of Freedom (GDF), as deﬁned by Ye (1998 JASA 93:120-131), represent the sen-
sitivity of model ﬁts to perturbations of the data. As such they can be computed for any statistical
model, making it possible, in principle, to derive the number of parameters in machine-learning ap-
proaches. Deﬁned originally for normally distributed data only, we here investigate the potential of this
approach for Bernoulli-data. GDF-values for models of simulated and real data are compared to model
complexity-estimates from cross-validation. Similarly, we computed GDF-based AICc for randomFor-
est, neural networks and boosted regression trees and demonstrated its similarity to cross-validation.
GDF-estimates for binary data were unstable and inconsistently sensitive to the number of data points
perturbed simultaneously, while at the same time being extremely computer-intensive in their calcu-
lation. Repeated 10-fold cross-validation was more robust, based on fewer assumptions and faster to
compute. Our ﬁndings suggest that the GDF-approach does not readily transfer to Bernoulli data and a
wider range of regression approaches.

1 Introduction

In many scientiﬁc ﬁelds, statistical models have become a frequently used tool to approach research
questions. Choosing the most appropriate model(s), i.e. the model(s) best supported by the data, however,
can be difﬁcult. Especially in ecological, sociological and psychological research, where data are often
sparse while systems are complex, the evidence one particular statistical model may not be exclusive.
The existence of alternative models, which ﬁt the data with comparable goodness, but yield considerably
different predictions is ubiquitous (e.g. Draper 1995, Madigan et al. 1995, Raftery 1996).

In ecological and evolutionary research, statistical procedures are dominated by an “information-
theoretical approach” (Burnham & Anderson 1998, 2002), which essentially means that model ﬁt is as-
sessed by Akaike’s Information Criterion (deﬁned as AIC = −2 log-likelihood+2 number of parameters:

∗carsten.dormann@biom.uni-freiburg.de

1

Akaike 1973). In this ﬁeld, the AIC has become the paradigmatic standard for model selection of likelihood-
based models as well as for determination of model weights in model averaging (e.g. Diniz-Filho et al.
2008, Mundry 2011, Hegyi & Garamszegi 2011). In recent years, and particularly in the ﬁeld of species
distribution analyses, non-parametric, likelihood-free approaches (‘machine learning’) have become more
prevalent and in comparisons typically show better predictive ability (e.g. Recknagel 2001, Elith et al.
2006, Olden et al. 2008, Elith & Leathwick 2009). However, these approaches do not allow the compu-
tation of an AIC, because many of such ‘black-box’ methods are neither likelihood-based, nor can one
readily account for model complexity, as the number of parameters does not reﬂect the effective degrees
of freedom (a phenomenon Elder (2003) calls the “paradox of ensembles”). Thus, at the moment ecol-
ogists are faced with the dilemma of either following the AIC-paradigm, which essentially limits their
toolbox to GLMs and GAMs, or use machine-learning tools and closing the AIC-door. From a statisti-
cal point of view, dropping an AIC-based approach to model selection and averaging is no loss, as an
alternative approximation of the Kullback-Leibler distance is possible through cross-validation.

In this study, we explore a potential avenue to unite AIC and machine learning, based on the con-
cept of Generalised Degrees of Freedom (GDF). We explore the computation of GDFs for Gaussian and
Bernoulli-distributed data as plug-in estimates of the number of parameters in a model. We also compare
such a GDF-AIC-based approach with cross-validation.

The paper is organised as follows. We ﬁrst review the Generalised Degrees of Freedom concept and
relate it to the degrees of freedom in linear models. Next, we brieﬂy illustrate the relation between cross-
validation and Kullback-Leibler divergence, as KL also underlies the derivation of the AIC. Through
simulation and real data we then explore the computation of GDFs for a variety of modelling algorithms
and its stability. The paper closes with a comparison of GDF-based AIC and cross-validation-derived
deviance, and comments on computational efﬁciency of the different approaches and consequences for
model averaging.

1.1 Generalised Degrees of Freedom

Generalised Degrees of Freedom (GDF), originally proposed by Ye (1998) and illustrated for machine
learning by Elder (2003), can be used as a measure of model complexity through which we can make
information theoretical approaches applicable to blackbox algorithms.

In order to understand the conceptual properties of this method, we can make use of a somewhat
simpler version of degrees of freedom (df). For a linear model m, y = Xβ + ε, df are computed as the
trace of the hat (or projection) matrix H, with elements hi j (e.g. Hastie et al. 2009, p. 153):

X(cid:0)XT X(cid:1)−1 XT(cid:17)
(cid:16)

dfm = trace (H) = trace

.

(1)

For a linear model this is the number of its independent parameters, i.e. the rank of the model. To expand
this concept to other, non-parametric methods, making it in fact independent of the actual ﬁtted model, the
deﬁnition above provides the basis for a generalised version of degrees of freedom. Thus, and according
to Ye (1998),

,

(2)

GDFm = trace (H) = ∑

i

hii = ∑

i

2

∂ ˆyi
∂ yi

(cid:98)y(cid:48)
i − ˆyi
i − yi
y(cid:48)

GDF ≈ ∑

i

where ˆyi is the ﬁtted value. That is to say, a model is considered the more complex the more adaptively it
ﬁts the data, which is reﬂected in higher GDF. Or in the words of Ye (1998, p. 120): GDF quantiﬁes the
“sensitivity of each ﬁtted value to perturbations in the corresponding observed value”. For additive error
models (i.e. Y = f (X) + ε, with var(ε) = σ 2

ε ), we can use the speciﬁc deﬁnition that

GDFm =

∑N
i=1 cov( ˆyi),yi

σ 2
ε

(3)

(Hastie et al. 2009, p. 233). Beyond the additive-error model more generally, however, we have to ap-
proximate eqn 2 in other ways. By ﬁtting the model with perturbed yi and assessing the response in ˆyi we
can evaluate ∂ ˆyi
∂ yi

so that,

,

(4)

i is perturbed in such a way that, for normally distributed data, y(cid:48)

where y(cid:48)
small relatively to the variance of y.

i = yi +N (µ = 0,σ ) with σ being

In order to adapt GDF to binary data we need to reconsider this procedure. Since a perturbation cannot
be achieved by adding small random noise to yi, the only possibility is to replace 0s by 1s and vice versa.
A perturb-all-data-at-once approach (Elder 2003) as for Gaussian data is thus not feasible. We explore
ways to perturb Bernoulli-distributed y below.

The equivalence of GDF and k in the linear model encourages us to use GDF as a plug-in estimator

of the number of parameters in the AIC computation.

1.2 Cross-validation and a measure of model complexity

Cross-validation is a standard approach to quantify predictive performance of a model, automatically
accounting for model complexity (by yielding lower ﬁts for too simple and too complex models, e.g.
Wood 2006, Hastie et al. 2009). Because each model is ﬁtted repeatedly, cross-validation is computation-
ally more expensive that the AIC, but the same problem arises for GDF: it requires many simulation to
estimate it stably (see below).

We decided to use the log-likelihood as measure of ﬁt for the cross-validation (Horne & Garton 2006),
with the following reasoning. Let fθ denote the model density of our data, y, and let ft denote the true
density. Thus the Kullback-Leibler (KL) divergence (Kullback & Leibler 1951) is

(cid:90) (cid:0)log ft − log f ˆθ

(cid:1) ft dy.

(5)
AIC is an estimate of this values (in the process taking an expectation over the distribution of ˆθ). Now

(cid:82) ftlog ft dy is independent of the model, and the same for all models under consideration (and hence gets

dropped from the AIC). So the important part of the KL-divergence is

ftlog f ˆθ dy

(6)

that is, the expected model log-likelihood, where the expectation is over the distribution of data not used
in the estimation of ˆθ. Expression 6 can be estimated by cross-validation:

(cid:96)CV = − K
∑

i=1

log f ˆθ [−i] (yi) ,

3

(7)

(cid:90)

−

where (cid:96)CV is the sum over K folds of the log likelihood of the test subset y[i], given the trained model
f ˆθ [−i], with parameter estimates θ [−i]. To put this on the same scale as AIC we multiply by −2 to obtain
the cross-validated deviance.

With the assumption of AIC and (leave-one-out) cross-validation being asymptotically equivalent
(Stone 1977) and given the deﬁnition of AIC, we argue that it should be possible to extract a measurement
from (cid:96)CV that quantiﬁes model complexity. Hence,

AIC = −2(cid:96)m + 2 ˆp ≈ −2(cid:96)CV
ˆp ≈ (cid:96)m − (cid:96)CV.

(8)

with ˆp representing (estimated) model complexity, (cid:96)m the maximum log-likelihood of the original (non-
cross-validated) model and N the number of data points. Embracing the small sample-size bias adjustment
of AIC (Sugiura 1978, Hurvich & Tsai 1989) we get:

AICc = −2(cid:96)m + 2 ˆp +

2 ˆp ( ˆp + 1)
N − ˆp− 1

≈ −2(cid:96)CV
ˆp ≈ ((cid:96)m − (cid:96)CV ) (N − 1)
(cid:96)m − (cid:96)CV + N .

(9)

Thus, we can compute both a cross-validation-based deviance that should be equivalent to the AIC,
−2(cid:96)CV , as well as a cross-validation alternative to GDF, based on the degree of overﬁtting of the original
model (eqns. 8 and 9).

This approach to computing model complexity is not completely unlike that of the DIC (Spiegelhalter
et al. 2002), where pD represents the effective number of parameters in the model and is computed
as the mean of the deviances in an MCMC-analysis minus the deviance of the mean estimates: pD =
D(θ ) − D( ¯θ ) = 2(cid:96)( ¯θ ) − 2(cid:96)(θ ) (Wood 2015). In eqn. 8 the likelihood estimate plays the role of (cid:96)( ¯θ ),
while the cross-validation estimates (cid:96)(θ ).

2 Implementing and evaluating the GDF-approach for normally

and Bernoulli-distributed data

We analysed simulated and real data sets using ﬁve different statistical models. Then we computed their
GDF, disturbing k data points at a time, with different intensity (only for normal data), and for different
values of k.

2.1 Data: simulated and real

First, we evaluated the GDF-approach on normally distributed data following Elder (2003), deliberately
using a relatively small data set: Nnorm = 250. The response y was simulated as (y ∼ N (β0 + β1x1 +
1 +β3x2 +β4x3x4,σ = 1), with β -values of −5,5,−10, 10 and 10, respectively), with x1...4 ∼ U (0,1).
β2x2
(This simulation was repeated to control for possible inﬂuences of the data itself on the resulting GDF,
but results were near-identical.)

4

We simulated binary data with Nbinom = 300 (effective sample size ESSbinom ≈ 150) and y∼ Bern(logit−1(β0 +

β1x1 +β2x2

1 +β3x2 +β4x3x4)), (with β -values of −6.66, 5, −10, 10, respectively), with x1...4 ∼ U (0,1).
Our real-world data comprised a fairly small data set (NPhyseter = 261, ESSPhyseter = 115), showing
the occurrence of sperm whales (Physeter macrocephalus) around Antarctica (collected during cetacean
IDCR-DESS SOWER surveys), and a larger global occurrence data set (NVulpes = 12722, ESSVulpes =
5401) for the red fox (Vulpes vulpes, provided by the International Union for Conservation of Nature
(IUCN)). We pre-selected predictors as described in (Dormann & Kaschner 2010, Dormann et al. 2010),
yielding the six and three (respectively) most important covariates. Since we are not interested in devel-
oping the best model, or comparing model performance, we did not strive to optimise the model settings
for each data set.

2.2

i over y(cid:48)

Implementing GDF for normally distributed data

(cid:16)(cid:98)y(cid:48)
A robust computation of GDF is a little more problematic than eq. 4 suggests. Ye (1998) proposed to ﬁt
i − yi) for each
a linear regression to repeated perturbations of each data point, i.e. to
is constant for all models, and(cid:98)yi is constant for the non-stochastic algorithms (GLM, GAM), the linear
repeatedly perturbed data point i, calculating GDF as the sum of the slopes across all data points. As yi
regression simpliﬁes to(cid:98)y(cid:48)
(cid:98)yi, and therefore apply the procedure also to randomForest, ANN and BRT.
Elder (2003) presents this so-called “horizontal” method (the perturbed y(cid:48) and ﬁtted(cid:98)y(cid:48) are stored as

i. For internally stochastic models we compute a mean(cid:98)y(cid:48)

i as plugin for

(cid:17)
i −(cid:98)y

over (y(cid:48)

different columns in two separate matrices, which are then regressed row-wise) as more robust compared
to the “vertical” method (where for each perturbation a GDF is computed for each column in these
matrices and later averaged across replicate perturbations). Convergence is poor when using the “vertical”
method, so we restricted the computation of GDF to the “horizontal” method.

2.3 GDF for binary data

While normal data can be perturbed by adding normal noise (see next section), binary data cannot. The
only option is to invert their value, i.e. 0 → 1 or 1 → 0. Clearly, we cannot perturb many data points
simultaneously this way, as that will dilute any actual signal in the data. However, for large data sets it is
computationally expensive to perturb all n data points individually, and repeatedly (to yield values for the
horizontal method, see above). We thus varied the number of data points to invert, k, to evaluate whether
we can raise k without biasing the GDF estimate.

2.4 How many data points to perturb simultaneously?

With the number of points to perturb, k, the number of replicates for each GDF-computation (ngdf) and the
amplitude of disturbance (for the normal data), we have three tuning parameters when computing GDF.
First, we calculated GDF for the simulated data and the sperm whale dataset with k taking up increas-
ing values (from 1 to close to n, or to ESS for binary data). Thus, random subsets of size k of the response
variable y were perturbed, yielding y(cid:48). After each particular perturbation the model was re-ﬁtted to y(cid:48). To
gain insight about the variance of the computed GDF values we repeated this calculation 100 times (due

5

to very high computational effort, the number of reruns had to be limited to 10 for both randomForest and
BRT).

The perturbation for the normally distributed y were also drawn from a normal distribution N (0,0.25·

σsimulation). We evaluated the sensitivity to this parameter by setting it to 0.125 and 0.5.

2.5 Modelling approaches

We analysed the data using Generalised Linear Model (GLM), Generalised Additive Model (GAM),
randomForest, feed-forward Artiﬁcial Neural Network (ANN) and Boosted Regression Trees (BRT). For
the GLM, GDF should be identical to the trace of the Hessian (and the rank of the model), hence the GLM
serves as benchmark. For GAMs, different ways to compute the degrees of freedom of the model have
been proposed (Wood 2006), while for the other three methods the intrinsic stochasticity of the algorithm
(or in the case of ANN of the initial weights) may yield models with different GDFs each time.

All models were ﬁtted using R (Team 2014) and packages gbm (for BRTs: Ridgeway et al. 2013,
interaction depth=3, shrinkage=0.001, number of trees=3000, cv.folds=5), mgcv (for GAMs: Wood 2006,
thin plate regression splines), nnet (for ANNs: Venables & Ripley 2002, size=7, decay=0.03 for simulated
and decay=0.1 for real data, linout=TRUE for normal data), randomForest (Liaw & Wiener 2002) and
core package stats (for GLMs, using also quadratic terms and ﬁrst-order interactions). R-code for all
simulation as well as data are available on https://github.com/biometry/GDF.

2.6 Computation of AIC and AIC-weights, from GDF and cross-validation

In addition to the number of parameters, the AIC-formula requires the likelihood of the data, given the
model. As machine-learning algorithms may minimise a score function different from the likelihood, the
result probably differs from a maximum likelihood estimate. To calculate the AIC, however, we have to
assume that the distance minimised by non-likelihood methods is proportional to the likelihood, otherwise
the AIC would not be a valid approximation of the KL-distance. No such assumption has to be made for
cross-validation, as (cid:96)CV here only serves as a measure of model performance. For the normal data, we
compute the standard deviation of the model’s residuals as plug-in estimate of σ.

For the binary data, we use the model ﬁts as probability in the Bernoulli distribution to compute the
likelihood of that model. We then calculated the AIC for all considered models based on their GDF-value.
Due to the small sample sizes, we used AICc (Sugiura 1978, Hurvich & Tsai 1989):

AICc = −2(cid:96)m + 2GDF +

GDF (GDF + 1)
N − GDF− 1

.

(10)

We used (10-fold) cross-validation, maintaining prevalence in the case of binary data, to compute the
log-likelihood of the cross-validation, yielding (cid:96)CV. To directly compare it to AICc, we multiplied (cid:96)CV
with −2. The cross-validation automatically penalises for overﬁtting by making poorer predictions.

For model averaging, we computed model weights wm for each model m, once for the GDF-based
AICc and for the cross-validation log-likelihood, using the equation for Akaike-weights (Turkheimer

6

et al. 2003, Burnham & Anderson 2002, p. 75):

wAICc
m =

e− 1
2 ∆AICc
r=1 e− 1
∑n

m
2 ∆AICc

r

,

(11)

m = AICcm − AICcmin, taking the smallest AICc, i.e. the AICc of the best of the candidate

where ∆AICc
models as AICcmin; n is the number of models to be averaged over.

The same idea can be applied to cross-validated log-likelihoods, so that

where ∆CV

m = (cid:96)CVmax − (cid:96)CVm, with (cid:96)CVmax being the largest cross-validated log-likelihood in the model set.

wCV
m =

e∆CV
m
∑n
r=1 e∆CV

r

,

(12)

3 Results

3.1 GDF conﬁguration analysis

For normally distributed data, increasing the number of points perturbed simultaneously typically slightly
increased the variance of the Generalised Degrees of Freedom calculated for the model (Fig. 1 left column,
GLM, GAM, but not for randomForest and ANN). For GLM and GAM, GDF computations yielded
exactly the same value as the model’s rank (indicated by the dashed horizontal line).

For simulated Bernoulli data (Fig. 1 central column), we also observed an effect of the number of
points perturbed on the actual GDF value, which decreased for GLM, GAM and ANN, but increased for
BRTs with the number of data points perturbed. Several data points needed to be perturbed (> 20) to yield
an approximately correct estimate. More worryingly, GDF depended non-linearly on the number of data
points perturbed, with values varying by a factor of 2 for GAM and ANN. For GAM the sensitivity occurs
because the smoothing parameter selection is sensitive to the quite severe information loss as more and
more data are perturbed. GLM and BRT yielded more consistent but still systematically varying GDF-
estimates.

The same pattern was observed for the sperm whale data (Fig. 1, right column). For the GLM, there
was still some bias observable, also in the sperm whale-case study’s GLM. We attribute it to the fact that
by perturbing the binary data we also alter the prevalence.

The two replicate simulations yielded consistent estimates, except in the case of the normal GAM
and normal BRT. This suggests that both methods “ﬁtted into the noise” speciﬁc to the data set, while the
other methods did not. We did not observe this phenomenon with the binary data.

For randomForest, GDF-estimates centre around 0, meaning that perturbations of the data did not
affect the model predictions. This is to some extent explicable by the stochastic nature of randomForest,
giving different predictions when ﬁtted to the same data. We interpret the value of 0 as the perturbations
creating less variability than the intrinsic stochasticity of this approach.

This is not a general feature of stochastic approaches, as in neural networks and boosted regression
trees the intrinsic stochasticity seems to be much less inﬂuential, and both approaches yielded relatively

7

Gaussian

Bernoulli

Sperm Whale

)
F
D
G

(

m
o
d
e
e
r
F
f
o

s
e
e
r
g
e
D
d
e
s
i
l
a
r
e
n
e
G

data points perturbed simultaneously (k)

GLM

GAM

randomForest

ANN

BRT

Figure 1: Models’ GDFs as a function of the numbers of data points perturbed (k) (ranked abscissa) for the
simulated Gaussian (left), Bernoulli (centre) and sperm whale data (right column) and the ﬁve model types
represent the mean GDF for the two replications at each level of k (in
(rows). The ﬁlled red and black dot
light-grey open dots
represents the number of
model parameters of the GLM.

and black open dots ). The dashed line in the ﬁrst row

8

1214161820llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1214161820lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll5678910lllllllll5101520llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1015202530lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1618202224lllllllll−4−2024llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−4−2024lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−4−2024lllllllll40455055606570llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll5101520lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1520253035lllllllll3638404244llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1357950200303234363840lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1594590135llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll303234363840lllllllll153070110GLM

GAM

randomForest

)
F
D
G

(

m
o
d
e
e
r
F
f
o

s
e
e
r
g
e
D
d
e
s
i
l
a
r
e
n
e
G

ANN

BRT

perturbed data points at once (k)

Figure 2: Perturbation magnitude: Estimated GDF along an increasing numbers of perturbed data points (k)
for the simulated Gaussian data. For each k there are 100 GDF replications computed with σ = 0.125σy ( ),
σ = 0.25σy ( ) and σ = 0.5σy ( ), hence lower to higher perturbation magnitude. The dotted green line
represents the model’s rank (GLM) and the sum of estimated degrees of freedom (GAM). Note that x-axis is
rank-transformed.

consistent GDF-values. The actual GDF estimate of course depends on the settings of the methods and
should not be interpreted as representative.

To compute the GDF for normally distribute data, we have to choose the strength of perturbation.
The GDF-value is robust to this choice, unless many data points are disturbed (Fig. 2). Only for BRT
does increasing the strength of perturbation lead to a consistent, but small, decrease in GDF-estimates,
suggesting again that BRTs ﬁt into the noise.

Estimates of GDF for randomForest and ANN (but not BRT) respond with decreasing variance to
increasing the strength of perturbation. Increasing the intensity of perturbation seems to overrule their
internal stochasticity.

It seems clear from the results presented so far that no single best perturbation strength and optimal
proportion of data to perturb exists for all methods. For the following analyses, we use, for normal data,
perturbation = 0.25σy and k = N (except for GAM, where k = 0.2N); and for Bernoulli data k = 0.5N
(except for BRT and ANN, where k = 0.04N).

3.2 Efﬁciency of GDF and cross-validation computations

Both GDF and cross-validation require multiple analyses. For the GDF, we need to run several perturba-
tions, and possibly replicate this many times. For cross-validation, we may also want to repeatedly per-
form the 10-fold cross-validation itself to yield stable estimates. The mean GDF and CV-log-likelihood

9

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll101214161820llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll68101214llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−10−50510151325125250llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll304050607080151325125250llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll353637383940151325125250llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllF
D
G

V
C

m
(cid:96)
∆

Number of Replicates/ Model runs

Figure 3: Development of mean Generalised Degrees of Freedom (GDF, left) and cross-validation log-
likelihood-differences (∆(cid:96)CV
m , right) over 1000 replicates (250000 and 10000 model runs, respectively). GDF
computation with k = 50 data points perturbed per model run and 50 internal replicates for the Gaussian sim-
ulation data (N = 250), i.e. 250 model evaluations per replicate. ∆(cid:96)CV
m derives from 10-fold cross-validation,
i.e. 10 model evaluations per replicate. The dashed lines display the mean GDF and ∆(cid:96)CV
m , respectively, of all
replicates ± one standard error.

((cid:96)CV ) over 1-1000 replicates are depicted in Fig. 3. With 100 runs, both estimates have stabilised, but
100 runs for GDF represent 25000 model evaluations (due to the k = 50 perturbations and 50 internal
replicates for a data set of N = 250), while for 10-fold cross-validation these represent only 1000 model
evaluations, making it 25 times less costly.

3.3 GDF-based AIC vs Cross-Validation

We analysed four data sets with the above settings for GDF and cross-validation, two simulated (Gaussian
and Bernoulli) and two real-world data sets (sperm whale and red fox geographic distributions).

Both Generalised Degrees of Freedom (GDFm) and cross-validation log-likelihood-differences (∆(cid:96)CV
m )
measure model complexity in an asymptotically equivalent way (section 1.2). For ﬁnite data sets both
approaches yield identical rankings of model complexity but are rather different in absolute value (eqn. 1).
Particularly the red fox-case study yields very low GDF-estimates for GLM, GAM and randomForest,
while their cross-validation model complexity is much higher.

Model complexity is only one term in the AIC-formula (eqn. 8) and for large data sets the log-
likelihood will dominate. To put the differences between the two measures of model complexity into
perspective, we computed AICGDF for all data sets and compared it to the equivalent cross-validation
deviance (−2(cid:96)CV).

Across the entire range of data sets analysed both approaches yield extremely similar results (Fig. 4
right). Within each data set, however, the pattern is more idiosyncratic, revealing a high sensitivity to low
sample size (N < 1000, i.e. all data sets except the red fox).

10

0200400600800100014.614.815.015.215.40500001000001500002000002500000200400600800100015.516.016.517.017.50200040006000800010000Table 1: Measures of model complexity (mean ± standard deviation): Generalised Degrees of Freedom (GDF)
and ∆(cid:96)CV
m derived from cross-validation (see section 1.2) for Gaussian and Bernoulli simulation data and real-
world sperm whale and red fox distribution data. The true ranks for the GLMs are 15, 15, 7 and 8, respectively.

Model

GDF

∆(cid:96)CV
m

GDF

∆(cid:96)CV
m

GLM
GAM

GAUSSIAN SIMULATION
16.7± 1.97
15.1± 0.30
10.0± 0.41
10.7± 2.09
2.8± 2.67
randomForest −0.04± 0.33
69.4± 5.76
58.7± 0.82
38.5± 0.28
45.1± 2.17

ANN
BRT

BERNOULLI SIMULATION
19.4± 3.18
14.5± 0.41
11.0± 0.58
27.0± 3.82
1.3± 2.10
−0.21± 0.41
12.1± 1.67
12.6± 0.25
34.2± 0.17
20.3± 1.29

SPERM WHALE

6.8± 0.24
19.2± 0.59

7.2± 1.40
29.8± 7.63
randomForest −0.50± 0.26 −1.1± 2.66
35.0± 4.96
26.0± 1.29

29.3± 0.49
37.0± 0.17

GLM
GAM

ANN
BRT

RED FOX

7.9± 0.38
9.0± 0.21
1.11± 5.83
50.3± 0.66
59.0± 0.12

37.39± 13.0
37.08± 10.4
31.7± 13.22
64.2± 80.69
54.6± 2.30

V
C

(cid:96)
2
−
e
c
n
a
i
v
e
d

d
e
t
a
d
i
l
a
v
-
s
s
o
r
C

GDF-based AIC

Figure 4: Cross-validated deviance versus GDF-based AICc. Error bars display ± one standard error (some-
times too small to be visible). The identity line (y = x) indicates the equivalence of both measures. Colours rep-
); sperm whale
resent the four datasets: Gaussian simulated (top-left,
(bottom-left, ) and red fox (bottom-centre, ) data. In the right panel per datum GDF and −2(cid:96)CV, respectively,
i.e. divided by the number of data points.

) and Bernoulli simulated (top-centre,

11

lllll11001150120012501100115012001250GLMGAMrFANNBRTlllll160180200220160170180190200210220230GLMGAMrFANNBRTlllll250300350400250300350400GLMGAMrFANNBRTlllll550065007500850055006000650070007500800085009000GLMGAMrFANNBRTlllll012345012345lllllllllllllllTable 2: Model weights: Comparison of Akaike weights wAIC and cross-validation weights wCV (see eq. 12)
for Gaussian and Bernoulli simulation data and real-life sperm whale and red fox abundance data. The “surpris-
ingly” good performance of GLMs in the sperm whale case study remains unexplained, but is fully reproducible.

MODEL

wAIC

wCV

wAIC

wCV

GAUSSIAN SIMULATION
0.9998
0.9996

1.548· 10−4
1.407· 10−16
1.158· 10−18
8.647· 10−13

4.390· 10−4
4.772· 10−17
2.632· 10−26
4.001· 10−16

SPERM WHALE

0.9999

1.373· 10−4
3.079· 10−7
5.369· 10−15
7.473· 10−12

0.9997

5.387· 10−10
8.966· 10−7
6.558· 10−18
2.996· 10−5

BERNOULLI SIMULATION

0.9762

4.310· 10−4
2.319· 10−2
1.759· 10−4
4.131· 10−16

0.4682

5.185· 10−10

0.4960

3.578· 10−2
2.381· 10−6

RED FOX

0
0
1
0
0

0
0
1
0
0

GLM
GAM

rF

ANN
BRT

GLM
GAM

rF

ANN
BRT

3.4 GDF, ∆(cid:96)CV

m and model weights

For all four datasets one modelling approach always substantially outperforms the others, making model
averaging an academic exercise. We compare model weights (according to eqns 11 and 12) purely to quan-
tify the differences in AICGDF and cross-validation deviance for model averaging. Only for the Bernoulli
simulation is the difference noticeable (Table 2). Here GLM and randomForest share the model weight
when quantiﬁed based on cross-validation, while for the GDF-approach GLM takes all the weight.

4 Discussion

So far, Ye’s (1998) Generalised Degrees of Freedom-concept did not attract much attention in the sta-
tistical literature, even though it builds on established principles and applies to machine learning, where
model complexity is unknown. Shen & Huang (2006) have explored the perturbation approach to GDFs
in the context of adaptive model selection for linear models and later extended it to linear mixed effect
models (Zhang et al. 2012). The also extended it to the exponential family (including the Bernoulli dis-
tribution) and even to classiﬁcation and regression trees (Shen et al. 2004). Our study differs in that the
models considered are more diverse and internally include weighted averaging, which clearly poses a
challenge to the GDF-algorithm.

12

4.1 GDF for normally distributed data

For normally distributed data our explorations demonstrate a low sensitivity to the intensity of pertur-
bation used to compute GDFs. Furthermore, across the ﬁve modelling approaches employed here, GDF
estimates are stable and constant for different numbers of data points perturbed simultaneously.

GDF-estimates were consistent with the rank of GLM models and in line with the estimated degrees
of freedom reported by the GAM. For neural networks and boosted regression trees, GDF-estimates
appear plausible, but cannot be compared with any self-reported values. Compared to the cross-validation
method, GDF values are typically, but not consistently, lower by 10-30% (Table 1).

For randomForest, GDF-estimates were essentially centred on zero. It seems strange to ﬁnd that an
algorithm that uses hundreds of classiﬁcation and regression trees internally to actually have no (or even
negative) degrees of freedom. We expected a low value due to the averaging of submodels (called the
‘paradox of ensembles’ by Elder 2003), but not such a complete insensitivity to perturbations. (Within
this study, SH and CFD independently reprogrammed our GDF-function to make sure that this was not
due to a programming error.) As eqn. 4 shows, the perturbation of individual data point are compared
to the change in the model expectation for this data point, and then summed over all data points. To
yield a GDF of 0, the change in expectation (numerator) must be much smaller than the perturbation
itself (denominator). This is possible when expectations are variable due to the stochastic nature of the
algorithm. It seems that randomForest is much more variable than the other stochastic approaches of
boosting and neural networks.

4.2 Bernoulli GDF

Changing the value of Bernoulli-data from 0 into 1 (or vice versa) is a stronger perturbation than adding
a small amount of normal noise to Gaussian data. As our exploration has shown, the GDF for such
Bernoulli-data is indeed much less well-behaved than for the normal data. Not only is the estimated GDF
dependent on the number of data points perturbed, also is this dependence different for each modelling
approach we used. This makes GDF-computation impractical for Bernoulli data. As a consequence, we
did not attempt to extend GDFs in this way to other distributions, as in our perception only a general,
distribution- and model-independent algorithm is desirable.

4.3 GDF vs model complexity from cross-validation

Cross-validation is typically used to get a non-optimistic assessment of model ﬁt (e.g. Hawkins 2004).
As we have shown, it can also be used to compute a measure of model complexity similar (in principle)
to GDF (eqn. 8 and Table 1). Both express model complexity as the effective number of parameters ﬁtted
in the model. GDF and cross-validation-based model complexity estimator ∆(cid:96)CV
m are largely similar, but
may also differ substantially (Table 4, red fox case study). Since the ‘correct’ value for this estimator is
unknown, we cannot tell which approach actually works better. Given our inability to choose the optimal
number of data points to perturb (except for GLM), we prefer ∆(cid:96)CV
m , which does not make any such
assumption.

13

4.4 Remaining problems

To make the GDF approach more generally applicable, a new approach has to be found. The original idea
of Ye (1998) is appealing, but not readily transferable in the way we had hoped.

Another problem, even for Gaussian data where this approach seems to be performing ﬁne, is the high
computational burden. GDF-estimation requires tens of thousands of model evaluations, giving it very
limited appeal, except for small data sets and fast modelling approaches. Cross-validation, as alternative,
is at least an order of magnitude faster, but still requires around 1000 evaluations. If the aim is to compute
model weights for model averaging, no precise estimation of model complexity is needed and even the
results of a single 10-fold cross-validation based on eqn. 8 can be used. It was beyond the scope of this
study to develop an efﬁcient cross-validation-based approach to compute degrees of freedom, but we
clearly see this as a more promising way forward than GDF.

4.5 Alternatives to AIC

The selection of the most appropriate statistical model is most commonly based on Kullback-Leibler
(KL) discrepancy (Kullback & Leibler 1951): a measure representing the distance between the true and
an approximating model. Thus, we assume that a model m, for which the distance to the true model
is minimal, is the KL-best model. Yet, since KL-discrepancy is not observable, even if a true model
existed, many statisticians have attempted to ﬁnd a metric approximation (e.g. Burnham & Anderson
2002, Burnham et al. 1994). Akaike (1973), who proposed this measure as the basis for model selection
in the ﬁrst place, developed the AIC to get around the discussed problem.

The point of the cross-validated log-likelihood is that we do away with the approximation that yields
the degrees of freedom term in the AIC, instead estimating the model-dependent part of the KL diver-
gence directly. This approach is disadvantageous if AIC can be computed from a single model ﬁt. But
if the EDF terms for the AIC would require repeated model ﬁts then there is no reason to use the AIC-
approximation to the KL-divergence, rather than a more direct estimator. If leave-one-out cross-validation
is too expensive, then we can leave out several, at the cost of some Monte-Carlo variability (resulting from
the fact that averaging over all possible left out sets is generally impossible).

5 Conclusion

We have shown that the idea of using GDFs to extend information-theoretical measures of model ﬁt (such
as AIC) to non-likelihood models is burdened with large computational costs and yields variable results
for different modelling approaches. Cross-validation is more variable than GDFs, but a more direct way
to compute measures of model complexity, ﬁt and weights (in a model averaging context). As cross-
validation may, but need not, employ the likelihood ﬁt to the hold-out, it appears more plausible for
models that do not make likelihood assumptions. Thus, we recommend repeated (> 100 times) 10-fold
cross-validation to estimate any of the statistics under consideration.

14

Acknowledgements

This work was partly performed on the computational resource bwUniCluster funded by the Ministry of
Science, Research and Arts and the Universities of the State of Baden-Wrttemberg, Germany, within the
framework program bwHPC.

References

Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In Second

International Symposium on Information Theory (pp. 267–281).: Akademinai Kiado.

Burnham, K. P. & Anderson, D. R. (1998). Model Selection and Multimodel Inference: A Practical

Information-Theoretic Approach. Berlin: Springer.

Burnham, K. P. & Anderson, D. R. (2002). Model Selection and Multimodel Inference: A Practical

Information-Theoretic Approach. Berlin: Springer. 2nd edition.

Burnham, K. P., Anderson, D. R., & White, G. C. (1994). Evaluation of the Kullback-Leibler discrepancy
for model selection in open population capture-recapture models. Biometrical Journal, 36(3), 299–315.

Diniz-Filho, J. A. F., Rangel, T. F. L. V. B., & Bini, L. M. (2008). Model selection and information theory

in geographical ecology. Global Ecology and Biogeography, 17, 479–488.

Dormann, C. & Kaschner, K. (2010). Where’s the sperm whale? A species distribution example analysis.

http://www.mced-ecology.org/?page_id=355.

Dormann, C. F., Gruber, B., Winter, M., & Herrmann, D. (2010). Evolution of climate niches in european

mammals? Biology Letters, 6, 229–232.

Draper, D. (1995). Assessment and propagation of model uncertainty. Journal of the Royal Statistical

Society Series B, 57, 45–97.

Elder, J. F. (2003). The generalization paradox of ensembles. Journal of Computational and Graphical

Statistics, 12(4), 853–864.

Elith, J., Graham, C. H., Anderson, R. P., Dudk, M., Ferrier, S., Guisan, A., Hijmans, R. J., Huettmann,
F., Leathwick, J. R., Lehmann, A., Li, J., Lohmann, L. G., Loiselle, B. A., Manion, G., Moritz, C.,
Nakamura, M., Nakazawa, Y., Overton, J. M., Peterson, A. T., Phillips, S. J., Richardson, K., Scachetti-
Pereira, R., Schapire, R. E., Sobern, J., Williams, S., Wisz, M. S., Zimmermann, N. E., & Araujo, M.
(2006). Novel methods improve prediction of species’ distributions from occurrence data. Ecography,
29(2), 129–151.

Elith, J. & Leathwick, J. R. (2009). Species distribution models: ecological explanation and prediction

across space and time. Annual Review of Ecology, Evolution, and Systematics, 40(1), 677–697.

Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining,

Inference, and Prediction, volume 2. Berlin: Springer.

15

Hawkins, D. M. (2004). The problem of overﬁtting. Journal of Chemical Information and Computer

Sciences, 44(1), 1–12.

Hegyi, G. & Garamszegi, L. Z. (2011). Using information theory as a substitute for stepwise regression

in ecology and behavior. Behavioral Ecology and Sociobiology, 65(1), 69–76.

Horne, J. S. & Garton, E. O. (2006). Likelihood cross-validation versus least squares cross-validation for
choosing the smoothing parameter in kernel home-range analysis. Journal of Wildlife Management,
70, 641–648.

Hurvich, C. M. & Tsai, C.-L. (1989). Regression and time series model selection in small samples.

Biometrika, 76(2), 297–307.

Kullback, S. & Leibler, R. A. (1951). On information and sufﬁciency. The Annals of Mathematical

Statistics, 1, 79–86.

Liaw, A. & Wiener, M. (2002). Classiﬁcation and regression by randomForest. R News, 2(3), 18–22.

Madigan, D., York, J., & Allard, D. (1995). Bayesian graphical models for discrete data. International

Statistical Review / Revue Internationale de Statistique, 63(2), 215–232.

Mundry, R. (2011). Issues in information theory-based statistical inference – commentary from a fre-

quentist’s perspective. Behavioral Ecology and Sociobiology, 65(1), 57–68.

Olden, J. D., Lawler, J. J., & Poff, N. L. (2008). Machine learning methods without tears: A primer for

ecologists. The Quarterly Review of Biology, 83(2), 171–193.

Raftery, A. E. (1996). Approximate bayes factors and accounting for model uncertainty in generalised

linear models. Biometrika, 83(2), 251–266.

Recknagel, F. (2001). Applications of machine learning to ecological modelling. Ecological Modelling,

146(1–3), 303–310.

Ridgeway, G. et al. (2013). gbm: Generalized Boosted Regression Models. R package version 2.1.

Shen, X. & Huang, H.-C. (2006). Optimal model assessment, selection, and combination. Journal of the

American Statistical Association, 101(474), 554–568.

Shen, X., Huang, H.-C., & Ye, J. (2004). Adaptive model selection and assessment for exponential family

distributions. Technometrics, 46(3), 306–317.

Spiegelhalter, D. J., Best, N. G., Carlin, B. P., & van der Linde, A. (2002). Bayesian measures of model

complexity and ﬁt. Journal of the Royal Statistical Society B, 64, 583639.

Stone, M. (1977). An asymptotic equivalence of choice of model by cross-validation and akaike’s crite-

rion. Journal of the Royal Statistical Society. Series B (Methodological), 39(1), 44–47.

Sugiura, N. (1978). Further analysts of the data by akaike’ s information criterion and the ﬁnite correc-

tions. Communications in Statistics - Theory and Methods, 7(1), 13–26.

16

Team, R. C. (2014). R: A Language and Environment for Statistical Computing. R Foundation for

Statistical Computing, Vienna, Austria.

Turkheimer, F. E., Hinz, R., & Cunningham, V. J. (2003). On the undecidability among kinetic models:
From model selection to model averaging. Journal of Cerebral Blood Flow & Metabolism, 23(4),
490–498.

Venables, W. N. & Ripley, B. D. (2002). Modern Applied Statistics with S. New York: Springer, 4th

edition.

Wood, S. (2006). Generalized Additive Models: An Introduction with R. New York: Chapman and

Hall/CRC.

Wood, S. N. (2015). Core Statistics. Cambridge: Cambridge University Press.

Ye, J. (1998). On measuring and correcting the effects of data mining and model selection. Journal of

the American Statistical Association, 93(441), 120–131.

Zhang, B., Shen, X., & Mumford, S. L. (2012). Generalized degrees of freedom and adaptive model
selection in linear mixed-effects models. Computational Statistics and Data Analysis, 56(3), 574586.

17

