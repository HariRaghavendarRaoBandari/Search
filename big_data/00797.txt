6
1
0
2

 
r
a

 

M
7
1

 
 
]

Y
C
.
s
c
[
 
 

3
v
7
9
7
0
0

.

3
0
6
1
:
v
i
X
r
a

The SPHERE Challenge

Activity Recognition with Multimodal Sensor Data

Niall Twomey1, Tom Diethe1, Meelis Kull1, Hao Song1, Massimo Camplani1,
Sion Hannuna1, Xenofon Fafoutis1, Ni Zhu1, Pete Woznowski1, Peter Flach2,

and Ian Craddock1

1 Department of Electrical and Electronic Engineering, University of Bristol, UK,

2 Department of Computer Science, University of Bristol, UK,

(cid:109) http://irc-sphere.ac.uk/sphere-challenge/home

(cid:66) firstname.lastname@bristol.ac.uk

Abstract. This paper outlines the Sensor Platform for HEalthcare in
Residential Environment (SPHERE) project and details the SPHERE
challenge that will take place in conjunction with European Conference
on Machine Learning and Principles and Practice of Knowledge Discov-
ery (ECML-PKDD) between March and July 2016. The SPHERE chal-
lenge is an activity recognition competition where predictions are made
from video, accelerometer and environmental sensors. Monitory prizes
will be awarded to the top three entrants, with e1,000 being awarded to
the winner, e600 being awarded to the ﬁrst runner up, and e400 being
awarded to the second runner up. The dataset can be downloaded from
the University of Bristol’s data servers 3 [?].

1 Background and Summary

In this section we ﬁrst describe the SPHERE project, then the SPHERE chal-
lenge is described, and outline of the remainder of the paper.

1.1 The SPHERE Project

Obesity, depression, stroke, falls, cardiovascular and musculoskeletal disease are
some of the biggest health issues and fastest-rising categories of health-care costs.
The ﬁnancial expenditure associated with these is widely regarded as unsustain-
able and the impact on quality of life is felt by millions of people in the UK each
day. Smart technologies can unobtrusively quantify activities of daily living, and
these can provide long-term behavioural patterns that are objective, insightful
measures for clinical professionals and caregivers.

To this end the EPSRC-funded “Sensor Platform for HEalthcare in Residen-
tial Environment (SPHERE)” Interdisciplinary Research Collaboration (IRC)4

3 https://data.bris.ac.uk/data/dataset/8gccwpx47rav19vk8x4xapcog
4 http://www.irc-sphere.ac.uk/

[11,12,3] has designed a multi-modal sensor system driven by data analytics re-
quirements. The system is under test in a single house, and will be deployed in
a general population of 100 homes in Bristol (UK). The data sets collected will
be made available to researchers in a variety of communities.

Data is collected from the following three sensing modalities:

1. wrist-worn accelerometer;
2. RGB-D cameras (i.e. video with depth information); and
3. passive environmental sensors.

With these sensor data, we can learn patterns of behaviour, and can track the
deterioration/progress of persons that suﬀer or recover from various medical
conditions. To achieve this, we focus activity recognition over multiple tiers,
with the two main prediction tasks of SPHERE including:

1. prediction of Activities of Daily Living (ADL) (e.g. tasks such as meal prepa-

ration, watching television); and

2. prediction of posture/ambulation (e.g. walking, sitting, transitioning).

Reliable predictions of ADL allows us to model behaviour and of residents
over time, e.g. what does a typical day consist of, what times are particular activ-
ities performed etc. Prediction of posture and ambulation will complement ADL
predictions, and can inform us about the physical well-being of the participant,
how mobile/responsive is the participant, how activie/sedintary, etc.

1.2 The SPHERE Challenge

The task for the SPHERE challenge is to predict posture and ambulation labels
given the sensor data from the recruited participants.

We will henceforth refer to posture/ambulation as ‘activity recognition’ for
brevity. It is worth noting that the term activity recognition has diﬀerent in-
terpretations when viewed from accelerometer, video, and environmental sensor
perspectives. The deﬁnition of activities used in challenge most closely aligns to
the terminology used in the ﬁeld of accelerometer-based prediction.

For this task, accelerometer, RGB-D and environmental data is provided.
Accelerometer is samplled at 20 Hz and given in its raw format (see Section
2.1). Raw video is not given in order to preserve anonymity of the participants.
Instead, extracted features that relate to the centre of mass and bounding box of
the identiﬁed persons are provided (see Section 2.2). Environmental data consists
of Passive Infra-Red (PIR) sensors, and these is given in raw format (see Section
2.3).

Twenty (posture/ambulation) activities labels are annotated in our dataset,

and these are enumerated below together with short descriptions:

2

1. a ascend: ascent stairs;
2. a descend: descent stairs;
3. a jump: jump;
4. a loadwalk: walk with load;
5. a walk: walk;
6. p bent: bending;
7. p kneel: kneeling;
8. p lie: lying;
9. p sit: sitting;
10. p squat: squatting;

11. p stand: standing;
12. t bend: stand-to-bend;
13. t kneel stand: kneel-to-stand;
14. t lie sit: lie-to-sit;
15. t sit lie: sit-to-lie;
16. t sit stand: sit-to-stand;
17. t stand kneel: stand-to-kneel;
18. t stand sit: stand-to-sit;
19. t straighten: bend-to-stand; and
20. t turn: turn

The preﬁx ‘a ’ on a label indicates an ambulation activity (i.e. an activ-
ity requiring of continuing movement), the preﬁx ‘p ’ indicate static postures
(i.e. times when the participants are stationary), and the preﬁx ‘t ’ indicate
posture-to-posture transitions. These labels are the target variables that are to
be predicted in the challenge.

The SPHERE challenge will take place in conjunction with the ECML-PKDD5

conference, and will consist of two stages:

– Stage 1. Scripted Data: The ﬁrst stage of the challenge uses sensor data
that was recorded when the participants performed a pre-deﬁned script.
This script is described in later sections of this document. This data will be
available from the challenge start to mid-April.

– Stage 2. Scripted + Real Data: The scripted data from Stage 1 will
be augmented with naturalistic data that was recorded from participants
that did not follow a pre-deﬁned script. Both train and test datasets will be
augmented with this new data. This data will be available from mid-April
to challenge end.

Prizes will be awarded to the ﬁrst three winners, with e1,000 being awarded
to the winner, e600 to the runner up, and e400 to the second runner up. Perfor-
mance is evaluated using the weighted Brier score (see Section 6 for more details
on performance evaluation). Winning participants must participate in the work-
shop that is associated with the challenge. Participation rules and eligibility
criteria are detailed in Section 6.

1.3 Structure of this Paper

The remainder of this paper is structured as follows: in Section 2 the set of
sensors used in this dataset are described in greater detail, Section 3 provides
details regarding the annotation process, Section 4 describes the primary task of
this challenge in greater detail, Section 5 provides a description of the format of
the data, and in Section 6 a link to the full set of rules governing this eligibility
for participation and winning this challenge is provided.

5 http://www.ecmlpkdd2016.org

3

2 Sensors

The following subsections describe the sensing modalities that are found in the
smart home. All sensors are synchronised with network time protocol (NTP).

2.1 Accelerometers

Participants wore a device equipped with a tri-axial accelerometer on the domi-
nant wrist, attached using a strap. The device wirelessly transmits data using the
Bluetooth Low Energy (BLE) standard to several access points (receivers) posi-
tioned within the house. The outputs of these sensors are a continuous numerical
stream of the accelerometer readings (units of g, i.e. approximately 9.81 m s−2).
Accompanying the accelerometer readings are the received signal strength indi-
cations (RSSI) that were recorded by each access point (in units of dBm), and
these data will be informative for indoor localisation. The accelerometers record
data at 20 Hz, and the accelerometer ranges are between ±8 g. RSSI values are
also recorded at 20 Hz, and values are no lower than 110 dBm.

Due to the nature of the sensing platform, there may be missing packets from
the data. Recent accelerometer work done by SPHERE researchers on activity
recognition with accelerometers includes [4,8].

2.2 RGB-D Cameras

Video recordings were taken using ASUS Xtion PRO RGB-Depth (RGB-D) cam-
eras6. Automatic detection of humans was performed using the OpenNI library7,
and false positive detections were manually removed by the organisers by visual
inspection. Three RGB-D cameras are installed in the SPHERE house, and these
are located in the living room, hallway, and the kitchen. No cameras are located
elsewhere in the residence.

In order to preserve the anonymity of the participants the raw video data
are not shared. Instead, the coordinates of the 2D bounding box, 2D centre of
mass, 3D bounding box and 3D centre of mass are provided.
The units of 2D coordinates are in pixels (i.e. number of pixels down and
right from the upper left hand corner) from an image of size 640 × 480 pixels.
The coordinate system of the 3D data is axis aligned with the 2D bounding
box, with a supplementary dimension that projects from the central position of
the video frames. The ﬁrst two dimensions specify the vertical and horizontal
displacement of a point from the central vector (in millimetres), and the ﬁnal
dimension speciﬁes the projection of the object along the central vector (again,
in millimetres).

RGB-D data is very valuable in indoor environments as it can facilitate im-
provement in accuracy for fundamental computer vision tasks such as tracking
[2] while also enabling speciﬁc analysis at higher levels. RGB-D data collected

6 https://www.asus.com/3D-Sensor/Xtion_PRO/
7 https://github.com/OpenNI/OpenNI

4

in the SPHERE house has been used for example for speciﬁc action recognition
[6] and action quality estimation [7].

2.3 Environmental Sensors

The environmental sensing nodes are built on development platforms (Libelium,
with CE marking)8, powered by batteries or/and 5 V DC converted from mains.
PIR sensors are employed to detect presence in the data. Values of 1 indicate that
motion was detected, whereas values of 0 mean that no motion was detected. A
number of methods that deal with environmental sensor data are detailed in the
following [9,10].

2.4 Recruitment and Sensor Layout

Participants were recruited to perform activities of daily living in the SPHERE
house while the data recorded by the sensors was logged to a database. Ethical
approval was secured from the University of Bristol’s ethics committee to conduct
data collection, and informed consent was obtained from healthy volunteers.

In the ﬁrst stage of data collection, participants were requested to follow a
pre-deﬁned script9. In the second stage, participants were recorded and data was
annotated in a non-scripted naturalistic setting.

Figures 1a and 1b show the ﬂoor plan of the ground and ﬁrst ﬂoors of the

smart environment respectively.

3 Annotation

A team of 12 annotators were recruited and trained to annotate the set of ac-
tivities outined in the introduction. To support the annotation process, a head
mounted camera (Panasonic HX-A500E-K 4K Wearable Action Camera Cam-
corder) recorded 4K video at 25 FPS to an SD-card. This data is not shared
in this dataset, and is used only to assist the annotators. Synchronisation be-
tween the NTP clock and the head-mounted camera was achieved by focusing
the camera on an NTP-synchronised digital clock at the beginning and end of
the recording sequences.

An annotation tool called ELAN10 was used for annotation. ELAN is a tool
for the creation of complex annotations on video and audio resources, developed
by the Max Planck Institute for Psycholinguistics in Nijmegen, The Netherlands.
Room occupancy labels are also given in the training sequences. While perfor-
mance evaluation is not directly aﬀected by room prediction on this, participants
may ﬁnd that modelling room occupancy may be informative for prediction of
posture and ambulation.

8 http://www.libelium.com/
9 Details

script

of

the

can be

found at

http://www.irc-sphere.ac.uk/

sphere-challenge/script

10 https://tla.mpi.nl/tools/tla-tools/elan/

5

F
i
g
.
1
:

T
h
e

ﬂ
o
o
r
p
l
a
n

o
f

t
h
e

S
P
H
E
R
E
s

m
a
r
t

h
o
m
e
.

(
a
)

G
r
o
u
n
d

ﬂ
o
o
r

(
b
)

F
i
r
s
t

ﬂ
o
o
r

6

3.7m3.8	m2.8	m1.36	m0.85	m1.2m1.8	wx2h0.73	m0.7mx0.3mGardenGardenDoor	to	basement3.15	mskylightCooker/ovensinkFridge3.2	mwmChimney	breastMirrorWindowShelvesRadiatorstairsCeiling	heightRoom	measurementporche10	steps3.6	m2.9	m2.8	m1.13	wx1.7h3.15	m4	steps2.8	m5.2m1m0.8mx0.3mSPHERE-134-AD1CONFIDENTIALCamera	locationsCeiling	heightChimney	breastMirrorWindowShelves/wadrobesRadiatorstairsRoom	measurementBath/showersinksink5	steps1	steps3.8	m3.6	m2.5	m2.5	m3.4	m3.6	m1.8m3.2m2.2m0.9m0.8x0.3m5m3.5	m1.5m0.9mSPHERE-134-AD2CONFIDENTIAL4 Task

The task is to predict posture, ambulation and transition activities for every
second of the test data using the sensor data provided. Training data will be in
the form of long sequences of data (tens of minutes in length), while test data
will be short sequences, typically of length 10-30 seconds. Predictions must be
made for every second of data.

4.1 Submission Files

sample submission.csv contains an example submission ﬁle11. Each row should
follow the Comma Separated Values (CSV) format below. In this enumerated
list, the the number indicates the column index of the CSV ﬁle.

1. test record id
2. start time
3. end time
4. P r(a ascend)
5. P r(a descend)
6. P r(a jump)
7. P r(a loadwalk)
8. P r(a walk)

9. P r(p bent)

10. P r(p kneel)
11. P r(p lie)
12. P r(p sit)
13. P r(p squat)
14. P r(p stand)
15. P r(t bend)
16. P r(t kneel stand)

17. P r(t lie sit)
18. P r(t sit lie)
19. P r(t sit stand)
20. P r(t stand kneel)
21. P r(t stand sit)
22. P r(t straighten)
23. P r(t turn)

Here, test record id is the name of the test directory, start time and
end time are time stamps indicating the window over which prediction is to be
made (note that the diﬀerence between end time and start time will always
be 1 sec), and P r(.) is the predicted probability of each activity during the
window. As an example, we would expect 10 predictions for a sequence of length
10 seconds.

4.2 Performance Evaluation

As both the targets and predictions are probabilistic, classiﬁcation performance
is evaluated with weighted Brier score[1]:

N(cid:88)

C(cid:88)

BS =

1
N

n=1

c=1

wc (pn,c − yn,c)2

(1)

where N is the number of test sequences, C is the number of classes, wc is the
weight for each class, pn,c is the predicted probability of instance n being from
class c, and yn,c is the proportion of annotators that labelled instance n as arising

11 This ﬁle is generated with: https://github.com/IRC-SPHERE/sphere-challenge/

blob/master/sample_submission.py

7

from class c. Lower Brier score values indicate better performance, with optimal
performance achieved with a Brier score of 0.
For the ﬁrst phase of the challenge (i.e. with scripted data only) the class
weights will be uniform, i.e. wc = 1/C, ∀ 1 ≤ c ≤ C. For the second phase
(i.e. with both scripted and real data) the weights will be adjusted to place
more emphasis on the more diﬃcult tasks. The changes to the weights will be
announced on the challenge website and this document will be updated to reﬂect
the changes.

5 Data

This section outlines the format of the data records available from this challenge.

5.1 Directory Structure

Training data and testing data can be found in the ‘train’ and ‘test’ sub-
directories respectively. The recorded data are collected under unique codes (each
recording will be referred to as a ‘data sequence’). Timestamps are re-based to
be relative to the start of the sequences, i.e. each sequence always starts from
t = 0.

Each data sequence contains the following essential ﬁles:

1. targets.csv
2. pir.csv
3. video hallway.csv
4. video living room.csv
5. video kitchen.csv
6. meta.json

Two additional ﬁles are also provided in the training sequences:

1. annotations *.csv
2. location *.csv

The data from annotations *.csv is used to create the targets.csv ﬁle.

locations.csv is available for participants that want to model the location.

A git repository is publicly available 12. In this repository a number of
scripts for visualisation, benchmarking and data processing are available. (All
subsequent sensor images were generated using these scripts.)

5.2 targets.csv (train only)

This ﬁle contains the probabilistic targets for classiﬁcation. Multiple annotators
may have annotated each sequence, and this ﬁle aggregates all of the annota-
tions over 1 sec windows. These ﬁles can be created from annotations *.csv,

12 https://github.com/IRC-SPHERE/sphere-challenge/

8

Fig. 2: Example PIR for training record 00001. The black lines indicate the time
durations where a PIR is activated. The blue and green wide horizontal lines
indicate the room occupancy labels as given by the two annotators that labelled
this sequence.

and the following script shows how they are generated: https://github.com/
IRC-SPHERE/sphere-challenge/blob/master/gen_targets.py.

The following 20 activities are labelled:

annotations = (’a_ascend’, ’a_descend’, ’a_jump’, ’a_loadwalk’,
’a_walk’, ’p_bent’, ’p_kneel’, ’p_lie’, ’p_sit’,
’p_squat’, ’p_stand’, ’t_bend’, ’t_kneel_stand’,
’t_lie_sit’, ’t_sit_lie’, ’t_sit_stand’,
’t_stand_kneel’, ’t_stand_sit’, ’t_straighten’,
’t_turn’)

where the preﬁx naming convention is the same as that described earlier.

targets.csv ﬁles contain 22 columns:

1. start: The starting time of the window (relative to the start of the sequence)
2. end: The ending time of the window (relative to the start of the sequence)
3. targets: Columns 3-22: the 20 probabilistic targets.

5.3 pir.csv (train + test)

This ﬁle contains the start time and duration for all PIR sensors in the smart
environment. PIR sensors are located in the following rooms:

pir_locations = (’bath’, ’bed1’, ’bed2’, ’hall’, ’kitchen’,

’living’, ’stairs’, ’study’, ’toilet’)

The columns of this CSV ﬁle are:

1. start: the start time of the PIR sensor (relative to the start of the sequence)
2. end: the end time of the PIR sensor (relative to the start of the sequence)
3. name: the name of the PIR sensor being activated (from the pir locations

list)

4. index: the index of the activated sensor from the pir locations

Example PIR sensor activations and ground truth locations are overlaid in Fig-
ure 2. Note, the PIR sensors can be noisy in nature.

9

(a) Acceleration signal trace shown over a 5 minute time period. Annotations
are overlaid.

(b) RSSI values from the four access points. Room occupancy labels are shown
by the the horizontal lines.

Fig. 3: Example acceleration and RSSI signals for training record 00001. The line
traces indicate the accelerometer/RSSI values recorded by the access points. The
horizontal lines indicate the ground-truth as provided by the annotators (two
annotators annotated this record, and their annotations are depicted by the
green and blue traces respectively).

5.4 meta.json (train + test)

This ﬁle contains meta-data regarding the data records regarding the duration
of the sequence. This data can be inferred from the sensor data but is provided
for convenience.

5.5 acceleration.csv (train + test)

The acceleration ﬁle consists of eight columns:

10

Fig. 4: Example centre 3d for training record 00001. The horizontal lines indi-
cate annotated room occupancy. The blue, green and red traces are the x, y,
and z values for the 3D centre of mass.

1. t: this is the time of the recording (relative to the start of the sequence)
2. x/y/z: these are the acceleration values recorded on the x/y/z axes of the

accelerometer.

3. Kitchen AP/Lounge AP/Upstairs AP/Study AP: these specify the

RSSI of the acceleration signal as received by the access kitchen/lounge/upstairs
access points. Empty values indicate that the access point did not receive
the packet.

Sample acceleration and RSSI with overlaid annotations are shown in Figure 3.
Figure 3a shows the acceleration signals overlaid with activity labels, and Fig-
ure 3b shows the RSSI signal information with the overlaid room occupancy.

5.6 video *.csv (train + test)

The following columns are found in the video hallway.csv, video kitchen.csv
and video living room.csv ﬁles:

1. t: The current time (relative to the start of the sequence)
2. centre 2d x/centre 2d y: The x- and y-coordinates of the centre of the

2D bounding box.

11

3. bb 2d br x/bb 2d br y: The x and y coordinates of the bottom right (br)

corner of the 2D bounding box

4. bb 2d tl x/bb 2d tl y: The x and y coordinates of the top left (tl) corner

of the 2D bounding box

5. centre 3d x/centre 3d y/centre 3d z: the x, y and z coordinates for the

centre of the 3D bounding box

6. bb 3d brb x/bb 3d brb y/bb 3d brb z: the x, y, and z coordinates for

the bottom right back (brb) corner of the 3D bounding box

7. bb 3d ﬂt x/bb 3d ﬂt y/bb 3d ﬂt z: the x, y, and z coordinates of the

front left top (ﬂt) corner of the 3D bounding box.

Example 3D centre of mass data is plotted for the hallway, living room and
kitchen cameras in Figure 4. Room occupancy labels are overlaid on these, where
we can see strong correspondence between the detected persons and room occu-
pancy.

5.7 Supplementary Files

The following two sets of ﬁle need not be used for the challenge, but are in-
cluded to facilitate users that wish to perform additional modelling of the sen-
sor environment. For example, indoor localisation can be modelled with the
locations.csv ﬁle, c.f. [5].

annotations *.csv (train only):

This ﬁle provides the individual annotations as provided by the annotators.
The target variables are the same as for targets.csv. The annotations *.csv
contains the following four columns:

1. start: the start time of the activity (relative to the start of the sequence)
2. end: the end time of the activity (relative to the start of the sequence)
3. name: the name of the label (from the annotations list)
4. index: the index of the label name starting at 0

location *.csv (train only):

This ﬁle provides the annotation labels for room occupancy. The following

rooms are labelled:

location_names = (’bath’, ’bed1’, ’bed2’, ’hall’, ’kitchen’,

’living’, ’stairs’, ’study’, ’toilet’)

location.csv contains the following four columns:

1. start: the time a participant entered a room (relative to the start of the

sequence)

2. end: the time the participant left the room (relative to the start of the

sequence)

3. name: the name of the room (from the location names list)
4. index: the index of the room name starting at 0

12

6 Eligibility and Rules

The full set of rules for the challenge can be found at:
http://irc-sphere.ac.uk/sphere-challenge/rules

Acknowledgements

This work was performed under the SPHERE IRC funded by the UK Engineering
and Physical Sciences Research Council (EPSRC), Grant EP/K031910/1.

References

1. Brier, G.W.: Veriﬁcation of forecasts expressed in terms of probability. Monthly

weather review 78(1), 1–3 (1950)

2. Camplani, M., Hannuna, S., Mirmehdi, M., Damen, D., Paiement, A., Tao, L.,
Burghardt, T.: Real-time rgb-d tracking with depth scaling kernelised correlation
ﬁlters and occlusion handling. In: Xianghua Xie, M.W.J., Tam, G.K.L. (eds.) Pro-
ceedings of the British Machine Vision Conference (BMVC). pp. 145.1–145.11.
BMVA Press (September 2015), https://dx.doi.org/10.5244/C.29.145

3. Diethe, T., Flach, P.A.: Smart-homes for eHealth: Uncertainty management and

calibration. In: NIPS 2015 Workshop on Machine Learning in Healthcare (2015)

4. Diethe, T., Twomey, N., Flach, P.: Active transfer learning for activity recog-
nition. In: European Symposium on Artiﬁcial Neural Networks, Computational
Intelligence and Machine Learning

5. Fafoutis, X., Mellios, E., Twomey, N., Diethe, T., Hilton, G., Piechocki, R.: An rssi-
based wall prediction model for residential ﬂoor map construction. In: Proceedings
of the 2nd IEEE World Forum on Internet of Things (WF-IoT) (2015)

6. Tao, L., Hannuna, S., Burghardt, T., Camplani, M., Paiement, A., Aldamen, D.,
Mirmehdi, M., Craddock, I.: A Comparative Home Activity Monitoring Study
using Visual and Inertial Sensors (2015)

7. Tao, L., Paiement, A., Aldamen, D., Mirmehdi, M., Hannuna, S., Camplani, M.,
Burghardt, T., Craddock, I.: A comparative study of pose representation and dy-
namics modelling for online motion quality assessment. Computer Vision and Im-
age Understanding (11 2015)

8. Twomey, N., Diethe, T., Flach, P.: Bayesian active learning with evidence-based
instance selection. In: Workshop on Learning over Multiple Contexts, European
Conference on Machine Learning (ECML’15) (2015)

9. Twomey, N., Diethe, T., Flach, P.: Unsupervised learning of sensor topologies for
improving activity recognition in smart environments. To appear in Neurocomput-
ing (2016)

10. Twomey, N., Flach, P.: Context modulation of sensor data applied to activity
recognition in smart homes. In: Workshop on Learning over Multiple Contexts,
European Conference on Machine Learning (ECML’14) (2014)

11. Woznowski, P., Fafoutis, X., Song, T., Hannuna, S., Camplani, M., Tao, L.,
Paiement, A., Mellios, E., Haghighi, M., Zhu, N., Hilton, G., Damen, D.,
Burghardt, T., Mirmehdi, M., Piechocki, R., Kaleshi, D., Craddock, I.: A multi-
modal sensor infrastructure for healthcare in a residential environment. In: Int.
Conf. Communications (ICC) Workshops, in press (2015)

13

12. Zhu, N., Diethe, T., Camplani, M., Tao, L., Burrows, A., Twomey, N., Kaleshi,
D., Mirmehdi, M., Flach, P.A., Craddock, I.: Bridging e-health and the internet
of things: The SPHERE project. IEEE Intelligent Systems 30(4), 39–46 (2015),
http://doi.ieeecomputersociety.org/10.1109/MIS.2015.57

14

