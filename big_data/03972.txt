RUNNING HEADER

1

Laplacian Eigenmaps from Sparse, Noisy Similarity

Measurements

Keith Levin, Vince Lyzinksi

6
1
0
2

 
r
a

 

M
2
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
2
7
9
3
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—Manifold learning and dimensionality reduction
techniques are ubiquitous in science and engineering, but can
be computationally expensive procedures when applied to large
data sets or when similarities are expensive to compute. To
date,
little work has been done to investigate the tradeoff
between computational resources and the quality of learned
representations. We present both theoretical and experimental
explorations of this question. In particular, we consider Laplacian
eigenmaps embeddings based on a kernel matrix, and explore
how the embeddings behave when this kernel matrix is corrupted
by occlusion and noise. Our main theoretical result shows that
under modest noise and occlusion assumptions, we can (with
high probability) recover a good approximation to the Laplacian
eigenmaps embedding based on the uncorrupted kernel matrix.
Our results also show how regularization can aid this approxima-
tion. Experimentally, we explore the effects of noise and occlusion
on Laplacian eigenmaps embeddings of two real-world data sets,
one from speech processing and one from neuroscience, as well
as a synthetic data set.

I. INTRODUCTION AND MOTIVATION

M ANIFOLD-BASED dimensionality reduction tech-

niques operate under the assumption that data observed
in a high-dimensional space lie on a low-dimensional mani-
fold [4], [5], [7], [53], [57]. Owing to the ubiquitousness of
large high-dimensional data sets, these techniques have been
well studied in the literature, with applications across many
disparate ﬁelds; see [60] for an excellent survey of the relevant
literature. In addition to the classical linear techniques (see,
for example, PCA [38], MDS [7], CCA [30], [34], among
others), numerous manifold embedding procedures have been
proposed to discover intrinsic low-dimensional structure in
nonlinear data (see, for example, ISOMAP [57], Laplacian
eigenmaps [4], among others). These nonlinear techniques
often assume the data is locally linear, and attempt to preserve
this local geometry in the embedding. Due to the locality-
preserving nature of these algorithms, they tend to be empir-
ically robust to modest noise and outliers [4], though general
theoretical results in this direction are comparatively few.

Herein, we theoretically and practically explore the robust-
ness of Laplacian eigenmaps to very general noise conditions.
The present work differs from most manifold embedding

Keith Levin is with the Department of Computer Science, Johns Hopkins

University, Baltimore, MD, 21218 USA klevin@jhu.edu

Vince Lyzinski

is with the Department of Applied Mathematics and
Statistics and the Human Language Technologies Center of Excellence, Johns
Hopkins University, Baltimore, MD, 21218 USA vlyzins1@jhu.edu

This work is partially supported by the XDATA program of the Defense
Advanced Research Projects Agency (DARPA) administered through Air
Force Research Laboratory contract FA8750-12-2-0303; the DARPA SIM-
PLEX program SPAWAR contract N66001-15-C-4041; and DARPA GRAPHS
contract N66001-14-1-4028.

robustness results in two key ways: ﬁrst, we assume that
the uncertainty lies not in the observations themselves, but
rather in our measurement of the pairwise similarities or
distances used to construct the kernel matrix. Second, the noise
model we work under is entirely nonparametric– indeed, we
make no distributional assumptions on the noise other than
unbiasedness (see Equation (2) below).

A. Problem Description

Suppose that X is a set of objects, with a notion of similarity
captured by a kernel function κ : X ×X → [0,∞); i.e., x, y ∈
X are similar if κ(x, y) is large, and x, y ∈ X are not similar
if κ(x, y) is small. Given n observations x1, x2, . . . , xn ∈ X ,
we can represent their similarities via a hollow, undirected
weighted graph with adjacency matrix K given by

(cid:40)

Kij =

κ(xi, xj)
0

if i (cid:54)= j
otherwise.

(1)

Manifold-based dimensionality reduction techniques seek to
recover the low-dimensional structure intrinsic in the similar-
ities captured by K . We note that some manifold embedding
algorithms rely on distance or disimilarity measures rather than
similarities, but the distinction is immaterial here.

The quality of the embedding of K depends upon the
quality of the similarity measure κ and upon our ability to
compute the similarity accurately. If κ(·,·) only approximately
captures the “correct” notion of similarity between the obser-
vations, it is natural to ask how this inﬂuences the quality
of the embedding. Similarly, when κ(x, y) is expensive to
compute, we might ask whether an embedding of similar
quality is possible based on an inexpensive approximation
or by computing κ(x, y) for only a fraction of all pairs of
observations, and inferring the rest of K , for example, by
applying Chatterjee’s universal singular value thresholding
(USVT) [15].

The Laplacian eigenmaps embeddings constructed in [44]
serve as an illustrative example. The authors’ data consists
of a set of 10, 383 word examples X , each represented by
a time series of acoustic feature vectors. For word examples
xi, xj ∈ X , the corresponding entry in the kernel matrix is

Kij = exp{−d2(xi, xj)/σ2},

where d(xi, xj) is a non-decreasing function of the dynamic
time warping (DTW) alignment cost [54] between xi and
xj. We refer the reader to [44] and references therein for
technical details. The inadequacies of DTW as a measure of
word similarity are well documented in the speech processing

RUNNING HEADER

2

(cid:40)

literature [44], [45]. Additionally, DTW cost is computation-
ally expensive, requiring time that scales as the product of
the lengths of the two sequences to be aligned. As such, a
fast estimate of d(xi, xj) or κ(xi, xj) is acceptable, and it
is preferable to avoid computing all O(n2) DTW alignments
required to populate the kernel matrix.

B. Our Model

In light of the above, we consider the following model. We
assume a ﬁxed set of observations x1, x2, . . . , xn ∈ X , and a
similarity function κ deﬁned on X ×X , giving rise to a true but
unknown symmetric kernel matrix K = [Kij] ∈ [0, 1]n×n.
The embedding learned from K is—given the kernel, data,
and embedding procedure—the best embedding we could hope
to learn from the observed data. Suppose, however, that rather
than observing K , we observe a random symmetric matrix
Y ∈ Rn×n, whose entries are generated independently as

Yij = Yji =

Kij with probability p
0

with probability (1 − p),

(2)
where the Kij ∈ [0, 1] are independent random variables
with EKij = Kij and p ∈ [0, 1] is the probability that we
observe a given entry of K. We can think of K as a corrupted
version of K , with errors reﬂecting either (a) our failure
to fully capture the correct notion of similarity on X or (b)
approximation error arising from estimating a computationally
expensive κ(x, y). Similarly, we can view the sparsity of the
matrix Y as reﬂecting the fact that in the case where either
(a) κ(x, y) is computationally intensive, or (b) n is large, we
would like to avoid computing all O(n2) pairwise similarities
when constructing our embeddings.

In this paper, we theoretically and practically explore under
what conditions it is suitable to use the embedding learned
from Y in place of K . Under such conditions, we can
potentially obtain embeddings with quality comparable to
those produced from K , at a greatly reduced computational
cost. In the present work, we consider the performance of
Laplacian eigenmaps [4], [5] under this model, but we believe
that the results extend to other embedding techniques, as well.

C. Laplacian Eigenmaps

As originally described in [4], [5], Laplacian eigenmaps
embeds the observed data X into Rd by ﬁrst constructing the
k-nearest-neighbor (k-NN) or -graph G = (V, E) from X .
In the k-NN graph, an edge is present between i and j if xi
is among the k nearest neighbors (according to some distance
deﬁned on X ) of xj or vice versa. In the -graph, i and j are
adjacent if (cid:107)xi − xj(cid:107)2 <  for a given threshold parameter .
We deﬁne W , the weighted adjacency matrix of G, by

(cid:40)Kij

Wij =

if {i, j} ∈ E
else,

0

(cid:80)
and let D ∈ Rn×n be the diagonal matrix deﬁned by Dii =
j Wij for i ∈ [n]. Then the normalized weighted graph
Laplacian of G [20] is given by L (W ) = D−1/2W D−1/2.
If the eigendecomposition of L (W ) is given by L (W ) =

U ΛU(cid:62) with the diagonal entries of Λ nonincreasing, then
Laplacian eigenmaps embeds X via U [:, 2 : d + 1]—the ﬁrst d
nontrivial eigenvectors of L (W ). (note that U [:, 1] = (cid:126)1, the
trivial all-ones vector). This embedding optimally preserves
the local geometry of X in a least squares sense.

the sense that some or all of the row sums(cid:80)n

In the event that K is noisily and incompletely observed as
Y , how does the d-dimensional Laplacian eigenmaps embed-
ding produced from Y compare with that of K ? Our main
theorem, Theorem 1, deals with the regularized matrix [Yij +r]
rather than Y itself, owing to the fact that when the observation
rate p is small, the matrix pK = EY may be quite sparse, in
j=1 pKij are too
small to guarantee necessary concentration inequalities [41],
[48], [58]. Regularization prevents this pitfall, at
the cost
of changing the matrix to which we converge. We discuss
regularization at more length in Subsection II-C. Intuitively,
our main theorem states that the embedding produced from a
regularized version of Y is similar to that produced by K .
This implies that we can avoid the O(n2) exact computations
for K , using instead the potentially less computationally
expensive Y , with little loss in downstream performance.
Remark 1. We depart from Laplacian eigenmaps as described
by Belkin and Niyogi [4] in that we do not attempt to build
a k-NN graph or -graph from X . However, most commonly-
used kernel functions (e.g., the Gaussian kernel) are such that
K approximates a k-NN or -graph, with Y then a noisily-
observed subgraph of K .

D. Roadmap

The remainder of the paper is organized as follows. In
Section II we give an overview of existing work on robust
manifold dimensionality reduction and related problems. We
present our theoretical results in Section III, and explore these
results experimentally in Section IV. We close with a brief
discussion in Section V.

II. RELATED WORK

A. Manifold Learning

Manifold learning is a general class of techniques for
nonlinear dimensionality reduction that seek to embed a col-
lection of observations into Euclidean space in a way that
preserves some aspect of the structure of those observations.
For example, given a collection of objects and some notion
of distance on those objects, we may wish to embed the
objects into Euclidean space in such a way that all pairwise
distances are (approximately) preserved [36], [46], [47]. A host
of different embedding techniques have been proposed in the
literature (see, for example, [23], [24], [26], [32], [53], [57])
to preserve the numerous different notions of structure in the
data. As outlined in [63], it is possible to view many of these
approaches as special cases of a more general framework

There is a large amount of literature dedicated to improving
the performance of manifold learning and dimensionality
reduction algorithms in the presence of noise and missing
data; see, for example, [10], [14], [31], [55]. The present work
differs from most such results in the following key ways:

RUNNING HEADER

3

We assume that the uncertainty lies not in the observations
themselves, but rather in the computation of the pairwise sim-
ilarities or distances used to construct the kernel matrix, and
our model of this uncertainty is nonparametric. Additionally,
we make no assumption that the observations lie in Euclidean
space. Rather, the objects under study are arbitrary (e.g., they
may be time series, graphs, etc.), and the information about
the geometry of X comes through the kernel function κ(x, y).
With the rise of big data, and the continued popularity of
kernel methods, much research has been devoted to speeding
up the construction and embedding of the kernel matrix,
for example by speeding up evaluation of the kernel func-
tion itself [43], [62], construction of the kernel matrix as a
whole [28], or the embedding procedured [3], [9]. However,
it is often the case that construction of the kernel matrix
is the major bottleneck in machine learning systems [33],
[44], [45]. In the model we consider, the construction of the
noisy, partially observed kernel matrix Y allows for potentially
dramatic speedups compared to the computation of the full,
clean kernel K . A similarly-motivated idea was explored
in [17],
in which the authors presented a pair of divide-
and-conquer algorithms for approximately constructing k-NN
graphs on n observations in d-dimensional Euclidean space.
However, unlike our current approach, they do not consider
noise in the observations themselves or in the assessment of
distances between observations.

Another close analogue to our present work is the paper of
Rohe, Chatterjee and Yu [52]. In [52], the authors theoretically
and empirically explored the robustness properties of spectral
clustering: i.e., Laplacian eigenmaps applied to a binary graph
adjacency matrix to obtain an embedding in Rd prior to k-
means clustering. In the language of the present paper, they
considered the inner product kernel matrix K ∈ Rn×n on
a ﬁxed (but unknown) subset X ⊂ Rd. From this kernel,
they observe the symmetric matrix Y ∈ {0, 1}n×n with
independent entries deﬁned by

(cid:40)

Yij = Yji =

1 with probability κij
0 with probability (1 − κij).

(3)

They compared the Laplacian spectral embedding based on
K with that based on Y . Their key result showed that,
under some mild assumptions on the spectrum of L (K ) (the
normalized Laplacian of K ), the eigenspace of L (Y ) does
not signiﬁcantly differ from the corresponding eigenspace of
L (K ) (after suitable rotation). As a result, they prove that
spectral clustering of L (Y ) consistently estimates the clusters
obtained by spectrally clustering L (K ). While our main
theorem uses results ([52, Prop. 2.1 and Thm. 2.2]) developed
in that paper,
the generality of our occlusion model (2)
compared to (3) requires new proof techniques. Additionally,
our manifolds do not necessarily have a well-deﬁned cluster
structure (as the stochastic blockmodel graphs of [52] do),
and so we do not consider consistency of clustering of our
embedding. Rather, in Theorem 1, we prove that the relevant
eigenvectors of L (Y ) do not signiﬁcantly differ from the
corresponding eigenvectors of L (K ). As in [52], we expect
the consistency of subsequent inference to similarly follow.

B. Matrix Completion and Data Imputation

A natural approach to applying Laplacian eigenmaps to Y is
to ﬁrst impute the missing entries of Y using matrix comple-
tion techniques. For example, with the additional assumption
that K is approximately low-rank, it would be possible to
impute the missing data via the techniques developed in
compressed sensing [12]. While some compressed sensing
papers have considered matrix completion in the presence of
both noise and occlusion [11], [19], most also require bounds
on the incoherence of matrix K , a requirement that need not
hold in general for the kernel matrices we consider here.

Some recent matrix completion work has considered the
problem of imputing missing entries in a distance matrix [1],
[37], [59]. Among these works, [37] is closest in spirit to
the problem considered in the present work. In [37],
the
authors considered the problem of placing n objects into d-
dimensional Euclidean space based on noisy, occluded mea-
surements of the O(n2) pairwise distances. Their semideﬁnite
programming-based approach solves this problem under a very
general error model, where nothing is known about the errors
other than a bound on their magnitude. However, their model
differs from ours in two key ways. First, the observations in
question are assumed to be points in d-dimensional Euclidean
space, while ours need only be endowed with a kernel func-
tion. Second, their assumption is that distance measurements
are taken on all pairs of points within a ﬁxed radius of one
another. However, under our model, all entries of K are
equally likely to be (noisily) observed.

Chatterjee [15] considered the problem of completing an
arbitrary matrix based on partial, noisy observations, with no
speciﬁc assumptions on the matrix structure. His universal
singular value thresholding (USVT) procedure constructs a
minimax optimal estimate for K based on its occluded, noisy
measurement Y (as deﬁned in (2)). Though we believe that
the results obtained in this paper would hold in a qualitatively
similar way if we used USVT applied to matrix Y prior to
embedding, analyzing the behavior of the USVT estimate of
K under the graph Laplacian is theoretically challenging, and
we do not pursue it further here. In empirical comparisons, we
found our method and Chatterjee’s USVT performed nearly
identically across our data sets. We do note that USVT requires
an expensive SVD computation, and yields a dense matrix as
an estimate of K , instead of the sparse Y , which may be
computationally intractable for large n.

C. Matrix Concentration

Recent years have seen a ﬂurry of results proving concentra-
tion results for sums of random matrices [2], [16], [39], [41],
[42], [48], [50], [58], in the spirit of the well-established scalar
analogues [8]. Many existing concentration results require
assumptions about the density of the underlying graphs [48],
[52]. For example, many such results hold only in the dense
regime and require a lower bound on the average degree (i.e.,
a lower bound on the row sums of the expected value of
the random matrix). It is well known that the high variance
associated with small average degree precludes concentration
of the Laplacian for general weighted graphs [22], [27], [40],

RUNNING HEADER

4

[41]. This is an issue for the problem considered in the present
work, especially when the observation rate p is small.

Existing empirical and theoretical results show that regular-
ization yields the desired concentration of the graph Laplacian
for sparse graphs (see [2], [16], [39], [41], [42], [50] and
references therein). This regularization typically takes the
form of either adding a small number to each entry of the
adjacency matrix, as in [41], or by adding to the degree
matrix directly, as in [50]. Our result draws on this line of
work by investigating the behavior of the Laplacian eigenmaps
embeddings when regularization is applied. In this sense, the
current work is a natural outgrowth of [52] and [41] in that
the former considers concentration of the Laplacian eigenmaps
embeddings under the Frobenius norm, and the latter considers
concentration of the regularized graph Laplacian under the
spectral norm. We follow the former of these two works
and consider concentration under the Frobenius norm, rather
than spectral norm. This differs from the bounds established
in [41], [42], [48], [58], which show concentration of the
adjacency matrix and graph Laplacian under the spectral norm.
We prefer the Frobenius norm formulation of Theorem 1, as
the Frobenius norm between the (suitably rotated) eigenspaces
has a natural interpretation as the Procrustes alignment error
of the orthogonal bases of the two different embeddings.

III. MAIN RESULTS

In the event that the kernel matrix K is noisily, partially
observed as Y , our goal is to theoretically and empirically
understand the impact this observation error will have on the
embedding obtained via Laplacian eigenmaps. We prove that
Laplacian eigenmaps is indeed robust to certain amounts of
both occlusion and noise by ﬁrst proving that (a suitably
regularized version of) L 2(Y ) suitably concentrates about
(a regularized version of) L 2(pK ). Combining this result
with the Davis-Kahan theorem [25], we obtain in Theorem 1
a guarantee that the embedding learned from the occluded
noisy kernel matrix is similar (up to rotation) to that learned
from the regularized clean kernel matrix. We provide relevant
details below and in the appendix.
Let G = (V, E) be an undirected, loop-free, weighted graph
on n vertices with edge weights wij ≥ 0. We represent G by
its adjacency matrix A ∈ Rn×n, with entries

(cid:40)

Aij = Aji =

wij
0

if {i, j} ∈ E
if {i, j} /∈ E.

Given A, we deﬁne its normalized graph Laplacian by

L (A) = D(A)−1/2AD(A)−1/2,

where D(A) ∈ Rn×n is the degree matrix of graph G, a
j=1 Aij. We deﬁne its

diagonal matrix with D(A)ii = (cid:80)n
1/(cid:112)D(A)ii

(cid:16)D(A)−1/2(cid:17)

inverse square root by

(cid:40)

if D(A)ii (cid:54)= 0
otherwise.

=

ii

0

We note that the graph Laplacian as we have deﬁned it differs
from the more commonly used D(A)−1/2(I − A)D(A)−1/2
(e.g., in [20]). We will be interested in the eigenspace of

L (A), and one can easily check that both our L (A) and the
more commonly used deﬁnition have the same eigenspaces.
In general, neither the adjacency matrix nor the graph
Laplacian of sparse random graphs concentrate about their
means owing to high variance in degree distributions [27],
[41], [42]. This suggests that we should not expect that L (Y )
will concentrate for arbitrary kernel matrices, and hence we
turn to regularization. Let J ∈ Rn×n denote the matrix of all
ones. Our main result will require us to bound (cid:107)L 2(Y +rJ)−
L 2(K + rJ)(cid:107)F , where Y is the sparse, noisy version of K
as speciﬁed in (2). We deal with the squared Laplacians for
reasons discussed in [52, Section 2]. Namely, we require that
L (Y + rJ) converge to L (pK + rJ) in Frobenius norm. To
ensure convergence for a suitably broad class of matrices, we
must instead consider the squared Laplacians in combination
with the following Lemma, proved in [52], which ensures that
if certain eigenvectors of L 2(Y + rJ) converge, then so do
the relevant eigenvectors of L (Y + rJ).
Lemma 1 ([52, Lemma 2.1]). Let B ∈ Rn×n be symmetric.
1) λ2 is an eigenvalue of B2 if and only if λ is an eigenvalue

of B.

2) If Bx = λx, then B2x = λ2x.
3) If B2x = λ2x,

then x can be written as a linear
combination of eigenvectors of B with corresponding
eigenvalues λ or −λ.

Our main theorem, Theorem 1, shows that the spans of the
eigenvectors corresponding to the largest eigenvalues of the
population Laplacian and the Laplacian of the sparse noisy
kernel matrix Y are close. As a consequence, subsequent
inference performed on the Laplacian eigenmaps embeddings
will be robust to the errors introduced in Y , since the em-
beddings will be (nearly) isometric to one another. In the
statement of the theorem, we include subscript or superscript
n on all quantities that depend on n, though we will drop
these subscripts in the sequel for notational convenience. For
a matrix B ∈ Rn×n,
let λ(B) denote the (multi-)set of
eigenvalues of B. For a set S ⊂ R, deﬁne λS(B) = λ(B)∩ S.
Theorem 1. For an open interval Sn ⊂ R,
let kn =
|λSn (L (Y (n) + rnJ))| be the cardinality of λSn(L (Y (n) +
rnJ)) (counting multiplicities), and let Xn ∈ Rn×k be the
matrix whose columns form an orthonormal basis for the
subspace spanned by the eigenvectors of L (Y (n) + rnJ)
with corresponding eigenvalues in λSn (L (Y (n) + rnJ)). Let
¯kn = |λSn (L (pK (n) + rnJ))| and let Xn be the analogue
of Xn for L (pK (n) + rJ). Deﬁne

δn = inf{|(cid:96) − s| : (cid:96) ∈ λSc (L (pK + rJ)), s ∈ S}.
(4)
Let rn depend on n in such a way that rn ≥ n−1 log n for
suitably large n. There exists a positive integer N such that
n ≥ N implies that kn = ¯kn, and there exists orthonormal
rotation matrix On such that

(cid:32)

(cid:33)

.

(cid:107)Xn − XnOn(cid:107)F = O

log1/2 n
δnrnn1/2

Proof. Combining Theorems 2 and 3 below yields the result.

RUNNING HEADER

5

Fig. 1. Points sampled from a 3-dimensional Swiss roll.
Remark 2. A key difference between the main theorem in [52]
and our result
is that we do not require a restriction on
the degrees of pK directly. Rather, we use regularization to
ensure that no row sum is too small. We note that letting
p = 1 and making minor adjustments to the arguments in
our concentration inequalities (namely, lower bounds on the
entries of the degree matrix D), we recover the main result
of [52], with a slightly better convergence rate. Namely, if we
deﬁne τ = n−1 mini∈[n] Dii, our result has τ−1 controlling
to rate of convergence of the eigenspaces rather than τ−2 as
in [52] (with dependence on n and δ unchanged)

Our main tool for proving Theorem 1 is the Davis-Kahan

theorem [25], which we use in the form presented in [52].
Theorem 2. Let S ⊂ R be an interval and let X be an
orthonormal matrix whose columns span the same subspace
as that spanned by the eigenvectors of L 2(pK ) with corre-
sponding eigenvalues in

λS(L 2(pK + rJ)) = S ∩ λ(L 2(pK + rJ)).

δ2

F

.

Deﬁne X analogously for L 2(Y + rJ). Let δ be deﬁned for
L 2(pK + rJ) as in (4).
If X and X are of the same dimension, then there exists
orthonormal matrix O, which depends on X and X, such that

(cid:107)X − X O(cid:107)2

F ≤ (cid:107)L 2(Y + rJ) − L 2(pK + rJ)(cid:107)2

1
2
To apply Theorem 2 toward Theorem 1, we need a con-
centration bound for L 2(Y + rJ) about L 2(pK + rJ). We
note that Y, K , J and r all implicitly depend on n, a fact
that we do not generally make explicit in the sequel for ease
of notation, but which we highlight here for clarity. For each
n = 1, 2, . . . , let K (n) be a weighted adjacency matrix for a
graph on n points in X as deﬁned in (1). Similarly, let Y (n) be
the corresponding sparse noisy kernel matrix as deﬁned in (2).
Theorem 3. Assume that regularization parameter r grows
with n in such a way that r = ω(n−1 log n). There exist
constants C, c > 0 such that for suitably large n,

(cid:107)L 2(Y + rJ) − L 2(pK + rJ)(cid:107)F ≤ C

log1/2 n
rn1/2

with probability at least 1 − n−c.
Proof. The proof of this theorem is given in the Appendix.

Fig. 2. Relative error (RelErr) in recovering the clean embedding of the
high-dimensional Swiss roll as a function of noise and occlusion. Each tile
reﬂects the mean of 50 independent trials. We see that recovery is possible
with low relative error except in the extreme case of simultaneous high-noise
and heavy occlusion, suggesting that the embeddings are robust to both noise
and occlusion of the kernel matrix.

Remark 3. A number of results exist concerning concen-
tration of the adjacency matrix and the graph Laplacian of
random graphs (see, for example, [27], [41], [42], [48], [52],
[58]). In general, these results show that the graph Laplacian
concentrates in spectral norm about its mean when the quantity
d = n max1≤i<j(cid:54)=n pij is of size Ω(log n) (here pij is the
probability of an edge appearing between nodes i and j in
the random graph). Our result differs from most of these, in
that we are concerned with concentration under the Frobenius
norm, rather than the spectral norm. We obtain results in a
similar regime, as captured by our lower bound requirements
on the regularization term r.

The other tool necessary to apply Theorem 2 in the present
setting is an assumption on the growth rate of the spectral gap
δ as deﬁned in (4). δn measures how well the eigenvalues in
λS(L 2(pK (n))) are isolated from the rest of the spectrum.
We will require that δn grows in such a way that for suitably
large n, the eigenvalues falling in Sn are those corresponding
to the eigenvectors of interest. The existence of this eigengap is
crucial for the application of the Davis-Kahan Theorem [25],
[52]. The eigengap depends on the matrix pK (n) (i.e., on
the topology of the graph this matrix encodes). As discussed
in [61], the existence of such a gap is a reasonable assumption
when, for example, the data set (viewed through similarity
function κ) has a cluster structure.

Typically, computing the Laplacian eigenmaps embedding
of a data set is not an end in itself, but rather a processing
step performed prior to subsequent inference, classiﬁcation,
or data exploration. Such tasks depend entirely upon the
geometry of the embedded data points produced by Laplacian
eigenmaps. If the geometry of the points produced from the
inexpensive embedding based on Y is approximately equal (up
to rotation) to that of the embedding based on K , then we
can expect comparable performance on downstream tasks that
are invariant under rotations of the data (e.g., clustering). Thus,
our results show that we can obtain performance comparable to
that obtained when using the dense, computationally intensive
K while avoiding the expense of working with K directly.

1e−031e+001e+030.000.250.500.751.00Observation ratea (log scale)0.51.0RelErrRUNNING HEADER

6

Fig. 3. Relative error in recovering the Laplacian eigenmaps embedding of the high-dimensional Swiss roll as a function of dimension at different values
of ﬁdelity parameter α (left) and different observation rates p (right). The true underlying dimension of the data is highlighted in red. Each data point is the
mean of 50 independent trials, with error bars indicating one standard error. We see a pattern typical of model selection problems, in which the expressiveness
of the model (i.e., higher embedding dimension) comes at the cost of increased variance (i.e., higher relative error in recovering the clean embedding).

IV. EXPERIMENTS

In this section, we present simulation and real-world data

to complement our theoretical results in Section III.

A. Data Sets

We consider three data sets, one synthetic, one from con-

nectomics, and one from the speech processing literature.

a) Synthetic Data (Figures 1, 2 and 3): We consider
a high-dimensional analogue of the 3-dimensional swiss roll
manifold (see Figure 1). We sample n points uniformly at
random from the d∗-dimensional unit cube and embed those
points into (d∗ + 1)-dimensional space by applying the swiss
roll transform

(x, y) (cid:55)→ (cx cos(cx), y, cx sin(cx)),

x ∈ R, y ∈ Rd∗−1

where c controls the curvature of the manifold. In all exper-
iments we use n = 5000, d∗ = 6 and c = 5. We chose
this higher-dimensional version of the well-understood, simple
swiss roll manifold to examine the effect of both under- and
over-estimating the dimension d∗. We obtain a kernel matrix
K from these points by applying a Gaussian kernel with
bandwidth σ. Results are fairly stable for a wide range of
values of σ. We use σ = 0.2 in all experiments, while
stressing that the task of selecting parameters in dimensionality
reduction techniques warrants much additional study.

b) C. elegans Connectome (Figure 6): We consider the
task of clustering the 253 non-isolated neurons in the C. ele-
gans, a nematode commonly used as a simple biological model
(see [18] and citations therein). These neurons are categorized
according to their function: sensory neurons, interneurons and
motor neurons, which make up 27.96%, 29.75% and 42.29%
of the connectome, respectively. Our data consists of the
symmetric binary adjacency matrix corresponding to the C.
elegans brain graph, in which each node corresponds to an
individual neuron, with an edge between two neurons if they
share a synapse. As discussed in [18], this brain graph can be
constructed in multiple ways. Here we consider the subgraph
of the chemical connectome induced by the non-isolated
vertices of the electrical gap junction connectome. Our goal

is to embed the nodes of this graph via Laplacian eigenmaps
so that clustering (e.g., by k-means) recovers the three neuron
categories enumerated above. We assess the quality of these
embeddings using adjusted Rand index (ARI) [35], which
measures how well two partitions agree, adjusted for chance.

Fig. 4.
Performance on the speech task, measured by average precision,
as a function of embedding dimension. We see that performance peaks at
an embedding dimension of d = 500, with a severe degradation in the case
where embedding dimension is chosen too small.

c) Speech Data (Figures 4, 5 and 7): We consider a
speech processing data set used in [44], [45], consisting of
10, 383 spoken word examples, representing 5, 539 distinct
word types. We refer the reader to [44] for technical details.
Using DTW alignment cost, we can deﬁne a radial basis kernel
on the set of word examples to obtain a 10, 383 × 10, 383
kernel matrix that serves as our starting point for constructing
embeddings. The evaluation developed in [13] measures how
well a representation distinguishes word types using average
precision (AP), which averages the precision of a classiﬁer
over all possible operating points. The measure runs between
0 and 1, with 1 representing perfect performance. Performance
on the speech data set
in question varies with technique,
including choice of acoustic features, and better performance
than that reported here has been obtained. However, the aim
of this paper is not
that performance, but rather
to examine how noise and occlusion inﬂuence performance

to best

llllllllllllllllllllllllllllllllllll0.00.20.40.6123456789101112Target dimensionError (per dimension)alll1e−040.1100llllllllllllllllllllllllllllllllllll0.00.20.40.6123456789101112Target dimensionError (per dimension)plll0.10.50.9lllllllll0.000.050.100.15101001000Target dimension (log scale)Average precisionRUNNING HEADER

7

Fig. 5. Average precision (AP) on the speech data set as a function of occlusion and noise level for different embedding dimensions d. Each tile is the mean
of ten independent trials. We see that performance degrades similarly for all three target dimensions in the presence of noise and occlusion.

for one speciﬁc choice of parameters (we mean here both
parameters in the sense of kernel bandwidth and in the sense
of representation, such as choice of acoustic features).

B. Noise Conditions

We consider the effects of additive noise and occlusion
both in isolation and in tandem on the quality of Laplacian
eigenmaps embeddings.
a) Additive Noise: Given a kernel matrix K ∈ [0, 1]n×n,
we produce a random matrix K ∈ [0, 1]n×n where Kii = 0
for all i ∈ [n], Kji = Kij for all j < i, and {Kij}1≤i<j≤n are
independent with Kij beta-distributed with EKij = Kij. We
constrain the expected value of beta-distributed Kij in this
way by ﬁxing one of the two shape parameters of the beta
distribution, and varying the other to change the variance of
the Kij. In particular, Kij ∼ Beta(αij, γij) with αij > 0
and γij > 0. ﬁxing γij = αij(1 − Kij)/Kij ensures that
EKij = Kij with

Var Kij =

ij (1 − Kij)
K 2
αij + Kij

,

so that we can vary our level of uncertainty on the Kij
variables by varying αij. We select a single global value
α > 0, and take Kij ∼ Beta(α, α(1− Kij)/Kij). In the limit
α → 0, the Kij are simply Bernoulli random variables with
probability of success pij = Kij. In the limit α → ∞, we have
Kij = Kij almost surely. Thus, we can think of our parameter
α as a measure of the accuracy of our measurements of K .
We note also that our parameterization implies that the Kij
variables do not all have the same variance. Rather, variances
are smaller for Kij nearer to 0 and 1.

b) Occlusion: We observe an occluded version of K ,
where entries above the diagonal are observed independently
with probability p. We proceed with our embedding using this
sparse kernel matrix, with zeros in the unobserved entries.

c) Additive Noise with Occlusion: This condition com-
bines the preceding two. We observe an occluded, noisy
version of matrix K . That
is, we generate noisy matrix
K from K with entries drawn independently from suitably
chosen beta-distributions, then occlude K by independently
observing entries with probability p.

C. Effect of Noise and Occlusion on Embeddings

Our main theoretical result suggests that Laplacian eigen-
maps embeddings should be robust to noise and occlusion.
Figure 2 shows how noise and occlusion inﬂuence the error in

recovering the clean Laplacian eigenmaps embedding. Here,
the target dimension is ﬁxed at d = d∗ = 6, while the noise
and occlusion vary on the two axes. Each tile is the relative
error averaged over 50 independent trials. We see that the clean
Laplacian eigenmaps embedding is recovered with low error
over a wide range of noise levels and occlusion rates, with
performance degrading only when the observation rate goes
below 0.25 in high-noise conditions.

Figure 5 further illuminates the results seen in the synthetic
data. Rather than looking at the relative error in recovering
the clean embedding, we examine how noise and occlusion
in the kernel matrix inﬂuence performance on the down-
stream speech task of distinguishing word types. The plot
shows average precision as a function of both noise level
and occlusion for three different embedding dimensions. We
see that performance decays similarly in all three embedding
dimensions, but that choice of embedding dimension has a
large effect on overall performance. For example, comparing
the d = 100 case in (a) with the d = 500 case in (b), we see
that both exhibit similar deterioration patterns with respect
to noise level and observation rate, but the 500-dimensional
embeddings out-perform the 100-dimensional ones when noise
and occlusion are not so severe as to drown out the signal
present in the kernel matrix.

D. Model Misspeciﬁcation

Selecting the target dimension is of the utmost importance
for good embeddings. Figure 3 shows how embedding dimen-
sion interacts with noise and occlusion on the synthetic data.
The two plots show that relative error in recovering the clean
embedding is smaller at lower target dimensionalities, and this
pattern holds over a wide range of noise levels and occlusion
rates. In particular, we note that relative error in the presence
of high noise and high occlusion remains comparable to the
relative error in low noise and low occlusion conditions. Of
course, this only tells part of the story. Figure 4 shows average
precision on the speech data set under clean conditions, as a
function of embedding dimension. While a low-dimensional
embedding performed under noise or occlusion might very
closely resemble the corresponding clean embedding as in
Figure 3, Figure 4 suggests that such an embedding would
not yield satisfactory performance on downstream tasks such
as classiﬁcation. Indeed, we see here a pattern typical of
model selection tasks: one must balance estimation error of
model parameters against error in ﬁtting the observed data. The
issue of model selection and related issues in clustering and

1e−021e+001e+020.000.250.500.751.00Observation ratea (log scale)0.000.050.100.15APd = 1001e−021e+001e+020.000.250.500.751.00Observation ratea (log scale)0.000.050.100.15APd = 5001e−021e+001e+020.000.250.500.751.00Observation ratea (log scale)0.000.050.100.15APd = 1000RUNNING HEADER

8

Fig. 6. Adjusted Rand index (ARI) on the C. elegans data set for different levels of regularization as a function of dimension at different values of observation
rates p. Each data point is the mean of 50 independent trials. We see that regularization enables us to accurately cluster the neurons even when much of the
structure of the brain graph is occluded, with performance consitently superior to that obtained without regularization.

dimensionality reduction has been addressed elsewhere in the
literature [29], [51], [56] and warrants attention, but it is not
our focus here. We wish only to highlight the fact that small
error in recovering the underlying clean Laplacian eigenmaps
embedding is not in itself sufﬁcient for good performance in
downstream tasks. The noisy embedding can only be as good
as the clean embedding we are attempting to recover.

E. Effect of regularization

In the setting of the current work, when the observation
rate p is too small, we are in the sparse graph setting [2],
[16], [39], [41], [42], [50]. As such, it is natural to consider
whether applying regularization might ease the deterioration
of embedding quality when the observation rate is small.
We follow the regularization procedure described in [41],
in which the regularization parameter r is added to each
entry of the observed matrix. That is, letting Y denote the
occluded version of the noisy matrix K, we apply Laplacian
eigenmaps to the matrix [Yij + r] rather than Y itself. Our
main theoretical results suggest that under suitable conditions,
such an approach will be beneﬁcial. The C. elegans brain
graph is extremely sparse, and occlusion only makes this
sparsity more dramatic. Figure 6 shows how regularization
inﬂuences downstream performance on the C. elegans data
under different levels of occlusion. We see that when r is
chosen too small, regularization is not enough to signiﬁcantly
change the learned embedding. Similarly, when r is chosen
too large (a condition we do not show in the plots), regular-
ization overpowers the signal present in the occluded matrix.
However, with the C. elegans data, we see that there exists a
level (r ≈ 0.01) at which regularization greatly improves ARI,
even when only half of the edges of the graph are known. We
note that embeddings produced by the regularization procedure
described in [50] resulted in nearly identical performance.

The performance seen here is especially exciting from the
neuroscience standpoint– these results suggest that we can
recover structural and functional information in connectome
data even when accurate assessment of all possible neural
connections is impossible. We note the similarity of this phe-
nomenon to that explored in [49], where the authors considered
graph inference in the setting where one can trade the accuracy
of edge assessment against the number of edges assessed.

We close by illustrating conditions under which regular-
ization does not appear to be a beneﬁt. One would think,
initially, and especially given the improvement seen in the C.
elegans data, that regularization would yield similar gains in

our speech task. Figure 7 shows how regularization inﬂuences
downstream performance on the speech task. We see that
regularization does not appear to confer the beneﬁt seen in
the C. elegans data. Crucially, however, moderate amounts of
regularization do not appear have any adverse effects on aver-
age precision. One possible explanation for this phenomenon
comes from the fact that the kernel bandwidth used in [44]
was chosen so as to give the best possible average precision on
precisely the task we are using for evaluation. That is, since the
kernel bandwidth has already been tuned so as to yield high-
quality embeddings, regularization can do little to improve
the embeddings. But this explanation does not account for
the fact that regularization does not appear to confer any
protection against occlusion and noise in the kernel matrix.
It is possible that the speech data set is such that the kernel
matrix is sparse enough that regularization does nothing to pull
us toward a better embedding. We leave further exploration of
this phenomenon to future work.

V. DISCUSSION

We have presented an analysis of the concentration of the
graph Laplacian of certain kernel matrices under occlusion
and noise. Crucial to our bound was the presence of a certain
structure in the kernel matrix that ensures concentration of the
row-sums. Experiments on both synthetic and real data show
that a concentration phenomenon similar to that predicted by
the theory is present, and has effects both on performance in
downstream tasks and on the model selection problem. We
close by brieﬂy mentioning some directions for future work.

A. Adaptive Techniques

The regularization used here was applied uniformly to every
vertex of the graph, but regularization is only required to
control the high variance associated with small-degree nodes.
In light of this, one might consider regularization techniques
that apply only to nodes that require it. It is unclear a priori
whether such an approach would be advantageous, since by
design the regularization techniques applied to graphs are
such that they already do little to change the behavior of
high-degree nodes. However, it stands to reason that a well-
designed adaptive technique might enable convergence of the
regularized estimate to the true expected graph, rather than
to its regularized counterpart as in the current work. For
example, if only a small fraction of the nodes in a given graph
require regularization, then the Frobenius error between the

llllllllllllllllllllllllllllllll0.000.050.100.150.20110100Target dimensionARIrllll00.010.11p = 0.5llllllllllllllllllllllllllllllll0.000.050.100.150.20110100Target dimensionARIrllll00.010.11p = 0.75llllllllllllllllllllllllllllllll0.000.050.100.150.20110100Target dimensionARIrllll00.010.11p = 1RUNNING HEADER

9

(a)

(c)

(b)

(d)

Fig. 7. Average precision on the speech data set as a function of embedding dimension for different levels of regularization under varying amounts of noise
and occlusion: (a) α = 10, p = 0.7, (b) α = 10, p = 1.0, (c) α = 100, p = 0.7, (d) α = 100, p = 1.0. Each data point is the mean of 10 independent
trials. We see that while regularization does not provide the stunning improvement that it does on the C. elegans graph, moderate regularization at least does
not noticeably harm average precision.

regularized and non-regularized adjacency matrix can still go
to zero even if r = Ω(n−2).

In a similar vein, it stands to reason that a technique that
evaluates entries of the kernel matrix adaptively rather than
the edge-independent occlusion model considered here might
achieve more accurate recovery of the clean embeddings.

B. Chatterjee’s USVT

As alluded to in Section II, a natural approach to the prob-
lem considered in this paper would be to apply Chatterjee’s
USVT [15] to the occluded, noisy kernel matrix Y . Applying
USVT in the speech task considered in Section IV yields
results essentially identical to those reported using Y alone
at all noise and occlusion rates. Indeed, USVT performed
remarkably similarly to our method on all three data sets. We
ﬁnd this fact interesting in and of itself and worthy of further
exploration– Chatterjee’s theoretical results suggest that USVT
should perform reasonably even at observation rates smaller
than those required by our analysis, but we observe little to no
difference in performance on the downstream tasks of neuron
classiﬁcation or word discrimination.

C. Graph Construction

We have largely ignored the problem of constructing the
k-NN or -graph, the ﬁrst step in Laplacian eigenmaps and
spectral clustering. Rather than using either of these construc-
tions, we have relied on the fact that by assumption our kernel
matrix can be made to resemble these graphs by using, for
example, a Gaussian kernel. We believe that the our analysis
can be extended to many of these constructions simply by
taking advantage of this resemblance. We leave this extension
for future work.

D. Other Dimensionality Reduction Techniques

To what extent are different embedding techniques robust
to uncertainty in similarity measures (as opposed to errors on
the observations themselves)? To the best of our knowledge,
MDS and Laplacian eigenmaps remain the only techniques
for which such questions have been explored. We believe that
analyses similar to that pursued in the current work should
apply to other dimensionality reduction techniques. Indeed,
given the results in [63], it would be a surprise to learn that
no such general result is possible.

ACKNOWLEDGMENTS

The authors thank Carey E. Priebe and Minh Tang for
helpful discussion in formulating the problem setup and for
their insightful feedback and discussion.

APPENDIX A

CONCENTRATION OF L 2(Y + rJ)

In what follows, we suppress dependence on n for ease of
notation. We remind the reader that all quantities involved,
including the sizes of the matrices, the regularization param-
eter r and observation rate p all implicitly depend on n. We

let (cid:98)Y = Y + rJ denote the regularized version of matrix Y ,
and deﬁne (cid:98)D to be the corresponding degree matrix, so that
(cid:98)Dii = nr +(cid:80)n
pK as (cid:99)K = pK + rJ, with (cid:98)D deﬁned as the corresponding
degree matrix, (cid:98)Dii = nr +(cid:80)n

j=1 Yij. We deﬁne the regularized version of

j=1 pKij.

Throughout, we let C > 0 denote a constant (i.e., not
depending on n), which may change from line to line or from
one lemma to another. We will let β and γ denote quantities
(both depending on n) that will control convergence of the

llllllllllllllllllllllllllllllll0.000.050.100.15101001000Target dimensionAverage precisionrllll01e−061e−040.01llllllllllllllllllllllllllllllll0.000.050.100.15101001000Target dimensionAverage precisionrllll01e−061e−040.01llllllllllllllllllllllllllllllll0.000.050.100.15101001000Target dimensionAverage precisionrllll01e−061e−040.01llllllllllllllllllllllllllllllll0.000.050.100.15101001000Target dimensionAverage precisionrllll01e−061e−040.01RUNNING HEADER

10

node degrees and the Frobenius norm in Theorem 3, respec-
tively. We will see that the constraints on β and γ required
for our concentration bounds are such that when we plug in
γ = C(cid:48)n−1/2r−1 log1/2 n and β = C(cid:48)(cid:48)n−1/2r−1/2 log1/2 n
for suitably chosen constants C(cid:48), C(cid:48)(cid:48) > 0, we obtain the bound
claimed in Theorem 3. We will require that β → 0 as n → ∞,
i.e., that r = ω(n−1 log n).

of (cid:98)Y concentrate about their expected value.

We ﬁrst establish that with high probability, the row sums

β2r
1 + β

Lemma 2. Suppose that there exists constant c1 > 0 such that
for all suitably large n we have
≥ c1

log n

(5)

.

Then for all suitably large n, with probability at least n1−c1,

it holds for all i ∈ [n] that |(cid:98)Dii − (cid:98)Dii| ≤ β(cid:98)Dii.
n(cid:88)
(cid:98)Dii − (cid:98)Dii =

Proof. Fix i ∈ [n]. By deﬁnition,

(Yij + r) − (pKij + r) =

n(cid:88)

Yij − pKij,

n

j=1

j=1

and EYij = pKij. Applying a standard Chernoff-style
bound [21], we have

Pr

where V =(cid:80)n

j=1

we have

(cid:104)|(cid:98)Dii − (cid:98)Dii| ≥ β(cid:98)Dii

EY 2

pEK 2

ij ≤ p

ij. Since

n(cid:88)

(cid:105) ≤ 2 exp
n(cid:88)
(cid:105) ≤ 2 exp
(cid:104)|(cid:98)Dii − (cid:98)Dii| ≥ β(cid:98)Dii

j=1

j=1

V =

(cid:104)|(cid:98)Dii − (cid:98)Dii| ≥ β(cid:98)Dii

Pr

(cid:41)

,

ii

(cid:40) −3β2(cid:98)D 2
6V + 2β(cid:98)Dii
Kij ≤ (cid:98)Dii,
(cid:26)−Cβ2

(cid:98)Dii

(cid:27)

,

(cid:105) ≤ n−c1.

Pr

where C > 0 is a constant. Since (cid:98)Dii ≥ nr by virtue of

1 + β

regularization, our assumption in (5) ensures that

Applying the union bound over all i ∈ [n] yields the result.
Lemma 3. Suppose that γ depends on n in such a way that
there exist constants C(cid:48), C(cid:48)(cid:48) > 0 so that for suitably large n,

and

C(cid:48)γ2 ≥ 16

n2r3 +

16
n2

.

γ ≥ C(cid:48)(cid:48) log1/2 n
n3/2r2
n(cid:88)
ik − (cid:99)K 2
((cid:98)Y 2
(cid:98)D 2
ii(cid:98)D 2

ik)2

k=1

kk

n(cid:88)

i=1

≤ Cγ2,

Then there exists a constant c2 > 0 such that with probability
at least 1 − n−c2, we have

, where C > 0 is a
We wish to bound Pr
constant to be determined. Rather than bound this directly, we
will bound

.

ik

kk

i,k

i,k

Pr

Xik =



Xik ≥ γ2

i,k Xik ≤ C(cid:48)γ2.

Proof. For ease of notation, deﬁne

A standard Chernoff-style bound lets us write

(cid:16)(cid:98)Y 2
(cid:17)2
ik − (cid:99)K 2
ii(cid:98)D 2
(cid:98)D 2
(cid:104)(cid:80)
i,k Xik ≥ Cγ2(cid:105)
(cid:88)
Xik − E(cid:88)
and show that E(cid:80)
 ≤ exp
(cid:88)
(cid:26) −3γ4
(cid:17)4
E(cid:16)(cid:98)Y 2
ik − (cid:99)K 2
(cid:88)
(cid:98)D 4
ii(cid:98)D 4
(cid:110)
(cid:111)
1/((cid:98)D 2
ii(cid:98)D 2
kk) : i, k ∈ [n]
 ≤ exp
(cid:26) −3(γnr)4
Xik ≥ γ2 + E(cid:88)

Xik ≥ γ2 + E(cid:88)

Bounding V ≤ n−6r−8 and M ≤ (nr)−4,

(cid:88)

and M = max

(cid:88)

EX 2

where

ik =

V =

Xik

Xik

Pr

Pr

i,k

i,k

i,k

i,k

kk

ik

,

.

(cid:27)

,

(cid:27)

,

6V + 2γ2M

6n−2r−4 + 2γ2

i,k

i,k

i,k

i,k

Pr

Xik

and using our assumption in (7) to lower bound the denom-
inator inside the exponent by Ω(nγ2), we can guarantee the
existence of a constant c2 > 0 such that

 ≤ n−c2.
(cid:88)
Xik ≥ γ2 + E(cid:88)
It remains for us to show that E(cid:80)
(cid:17)
E(cid:16)(cid:98)Y 4
i,k Xik ≤ C(cid:48)γ2. We have
ik + (cid:99)K 4
n(cid:88)
n(cid:88)
n(cid:88)
Xik ≤ n(cid:88)
ii(cid:98)D 2
(cid:98)D 2
ik + r4(cid:1) + (cid:99)K 4
8(cid:0)pEK 4
n(cid:88)
≤ n(cid:88)
(cid:98)D 2
ii(cid:98)D 2

where we have used the fact that (a + b)2 ≤ 2a2 + 2b2 for all

a, b ∈ R. We note ﬁrst that since (cid:98)Dii ≥ nr for all i ∈ [n], we

(8)

k=1

k=1

k=1

i=1

i=1

i=1

E

kk

kk

ik

ik

,

n(cid:88)

i=1

n(cid:88)

n(cid:88)

n(cid:88)

i=1

k=1

and

≤ 1
r

≤ 1
n2 .

ii(cid:98)D 2
r4(cid:98)D 2

1(cid:98)Dii
ik ≤ EKik = Kik and applying (9), we have
n(cid:88)
(cid:98)D 2
ii(cid:98)D 2

1(cid:98)Diin2r2

≤ n(cid:88)

≤ 1

n2r3 .

pEK 4

kk

i=1

kk

k=1

ik

(10)

(9)

Noting that EK 4

(6)

(7)

have

where C > 0 is a constant.

i=1

RUNNING HEADER

Recalling that (cid:99)Kik = pKik + r by deﬁnition and applying
the deﬁnition of (cid:98)Dii, (9) implies
n(cid:88)

n(cid:88)

n(cid:88)

ik + r4

i=1

k=1

(cid:99)K 4
ii(cid:98)D 2
ik(cid:98)D 2
n(cid:88)

kk

i=1

≤ 8p3
n2r2
≤ 8p3

n2r3 +

8
n2 .

≤ 8

i=1

+ 8

1(cid:98)Dii

n(cid:88)
n(cid:88)

k=1

i=1

p4K 4

ii(cid:98)D 2
(cid:98)D 2
n(cid:88)
ii(cid:98)D 2
r4(cid:98)D 2

k=1

kk

kk

Combining this with (8) and (10) and applying (6) completes
the proof.
Lemma 4. Under the same conditions as Lemma 2, and
assuming there exists a constant C > 0 such that

Cγ2 ≥ β2
nr2 ,

(11)

i=1

i=1

(cid:96)=1

(cid:96)=1

k=1

k=1

i(cid:96) )

i(cid:96) )

≤ Cγ2.

≤ (1 + p + 2r)2

with probability at least n1−c1, we have

n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)

i(cid:96) − (cid:99)K 2
Proof. Observing that (cid:98)Yik + (cid:99)Kik ≤ 1 + p + 2r,
n(cid:88)

n(cid:88)
ik − (cid:99)K 2
ik)((cid:98)Y 2
((cid:98)Y 2
(cid:98)D 2
ii(cid:98)Dkk(cid:98)D(cid:96)(cid:96)
i(cid:96) − (cid:99)K 2
ik − (cid:99)K 2
ik)((cid:98)Y 2
((cid:98)Y 2
ii(cid:98)Dkk(cid:98)D(cid:96)(cid:96)
(cid:98)D 2
n(cid:88)
n(cid:88)
n(cid:88)
((cid:98)Yik − (cid:99)Kik)((cid:98)Yi(cid:96) − (cid:99)Ki(cid:96))
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ β(cid:98)Dii,
(cid:98)Yik − (cid:99)Kik
and hence, since p, r ∈ [0, 1] and (cid:98)Dii ≥ nr,
n(cid:88)
ik − (cid:99)K 2
i(cid:96) − (cid:99)K 2
ik)((cid:98)Y 2
((cid:98)Y 2
(cid:98)D 2
ii(cid:98)Dkk(cid:98)D(cid:96)(cid:96)

By Lemma 2, with probability at least 1 − n1−c1, it holds for
all i ∈ [n] that

≤ 16β2
nr2 .

n(cid:88)

n(cid:88)

(cid:98)D 2

n2r2

i(cid:96) )

k=1

k=1

(cid:96)=1

i=1

i=1

k=1

(cid:96)=1

ii

.

Our assumption in (11) yields the desired result.

i,j,k,(cid:96)

p4KikKjkKi(cid:96)Kj(cid:96)

(cid:98)Dii(cid:98)Djj(cid:98)Dkk(cid:98)D(cid:96)(cid:96)

Lemma 5. (cid:88)
(i) (cid:98)Dii ≥ rn for all i ∈ [n],
(iii) (cid:80)n
we have(cid:88)

Proof. Using the following facts:
(ii) Kik ∈ [0, 1] for all i, j ∈ [n],

k=1 pKik ≤ (cid:98)Dii for all i ∈ [n],
(cid:88)
(cid:98)Dii(cid:98)Djj(cid:98)Dkk(cid:98)D(cid:96)(cid:96)
(cid:88)

p4KikKjkKi(cid:96)Kj(cid:96)

i,j,k,(cid:96)

i,j,k

≤ p
nr
≤ p
nr

i,j,k

n(cid:88)

pKj(cid:96)

(cid:96)=1

≤ p
r

.

p2KikKjk

(cid:98)Dii(cid:98)Djj(cid:98)Dkk
(cid:98)Dii(cid:98)Dkk

p2KikKjk

(i,j,k,(cid:96))∈T

≤ p
r

.

11

Lemma 6. For ease of notation, let

((cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk)((cid:98)Yi(cid:96)(cid:98)Yj(cid:96) − (cid:99)Ki(cid:96) (cid:99)Kj(cid:96))

Xijk(cid:96) =

(12)
and deﬁne T = {(i, j, k, (cid:96)) : i, j, k, (cid:96) ∈ [n] distinct.}. There
exists a constant C > 0 such that

(cid:98)Dii(cid:98)Djj(cid:98)Dkk(cid:98)D(cid:96)(cid:96)

(cid:88)

(i,j,k,(cid:96))∈T

Var Xijk(cid:96) ≤ C

n4r5 .

ijk(cid:96)

ijk(cid:96)

= d−2

E(cid:104)(cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk

Proof. Since i, j, k, (cid:96) are distinct for each (i, j, k, (cid:96)) ∈ T ,
VarXijk(cid:96) = EX 2

(cid:105)2
(cid:105)2 E(cid:104)(cid:98)Yi(cid:96)(cid:98)Yj(cid:96) − (cid:99)Ki(cid:96) (cid:99)Kj(cid:96)
where dijk(cid:96) = (cid:98)Dii(cid:98)Djj(cid:98)Dkk(cid:98)D(cid:96)(cid:96). Expanding (cid:98)Yik = Yik + r and
(cid:99)Kik = pKik + r and using linearity of expectation, we have
E(cid:104)(cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk
(cid:105)2
= E(cid:2)YikYjk − p2KikKjk

+ r(Yik − pKik) + r(Yjk − pKjk)(cid:3)2

,

= Var YikYjk

+ r(r + 2pKjk) Var Yik + r(r + 2pKik) Var Yjk.

For ease of notation, deﬁne
Qijk = p2KikKjk + r(r + 2p)pKik + r(r + 2p)pKjk.

The Bhatia-Davis inequality [6] states that if a random variable
Z satisﬁes Pr[m ≤ Z ≤ M ] = 1, then Var Z ≤ (EZ −
m)(M − EZ). Combining this with the fact that Kik ∈ [0, 1]
for all i, k ∈ [n], we have Var YikYjk ≤ p2KikKjk and
Var Yik ≤ pKik, and hence

E(cid:104)(cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk

(cid:105)2 ≤ Qijk.

Combining this with (12), we have
Var Xijk(cid:96) ≤ d−2

ijk(cid:96)QijkQij(cid:96).

Summing, we have

(cid:88)

d−2
ijk(cid:96)QijkQij(cid:96)

(i,j,k,(cid:96))∈T
d−2
ijk(cid:96)p4KikKjkKi(cid:96)Kj(cid:96)

=

(cid:88)

Var Xijk(cid:96) ≤ (cid:88)
(cid:88)
(cid:88)
(cid:88)

(i,j,k,(cid:96))∈T

(i,j,k,(cid:96))∈T

(i,j,k,(cid:96))∈T

+ 2

+ 4

+ 2

(i,j,k,(cid:96))∈T

d−2
ijk(cid:96)r(r + 2p)p3KikKjkKj(cid:96)

d−2
ijk(cid:96)r2(r + 2p)2p2KikKjk

d−2
ijk(cid:96)r2(r + 2p)2p2KikKj(cid:96)

(r + 2p)2

≤ p

(r + 2p)
n4r4 + 4

where we have used (cid:98)Dii ≥ nr along with Lemma 5 to

n4r5 + 4

bound the ﬁrst sum after the equality, and the other sums are
bounded using reasoning nearly identical to that in the proof
of Lemma 5. The result then follows from r, p ∈ [0, 1].

n4r4

,

RUNNING HEADER

12

Letting (i, j, k, (cid:96)) ∼ (a, b, c, d) denote the fact

that
(a, b, c, d) is a permutation of (i, j, k, (cid:96)), we can bound the
sum of the covariances under consideration by

(cid:88)

(i,j,k,(cid:96))∈T

(cid:88)

(a,b,c,d)∼(i,j,k,(cid:96))

≤ 2C(1 + r)4 (cid:88)

Cov (Xijk(cid:96), Xabcd)

(cid:99)Kik (cid:99)Kjk (cid:99)Ki(cid:96) (cid:99)Kj(cid:96)
kk(cid:98)D 2
jj(cid:98)D 2
ii(cid:98)D 2
(cid:98)D 2
(cid:88)
(cid:99)Kik (cid:99)Kj(cid:96) (cid:99)Kjk
kk(cid:98)D 2
jj(cid:98)D 2
ii(cid:98)D 2
(cid:98)D 2

(cid:96)(cid:96)

(cid:96)(cid:96)

+ 2C(1 + r)

(i,j,k,(cid:96))∈T
≤ C(1 + r)5 + 2C(1 + r)

,

n4r5

.

i,j,k,(cid:96)

(13)

(cid:88)

Lemma 7. There exists a constant C > 0 such that

{(i,j,k,(cid:96)),(i(cid:48),j(cid:48),k(cid:48),(cid:96)(cid:48))}∈(T
2)

Cov (Xijk(cid:96), Xi(cid:48)k(cid:48)j(cid:48)(cid:96)(cid:48)) ≤ C

n3r4 .

Proof. Recall that

((cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk)((cid:98)Yi(cid:96)(cid:98)Yj(cid:96) − (cid:99)Ki(cid:96) (cid:99)Kj(cid:96))

(cid:98)Dii(cid:98)Djj(cid:98)Dkk(cid:98)D(cid:96)(cid:96)

Xijk(cid:96) =

Consider ﬁrst the situation where (a, b, c, d) is a permutation
of (i, j, k, (cid:96)). Call this permutation σ ∈ S4. Since (i, j, k, (cid:96))
and (a, b, c, d) are distinct, σ is not the identity permutation,
but σ may be such that Xijk(cid:96) = Xabcd as happens when, for
example, i = a, j = b, k = d, (cid:96) = c.

By symmetry, it sufﬁces to consider three cases.

EXijk(cid:96)Xabcd =

a) Case 1: {i, j} = {a, b} : In this case, we can assume
without loss of generality (by symmetry) that i = b, j = a,
k = d and (cid:96) = c, so that

E(cid:104)
((cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk)2((cid:98)Yi(cid:96)(cid:98)Yj(cid:96) − (cid:99)Ki(cid:96) (cid:99)Kj(cid:96))2(cid:105)
jj(cid:98)D 2
ii(cid:98)D 2
(cid:98)D 2
kk(cid:98)D 2
Var(cid:98)Yik(cid:98)Yjk Var(cid:98)Yi(cid:96)(cid:98)Yj(cid:96)
(cid:98)D 2
ii(cid:98)D 2
jj(cid:98)D 2
kk(cid:98)D 2
≤ (1 + r)4 (cid:99)Kik (cid:99)Kjk (cid:99)Ki(cid:96) (cid:99)Kj(cid:96)
(cid:98)D 2
ii(cid:98)D 2
jj(cid:98)D 2
kk(cid:98)D 2
equality and the fact that 0 ≤ (cid:98)Yik ≤ 1 + r.

where the last inequality follows from the Bhatia-Davis in-

b) Case 2: {i, j} = {a, c} : Without loss of generality,

=

(cid:96)(cid:96)

(cid:96)(cid:96)

(cid:96)(cid:96)

,

EXijk(cid:96)Xabcd =

assume that i = a, j = c, k = b and (cid:96) = d. We have

(cid:99)Kik (cid:99)Kij (cid:99)Kj(cid:96) (cid:99)Kk(cid:96) Var(cid:98)Yjk Var(cid:98)Yi(cid:96)
(cid:98)D 2
ii(cid:98)D 2
jj(cid:98)D 2
kk(cid:98)D 2
≤ (1 + r)2 (cid:99)Kik (cid:99)Kj(cid:96) (cid:99)Kjk (cid:99)Ki(cid:96)
jj(cid:98)D 2
kk(cid:98)D 2
ii(cid:98)D 2
(cid:98)D 2
and the fact that (cid:99)Kik ≤ 1 + r.

(cid:96)(cid:96)

(cid:96)(cid:96)

,

where the inequality follows from the Bhatia-Davis inequality

c) Case 3: {i, j} = {c, d} : Without loss of generality,

assume that i = c, j = d, k = a and (cid:96) = b. Then

EXijk(cid:96)Xabcd

E(cid:98)Yik(cid:98)Yj(cid:96)((cid:98)Yjk +(cid:98)Yi(cid:96))2 − (cid:99)Kik (cid:99)Kj(cid:96)( (cid:99)Kjk + (cid:99)Ki(cid:96))2
(cid:98)D 2
ii(cid:98)D 2
jj(cid:98)D 2
kk(cid:98)D 2
(cid:99)Kik (cid:99)Kj(cid:96) Var((cid:98)Yjk +(cid:98)Yi(cid:96))
jj(cid:98)D 2
ii(cid:98)D 2
(cid:98)D 2
kk(cid:98)D 2
(cid:17)
(cid:16)
(cid:99)Kik (cid:99)Kj(cid:96)
Var(cid:98)Yjk + Var(cid:98)Yi(cid:96)
jj(cid:98)D 2
ii(cid:98)D 2
(cid:98)D 2
kk(cid:98)D 2

(cid:96)(cid:96)

(cid:96)(cid:96)

(cid:96)(cid:96)

.

=

=

=

Now, consider the situation where (i, j, k, (cid:96)) is not a per-
mutation of (a, b, c, d). Clearly, if {i, j, k, (cid:96)}∩{a, b, c, d} = ∅,
then Cov(Xijk(cid:96), Xabcd) = 0. Indeed, Cov(Xijk(cid:96), Xabcd) (cid:54)= 0

requires that each term of the form ((cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk) be
since otherwise a term of the form E((cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk)

dependent on one of the other three such terms in XijklXabcd,

factors out and the covariance is zero. Indeed, only one
other choice (up to permutations of the indices) of (i, j, k, (cid:96))
and (a, b, c, d) gives rise to a non-zero covariance, namely
EXijk(cid:96)Xibk(cid:96). By symmetry, to handle the terms of this form,
it will sufﬁce for us to bound

Using the fact
inequality, and applying reasoning similar to that in Lemma 5,

(i,j,k,(cid:96))∈T

b(cid:54)∈{i,j,k,(cid:96)}

b(cid:54)∈{i,j,k,(cid:96)}

(cid:88)

Cov(Xijk(cid:96), Xibk(cid:96))

Cov(Xijk(cid:96), Xibk(cid:96)).

(cid:88)
that Var(cid:98)Yik ≤ (cid:99)Kik by the Bhatia-Davis
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:99)Kjk (cid:99)Kbk (cid:99)Kj(cid:96) (cid:99)Kb(cid:96) Var(cid:98)Yik Var(cid:98)Yi(cid:96)
kk(cid:98)D 2
ii(cid:98)D 2
(cid:98)D 2
(cid:96)(cid:96)(cid:98)Djj(cid:98)Dbb
(cid:99)Kjk (cid:99)Kbk (cid:99)Kj(cid:96) (cid:99)Kb(cid:96) (cid:99)Kik (cid:99)Ki(cid:96)
(cid:98)D 2
kk(cid:98)D 2
ii(cid:98)D 2
(cid:96)(cid:96)(cid:98)Djj(cid:98)Dbb
n(cid:88)
(cid:99)Kjk (cid:99)Kj(cid:96) (cid:99)Kik (cid:99)Ki(cid:96)
(cid:99)Kb(cid:96)(cid:98)D(cid:96)(cid:96)
(cid:98)D 2
ii(cid:98)D 2
kk(cid:98)D(cid:96)(cid:96)(cid:98)Djj
(cid:99)Kjk (cid:99)Kik
(cid:98)Dii(cid:98)D 2
kk(cid:98)Djj
(cid:99)Kjk (cid:99)Kik
(cid:98)D 2

≤ (1 + r)2
n3r4

i,j,k∈[n] distinct

b(cid:54)∈{i,j,k,(cid:96)}

b(cid:54)∈{i,j,k,(cid:96)}

(i,j,k,(cid:96))∈T

i,j,k∈[n] distinct

b=1

kk

.

(i,j,k,(cid:96))∈T

we have(cid:88)
(cid:88)
≤ (cid:88)

=

(i,j,k,(cid:96))∈T

(i,j,k,(cid:96))∈T
≤ (1 + r)

nr

≤ (1 + r)2
(nr)2

≤ (1 + r)2
(nr)4

Combining this with (13) and noting that r > n−1 implies

(n3r4)−1 ≥ (n4r5)−1, we have our result.

Lemma 8. Deﬁne variables

((cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk)((cid:98)Yi(cid:96)(cid:98)Yj(cid:96) − (cid:99)Ki(cid:96) (cid:99)Kj(cid:96))

(cid:98)Dii(cid:98)Djj(cid:98)Dkk(cid:98)D(cid:96)(cid:96)

Xijk(cid:96) =

RUNNING HEADER

13

and let T = {(i, j, k, (cid:96)) : i, j, k, (cid:96) ∈ [n] distinct.}. There exist
constants C, Cγ > 0 such that with probability at least 1 −

≤ Cγ2.

(14)

(i,j,k,(cid:96))∈T Xijk(cid:96)

.

(i,j,k,(cid:96))∈T

Proof. By Chebyshev’s inequality,

(cid:98)Dii(cid:98)Djj(cid:98)Dkk(cid:98)D(cid:96)(cid:96)
 ≤ Var(cid:80)

Cγ(γ4n3r4)−1,(cid:88)
((cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk)((cid:98)Yi(cid:96)(cid:98)Yj(cid:96) − (cid:99)Ki(cid:96) (cid:99)Kj(cid:96))
 (cid:88)
(cid:88)
(cid:88)

Xijk(cid:96) ≥ Cγ2

(i,j,k,(cid:96))∈T

(i,j,k,(cid:96))∈T

We have

Xijk(cid:96)

C 2γ4

Var

Pr

=

Var Xijk(cid:96)

(cid:88)

(i,j,k,(cid:96))∈T

+

{(i,j,k,(cid:96)),(i(cid:48),j(cid:48),k(cid:48),(cid:96)(cid:48))}∈(T
2)

Cov (Xijk(cid:96), Xi(cid:48)k(cid:48)j(cid:48)(cid:96)(cid:48)) .

Lemma 6 ensures that the ﬁrst of these two sums is bounded
by

(cid:88)

(i,j,k,(cid:96))∈T

Var Xijk(cid:96) ≤ C(cid:48)
n4r5 ,

where C(cid:48) > 0 is a constant, and Lemma 7 ensures that
Cov (Xijk(cid:96), Xi(cid:48)k(cid:48)j(cid:48)(cid:96)(cid:48)) ≤ C(cid:48)(cid:48)

n3r4

{(i,j,k,(cid:96)),(i(cid:48),j(cid:48),k(cid:48),(cid:96)(cid:48))}∈(T
2)

for some constant C(cid:48)(cid:48) > 0. Since (n4r5)−1 ≤ (n3r4)−1 for
r > 1/n, we have

(cid:88)
 (cid:88)

 ≤ C(cid:48) + C(cid:48)(cid:48)

Cγ4n3r4 .

Pr

(i,j,k,(cid:96))∈T

Xijk(cid:96) ≥ Cγ2

Choosing Cγ = (C(cid:48) + C(cid:48)(cid:48))/C yields the result.
Lemma 9. Under the conditions of the above lemmata, there
exist constants c, C > 0 such that for all suitably large n, with
probability at least 1 − 3n−c, we have

(cid:107)(cid:99)L (cid:99)L − ((cid:98)D−1/2(cid:98)Y (cid:98)D−1/2)2(cid:107)F ≤ Cγ.

F

=

i,j,k,(cid:96)

Proof. Expanding the sum and recalling our earlier deﬁnition
of T = {(i, j, k, (cid:96)) : i, j, k, (cid:96) ∈ [n] distinct.}, we have

(cid:107)(cid:99)L (cid:99)L − ((cid:98)D−1/2(cid:98)Y (cid:98)D−1/2)2(cid:107)2
(cid:88)
((cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk)((cid:98)Yi(cid:96)(cid:98)Yj(cid:96) − (cid:99)Ki(cid:96) (cid:99)Kj(cid:96))
(cid:98)Dii(cid:98)Djj(cid:98)Dkk(cid:98)D(cid:96)(cid:96)
(cid:88)
n(cid:88)
ik − (cid:99)K 2
((cid:98)Y 2
(cid:98)D 2
ii(cid:98)D 2
n(cid:88)
(cid:88)
(cid:88)
ik − (cid:99)K 2
i(cid:96) − (cid:99)K 2
ik)((cid:98)Y 2
((cid:98)Y 2
(cid:98)D 2
ii(cid:98)Dkk(cid:98)D(cid:96)(cid:96)
(cid:88)
((cid:98)Yik(cid:98)Yjk − (cid:99)Kik (cid:99)Kjk)((cid:98)Yi(cid:96)(cid:98)Yj(cid:96) − (cid:99)Ki(cid:96) (cid:99)Kj(cid:96))
(cid:98)Dii(cid:98)Djj(cid:98)Dkk(cid:98)D(cid:96)(cid:96)

ik)2

k(cid:54)=i

k(cid:54)=i

(cid:96)(cid:54)=i

i(cid:96) )

i=1

i=1

+

=

+

kk

(i,j,k,(cid:96))∈T

Each of these three summations is bounded (with high prob-
ability) by Cγ2 by Lemmata 3, 4 and 8, respectively. Let
constants c1, c2 > 0 be as deﬁned in Lemma 2 and Lemma 3
respectively, and choose c3 > 0 so that Cγ(γ4n3r4)−1 ≤ n−c3
for suitably large n, where Cγ is as deﬁned in Lemma 8.
By an application of the union bound, we have that with
probability at least 1 − (n1−c1 + n−c2 + n−c3), all three
sums are bounded at once, and the result follows by taking
c = min{c1 − 1, c2, c3},
Lemma 10. Suppose that β depends on n in such a way that
β → 0 as n → ∞. Under the conditions of Lemma 2, there
exists a constant C > 0 such that with probability at least
1 − n1−c1,

(cid:107)(cid:98)L(cid:98)L − ((cid:98)D−1/2(cid:98)Y (cid:98)D−1/2)2(cid:107)F ≤ C

β

.

1

r1/2

for all i, j, k ∈ [n] we have

Proof. Under the conditions of Lemma 2, with probability at

least 1− n1−c1 it holds for all i ∈ [n] that |(cid:98)Dii−(cid:80)n
k=1(cid:98)Yik| ≤
β(cid:98)Dii. It follows that for a suitably chosen constant C(cid:48) > 0,
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
ii (cid:98)D 1/2
(cid:98)D 1/2
jj (cid:98)Dkk
jj (cid:98)Dkk
ii (cid:98)D1/2
(cid:98)D1/2
|(cid:98)Dii −(cid:80)n
k=1(cid:98)Yik| ≤ β(cid:98)Dii for all i ∈ [n], we have
ii (cid:98)D 1/2
(cid:98)D 1/2
jj (cid:98)Dkk
(cid:98)D 1/2
ii (cid:98)D 1/2
jj (cid:98)Dkk
≤ (1 − β)−2

To see why this is the case (here we are following the
argument motivating Equation A.6 in [52]), note that when

jj (cid:98)Dkk
ii (cid:98)D1/2
(cid:98)D1/2

ii (cid:98)D 1/2
(cid:98)D 1/2
jj (cid:98)Dkk

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤

(1 + β)−2

. (15)

C(cid:48)β

−

≤

1

1

,

and Equation 15 follows by noting that by choosing n suitably
large (and choosing a suitable constant C(cid:48)(cid:48) > 0), we can make
β arbitrarily small, and thus

(1 + β)−2 ≥ β−2 − 1
(1 − β)−2 = 1 +

(β−1 + 1)2 =
β−1 − 1

2

+

≥ 1 − C(cid:48)(cid:48)β,

β−1 − 1
β−1 + 1
(β−1 − 1)2 ≤ 1 + C(cid:48)(cid:48)β.
1
(cid:98)Yik(cid:98)Yjk(cid:98)Yi(cid:96)(cid:98)Yj(cid:96)
(cid:98)Dii(cid:98)Djj(cid:98)Dkk(cid:98)D(cid:96)(cid:96)
k=1(cid:98)Yik ≤ (1 + β)(cid:98)Dii for
(cid:98)Yik(cid:98)Yjk(cid:98)Yi(cid:96)(cid:98)Yj(cid:96)
(cid:98)Dii(cid:98)Djj(cid:98)Dkk(cid:98)D(cid:96)(cid:96)

.

i,j,k,(cid:96)

Using (15), we have

F ≤ C(cid:48)β2 (cid:88)
(cid:107)(cid:98)L(cid:98)L − ((cid:98)D−1/2(cid:98)Y (cid:98)D−1/2)2(cid:107)2
Under the same event, we have (cid:80)n
(cid:98)Yjk ≤ (1 + r), and (cid:98)Dii ≥ nr, it follows that
F ≤ C(cid:48)β2 (cid:88)
(cid:107)(cid:98)L(cid:98)L − ((cid:98)D−1/2(cid:98)Y (cid:98)D−1/2)2(cid:107)2

i,j,k,(cid:96)

all i ∈ [n], and making repeated use of this and the facts that

≤ β2(1 + r)(1 + β)3

r

.

The result follows the fact that both r and β are bounded
above by 1.

result

To obtain our

take γ =
C(cid:48)n−1/2r−1 log1/2 n and β = C(cid:48)(cid:48)n−1/2r−1/2 log1/2 n for
suitably large constants C(cid:48), C(cid:48)(cid:48) > 0. Note ﬁrst that these
choices of γ and β satisfy all of the constraints of the lemmata

in Theorem 3,

.

RUNNING HEADER

14

required for Lemma 9, so long as r = ω(n−1 log n). Further,
note that β/r1/2 = Cγ for some constant C > 0, and hence

Lemma 10 implies that (cid:107)(cid:98)L(cid:98)L − ((cid:98)D−1/2(cid:98)Y (cid:98)D−1/2)2(cid:107)F ≤ Cγ

with high probability. Combining Lemma 9 and Lemma 10
and applying the triangle inequality then yields Theorem 3.

REFERENCES

[1] A. Y. Alfakih, A. Khandani, and H. Wolkowicz. Solving euclidean
distance matrix completion problems via semideﬁnite programming.
Computational Optimization and Applications, 12:13–30, 1999.

[2] A. A. Amini, A. Chen, P. J. Bickel, and E. Levina. Pseudo-likelihood
methods for community detection in large sparse networks. The Annals
of Statistics, 41(4):2097–2122, 2013.

[3] J. Baglama and L. Reichel. Augmented implicitly restarted Lanczos
SIAM Journal on Scientiﬁc Computing,

bidiagonalization methods.
27(1):19–42, 2005.

[4] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality
reduction and data representation. Neural Computation, 15(6):1373–
1396, 2003.

[5] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A
geometric framework for learning from labeled and unlabeled examples.
Journal of Machine Learning Research, 7, 2006.

[6] R. Bhatia and C. Davis. A better bound on the variance. The American

Mathematical Monthly, 107(4):353–357, 2000.

[7] I. Borg and P. J. F. Groenen. Modern multidimensional scaling: Theory

and applications. Springer Science & Business Media, 2005.

[8] S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: a
Nonasymptotic Theory of Independence. Oxford University Press, 2013.
low-rank modiﬁcations of the thin singular value
decomposition. Linear algebra and its applications, 415(1):20–30, 2006.
[10] E. J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principle

[9] M. Brand.

Fast

component analysis? Journal of the ACM, 58(3), 2011.

[11] E. J. Cand`es and Y. Plan. Matrix completion with noise. Proceedings

of the IEEE, 98(6):925–936, 2009.

[12] E. J. Cand`es and B. Recht. Exact matrix completion via convex
optimization. Foundations of Computational Mathematics, 9(6):717 –
772, 2009.

[13] M. A. Carlin, S. Thomas, A. Jansen, and H. Hermansky. Rapid
In

evaluation of speech representations for spoken term discovery.
Proceedings of Interspeech, 2011.

[14] H. Chang and D.-Y. Yeung. Robust locally linear embedding. Pattern

Recognition, 39(6):1053–1065, 2006.

[15] S. Chatterjee. Matrix estimation by universal singular value thresholding.

The Annals of Statistics, 43(1):177–214, 2015.

[16] K. Chaudhuri, F. Chung, and A. Tsiatas. Spectral clustering of graphs
with general degrees in the extended planted partition model. In Journal
of Machine Learning Research Workshop and Conference Proceedings,
volume 23, 2012.

[17] J. Chen, H.-R. Fang, and Y. Saad.

Fast approximate kNN graph
construction for high dimensional data via recursive lanczos bisection.
Journal of Machine Learning Research, 10:1989–2012, 2009.

[18] L. Chen, J.T. Vogelstein, V. Lyzinski, and C. E. Priebe. A joint graph
inference case study: the c.elegans chemical and electrical connectomes.
Worm, Accepted or publication, 2016.

[19] Y. Chen, A. Jalali, S. Sanghavi, and C. Caramanis. Low-rank matrix
recovery from errors and erasures. IEEE Transactions on Information
Theory, 59(7):4324–4337, 2013.

[20] F. Chung. Spectral Graph Theory. American Mathematical Society,

1997.

[21] F. Chung and L. Lu. Concentration inequalities and martingale inequal-

ities: a survey. Internet Math, 3(1):79–127, 2006.

[22] F. Chung, L. Lu, and V. Vu. Spectra of random graphs with given
expected degrees. Proceedings of the National Academy of Sciences,
100(11):6313–6318, 2003.

[23] R. R. Coifman and S. Lafon. Diffusion maps. Applied and Computa-

tional Harmonic Analysis, 21(1):5–30, 2006.

[24] T. Cox and M. Cox. Multidimensional scaling. Monographs on Statistics

and Applied Probability, 88, 2001.

[25] C. Davis and W. M. Kahan. the rotation of eigenvectors by a perturba-

tion. SIAM Journal of Numerical Analysis, 7(1), March 1970.

[26] D. L. Donoho and C. Grimes. Hessian eigenmaps: Locally linear
embedding techniques for high-dimensional data. Proceedings of of
the National Academy of Sciences of the United States of America,
100(10):5591–5596, 2003.

[27] U. Feige and E. Ofek. Spectral techniques applied to sparse random

graphs. Random Structures and Algorithms, 27(2), 2005.

[28] S. Fine and K. Scheinberg. Efﬁcient SVM training using low-rank kernel
Journal of Machine Learning Research, 2:243–264,

representations.
December 2001.

[29] C. Fraley and A. E. Raftery. Model-based clustering, discriminant
analysis and density estimation. Journal of the American Statistical
Association, 97(458):611 – 631, 2002.

[30] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation
analysis: An overview with application to learning methods. Neural
computation, 16(12):2639–2664, 2004.

[31] M. Hein and M. Maier. Manifold denoising.

In Advances in Neural

Information Processing Systems 19, 2006.

[32] G. E. Hinton and S. T. Roweis. Stochastic neighbor embedding.

In

NIPS, pages 833–840, 2002.

[33] T. Hofmann, B. Schlkopf, and A. J. Smola. Kernel methods in machine

learning. The Annals of Statistics, 36:1171–1220, 2008.

[34] H. Hotelling. Relations between two sets of variates. Biometrika, pages

[35] L. Hubert and P. Arabie. Comparing partitions. Journal of Classiﬁcation,

321–377, 1936.

2:193–218, 1985.

[36] P. Indyk. Algorithmic applications of low-distortion geometric embed-
dings. In 42nd Annual Symposium on Foundations of Computer Science,
2001.

[37] A. Javanmard and A. Montanari. Localization from incomplete noisy
distance measurements. Foundations of Computational Mathematics,
13(3):297–345, 2013.

[38] I. Jolliffe. Principal component analysis. Wiley, 2002.
[39] A. Joseph and B. Yu. Impact of regularization on spectral clustering.

Retrieved from arXiv, 2014.

[40] O. Klopp, A. B. Tsybakov, and N. Verzelen. Oracle inequalities for
network models and sparse graphon estimation. Retrieved from arXiv,
2015.

[41] C. M. Le, E. Levina, and R. Vershynin.

Sparse random graphs:
Regularization and concentration of the Laplacian. Retrieved from arXiv,
2015.

[42] C. M. Le and R. Vershynin. Concentration and regularization of random

graphs. Retrieved from arXiv, 2015.

[43] Q. Le, T. Sarl´os, and A. Smola. Fastfood – approximating kernel
expansions in loglinear time. In Proceedings of the 30th International
Conference on Machine Learning, 2013.

[44] K. Levin, K. Henry, A. Jansen, and K. Livescu. Fixed-dimension acous-
tic embeddings of variable-length segments in low-resource settings. In
Proc. ASRU, 2013.

[45] K. Levin, A. Jansen, and B. Van Durme. Segmental acoustic indexing

for zero resource keyword search. In Proc. ICASSP, 2015.

[46] N. Linial. Finite metric spaces– combinatorics, geometry and algorithms.
In In Proceedings of the International Congress of Mathematicians III,
pages 573–586, 2002.

[47] N. Linial, E. London, and Y. Rabinovich. The geometry of graphs and
some of its algorithmic applications. Combinatorica, 15:215–245, 1995.
[48] R. I. Oliviera. Concentration of the adjacency matrix and of the
Laplacian in random graphs with independent edges. Retrieved from
arXiv, 2010.

[49] C. E. Priebe, D. L. Sussman, M. Tang, and J. T. Vogelstein. Statistical
inference on errorfully observed graphs. Journal of Computational and
Graphical Statistics, 2014.

[50] T. Qin and K. Rohe. Regularized spectral clustering under the degree-
In Advances in Neural Information

corrected stochastic blockmodel.
Processing Systems, pages 3120–3128, 2013.

[51] A. E. Raftery and N. Dean. Variable selection for model-based
clustering. Journal of the American Statistical Association, 101(473):168
– 178, 2006.

[52] K. Rohe, S. Chatterjee, and B. Yu.

high-dimensional stochastic blockmodel.
39(4):1878–1915, 2011.

Spectral clustering and the
The Annals of Statistics,

[53] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally

linear embedding. Science, 290(5500), 2000.

[54] H. Sakoe and S. Chiba. Dynamic programming algorithm optimization
for spoken word recognition. IEEE Transactions on Acoustics, Speech
and Signal Processing, 26(1):43–49, 1978.

[55] N. Shahid, V. Kalofolias, X. Bresson, M. Bronstein, and P. Van-
dergheynst. Robust principle component analysis on graphs. Retrieved
from arXiv, 2015.

[56] R. Shibata. Consistency of model selection and parameter estimation.

Journal of Applied Probability, 23:127 – 141, 1986.

RUNNING HEADER

15

[57] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric
framework for nonlinear dimensionality reduction. Science, 290:2319–
2323, 2000.

[58] J. A. Tropp. User-friendly tail bounds for sums of random matrices.

Foundations of Computational Mathematics, 12(4):389–434, 2012.

[59] M. W. Trosset. Distance matrix completion by numerical optimization.

Computational Optimization and Applications, 17(1):11–22, 2000.

[60] L. J. P. van der Maaten, E. O. Postma, and H. J. van den Herik.
Dimensionality reduction: A comparative review. Journal of Machine
Learning Research, 10(1-41):66–71, 2009.

[61] U. von Luxburg. A tutorial on spectral clustering.

Statistics and

Computing, 17(4), 2007.

[62] C. K. I. Williams and M. Seeger. Using the Nystr¨om method to speed
up kernel machines. In Proceedings of Advances in Neural Information
Processing Systems 13, 2001.

[63] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph
embedding and extensions: A general framework for dimensionality
reduction. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 29(1), 2007.

