6
1
0
2

 
r
a

 

M
1
1

 
 
]

V
C
.
s
c
[
 
 

2
v
9
6
3
3
0

.

3
0
6
1
:
v
i
X
r
a

Summary Transfer: Exemplar-based Subset Selection for Video

Summarization

Ke Zhang

Department of Computer Science
University of Southern California

Los Angeles, CA 90089

zhang.ke@usc.edu

Fei Sha

Department of Computer Science

University of California
Los Angeles, CA 90095

feisha@cs.ucla.edu

Wei-Lun Chao

Department of Computer Science
University of Southern California

Los Angeles, CA 90089

weilunc@usc.edu

Kristen Grauman

Department of Computer Science

University of Texas at Austin

Austin, TX 78701

grauman@cs.utexas.edu

March 14, 2016

Abstract

Video summarization has unprecedented importance
to help us digest, browse, and search today’s ever-
growing video collections. We propose a novel sub-
set selection technique that leverages supervision in the
form of human-created summaries to perform automatic
keyframe-based video summarization. The main idea is to
nonparametrically transfer summary structures from an-
notated videos to unseen test videos. We show how to
extend our method to exploit semantic side information
about the video’s category/genre to guide the transfer pro-
cess by those training videos semantically consistent with
the test input. We also show how to generalize our method
to subshot-based summarization, which not only reduces
computational costs but also provides more ﬂexible ways
of deﬁning visual similarity across subshots spanning sev-
eral frames. We conduct extensive evaluation on several
benchmarks and demonstrate promising results, outper-
forming existing methods in several settings.

1 Introduction

The amount of video data has been explosively increas-
ing due to the proliferation of video recording devices
such as mobile phones, wearable and ego-centric cam-
eras, surveillance equipment, and others. According to
YouTube statistics, about 300 hours of video are uploaded

every minute [2]. To cope with this video data deluge, au-
tomatic video summarization has emerged as a promising
tool to assist in curating video contents for fast browsing
and search, without losing important information.

Video can be summarized at several levels of abstrac-
tion: keyframes [44, 10, 32, 26, 23], segment or shot-
based skims [34, 22, 33, 28], story-boards [6, 9], mon-
tages [15, 39] or video synopses [37].
In this paper,
we focus on developing learning algorithms for selecting
keyframes or subshots from a video sequence. Namely,
the input is a video and its subshots and the output is an
(ordered) subset of the frames or subshots in the video.

Inherently, summarization is a structured prediction
problem where the decisions on whether to include or ex-
clude certain frames or subshots into the subset are in-
terdependent. This is in sharp contrast to typical classiﬁ-
cation and recognition tasks where the output is a single
label.

The structured nature of subset selection presents a ma-
jor challenge. Current approaches rely heavily on several
heuristics to decide the desirability of each frame: repre-
sentativeness [14, 16, 34], diversity or uniformity [27, 44],
interestingness and relevance [15, 23, 28, 30, 34]. How-
ever, combining those frame-based properties to output an
optimal subset remains an open and understudied prob-
lem. In particular, researchers are hampered by the lack
of knowledge on the “global” criteria human annotators
presumably optimize when manually creating a summary.

1

Recently, initial steps investigating supervised learning
for video summarization have been made. They demon-
strate promising results [10, 12], often exceeding the con-
ventional unsupervised clustering of frames. The main
idea is to use a training set of videos and human-created
summaries as targets to adapt the parameters of a subset
selection model to optimize the quality of the summariza-
tion. If successful, a strong form of supervised learning
would extract high-level semantics and cues from human-
created summaries to guide summarization.

Supervised learning for structured prediction is a chal-
lenging problem in itself. Existing parametric techniques
typically require a complex model with sufﬁcient anno-
tated data to represent highly complicated decision re-
gions in a combinatorially large output space. In this pa-
per, we explore a nonparametric supervised learning ap-
proach to summarization. Our method is motivated by
the observation that similar videos share similar summary
structures. For instance, suppose we have a collection of
videos of wedding ceremonies inside churches. It is quite
likely good summaries for those videos would all contain
frames portraying brides proceeding to the altar, stand-
ing of the grooms and their best men, the priests’ preach-
ing, the exchange of rings, etc. Thus, if one such video is
annotated with human-created summaries, a clever algo-
rithm could essentially “copy and paste” the relative po-
sitions of the extracted frames in the annotated video se-
quence and apply them to an unannotated one to extract
relevant frames. Note that this type of transferring sum-
mary structures across videos need not assume a precise
matching of visual appearance in corresponding frames
— there is no need to have the same priest as long as the
frames of the priests in each video are sufﬁciently differ-
ent from other frames to be “singled out” as possible can-
didates.

The main idea of our approach centers around this
intuition, that is, non-parametric learning from exem-
plar videos to transfer summary structures to novel input
videos. In recent years, non-parametric methods in the vi-
sion literature have shown great promise in letting the data
“speak for itself”, though thus far primarily for traditional
categorization or regression tasks (e.g., label transfer for
image recognition [25, 40] or scene completion [13]).

How can summarization be treated non-parametrically?
A naive application of non-parametric learning to video
summarization would treat keyframe selection as a bi-
nary classiﬁcation problem—matching each frame in the
unannotated test video to the nearest human-selected
keyframes in some training video, and deciding indepen-
dently per frame whether it should be included in or ex-
cluded from the summary. Such an approach, however,

conceptually fails on two fronts. First, it fails to account
for the relatedness between a summary’s keyframes. Sec-
ond, it limits the system to inputs having very similar
frame-level matches in the annotated database, creating
a data efﬁciency problem.

Therefore, rather than transfer simple relevance labels,
our key insight is to transfer the structures implied by sub-
set selection. We show how kernel-based representations
of a video’s frames (subshots) can be used to detect and
align the meta-cues present in selected subsets.
In this
way, we compose novel summaries by borrowing recur-
ring structures in exemplars for which we have seen both
the source video and its human-created summary. A con-
ceptual diagram of our approach is shown in Fig. 1.

In short, our main contributions are an original model-
ing idea that leverages non-parametric learning for struc-
tured objects (namely, selecting subsets from video se-
quences), a summarization method that advances the
frontier of supervised learning for video summarization,
and an extensive empirical study validating the proposed
method and attaining far better summarization results than
competing methods on several benchmark datasets.

The rest of the paper is organized as follows. In sec-
tion 3, we describe our approach of nonparametric struc-
ture transfer. We report and analyze experimental results
in section 4 and conclude in section 5.

2 Related Work

A variety of video summarization techniques have been
developed in the literature [31, 41]. Broadly speak-
ing, most methods ﬁrst compute visual features at the
frame level, then apply some selection criteria to priori-
tize frames for inclusion in the output summary.

Keyframe-based methods select a subset of frames to
form a summary, and typically use low-level features
like optical ﬂow [42] or image differences [44]. Recent
work also injects high-level information such as object
tracks [26] or “important” objects [23], or takes user input
to generate a storyboard [9]. In contrast, video skimming
techniques ﬁrst segment the input into subshots using shot
boundary detection. The summary then consists of a se-
lected set of representative subshots [22, 33, 34].

Selection criteria for summaries often aim to retain di-
verse and representative frames [14, 16, 27, 34, 44]. An-
other strategy is to predict object and event saliency [15,
23, 30, 34], or to pose summarization as an anomaly de-
tection problem [18, 45]. When the camera is known to
be stationary, background subtraction and object track-
ing offer valuable cues about the salient entities in the
video [5, 37].

2

1 

k-th 
frame  

l-th 
frame  

tUDLQLQJ(cid:3)YLGHRV¶ 

2 

WHVW(cid:3)YLGHR¶V 

DPP Inference 

summarization kernels 

summarization kernel 

r-th 

training 
video 

simr(i, k) 

simr(j, l) 

test 
video 

1 

2 

3 

4 

5 

6 

i-th 
frame  

j-th 
frame  

Lr, kl 

video R 

video 1 

L
ij

=

ƒƒƒ
r
l

k

Lij 

sim ( , )sim ( , )
j l

i k

r

r

L
, 
r kl

3 

summarized 

video 

2 

3 

5 

Figure 1: The conceptual diagram of our approach, which leverages the intuition that similar videos share similar
summary structures. The main idea is nonparametric structure transfer, ie, transferring the subset structures in the
human-created summaries (blue frames) of the training videos to a new video. Concretely, for each new video, we
ﬁrst compute frame-level similarity between training and test videos (i.e., sim(·, ·), cf. eq. (4)). Then, we encode
the summary structures in the training videos with kernel matrices made of binarized pairwise similarity among their
frames. We combine those structures, factoring the pairwise similarity between the training and the test videos, into a
kernel matrix that encodes the summary structure for the test video, cf. eq. (7). Finally, the summary is decoded by
inputting the kernel matrix to a probabilistic model called the determinantal point process (DPP) to extract a globally
optimal subset of frames.

Whatever the above choices, existing methods are al-
most entirely unsupervised. For example, they employ
clustering to identify groups of related frames, and/or
manually deﬁne selection criteria based on intuition for
the problem. Some prior work includes supervised learn-
ing components (e.g., to generate regions with learned
saliency metrics [23], train classiﬁers for canonical view-
points [16], or recognize fragments of a particular event
category [36]), but they do not learn the subset selection
procedure itself.

Departing from unsupervised methods, limited recent
work formulates video summarization as a subset selec-
tion problem [12, 43, 17, 10]. This enables supervised
learning, exploiting knowledge encoded in human-created
summaries. In [12], a submodular function optimizes a
global objective function of the desirability of selected
frames, while [10] uses a probabilistic model that maxi-
mizes the probability of the ground-truth subsets.

The novelty of our approach is

to learn non-
parametrically from exemplar training videos to transfer
summary structures to test videos. In contrast to previ-
ous parametric models [12, 10], non-parametric learning
generalizes to new videos by directly exploiting patterns
in the training data. This has the advantage of generaliz-
ing locally within highly nonsmooth regions: as long as a
test video’s “neighborhood” contains an annotated train-
ing video, the summary structure of that training video
will be transferred. In contrast, parametric techniques typ-
ically require a complex model with sufﬁcient annotated
data to parametrically represent those regions. Our non-
parametric approach also puts design power into ﬂexible
kernel functions, as opposed to relying strictly on com-
binations of hand-crafted criteria (e.g., frame interesting-

ness, diversity, etc.).

3 Approach

We cast the process of extracting a summary from a video
as selecting a subset of items (i.e., video frames) from
a ground set (i.e., the whole video). Given a corpus of
videos and their human-created summaries, our learning
algorithm learns the optimal criteria for subset selection
and applies them to unseen videos to extract summaries.
The ﬁrst step is to decide on a subset selection model
that can output a structure (i.e., an ordered subset). For
such structured prediction problems, we focus on the de-
terminantal point process (DPP) [21] which has the ben-
eﬁts of being more computationally tractable than many
probabilistic graphical models [19]. Empirically, DPP has
been successfully applied to documentation summariza-
tion [20], image retrieval [7] and more recently, to video
summarization [10, 3].

We will describe ﬁrst DPP and how it can be used for
video summarization. We then describe our main ap-
proach in detail, as well as its several extensions.

3.1 Background

Let Y = {1, 2, · · · , N} denote a (ground) set of N items,
such as video frames. The ground set has 2N subsets.
The DPP over the N items assigns a probability to each
of those subsets. Let y ⊆ Y denote a subset and the prob-
ability of selecting it is given by

P (y; L) =

det(Ly)

det(L + I)

,

(1)

3

where L is a symmetric, positive semideﬁnite matrix and
I is an identity matrix of the same size of L. Ly is the
principal minor (sub-matrix) with rows and columns from
L indexed by the integers in y.

DPP can be seen conceptually as a fully connected N-
node Markov network where the nodes correspond to the
items. This network’s node-potentials are given by the di-
agonal elements of L and the edge potentials are given
by the off-diagonal elements in L. Note that those “po-
tentials” cannot be arbitrarily assigned — to ensure they
form a valid probabilistic model, the matrix L needs to
be positive semideﬁnite. Due to this constraint, L is of-
ten referred to as a kernel matrix whose elements can be
interpreted as measuring the pairwise compatibility.

Besides computational tractability which facilitates pa-
rameter estimation, DPP has an important modeling ad-
vantage over standard Markov networks. Due to the
celebrated Hammersley-Clifford Theorem, Markov net-
works cannot model distributions where there are zero-
probability events. On the other hand, DPP is capable of
assigning zero probability to absolutely impossible (or in-
admissible) instantiations of random variables.

To see its use for video summarization, suppose
there are two frames that are identical. For keyframe-
based summarization, any subset containing such iden-
tical frames should be ruled out by being assigned zero
probability. This is impossible in Markov networks — no
matter how small, Markov networks will assign strictly
positive probabilities to an exponentially large number
of subsets containing identical frames. For DPP, since
the two items are identical, they lead to two identical
columns/rows in the matrix L, resulting a determinant
zero (thus zero probability) for those subsets. Thus, DPP
naturally encourages selected items in the subset to be di-
verse, an important objective for summarization and in-
formation retrieval [21].

The mode of the distribution is the most probable subset

y∗ = arg maxy P (y; L) = arg maxy det(Ly).

(2)

This is an NP-hard combinatorial optimization problem,
and there are several approaches to obtaining approximate
solutions [21, 8].

The most crucial component in a DPP is its kernel ma-
trix L. To apply DPP to video summarization, we deﬁne
the ground set as the frames in a video and identify the
most desired summarization as the MAP inference result
of eq. (2). We compute L with a bivariate function over
two frames – we dub it the summarization kernel:

Lij = φ(vi)Tφ(vj )

(3)

where φ(·) is a function of the features vi (or vj) com-
puted on the i-th (or the j-th ) frames. There are several

choices. For instance, φ(·) could be an identity function,
a nonlinear mapping implied by a Gaussian RBF kernel,
or the output of a neural network [10].

As each different video needs to have a different ker-
nel, φ(·) needs to be identiﬁed from a sufﬁciently rich
function space so it generalizes from modeling the train-
ing videos to new ones. If the videos are substantially dif-
ferent, this generalization can be challenging, especially
when there are not enough annotated videos with human-
created summaries. Our approach overcomes this chal-
lenge by directly using the training videos’ kernel matri-
ces, as described below.

3.2 Non-parametric video summarization

Our approach differs signiﬁcantly from existing sum-
mary methods, including those based on DPPs. Rather
than learn a single function φ(·) and discard the train-
ing dataset, we construct L for every unannotated video
by comparing it to the annotated training videos. This
construction exploits two sources of information: 1) how
similar the new video is to annotated ones, and 2) how the
training videos are summarized. The former can be in-
ferred directly by comparing visual features at each frame,
while the latter can be “read off” from the human-created
training summaries.

We motivate our approach with an idealized exam-
ple that provides useful insight. Let us assume we are
given a training set of videos and their summaries D =
r=1 and a new video Y to be summarized. Sup-
{(Yr, yr)}R
pose this new video is very similar — we deﬁne similarity
more precisely later — to a particular Yr in D. Then we
can reasonably assume that the summary yr might work
well for the new video. As a concrete example, consider
the case where both Y and Yr are videos for wedding
ceremonies inside churches. We anticipate seeing similar
events across both videos: brides proceeding to the altars,
priests delivering speeches, exchanging rings etc. More-
over, similarity in their summaries exists on the higher-
order structural level: the relative positions of the sum-
mary frames of yr in the sequence Y are an informative
prior on where the frames of the summary y should be
in the new video Y. Speciﬁcally, as long as we can link
the test video to the training video by identifying similar
frames,1 we can “copy down”—transfer—the positions of
yr and lift the corresponding frames in Y to generate its
summary y.

While this intuition is conceptually analogous to the
familiar paradigm of nearest-neighbor classiﬁcation, our

1This task is itself not trivial, of course, but it does have the beneﬁt
of a rich literature on image matching and recognition work, including
efﬁcient search strategies.

4

approach is signiﬁcantly different. The foremost is that,
as discussed in section 1, we cannot select frames inde-
pendently (by nonparametrically learning its similarity to
those in the training videos). We need to transfer sum-
mary structures which encode interdependencies of se-
lecting frames. Therefore, a naive solution of representing
videos with ﬁx-length descriptors in Euclidean space and
literally pretending their summaries are “labels” that can
be transferred to new data is ﬂawed.

The main steps of our approach are outlined in Fig. 1.

We describe them in detail in the following.

where diag turns a vector into a diagonal matrix, Nr is
the number of frames in Yr and αr > 1 is an adjustable
parameter. The structure of Lr is intuitive: if a frame is in
the summary yr, then its corresponding diagonal element
is αr, otherwise 0.
It is easy to verify that Lr indeed
gives rise to the correct summarization. Note that αr > 1
is required. If αr = 1, any subset of yr is a solution to the
MAP inference problem (and we will be getting a shorter
summarization). If αr < 1, the empty set would be the
summary (as the determinant of an empty matrix is 1, by
convention).

Step 1: Frame-based visual similarity To infer simi-
larity across videos, we experiment with common ones in
the computer vision literature for calculating frame-based
similarity from visual features vi and vk extracted from
the corresponding frames:

i vk

sim1(i, k) = vT
sim2(i, k) = exp{− kvi − vkk2
sim3(i, k) = exp{−(vi − vk)TΩ(vi − vk)},

2 /σ}

(4)

where σ and Ω are adjustable parameters (constrained to
be positive or positive deﬁnite, respectively). These forms
of similarity measures are often used in vision tasks and
are quite ﬂexible, e.g., one can learn the kernel param-
eters for sim3. However, they are not the focus of our
approach — we expect more sophisticated ones will only
beneﬁt our learning algorithm. We also expect high-level
features (such as interestingness, objectness, etc.) could
also be beneﬁcial. In section 3.4 we discuss a generaliza-
tion to replace frame-level similarity with subshot-level
similarity.

Step 2: Summarization kernels for training videos
The summarization kernels {Lr}R
r=1 are not given to us in
the training data. However, note that the crucial function
of those kernels is to ensure that when used to perform the
MAP inference in eq. (2) to identify the summary on the
training video Yr , it will lead to the correct summariza-
tion yr (which is in the training set). This prompts us to
deﬁne the following idealized summarization kernels

Lr = αr


δ(1 ∈ yr)



0
...
0

0

δ(2 ∈ yr)

...
· · ·

· · ·
...
...
0

or more compactly,

0
...
0





δ(Nr ∈ yr)

(5)

Step 3: Transfer summary structure Our aim is now
to transfer the structures encoded by the idealized sum-
marization kernels from the training videos to a new (test)
video Y. To this end, we synthesize L for new video Y
out of {Lr}.

Let i and j index the video frames in Y, with k and l
for a speciﬁc training video Yr. Speciﬁcally, the “contri-
bution” from Yr to L is given by

rij = X

k

X

l

simr(i, k)simr(j, l)Lr,kl

(7)

where Lr,kl is the element in Lr, and simr(·, ·) measures
frame-based (visual) similarity between frames of Y and
Yr.

Fig. 1 illustrates graphically how frame-based similar-
ity enables transfer of structures in training summaries.
We gain further insights by examining the case when
the frame-based similarity simr(·, ·) is sharply peaked —
namely, there are very good matches between speciﬁc
pairs of frames (an assumption likely satisﬁed in the run-
ning example of summarizing wedding ceremony videos)

simr(i, m) ≫ simr(i, k), ∀ k 6= m
simr(j, n) ≫ simr(j, l),
∀ l 6= n.

Under these conditions,

rij ≈ simr(i, m)simr(j, n)Lr,mn.

(8)

(9)

Intuitively, if Y and Yr are precisely the same video (and
the video frames in Yr are sufﬁciently visually different),
then the matrix L would be very similar to Lr. Conse-
quently, the summarization yr, computed from Lr, would
be a good summary for Y.

To include all the information in the training data, we

sum up the contributions from all Yr and arrive at

Lr = αrdiag({δ(n ∈ yr)}Nr

n=1),

(6)

Lij = X

r

rij .

(10)

5

We introduce a few shorthand notions. Let Sr be a N ×
Nr matrix whose elements are simr(i, k), the frame-based
similarity between N frames in Y and Nr frames in Yr.
The kernel matrix L is thus

L = X

r

SrLrST
r .

(11)

Note that, L is for the test video with N frames — there is
no need for all the videos have the same length.

Step 4: Extracting summary Once we have computed
L for the new video, we use the MAP inference eq. (2)
to extract the summary as the most probable subset of
frames.

3.3 Learning

Our approach requires adjusting parameters such as α =
{α1, α2, · · · , αR} for the ideal summarization kernels
and/or Ω for computing frame-based visual similarity
eq. (4). We use maximum likelihood estimation to esti-
mate those parameters. Speciﬁcally, for each video in the
training dataset, we pretend it is a new video and formu-
late a kernel matrix

ˆLq = X

r

Sq

r LrSq

r

T, ∀, q = 1, 2, · · · , R.

(12)

We optimize the parameters such that the ground-truth
summarization yq attains the highest probability under
ˆLq,

α∗ = arg max

α

R

X

q=1

log P (yq; ˆLq).

(13)

We can formulate a similar criterion to learn the Ω pa-
rameter for sim3(·, ·). We carry out the optimization by
gradient descent. In our experiments, we set σ for sim2 to
be 1, with features normalized to have unit norm. Addi-
tional details are presented in the Supplementary Material
and omitted here for brevity.

3.4 Extensions
Category-speciﬁc summary transfer Video datasets
labeled with semantically consistent categories have been
emerging [36, 38]. We view categories as valuable prior
information that can be exploited by our nonparametric
learning algorithm. Intuitively, videos from the same cat-
egory (activity type, genre, etc.) are likely to be similar in
part, not only in visual appearance but also in high-level
semantic cues (such as how key events are temporally or-
ganized), resulting in a similar summary structures. We

6

extend our method to take advantage such optional side
information in two ways:

• hard transfer. We compare the new video from a
category c only to the training videos from the same
category c. Mathematically, for each video cate-
gory, we learn a category-speciﬁc set of α(c) =
{α(c)
r > 0 only when
the training video r belongs to category c.

R } such that α(c)

1 , α(c)

2 , · · · , α(c)

• soft transfer. We relax the requirement in hard trans-
fer such that α(c)
r > 0 even if the rth training video
is not from the category c. Note that while we uti-
lize structural information from all training videos,
the way we use them still depends on the test video’s
category.

Subshot-based summary transfer Videos can also be
summarized at the level of subshots. As opposed to se-
lecting keyframes, subshots contain short but contiguous
frames, giving a glimpse of a key event. We next extend
our subset selection algorithm to select a subset of sub-
shots.

To this end, in our conceptual diagram as in Fig. 1, we
replace computing frame-level similarity with subshot-
level similarity, where we compare subshots between the
training videos and the new video. We explore two possi-
ble ways to compute the frame-set to frame-set similarity:

• Similarity between averaged features. We represent
the subshots using the averaged frame-level feature
vectors within each subshot. We then compute the
similarity using the previously deﬁned sim(·, ·).

• Maximum similarity. We compute pairwise similar-
ity between frames within the subshots and select the
maximum value as the similarity between the sub-
shots.

Both of these two approaches reduce the reliance on
frame-based similarity deﬁned in the global
frame-
based descriptors of visual appearance, loosening the re-
quired visual alignment for discovering a good match—
especially with the latter max operator, in principle, since
it can ignore many unmatchable frames in favor of a sin-
gle strong link within the subshots. Moreover, the ﬁrst
approach can signiﬁcantly reduce the computational cost
of nonparametric learning as the amount of pairwise-
similarity computation now depends on the number of
subshots, which is substantially smaller than the number
of frames.

3.5 Implementation and computation cost
Computing Sr in eq. (11) is an O(N ×Pr Nr) operation.
For long videos, several approaches will reduce the cost
signiﬁcantly. First, it is a standard procedure to down-
sample the video (by a factor of 5-30) to reduce the num-
ber of frames for keyframe-based summarization. Our
subshot-based summarization can also reduce the compu-
tation cost, cf. section 3.4. Generic techniques should
also help — sim(·, ·) computes various forms of distances
among visual feature vectors. Thus, many fast search
techniques apply, such as locality sensitive hashing or tree
structures for nearest neighbor searches.

Features For OVP/YouTube/Kodak/SumMe, we en-
code each frame with an ℓ2-normalized 8192-dimensional
Fisher vector [35], computed from SIFT features. For
OVP/YouTube/Kodak, we also use color histograms. We
also experimented with features from a pre-trained convo-
lutional nets (CNN), details in the Suppl. For MED, we
use the provided 16512-dimensional Fisher vectors.

Evaluation metrics As in [10, 11, 12] and other prior
work, we evaluate automatic summarization results (A) by
comparing them to the human-created summaries (B) and
reporting the F-score (F), precision (P), and recall (R):

4 Experiment

Recall =

Precision =

× 100%,

#matched pairs

#frames (shots) in A
#matched pairs

#frames (shots) in B

× 100%,

(14)

(15)

(16)

We validate our approach on ﬁve benchmark datasets. It
outperforms competing methods in many settings. We
also analyze its strengths and weaknesses.

4.1 Setup

Data For keyframe-based summarization, we experi-
the Open Video Project
ment on three video datasets:
(OVP) [1, 4], the YouTube dataset [4], and the Kodak
consumer video dataset [29]. All the 3 datasets were
used in [10] and we follow the procedure described there
to preprocess the data, and to generate training ground-
truths from multiple human-created summaries. For the
YouTube dataset, in the following, we report results on 31
videos after discarding 8 videos that are neither “Sports”
nor “News” such that we can investigate category-speciﬁc
video summarization (cf. sec. 3.4). In Suppl., we report
results on the original dataset.

For subshot-based summarization (cf. sec. 3.4), we ex-
the portion of MED
periment on three video datasets:
with 160 annotated summaries [36], SumMe [11] and
YouTube. Videos in MED are pre-segmented into sub-
shots with the Kernel Temporal Segmentation (KTS)
method [36] and we observe those subshots. For SumMe
and YouTube, we apply KTS to generate our own sets
of subshots. MED has 10 well-deﬁned video categories
allowing us to experiment with category-speciﬁc video
summarization on it too. SumMe does not have seman-
tic category meta-data. Instead, its video contents have
a large variation in visual appearance and can be roughly
classiﬁed according to shooting style: still camera, ego-
centric, or moving.

Table 1 summarizes key characteristics of

those

datasets with details in the Supplementary Material.

F-scocre =

P · R

0.5(P + R)

× 100%.

For OVP/YouTube/Kodak, we follow [10] and utilize
the VSUMM package [4] for ﬁnding matching pairs of
frames. For SumMe, we follow the procedure in [11, 12].
For datasets with multiple human-created summaries, we
average or take the maximum over the number of human-
created summaries to obtain the evaluation metrics for
each video. We then average over the number of videos
to obtain the metrics for the datasets. More details are
included in the Supplementary Material.

Implementation details For each dataset, we randomly
choose 80% of the videos for training and use the remain-
ing 20% for testing, repeating for 5 or 100 rounds so that
we can calculate averaged performance and standard er-
rors. To report existing methods’ results, we use prior
published numbers when possible. We also implement
the VSUMM approach [4] and obtained code from the au-
thors for seqDPP [10] in order to apply them to several
datasets. We follow the practices in [10] so that we can
summarize videos sequentially. For MED, we implement
KVS [36] ourselves.

4.2 Main Results

We summarize our key ﬁndings in this section. For more
details, please refer to the Supplementary Material.

In Table 2, we compare our approach to both supervised
and unsupervised methods for video summarization. We
report published results in the table as well as results from
our own implementation of some methods. Only the best
variants of all methods are quoted and presented; others
are deferred to the Supplementary Material.

7

Table 1: Key characteristics of datasets used in our empirical studies. Most videos in these datasets have a duration
from 1 to 5 minutes.

# of
video

# of

category

# of Training

videos

# of Test

video

Type of

summarization

Dataset
Kodak
OVP

Youtube
SumMe
MED

18
50
31
25
160

-
-
2
-
10

14
40
31
20
128

4
10
8
5
32

keyframe
keyframe

Evaluation metrics
F-score in matching

selected frames

keyframe; subshot

selected frames; frames in selected subshots

subshot
subshot

frames in selected subshots
matching selected subshots

On all but one of the ﬁve datasets (OVP), our nonpara-
metric learning method achieves the best results. In gen-
eral, the supervised methods achieve better results than
the unsupervised ones. Note that even for datasets with
a variety of videos that are not closely visually similar
(such as SumMe), our approach attains the best result—it
indicates our method of transferring summary structures
is effective, able to build on top of even crude frame sim-
ilarities.

4.3 Detailed analysis
Advantage of nonparametric learning Nonparametric
learning enjoys the property of generalizing locally. That
is, as long as a test video has enough correctly annotated
training videos in its neighborhood, the summary struc-
tures of those videos will transfer. A parametric learning
method, on the other end, needs to learn both the loca-
tions of those local regions and how to generalize within
local regions. If there are not enough annotated training
videos covering the whole range of data space, it could be
difﬁcult for a parametric learning method to learn well.

We design a simple example to illustrate this point.
As it is difﬁcult to assess “similarity” to derive nearest
neighbors for video, we use a video’s category to de-
lineate those “semantically near”. Speciﬁcally, we split
YouTube’s 31 videos into two piles, according to their cat-
egories “Sports” or “News”. We then construct seqDPP,
a parametric learning model [10], using all the 31 videos,
as well as “Sports” or “News” videos only to summarize
test videos from either category. We then contrast to our
method in the same setting. Table 3 displays the results.

The results on the “News” category convincingly sug-
gest that the nonparametric approach like ours can lever-
age the semantically-close videos to outperform the para-
metric approach with the same amount of annotated
data—or even more data.
(Note that, the difference on
the “Sports” category is nearly identical.)

Table 3: Advantage of nonparametric summary transfer

Type of
test video

Sports
News

all
52.8
67.9

seqDPP

same as test

54.5
67.7

Ours
same as test

54.4
69.1

all
53.5
66.9

Table 4: Video category information helps summarization

Dataset

YouTube

MED
SumMe

w/o

category

60.0
28.9
39.2

category-speciﬁc
hard
61.5
30.4
40.9

soft
60.6
30.7
40.1

information can improve summarization (cf. contrasting
the column of “same as test” to “all”). Now we investi-
gate this advantage in more detail. Table 4 shows how our
nonparametric summary transfer can exploit video cate-
gory information, using the method explained in sec. 3.4.
Particularly interesting are our results on the SumMe
dataset, which itself does not provide semantically mean-
ingful categories. Instead, we generate two “fake” cate-
gories for it. We ﬁrst collapse the 10 video categories in
the dataset TVSum2 [38] into two super-categories (de-
tails in Suppl.) — these two super-categories are seman-
tically similar within each other, though they do not have
obvious visual similarity to videos in the SumMe. We
then build a binary classiﬁer trained on TVSum videos
but classify the videos in SumMe as “super-category I”
and “super-category II” and then proceed as if they are
ground-truth categories, as in MED and YouTube.

Despite this dichotomy being independently developed,
our summarization results on SumMe improve over using
all video data together.

Subshot-based summarization In section 3.4, we dis-
cuss an extension to summarize video at the level of se-

Advantage of exploiting category prior Table 3 al-
ready alludes to the fact that exploiting category side-

2We choose this one as it has raw videos for us to extract features
and have a larger number of labeled videos for us to build a category
classiﬁer

8

Table 2: Performance (F-score) of various video summarization methods. Numbers followed by citations are from
published results. Others are from our own implementation. Dashes denote unavailable/inapplicable dataset-method
combinations.

Unsupervised
DT
[32]

VSUMM1

VSUMM2

Kodak
OVP

YouTube

MED
SumMe

[4]
69.5
70.3
58.7
28.9
32.8

[4]
67.6
69.5
59.9
28.8
33.7

-

57.6

-
-
-

STIMO KVS Video MMR

Supervised

SumMe

Submodular

seqDPP Ours

[6]
-

63.4

-
-
-

[36]

[24]

[11]

[12]

-
-
-

20.6

-

-
-
-
-

-
-
-
-

-
-
-
-

26.6

39.3

39.7

[10]
78.9
77.7
60.1

-
-

82.3
76.5
61.8
30.7
40.9

Table 5: Subshot-based summarization on YouTube

Category
-speciﬁc

No
Yes

Subshot-based

Frame-
based Mean-Feature Max-similarity
60.0
61.5

60.9
61.8

60.7
61.6

Summary of nearest training video 

seqDPP 
(F = 62) 

Ours 
(F = 60) 

Ground-truth 

lecting subshots. This extension not only reduces com-
putational cost (as the number of subshots is signiﬁ-
cantly smaller than that of frames), but also provides ad-
ditional means of measuring similarity across videos be-
yond frame-level visual similarity inferred from global
frame-based descriptors. Next we examine how such ﬂex-
ibility can ultimately improve keyframe-based summa-
rization. Concretely, we ﬁrst perform subshot summariza-
tion, then pick the middle frame in each selected subshot
as the output keyframes. This allows us to directly com-
pare to keyframe-based summarization using the same F-
score metric.

Table 5 shows the results. Subshot-based summariza-
tion clearly improves frame-based – this is very likely due
to the more robust similarity measures now computed at
the subshot-level. The improvement is more pronounced
when a category prior is not used. One possible expla-
nation is that measuring similarity on videos from the
same categories is easier and more robust, whereas across
categories it is noisier. Thus, when a category prior is
not present, the subshot-based similarity measure beneﬁts
summarization more.

Other detailed analysis in Suppl. We summarize other
analysis results as follows. We show how to improve
frame-level similarity by learning better metrics. We also
show deep features, powerful for visual category recogni-
tion, is not particularly advantageous comparing to shal-
low features. We also show how category prior can still
be exploited even we do not know the true category of test
videos.

Figure 2: A failure case by our approach. Our sum-
mary misses the last two frames from the ground-truth
(red-boxed) as the test video (nature scene) transfers from
the nearest video with a semantically different category
(beach activity). See text.

4.4 Qualitative analysis

Fig. 2 shows a failure case by our method. Here the test
video depicts a natural scene, while its closest training
video depicts beach activities. There is a visual similarity
(e.g., in the swath of sky). However, semantically, these
two videos do not seem to be relevant and it is likely difﬁ-
cult for the transfer to occur. In particular, our results miss
the last two frames where there are a lot of grass. This
suggests one weakness in our approach: the formulation
of our summarization kernel for the test video does not
directly consider the relationship between its own frames
– instead, they interact through training videos. Thus,
one possible direction to avoid unreliable neighbors in the
training videos is to rely on the visual property of the
test video itself. This suggests future work on a hybrid
appraoch with parametric and nonparametric aspects that
complement each other.

Please see the Suppl. for more qualitative analysis and

example output summaries.

9

5 Conclusion

We propose a novel supervised learning technique for
video summarization. The main idea is to learn non-
parametrically to transfer summary structures from train-
ing videos to test ones. We also show how to exploit
side (semantic) information such as video categories and
propose an extension for subshot-based summarization.
Our method achieves promising results on several bench-
mark datasets, compared to an array of nine existing tech-
niques.

References

[1] Open video project. http://www.open-video.org/. 7
[2] Youtube statistics: https://www.youtube.com/yt/press/statistics.html.

1

[3] W.-L. Chao, B. Gong, K. Grauman, and F. Sha. Large-margin

determinantal point processes. In UAI, 2015. 3

[4] S. E. F. de Avila, A. P. B. Lopes, A. da Luz, and A. de Albu-
querque Ara´ujo. Vsumm: A mechanism designed to produce static
video summaries and a novel evaluation method. Pattern Recogni-
tion Letters, 32(1):56–68, 2011. 7, 9

[5] S. Feng, Z. Lei, D. Yi, and S. Z. Li. Online content-aware video

condensation. In CVPR, 2012. 2

[6] M. Furini, F. Geraci, M. Montangero, and M. Pellegrini. Stimo:
Still and moving video storyboard for the web scenario. Multime-
dia Tools and Applications, 46(1):47–69, 2010. 1, 9

[7] J. Gillenwater, A. Kulesza, and B. Taskar. Discovering diverse and

salient threads in document collections. In EMNLP, 2012. 3

[8] J. Gillenwater, A. Kulesza, and B. Taskar. Near-optimal MAP in-

ference for determinantal point processes. In NIPS, 2012. 4

[9] D. B. Goldman, B. Curless, D. Salesin, and S. M. Seitz. Schematic
storyboarding for video visualization and editing. 25(3):862–871,
2006. 1, 2

[10] B. Gong, W.-L. Chao, K. Grauman, and F. Sha. Diverse sequen-
tial subset selection for supervised video summarization. In NIPS,
2014. 1, 2, 3, 4, 7, 8, 9

[11] M. Gygli, H. Grabner, H. Riemenschneider, and L. Van Gool. Cre-

ating summaries from user videos. In ECCV. 2014. 7, 9

[12] M. Gygli, H. Grabner, and L. Van Gool. Video summarization by
learning submodular mixtures of objectives. In CVPR, 2015. 2, 3,
7, 9

[13] J. Hays and A. A. Efros. Scene completion using millions of pho-
tographs. ACM Transactions on Graphics (TOG), 26(3):4, 2007.
2

[14] R. Hong, J. Tang, H.-K. Tan, S. Yan, C. Ngo, and T.-S. Chua. Event
driven summarization for web videos. In SIGMM Workshop, 2009.
1, 2

[15] H.-W. Kang, Y. Matsushita, X. Tang, and X.-Q. Chen. Space-time

video montage. In CVPR, 2006. 1, 2

[16] A. Khosla, R. Hamid, C.-J. Lin, and N. Sundaresan. Large-scale
video summarization using web-image priors. In CVPR, 2013. 1,
2, 3

[17] G. Kim, L. Sigal, and E. P. Xing. Joint summarization of large-
scale collections of web images and videos for storyline recon-
struction. In CVPR, 2014. 3

[18] J. Kim and K. Grauman. Observe locally, infer globally: a space-
time mrf for detecting abnormal activities with incremental up-
dates. In CVPR, 2009. 2

[19] D. Koller and N. Friedman. Probabilistic graphical models: prin-

ciples and techniques. MIT press, 2009. 3

10

[20] A. Kulesza and B. Taskar. Learning determinantal point processes.

In UAI, 2011. 3

[21] A. Kulesza and B. Taskar. Determinantal point processes for ma-
chine learning. Foundations and Trends in Machine Learning, 5(2–
3), 2012. 3, 4

[22] R. Lagani`ere, R. Bacco, A. Hocevar, P. Lambert, G. Pa¨ıs, and B. E.
Ionescu. Video summarization from spatio-temporal features. In
ACM TRECVid Video Summarization Workshop, 2008. 1, 2

[23] Y. J. Lee, J. Ghosh, and K. Grauman. Discovering important peo-
In CVPR,

ple and objects for egocentric video summarization.
2012. 1, 2, 3

[24] Y. Li and B. Merialdo. Multi-video summarization based on video-

mmr. In WIAMIS Workshop, 2010. 9

[25] C. Liu, J. Yuen, and A. Torralba. Nonparametric scene parsing via

label transfer. IEEE PAMI, 33(12):2368–2382, 2011. 2

[26] D. Liu, G. Hua, and T. Chen. A hierarchical visual model for video
object summarization. IEEE PAMI, 32(12):2178–2190, 2010. 1, 2
[27] T. Liu and J. R. Kender. Optimization algorithms for the selection

of key frame sequences of variable length. In ECCV. 2002. 1, 2

[28] Z. Lu and K. Grauman. Story-driven summarization for egocentric

video. In CVPR, 2013. 1

[29] J. Luo, C. Papin, and K. Costello. Towards extracting semantically
meaningful key frames from personal video clips: from humans to
computers. IEEE TCSVT, 19(2):289–301, 2009. 7

[30] Y.-F. Ma, L. Lu, H.-J. Zhang, and M. Li. A user attention model

for video summarization. In ACM Multimedia, 2002. 1, 2

[31] A. G. Money and H. Agius. Video summarisation: A conceptual
framework and survey of the state of the art. Journal of Visual
Communication and Image Representation, 19(2):121–143, 2008.
2

[32] P. Mundur, Y. Rao, and Y. Yesha. Keyframe-based video sum-
International Journal on

marization using delaunay clustering.
Digital Libraries, 6(2):219–232, 2006. 1, 9

[33] J. Nam and A. H. Tewﬁk. Event-driven video abstraction and vi-
sualization. Multimedia Tools and Applications, 16(1-2):55–77,
2002. 1, 2

[34] C.-W. Ngo, Y.-F. Ma, and H. Zhang. Automatic video summariza-

tion by graph modeling. In ICCV, 2003. 1, 2

[35] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies

for image categorization. In CVPR, 2007. 7

[36] D. Potapov, M. Douze, Z. Harchaoui, and C. Schmid. Category-

speciﬁc video summarization. In ECCV. 2014. 3, 6, 7, 9

[37] Y. Pritch, A. Rav-Acha, A. Gutman, and S. Peleg. Webcam synop-

sis: Peeking around the world. In ICCV, 2007. 1, 2

[38] Y. Song, J. Vallmitjana, A. Stent, and A. Jaimes. Tvsum: Summa-

rizing web videos using titles. In CVPR, 2015. 6, 8

[39] M. Sun, A. Farhadi, B. Taskar, and S. Seitz. Salient montages from

unconstrained videos. In ECCV. 2014. 1

[40] A. Torralba, R. Fergus, and W. Freeman. 80 million tiny images:
a large dataset for non-parametric object and scene recognition.
PAMI, 2008. 2

[41] B. T. Truong and S. Venkatesh. Video abstraction: A systematic
review and classiﬁcation. ACM Transactions on Multimedia Com-
puting, Communications, and Applications (TOMM), 3(1):3, 2007.
2

[42] W. Wolf. Key frame selection by motion analysis.

In ICASSP,

1996. 2

[43] J. Xu, L. Mukherjee, Y. Li, J. Warner, J. M. Rehg, and V. Singh.
Gaze-enabled egocentric video summarization via constrained
submodular maximization. In CVPR, 2015. 3

[44] H. J. Zhang, J. Wu, D. Zhong, and S. W. Smoliar. An integrated
system for content-based video retrieval and browsing. Pattern
recognition, 30(4):643–658, 1997. 1, 2

[45] B. Zhao and E. P. Xing. Quasi real-time summarization for con-

sumer videos. In CVPR, 2014. 2

