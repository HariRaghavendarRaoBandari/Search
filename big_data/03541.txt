6
1
0
2

 
r
a

 

M
1
1

 
 
]

V
C
.
s
c
[
 
 

1
v
1
4
5
3
0

.

3
0
6
1
:
v
i
X
r
a

1

Watch-n-Patch: Unsupervised Learning of

Actions and Relations

Chenxia Wu, Jiemi Zhang, Ozan Sener, Bart Selman, Silvio Savarese, and Ashutosh Saxena

Abstract—There is a large variation in the activities that humans perform in their everyday lives. We consider modeling these
composite human activities which comprises multiple basic level actions in a completely unsupervised setting. Our model learns
high-level co-occurrence and temporal relations between the actions. We consider the video as a sequence of short-term action clips,
which contains human-words and object-words. An activity is about a set of action-topics and object-topics indicating which actions are
present and which objects are interacting with. We then propose a new probabilistic model relating the words and the topics. It allows
us to model long-range action relations that commonly exist in the composite activities, which is challenging in previous works.
We apply our model to the unsupervised action segmentation and clustering, and to a novel application that detects forgotten actions,
which we call action patching. For evaluation, we contribute a new challenging RGB-D activity video dataset recorded by the new
Kinect v2, which contains several human daily activities as compositions of multiple actions interacting with different objects. Moreover,
we develop a robotic system that watches people and reminds people by applying our action patching algorithm. Our robotic setup can
be easily deployed on any assistive robot.

Index Terms—Unsupervised Learning, Activity Discovery, Robot Application.

!

1 INTRODUCTION
The average adult forgets three key facts, chores or events every
day [2]. Hence it is important for a vision system to be able to
detect not only what a human is currently doing but also what he
forgot to do. For example in Fig. 1, someone fetches milk from
the fridge, pours the milk to the cup, takes the cup and leaves
without putting back the milk, then the milk would go bad. In this
paper, we focus on modeling these composite human activities
then detecting the forgotten actions for a robot, which learns from
a completely unlabeled set of RGB-D videos.

it

i.e.,

A human activity is composite,

is composed of
several basic level actions. For example, a composite activity
warming milk contains a sequence of actions: fetch-milk-from-
fridge, microwave-milk, put-milk-back-to-fridge, fetch-milk-from-
microwave, and leave. Modeling this poses several challenges.
First, some actions often co-occur in a composite activity but
some may not. Second, co-occurring actions have variations in
temporal orderings, e.g., people can ﬁrst put-milk-back-to-fridge
then microwave-milk instead of the inverse order in the above
example, as its ordering is more relevant to the action fetch-milk-
from-fridge. Moreover, these ordering relations could exist in both
short-range and long-range, e.g., pouring is followed by drink
while sometimes fetch-book is related to put-back-book with a

• Wu, Sener and Selman are with the Department of Computer
chenxi-

Ithaca, NY 14853. E-mail:

Science, Cornell University,
awu,ozan,selman@cs.cornell.edu

•
•

•

Zhang is with Didi Chuxing, China. Email: jmzhang10@gmail.com

Savarese is with the Department of Computer Science, Stanford University,
CA 94305. Email: ssilvio@cs.stanford.edu

Saxena is with Brain of Things Inc., Redwood City, CA 94062. Email:
asaxena@cs.stanford.edu

Parts of this work have been published in [57], [58] as the conference version.

Fig. 1: Our Watch-Bot understands what human is currently doing
by automatically segmenting the composite activity into basic level
actions. We propose a completely unsupervised approach to modeling
the human skeleton and object features to the actions, as well as
the pairwise action co-occurrence and temporal relations. Using the
learned model, our robot detects humans’ forgotten actions and
reminds them by pointing out the related object using the laser spot.

long read between them. Third, the objects the human interacting
with are also important to modeling the actions and their relations,
as same actions often have common objects in interaction.

The challenge that we undertake in this paper is: Can an al-
gorithm learn about the aforementioned relations in the composite
activities when just given a completely unlabeled set of RGB-D

videos?

Most previous works focus on action detection in a supervised
learning setting. In the training, they are given fully labeled actions
in videos [31], [42], [43], or weakly supervised action labels [9],
[13], or locations of human/their interacting objects [29], [37],
[50]. Among them, the temporal structure of actions is often
discovered by Markov models such as Hidden Markov Model
(HMM) [49] and semi-Markov [16], [45], or by linear dynamical
systems [5], or by hierarchical grammars [4], [28], [39], [52], [54],
or by other spatio-temporal representations [22], [24], [27], [38].
Object-in-use contextual information has also been commonly
used for recognizing actions [26], [27], [37], [54]. Besides relying
on the manually labeling, most of these works are based on RGB
features and only model the short-range relations between actions
(see Section 2 for details).

Unlike these approaches, we consider a completely unsu-
pervised setting. The novelty of our approach is the ability to
model the long-range action relations in the temporal sequence, by
considering pairwise action co-occurrence and temporal relations,
e.g., put-milk-back-to-fridge often co-occurs with and temporally
after (but not necessarily follows) fetch-milk-from-fridge. We also
use the more informative human skeleton features and RGB-D
object features, which have shown higher performance over RGB
only features for action recognition [26], [30], [59].

In order to capture the rich structure in the composite activity,
we draw strong parallels with the work done on document model-
ing from natural language (e.g., [8]) and proposed a Casual Topic
Model (CaTM). We consider an activity video as a document,
which consists of a sequence of short-term action clips contain-
ing human-skeleton-trajectories as human-words and interacting-
object-trajectories as object-words. An activity is about a set of
action-topics indicating which actions are present in the video,
such as fetch-milk-from-fridge in the warming milk activity, and a
set of object-topics indicating which object types are interacting.
We draw human-words from the action-topics, and object-words
from both action-topics and object-topics1. Then we model the
following (see Fig. 2):

• Action co-occurrence. Some actions often co-occur in the
same activity and may have the same objects. We model
the co-occurrence by adding correlated topic priors to the
occurrence of action-topics and object-topics, e.g., action-
topics fetch-book and put-back-book has strong correlations
and are also strongly correlated to object-topic book.

• Action temporal relations. Some actions often causally follow
each other, and actions change over time during the activity
execution. We model the relative time distributions between
every action-topic pair to capture the temporal relations.
We ﬁrst show that our model is able to learn meaningful
representations from the unlabeled composite activity videos. We
use the model to temporally segment videos to action segments
by assigning action-topics. We show that these action-topics are
promising to be semantically meaningful by mapping them to
ground-truth action classes and evaluating the labeling perfor-
mance.

We then show that our model can be used to detect forgotten
actions in the composite activity, a new application that we call
action patching. We enable a robot, which we call Watch-Bot,

1. Here we consider the same object type like book can be variant in
appearance in different actions such as close book in the fetch-book action
and open book in the reading action.

2

to detect humans’ forgotten actions as well as to localize the
related object in the scene. The setup of the robot can be easily
deployed on any assistive robot and applied to different areas
such as industry, medical work and home use. We evaluate the
action patching accuracy to show that the learned co-occurrence
and temporal relations are very helpful to inferring the forgotten
actions. We also show that our Watch-Bot is able to remind
humans of forgotten actions in the real-world robotic experiments.
We also provide a new challenging RGB-D activity video
dataset 2 recorded by the new Kinect v2 (see examples in Fig. 12),
in which the human skeletons are also recorded. It contains 458
videos of human daily activities as compositions of multiple
actions interacting with different objects, in which people forget
actions in 222 videos. They are performed by different subjects
in different environments with complex backgrounds. In robotic
experiments, we show that our Watch-Bot
is able to remind
humans of forgotten actions in the real-world experiments.

In summary, the main contributions of this work are:
• Our model is completely unsupervised thus being more useful

and scalable.

• Our model considers both the short-range and the long-
range action relations, showing the effectiveness in the action
segmentation and clustering.

• We show a new application by enabling a robot to remind

humans of forgotten actions in the real scenes.

• We provide a new challenging RGB-D activity dataset
recorded by the new Kinect v2, which contains videos of
multiple actions interacting with different objects.
The paper is organized as follows. Section 2 introduces the
related works. Section 3 outlines our approach to modeling the
composite activity. We present the visual features of the activity
video clip in Section 4. Section 5 gives the detailed description of
our learning model as well as its learning and inference. Section 6
introduces our watch-bot system to reminding of forgotten actions
using our learned model. We give an extensive evaluation and
discussion in the experiments in Section 7. Sections 8 concludes
the paper.

2 RELATED WORK
Action Recognition. Our work is related to the works on action
recognition in computer vision. There is a large number of works
on action recognition, which can be referred in recent surveys [3].
In this section, we cover the most related approaches. Most
previous works on action recognition are supervised [9], [13], [29],
[31], [34], [38], [42], [50]. Among them, the most popular are
linear-chain models such as hidden markov model (HMM) [49],
semi-Markov [16], [45] and the linear dynamic system [5]. They
focus on modeling the local transitions (between frames, temporal
segment, or sub-actions) in the activities. More complex hierar-
chical relations [28], [39], [52], [54] or graph relations [4], [46]
are considered in modeling actions in the complex activity. There
are also some works focusing on detecting local action patches,
primitives, trajectories or spatio-temporal features [19], [33], [35],
[62] without considering the high-level action relations. There also
exist some unsupervised approaches on action recognition. Yang
et al. [62] develop a meaningful representation by discovering
local motion primitives in an unsupervised way, then a HMM
is learned over these primitives. Jones et al. [20] propose an

2. The dataset and tools are released at http://watchnpatch.cs.cornell.edu.

unsupervised dual assignment clustering on the dataset recorded
from two views.

Although these approaches have performed well in different
areas, most of them rely on local relations between adjacent
clips or actions that ignore the long-term action relations and use
RGB visual features. Unlike these approaches, we use the richer
human skeleton and RGB-D features rather than the RGB action
features [21], [53]. We model the pairwise action co-occurrence
and temporal relations in the whole video, thus relations are
considered globally and completely with the uncertainty. We also
use the learned relations to infer the forgotten actions without any
manual annotations.
RGB-D and Human Skeleton Features. Action recognition
using human skeletons and RGB-D camera have shown the advan-
tages over RGB videos in many works. Skeleton-based approach
focus on proposing good skeletal representations [30], [43], [48],
[51], [59]. Besides of the human skeletons, we also detect the
human interactive objects in an unsupervised way to provide more
discriminate features. Object-in-use contextual information has
been commonly used for recognizing actions [26], [27], [37],
[54]. Moreover, Huet al. [17] propose a joint learning model to
simultaneously learn heterogenous features from RGB-D activity
videos. Most of them focus on designing or learning good action
features. They lost the high-level action relations which can be
captured in our model.
Bayesian Models. Our work is also related to the Bayesian
models. LDA [8] was the ﬁrst hierarchical Bayesian topic model
and widely used in different applications. The correlated topic
models [6], [23] add the priors over topics to capture topic
correlations. A topic model over absolute timestamps of words is
proposed in [55] and has been applied to action recognition [14].
However, the independence assumption of different topics would
lead to non smooth temporal segmentations. Recently, a multi-
feature max-margin hierarchical Bayesian model [60] is proposed
to jointly learn a high-level representation by combining a hierar-
chical generative model and discriminative maxmargin classiﬁers
in a uniﬁed Bayesian framework. Differently, our model considers
both correlations and the relative time distributions between topics
rather than the absolute time, which captures richer information of
action structures in the complex human activity.
Perception of Human Activities for Robotics. Our work is
also related to the works on recognizing human actions for
robotics [10], [25], [32]. Yang et al. [61] presented a system
that learns manipulation action plans for robot from unconstrained
youtube videos. Hu et al. [18] proposed an activity recognition
system trained from soft labeled data for the assistant robot.
Chrungoo et al.
[11] introduced a human-like stylized gestures
for better human-robot interaction. Piyathilaka et al. [40] used
3D skeleton features and trained dynamic bayesian networks for
domestic service robots. The output laser spot on object is also
related to the work ‘a clickable world’ [36], which selects the
appropriate behavior to execute for an assistive object-fetching
robot using the 3D location of the click by the laser pointer.
However, it is challenging to directly use these approaches to
detecting the forgotten actions and remind people.

3 OVERVIEW
We outline our approach in this section (see Fig. 2). The input to
our system is RGB-D videos with the 3D joints of human skeletons

3

Fig. 2: Video representation. (1) A video frames (fi) is ﬁrst decom-
posed into a sequence of overlapping ﬁxed-length temporal clips.
(2) The human-skeleton-trajectories/interactive-object-trajectories are
extracted from each clip, and we cluster them to form the human-
dictionary/object-dictionary. (3) Then the video is represented as a
sequence of human-word and object-word indices by mapping its
human-skeleton-trajectories/interactive-object-trajectories to the near-
est human-words/object-words in the dictionary. (4) An activity video
is about a set of action-topics/object-topics indicating which actions
are present and which types of objects are interacting with. (5)
We learn the mapping of action-words/object-words to the action-
topics/object-topics, as well as the co-occurrence and the temporal
relations between the topics. (6) We assign the topics to clips using
the learned model.

from Kinect v2. We ﬁrst decompose a video into a sequence of
overlapping ﬁxed-length temporal clips (step (1)). We then extract
the human-skeleton-trajectory features and the interacting-object-
trajectory features from the clips (introduced in Section. 4). The
human skeleton features and RGB-D object features have shown
higher performance over RGB only features for the human action
modeling [26], [30], [59].

In order to build a compact representation of the action
video, we draw parallels to document modeling in the natural
language [8] to represent a video as a sequence of words. We use
k-means to cluster the human-skeleton-trajectories/interacting-
object-trajectories from all the clips in the training set to form
a human-dictionary and an object-dictionary, where we use the
cluster centers as human-words and object-words ((2) in Fig. 2).
Then, the video can be represented as a sequence of human-
word and object-word indices by mapping its human-skeleton-
trajectories/interacting-object-trajectories to the nearest human-
words/object-words in the dictionary ((3) in Fig. 2). Also, an ac-
tivity video is about a set of action-topics indicating which actions
are present in the video, and a set of object-topics indicating which
object types are interacting in the actions ((4) in Fig. 2).

We then build an unsupervised learning model that models the
mapping of action-words/object-words to the action-topics/object-

topics, as well as the co-occurrence and the temporal relations
between the topics ((5) in Fig. 2). Using the learned model, we can
assign the action-topic/object-topic to each clip. So the continuous
clips with the same assigned action-topic form an action segment
((6) in Fig. 2).

The unsupervised action assignments of the clips are chal-
lenging because there is no annotation during the training stage.
Besides extracting rich visual features, we further consider the
relations among actions and objects. Unlike previous works, our
model captures long-range relations between actions e.g., put-
milk-back-to-fridge is strongly related to fetch-milk-from-fridge
even with pour and drink between them. We model all pairwise
co-occurrence and temporal casual relations between topics in a
video, using a new probabilistic model (introduced in Section 5).
Speciﬁcally, we use a joint distribution as the correlated topic
priors. They estimate which actions and objects are most likely
to co-occur in a video. And we use a relative time distributions
of topics to capture the temporal causal relations between actions,
which estimate the possible temporal ordering of the occurring
actions in the video.

u , x(2)

4 VISUAL FEATURES
We describe how we extract the visual features of a clip in this
section. We extract both human-skeleton-trajectory features and
the interacting-object-trajectory features from the output by the
Kinect v2 [1], which has an improved body tracker and the higher
resolution of RGB-D frame than the Kinect v1. The tracked human
skeleton has 25 joints in total. Let Xu = {x(1)
u ,··· , x(25)
u }
be the 3D coordinates of 25 joints of a skeleton in the current
frame u. We ﬁrst compute the cosine of the angles between the
connected body parts in each frame: α(pq) = (p(p)· p(q))/(|p(p)|·
|p(q)|), where the vector p(p) = x(i) − x(j) represents the body
part. The transition between the joint coordinates and angles in dif-
ferent frames can well capture the human body movements. So we
extract the motion features and off-set features [59] by computing
their Euclidean distances D(, ) to previous frame f m
u,u−1
and the ﬁrst frame f m
u−1)}pq;
u,u−1 = {D(x(i)
u , x(i)
f m
1 )}25
u,1 = {D(x(i)
u , x(i)
f m
Then we concatenate all f m
features of the clip.

i=1, f α
u,1 = {D(α(pq)
u
1
u,1 as the human
u,1, f α
u,u−1, f m

u,u−1 = {D(α(pq)
, α(pq)

u,1, f α
u−1)}25
i=1, f α

, α(pq)
)}pq.

u,1 in the clip:

u,u−1, f α

u,u−1, f α

u

Fig. 3: Examples of the human skeletons (red line) and the extracted
interacting objects (green mask, left: fridge, right: book).

We also extract the human interacting-object-trajectory based
on the human hands, image segmentation, motion detection and
tracking. To detect the interacting objects, ﬁrst we segment each
frame into super-pixels using a fast edge detection approach [12]
on both RGB and depth images. The RGB-D edge detection
provides richer candidate super-pixels rather than pixels to further
extract objects. We then apply the moving foreground mask [47]
to remove the unnecessary steady backgrounds and select those

4

super-pixels within a distance to the human hands in both 3D
points and 2D pixels. Finally, we collect the bounding boxes
enclosing these super-pixels as the potential interested objects (see
examples in Fig. 3).

We then track the bounding box in the segmented clip using
SIFT matching and RANSAC to get the trajectories. We use the
closest trajectory to the human hands for the clip. Finally, we
extract six kernel descriptors [41] from the bounding box of each
frame in the trajectory: gradient, color, local binary pattern, depth
gradient, spin, surface normals, and KPCA/self-similarity, which
have been proven to be useful features for RGB-D data [56]. We
concatenate the object features of each frame as the interacting-
object-trajectory feature of the clip.

5 LEARNING MODEL
In order to incorporate the aforementioned properties of activities,
we present a new generative model (see the graphic model in
Fig. 4 and the notations in Table 1). The novelty of our model
is the ability to capture both short-range and long-range relations
between actions in the compose activity videos in an unsupervised
way. Using these relations, we can simultaneously segment the
video and assign the action-topics as well as infer forgotten
actions.

Consider a collection of D videos (documents in the topic
model). Each video as a document d consists of Nd continuous
clips {cnd}Nd
n=1, each of which consists of a human-word wh
nd
mapped to the human-dictionary and an object-word wo
nd mapped
to the object-dictionary. We assign action-topic to each clip cnd
from K latent action-topics, indicating which action-topic they
belong to. We assign object-topic to each object-word wo
nd from
P latent object-topics, indicating which object-topic is interacting
within the clip. The assignments are denoted as z(1)
nd and z(2)
nd .
We use superscripts (1), (2) to denote action-topics and object-
topics respectively. After assignments, continuous clips with the
same action-topic compose an action segment in a video. All the
segments assigned with the same action-topic from the training set
compose an action cluster.

:d ), z(2)

dn ∼ M ult(π(2)

The topic model such as LDA [8] has been very common
for document modeling from language. We use a it to generate
a video document using a mixture of topics. Enable to model
human actions in the video, our model introduces co-occurrence
and temporal structure of topics instead of the topic independence
assumption in LDA.
Basic generative process. In a document d, we choose
dn ∼ M ult(π(1)
z(1)
:d ), where M ult(π) is
a multinomial distribution with parameter π. The human-word
nd is drawn from an action-topic speciﬁc multinomial distribu-
wh
k ∼ Dir(β(1)) is
tion φ(1)
z(1)
nd
the human-word distribution of action-topic k, sampled from a
Dirichlet prior with the hyperparameter β(1). While the object-
nd is drawn from an action-topic and object-topic spe-
word wo
ciﬁc multinomial distribution φ(12)
),
nd z(2)
z(1)
nd z(2)
z(1)
kp ∼ Dir(β(12)) is the object-word distribution of
where φ(12)
action-topic k and object-topic p. Here we consider the same
object type like book can be variant in appearance in different
actions such as a close book in fetch-book and a open book in read
action. So we consider the object-word distribution for different
combinations of the action topic and the object topic.

dn ∼ M ult(φ(12)

dn ∼ M ult(φ(1)

), where φ(1)

, wh

, wo

z(1)
dn

nd

nd

TABLE 1: Notations in our model.

5

Symbols Meaning
D
K
P
Nd
cnd
wh
nd
wo
nd
z(1)
nd
z(2)
nd
tnd
tmnd
π(1)
:d , π(2)
v(1)
:d , v(2)
φ(1)
k
φ(12)
kp

number of videos in the training database;
number of action-topics;
number of object-topics;
number of human-words/object-words in a video;
n-th clip in d-th video;
n-th human-word in d-th video;
n-th object-word in d-th video;
action-topic assignment of cnd;
object-topic assignment of wo
nd;
normalized timestamp of of cnd;
= tmd − tnd the relative time between cmd and cnd;

:d , π(2)

:d in d-th document;

:d the probabilities of action/object-topics in d-th document;
:d

the priors of π(1)
multinomial human-word distribution from action-topic k;
multinomial object-word distribution from
action-topic k and object-topic p;
multivariate normal distribution of v:d = [v(1)
relative time distribution of tmnd, between action-topic k, l;

:d , v(2)
:d ];

µ, Σ
θkl

Fig. 4: The graphic model of our causal topic model.

k−1(cid:89)
p−1(cid:89)

l=1

l=1

in document d, where(cid:80)K

Topic correlations. The co-occurrence such as action pour
and action drink, object book and action read, is useful to recog-
nizing the co-occurring actions/objects and also gives a strong evi-
dence for detecting forgotten actions. We model the co-occurrence
by drawing their priors from a mixture distribution. Let π(1)
kd , π(2)
pd
be the probability of action-topic k and object-topic p occurring
pd = 1. Instead
of sampling it from a ﬁx Dirichlet prior with parameter in LDA
that models them independently, we construct the probabilities by
a stick-breaking process as follows. The stick-breaking notion has
been widely used for constructing random weights [23], [44].

kd = 1,(cid:80)P

k=1 π(1)

p=1 π(2)

kd = Ψ(v(1)
π(1)
kd )

pd = Ψ(v(2)
π(2)
pd )

Ψ(v(1)

ld ), Ψ(v(1)

kd ) =

Ψ(v(2)

ld ), Ψ(v(2)

pd ) =

1

1 + exp(−v(1)
kd )

1

1 + exp(−v(2)
pd )

,

,

kd , v(2)

kd ), Ψ(v(2)
where 0 < Ψ(v(1)
tion, which satisﬁes Ψ(−v(1)
1 − Ψ(v(2)

pd ) < 1 is a classic logistic func-
kd ), Ψ(−v(2)
kd ) = 1 − Ψ(v(1)
pd ) =
kd , π(2)
pd .

pd serves as the prior of π(1)

pd ), and v(1)

In order to capture the correlations between action-topics and
object-topics, we draw the packed vector v:d = [v(1)
:d ] in
the stick-breaking notion from a mutivariate normal distribu-
tion N (µ, Σ). In practice, we use a truncated vector v(1)
(cid:80)K−1
:d =
Kd = 1 −
1d ,··· , v(1)
[v(1)
k=1 π(1)
kd ) as the probability of the ﬁnal

K−1,d] for (K-1) topics, and set π(1)

kd = (cid:81)K−1

k=1 Ψ(−v(1)

:d , v(2)

topic for a valid distribution. The same for v(2)
:d .

Relative time distributions. The temporal relations between
actions are also useful to discriminating the actions using temporal
ordering and inferring the forgotten actions using the temporal
context. We model the relative time of occurring actions by taking
their time stamps into account. We consider that the relative time
between two words are drawn from a certain distribution according

Fig. 5: The relative time distributions learned by our model on training
set (the blue dashed line) and the ground-truth histogram of the
relative time over the whole dataset (the green solid line).

to their topic assignments. In detail, let tnd, tmd ∈ (0, 1) be
the absolute time stamp of n-th word and m-th word, which is
normalized by the video length. tmnd = tmd − tnd is the relative
time of m-th clip relative to n-th clip. Then tmnd is drawn from a
certain distribution, tmnd ∼ Ω(θz(1)
are the
(cid:40)
parameters. Ω(θk,l) are K 2 pairwise action-topic speciﬁc relative
time distributions deﬁned as follows:
bk,l · N (t|θ+
k,l)
1 − bk,l · N (t|θ−
k,l)

if t ≥ 0,
if t < 0,

Ω(t|θk,l) =

), where θz(1)

md,z(1)

md,z(1)

(1)

nd

nd

An illustration of the learned relative time distributions are
shown in Fig. 5. We can see that the distributions we learned
correctly reﬂect the order of the actions, e.g., put-back-to-fridge
is after pour and can be before/after microwave, and the shape
is almost similar to the real distributions. Here the Bernoulli
distribution bk,l/1 − bk,l gives the probability of action k af-
ter/before the action l. And two independent normal distributions
N (t|θ+
k,l) estimate how long the action k is after/before
the action l3. Then the order and the length of the actions will be
captured by all these pairwise relative time distributions.

k,l)/N (t|θ−

3. Specially, when k = l, If two words are in the same segments, we draw t
from a normal distribution which is centered on zero, and the variance models
the length of the action. If not, it also follows Eq. (1) indicating the relative
time between two same actions. We also use functions tan(−π/2 + πt)(0 <
t < 1), tan(π/2 + πt)(−1 < t < 0) to feed t to the normal distribution so
that the probability is valid, that summits to one through the domain of t.

6

(a) Robot System.

(b) System Pipeline.

Fig. 6: (a). Our Watch-Bot system. It consists of a Kinect v2 sensor that inputs RGB-D frames of human actions, a laptop that infers the
forgotten action and the related object, a pan/tilt camera that localizes the object, mounted with a ﬁxed laser pointer that points out the object.
(b). The system pipeline. The robot ﬁrst uses the learned model to infer the forgotten action and the related object based on the Kinect’s input.
Then it maps the view from the Kinect to the pan/tilt camera so that the bounding box of the object is mapped in the camera’s view. Finally,
the camera moves until the laser spot lies in the bounding box of the target object.

5.1 Learning and Inference
Gibbs sampling is commonly used as a means of statistical
inference to approximate the distributions of variables when direct
sampling is difﬁcult [7], [23]. Given a video, the word wh
nd, wo
nd
and the relative time tmnd are observed. We can integrate out
k , Φ(12)
Φ(1)
since Dir(β(1)), Dir(β(12)) are conjugate priors for
the multinomial distributions Φ(1)
kp . We also estimate the
standard distributions including the mutivariate normal distribu-
tion N (µ, Σ) and the time distribution Ω(θkl) using the method
of moments, once per iteration of Gibbs sampling. Following the
convention, we use the ﬁxed symmetric Dirichlet distributions by
setting β(1), β(12) as 0.01.

k , Φ(12)

kp

Then we introduce how we sample the topic assignment
nd . We do a collapsed sampling as in LDA by calculating
nd = k|π(1)
kd ω(k, wh
nd = p|π(2)

z(1)
nd , z(2)
the posterior distribution of z(1)
:d , z(1)−nd, z(2)
nd)ω(k, z(2)
:d , z(2)−nd, z(1)

nd , z(2)
nd :
nd , tnd)
nd , wo
nd ) ∝ π(2)

nd)p(tnd|z(1)
pd ω(z(1)

p(z(1)
∝ π(1)
p(z(2)

:d , θ),
nd , p, wo

nd),

kd /π(2)

nd), ω(k, p, wo

In Eq. (2), note that the topic assignments are decided by
which actions/objects are more likely to co-occur in the video
(the occurance probabilities π(1)
kd ), the visual appearance of
nd)) and the
the word (the word distributions ω(k, wh
temporal relations (the relative time distributions p(tnd|z(1)
:d , θ)).
Due to the logistic stick-breaking transformation, the poste-
rior distribution of the topic priors v:d = [v(1)
:d ] does not
have a closed form. So we instead use a Metropolis-Hastings
:d|v:d, µ, Σ) =
independence sampler [15]. Let the proposals q(v∗
:d|µ, Σ) be drawn from the prior. The proposal is accepted
N (v∗
with probability min(A(v∗
A(v∗

:d, v:d), 1), where

:d , v(2)

:d, v:d)
p(v∗

:d|µ, Σ)(cid:81)Nd
p(v:d|µ, Σ)(cid:81)Nd
n=1 p(z(1)
Nd(cid:89)
n=1 p(z(1)
nd |v(1)∗
p(z(1)
)p(z(2)
nd |v(1)
K(cid:89)
p(z(1)
:d )p(z(2)
(cid:80)Nd
π(1)∗
kd
π(1)
kd

n=1 δ(z(1)

(
k=1

nd ,k)

n=1

:d

)

:d

nd |v(1)∗
)p(z(2)
nd |v(1)
:d )p(z(2)
nd |v(2)∗
)
:d
nd |v(2)
P(cid:89)
:d )

π(2)∗
pd
π(2)
pd

(
p=1

=

=

=

)q(v:d|v∗

nd |v(2)∗
nd |v(2)

:d

:d )q(v∗

:d, µ, Σ)
:d|v:d, µ, Σ)

(cid:80)Nd

)

n=1 δ(z(2)

nd ,p),

ω(k, wh

nd) =

ω(k, p, wo

nd) =

p(tnd|z(1)

:d , θ) =

,

N−nd
kwh + β(1)
N−nd
k + Nwh β(1)
N−nd
kpwo + β(12)
N−nd
kp + Nwoβ(12)
Ω(tmnd|θz(1)

Nd(cid:89)

,

m

which can be easily calculated by counting the number of words
assigned with each topic by z(1)
nd . Here the function δ(x, y) =
1 if only if x = y, otherwise equal to 0. The time complexity of
the sampling per iteration is O(NdD(max(NdK, P )))

nd , z(2)

For inference of a test video, we sample the unknown topic
:d using the

nd and the topic priors v(1)

assignments z(1)
learned parameters in the training stage.

:d , v(2)

nd , z(2)

),

(2)

md,k)Ω(tnmd|θk,z(1)

md

kwh /N−nd

where Nwh , Nwo is the number of unique word types in dic-
tionary, N−nd
kpwo denotes the number of instances of word
nd assigned with action-topic k/action-topic k and object-
nd/wo
wh
topic p, excluding n-th word in d-th document, and N−nd
/N−nd
denotes the number of total words assigned with action-topic
k/action-topic k and object-topic p. z(1)−nd/z(2)−nd denotes the
topic assignments for all words except z(1)
nd . The detailed
derivation of Eq. (2) is in the Appendix A.

nd /z(2)

kp

k

6 WATCH-BOT TO REMINDING OF FORGOTTEN
ACTIONS
The average adult forgets three key facts, chores or events every
day [2]. So it is important for a personal robot to be able to detect
not only what a human is currently doing but also what he forgot
to do. In this section, we describe a new robot system (see Fig. 6)

to detect the forgotten actions and remind people, which we called
action patching, using our learning model.

Note that detecting forgotten action is more challenging than
conventional action recognition, since what to infer is not shown
in the query video. Also, our model does not necessarily know
the semantic class of the actions. Instead it learns action clusters
and relations from the unlabeled action videos and use them to
detect forgotten actions and remind people. Therefore, modeling
rich relations from videos is important to providing evidence
for detecting forgotten actions. Our model models pairwise co-
occurrence and long-range temporal relations of actions/topics. As
a result, rather than only modeling the single action or the local
temporal transitions in the previous works, those actions occurred
with a relatively large time interval, occurred after the forgotten
actions, as well as the interacting objects can also be used to
detect forgotten actions in our model. For example, a put-back-
book might be forgotten as previously seen a fetch-book action
before a long read action, and seen a book and a leave action
indicates he really forgot it.

We enable a robot, that we call Watch-Bot, to detect humans’
forgotten actions as well as localize the related object in the current
scene. The robot consists of a Kinect v2 sensor, a pan/tilt camera
(which we call camera for brevity in this paper) mounted with a
laser pointer, and a laptop (see Fig. 6). This setup can be easily
deployed on any assistive robot. Taking the example in Fig. 1,
if our robot sees a person fetch a milk from the fridge, pour the
milk, and leave without putting the milk back to the fridge. Our
robot would ﬁrst detect the forgotten action and the related object
(the milk), given the input RGB-D frames and human skeletons
from the Kinect; then map the object from the Kinect’s view to
the camera’s view; ﬁnally pan/tilt the camera till its mounted laser
pointer pointing to the milk.

Our goal is to detect the forgotten action and then point out
the related object in the forgotten action using our learned model
(see Alg. 1). We ﬁrst use our model to segment the query video
into action segments (step 1,2 in Alg. 1), and then infer the most
possible forgotten action-topic and the related object-topic (step 4
in Alg. 1). Next we retrieve a top forgotten action segment from
the training database, containing the inferred forgotten action-
topic and the object-topic (step 5,6 in Alg. 1). Using the extracted
object in the retrieved segment, we detect the bounding box of the
related forgotten object in the Kinect’s view of the query video
(step 8,9,10 in Alg. 1). After that, we map the bounding box of
the object from the Kinect’s view to the camera’s view. Finally,
the pan/tilt camera moves until its mounted laser pointer points
out the related object in the current scene.

Patched Action and Object Inference. Our model infers
the forgotten action using the probability inference based on the
dependencies between actions and objects. After assigning the
action-topics and object-topics to a query video q, we consider
adding one additional clip ˆc consisting of ˆwh, ˆwo into q in each
action segmentation point ts (see Fig 7). Then the probabilities
of the missing action-topics km with object-topics pm in each
segmentation point ts can be compared following the posterior
distribution in Eq. (2):

7

Algorithm 1 Forgotten Action and Object Detection.
Input: RGB-D video q with tracked human skeletons.
Output: Claim no action forgotten, or output an action segment
with the forgotten action and a bounding box of the related object
in the current scene.
1. Assign the action-topics to clips and the object-topics to object-
words in q as introduced in Section 5.1.
2. Get the action segments by merging the continuous clips with
the same assigned action-topic.
3. If the assigned action-topics Ke in q contains all modeled action-
topics [1 : K], claim no action forgotten and return;
4. For each action segmentation point ts, each not assigned action-
topic km ∈ [1 : K] − Ke, and each object-topic pm ∈ [1 : P ]:

Compute the probability deﬁned in Eq. 3;

5. Select the top tree possible tuples (km, pm, ts), and get the
forgotten action segment candidate set Q which contains segments
with topics (km, pm);
6. Select the top forgotten action segment p from Q with the
maximum patch score(p);
7. If patch score(p) is smaller than a threshold, claim no action
forgotten and return;
8. Segment the current frame to super-pixels using edge detec-
tion [12] as in Section 3;
9. Select the nearest super-pixels to both extracted object bounding
box in q and p.
10. Merge the adjacent super-pixels and bound the largest one with
a rectangle as the output bounding box.
11. Return the top forgotten action segment and the object bounding
box.

where Ts is the set of segmentation points (t1, t2 in Fig. 7)
and Ke is the set of existing action-topics in the video
(fetch-book, etc. in Fig. 7). Thus [1 : K] − Ke are the miss-
ing topics in the video (put-down-items, etc.
in Fig. 7).
p(ts|z(1)
:d , θ), ω(km, wh), ω(km, pm, wo) can be computed as in
Eq. (2). Here we marginized ˆwh, ˆwo to avoid the effect of a
speciﬁc human-word or object-word. Note that, π(1)
pd gives
the probability of a missing action-topic with an object-topic
in the video decided by the correlation we learned in the joint
distribution prior, i.e., the close topics have higher probabilities to
occur in this query video. And p(ts|z(1)
:d , θ) measures the temporal
consistency of adding a new action-topic. And the marginized
wh,wo ω(km, wh)ω(km, pm, wo) give

word-topic distribution (cid:80)

kd , π(2)

the likelihood of the topic learned from training data.

Patched Action and Object Detection. Then we select the
top three tuples (km, pm, ts) using the above probability. The
action segments of action-topic km containing object-topic pm
in the training set consist a patched action candidate segment set
Q. We then select the patched action segment from Q with the
maximum patch score deﬁned in Eq. 4. In detail, we consider
that the front and the tail of the patched action segment fpf , fpt
should be similar to the tail of the adjacent segment in q before ts
and the front of the adjacent segment in q after ts: fqt, fqf . At the
same time, the middle of the patched action segment fpm should
be different to fqt, fqf , as it is a different action forgotten in the
video.4

patch score(p) = ave(D(fpm, fqf ), D(fpm, fqt))
−max(D(fpf , fqt), D(fpt, fqf )),

(4)

where D(, ) is the average pairwise distances between frames,
ave(, ), max(, ) are the average and max value. If the maximum

p(z(1)
∝ π(1)

ˆc = pm, tˆc = ts|other)

(cid:88)
ˆc = km, z(2)
kmdπ(2)
ts ∈ Ts, km ∈ [1 : K] − Ke,

pmdp(ts|z(1)

:d , θ)

wh,wo

s.t.

ω(km, wh)ω(km, pm, wo),

4. Here the middle, front, tail frames are 20%-length of segment centering
on the middle frame, starting from the ﬁrst frame, and ending at the last frame
in the segment respectively.

(3)

8

In order to get a variation in activities, we ask participants to
ﬁnish task with different combinations of actions and ordering.
Some actions occur together often such as fetch-from-fridge and
put-back-to-fridge while some are not always in the same video
such as take-item and read. Some actions are in ﬁx ordering such
as fetch-book and put-back-book while some occur in random
order such as put-back-to-fridge and microwave. Moreover, to
evaluate the action patching performance, 222 videos in the
dataset has action forgotten by people and the forgotten actions
are annotated. We give the examples of action classes in Fig. 12
and action sequences in Table 4.

7.2 Experimental Setting and Compared Baselines
We evaluate in two environments ‘ofﬁce’ and ‘kitchen’. In each
environment, we split the data into a train set with most full videos
(ofﬁce: 87, kitchen 119) and a few forgotten videos (ofﬁce: 10,
kitchen 10), and a test set with a few full videos (ofﬁce: 10,
kitchen 20) and most forgotten videos (ofﬁce: 89, kitchen 113). In
our experiments, we compare seven unsupervised approaches with
only action-topics. They are Hidden Markov Model (HMM), topic
model LDA (TM), correlated topic model (CTM), topic model
over absolute time (TM-AT), correlated topic model over absolute
time (CTM-AT), topic model over relative time (TM-RT) and our
causal topic model with only action-topics (CaTM-A) [57]. We
compare three methods with both action-topics and object-topics.
They are HMM with the object-topics (HMM-O), LDA with the
object-topics (TM-O) and our causal topic model with the object-
topics (CaTM-AO). All these methods use the same human skele-
ton and RGB-D features introduced in Section 4. We also evaluate
HMM and our model CaTM using the popular features for action
recognition, dense trajectories feature (DTF) [53], extracted only
in RGB videos5, named as HMM-DTF and CaTM-A-DTF, CaTM-
AO-DTF.

In the experiments, we set the number of topics and states of
HMM equal to or more than ground-truth classes. For correlated
topic models, we use the same topic prior in our model. For models
over absolute time, we consider the absolute time of each word
is drawn from a topic-speciﬁc normal distribution. For models
over relative time, we use the same relative time distribution as
in our model (Eq. (1)). The clip length of the action-words is
set to 20 frames, densely sampled by step one and the size of
action dictionary is set to 500. For patching, the candidate set
for different approaches consist of the segments with the inferred
missing topics by transition probabilities for HMM, the topic
priors for TM and CTM, and both the topic priors and the time
distributions for TM-AT, TM-RT, CTM-AT and our CaTM. Then
we use the same patch score as in Section 6 to select the top one
patched segments, and the average of the patch score computed
in a set of the segmented videos after training is set as the threshold
of claiming forgotten action.

7.3 Evaluation Metrics
Action Segmentation and Cluster Assignment. We want to
evaluate if the unsupervised learned action-topics and states of
HMM are semantically meaningful. In the unsupervised setting,
we need to map the assigned topics to the ground-truth labels
for evaluation. This could be done by counting the mapped frames

5. We train a codebook with the size of 2000 and encode the extracted DTF

features in each clip as the bag of features using the codebook.

Fig. 7: Illustration of patched action and object inference using
our model. Given a test video, we infer the forgotten action-topic and
object-topic in each segmentation point (t1, t2 as above). Then we
select the top segment from the inferred action-topic’s segment cluster
with the inferred object-topic with the maximum patch score.

score is below a threshold or there is no missing topics (i.e., Ke =
[1 : K]) in the query video, we claim there is no forgotten actions.
Then we detect the bounding box of the patched object. We ﬁrst
segment the current frame into super-pixels as in Section 3, second
search the nearest segments using the extracted object in the test
video and the patched action, ﬁnally merge the adjacent segments
into one segment and bound the largest segment with a bounding
box.

Real Object Pointing. We now describe how we pan/tilt
the camera to point out the real object in the current scene. We
ﬁrst compute the transformation homography matrix between the
frame of the Kinect and the frame of the pan/tilt camera using
keypoints matching and RANSAC, which can be done very fast
within 0.1 second. Then we can transform the detected bounding
box from the Kinect’s view to the pan/tilt camera’s view. Since the
position of the laser spot in the pan/tilt camera view is ﬁxed, next
we only need to pan/tilt the camera till the laser spot lies within the
bounding box of the target object. To avoid the coordinating error
caused by distortion and inconsistency of the camera movement,
we use an iterative search plus small step movement instead of
one step movement to localize the object (illustrated in Fig. 6). In
each iteration, the camera pan/tilt a small step towards to the target
object according to the relative position between the laser spot and
the bounding box. Then the homography matrix is recomputed in
the new camera view, so that the bounding box is mapped in the
new view. Until the laser spot is close enough to the center of the
bounding box, the camera stops moving.

7 EXPERIMENTS
7.1 Watch-n-Patch Dataset
We collect a new challenging RGB-D activity dataset recorded by
the new Kinect v2 camera. Each video in the dataset contains 2-7
actions interacting with different objects (see examples in Fig. 12).
The new Kinect v2 has higher resolution of RGB-D frames (RGB:
1920 × 1080, depth: 512 × 424) and improved body tracking of
human skeletons (25 body joints). We record 458 videos with a
total length of about 230 minutes. We ask 7 subjects to perform
human daily activities in 8 ofﬁces and 5 kitchens with complex
backgrounds. And in each environment the activities are recorded
in different views. It composed of fully annotated 21 types of
actions (10 in the ofﬁce, 11 in the kitchen) interacting with 23
types of objects. We also record the audio, though it is not used in
this paper.

TABLE 2: Results using the same number of topics as the ground-
truth action classes. HMM-DTF, CaTM-A-DTF, CaTM-AO-DTF use
DTF RGB features and others use our human skeleton and RGB-D
features. (top one is bold)

9

‘ofﬁce’

Seg-Acc

Seg-AP

Frame-Acc PA-Acc PO-Acc

(%)

HMM-DTF

HMM
HMM-O

TM
TM-O
CTM
TM-AT
CTM-AT
TM-RT

Ofﬂine Online Ofﬂine Online Ofﬂine Online
15.9
15.2
21.3
18.0
18.2
27.3
13.0
9.3
18.4
9.8
16.4
10.0
13.8
8.9
15.5
9.6
30.8
36.4
34.0
28.2
CaTM-A-DTF
35.0
CaTM-AO-DTF 28.5
38.5
30.6
33.2
41.2
Seg-Acc

20.2
24.7
25.3
20.3
24.6
20.2
18.6
19.6
38.1
37.4
37.9
39.9
40.1
Frame-Acc PA-Acc PO-Acc

21.4
25.9
26.2
20.9
22.3
18.1
25.4
25.3
29.0
28.3
30.6
33.1
33.0

20.7
24.8
23.1
19.6
19.6
15.8
19.0
19.8
30.2
27.4
29.5
34.6
36.0

23.6
33.3
32.2
13.3
15.7
13.3
12.0
10.8
39.5
33.7
36.2
41.5
46.2

9.4
14.0
19.4
9.2
12.2
5.9
3.7
6.8
30.9
27.0
29.1
32.9
35.2

CaTM-A
CaTM-AO
‘kitchen’

Seg-AP

36.4

10.5

20.4

30.5

-
-
-
-
-

-
-

-

-

(%)

Ofﬂine Online Ofﬂine Online Ofﬂine Online

HMM-DTF

HMM
HMM-O

TM
TM-O
CTM
TM-AT
CTM-AT
TM-RT

4.9
20.3
23.9
7.9
7.9
10.5
8.0
9.7
32.3
CaTM-A-DTF
26.9
CaTM-AO-DTF 27.2
33.2
32.1

CaTM-A
CaTM-AO

3.6
15.2
17.2
4.7
6.7
9.2
4.8
10.0
26.9
23.6
25.3
29.0
30.7

18.8
20.7
21.1
21.5
22.6
20.5
21.5
19.1
23.4
18.4
19.1
26.4
28.5

5.6
13.8
18.8
14.7
17.1
14.9
21.6
22.6
23.0
17.4
18.6
25.5
28.5

12.3
21.0
23.5
20.9
24.9
18.9
20.9
20.1
35.0
33.3
32.9
37.5
39.2

9.8
18.3
20.3
11.5
14.4
15.7
14.0
16.7
31.2
29.9
30.2
34.0
36.9

2.3
7.4
12.4
9.6
10.8
6.4
7.4
10.7
18.3
16.5
17.6
20.5
24.4

-
-
5.3
-
5.3
-
-
-
-
-

13.2

-

20.6

(cid:80)

(cid:80)

i δ(ki,k)δ(ci,c)

, where(cid:80)

between topics and ground-truth classes. Let ki, ci be the assigned
topic and ground-truth class of frame i. The count of a mapping is:
i δ(ki, k)δ(ci, c) is the number
mkc =
of frames assigned with topic k as the ground-truth class c and
normalized by the number of frames as the ground-truth class
i δ(ci, c). Then we can solve the following binary linear

c: (cid:80)

i δ(ci,c)

programming to get the best mapping:

(cid:88)
(cid:88)
xkc = 1, ∀c,

k,c

c

k

x

max

s.t. ∀k,

xkcmkc,
xkc ≥ 1,

must be mapped by at least one topic.

(cid:88)
xkc = 0. And (cid:80)
mapped to exact one class,(cid:80)

xkc ∈ {0, 1},
where xkc = 1 indicates mapping topic k to class c, otherwise
c xkc = 1 constrain that each topic must be
k xkc ≥ 1 constrain that each class
We then measure the performance in two ways. Per frame: we
compute frame-wise accuracy (Frame-Acc), the ratio of correctly
labeled frames. Segmentation: we consider a true positive if the
overlap (union/intersection) between the detected and the ground-
truth segments is more than a default threshold 40% as in [39].
Then we compute segmentation accuracy (Seg-Acc), the ratio
of the ground-truth segments that are correctly detected, and
segmentation average precision (Seg-AP) by sorting all action
segments output by the approach using the average probability
of their words’ topic assignments. All above three metrics are
computed by taking the average of each action class.

Forgotten Action and Object Detection. We also evaluate
the patching accuracy (PA-Acc) by the portion of correct patched
video, including correctly output the forgotten action segments or
correctly claiming no forgotten actions. We consider the output
action segments by the algorithm containing over 50% ground-
truth forgotten actions as correctly output the forgotten action

Fig. 8: Online segmentation Acc/AP varied with the number of topics
in the ‘ofﬁce’ dataset.

Fig. 9: Forgotten action/object detection accuracy varied with the
number of action-topics in the ‘ofﬁce’ dataset.

segments. We also measure the patching object detection accuracy
(PO-Acc) by the typical object detection metric, that considers a
true positive if the overlap rate (union/intersection) between the
detected and the ground-truth object bounding box is greater than
40%.

7.4 Results
Table 2 and Fig. 8 show the main results of our experiments.
We ﬁrst perform evaluation in the ofﬂine setting to see if actions
can be well segmented and clustered in the train set. We then
perform testing in an online setting to see if the new video from
the test set can be correctly segmented and the segments can
be correctly assigned to the action cluster. We can see that our
approach performs better than the state-of-the-art in unsupervised
action segmentation and clustering, as well as action patching. We
discuss our results in the light of the following questions.

Did modeling the long-range relations help? We studied
whether modeling the correlations and the temporal relations
between topics was useful. The approaches considering the tem-
poral relations, HMM, TM-RT, and our CaTM, outperform other
approaches which assume actions are temporal independent. This
demonstrates that understanding temporal structure is critical to
recognizing and patching actions. The approaches, TM-RT and
CaTM, which model both the short-range and the long-range
relations perform better than HMM only modeling local relations.
Also, the approaches considering the topic correlations CTM,
CTM-AT, and our CaTM perform better than the corresponding
non-correlated topic models TM, TM-AT, and TM-RT. Our CaTM,
which considers both the action correlation priors and the temporal
relations, shows the best performance.

How successful was our unsupervised approach in learning
meaningful action-topics? From Table 2, we can see that the
unsupervised learned action-topics is promising to be semantically

10

Fig. 10: An example of the robotic experiment. The robot detects the human left the food in the microwave, then points to the microwave.

TABLE 3: Robotic experiment results. The higher the better.

Succ-Rate(%)

Subj-AccScore(1-5)

Subj-HelpScore(1-5)

HMM-O
TM-O

CaTM-AO

37.5
29.2
62.5

2.1
1.8
3.5

2.3
2.0
3.9

meaningful even though ground-truth semantic labels are not
provided in the training. In order to qualitatively estimate the
performance, we give a visualization of our learned topics in
Fig. 11. It shows that the actions with the same semantic meaning
are clustered together though they are in different views and
motions. In addition to the one-to-one correspondence between
topics and semantic action classes, we also plot the performance
curves varied with the topic number in Fig. 8. It shows that if we
set the topics a bit more than ground-truth classes, the performance
increases since a certain action might be divided into multiple
action-topics. But as topics increase, more variations are also
introduced so that performance saturates.

RGB videos vs. RGB-D videos. In order to compare the
effect of using information from RGB-D videos, we also evaluate
our model CaTM and HMM using the popular RGB features for
action recognition (CaTM-A-DTF, CaTM-AO-DTF and HMM-
DTF in Table 2). Clearly,
the proposed human skeleton and
RGB-D features outperform the DTF features as more accurate
human motion and object are extracted.

How well did our new application of action patching
performs? From Table 2, we ﬁnd that the approaches learning the
action relations mostly give better patching performance. This is
because the learned co-occurrence and temporal structure strongly
help indicate which actions are forgotten. Our model capturing
both the short-range and long-range action relations shows the
best results.

How important is it to consider relations between actions
and objects? From the results, we can see that
the model
which did well in forgotten action detection also performed well
in detecting forgotten object. Since our model CaTM-AO well
considers the relations between the action and the object, it shows
better performance in both forgotten action and forgotten object
detection than those which models action and object independently
as well as CaTM-A which only models the actions.

7.5 Robotic Experiments
In this section, we show how our Watch-Bot reminds people of
the forgotten actions in the real-world scenarios. We test each
two forgotten scenarios in ‘ofﬁce’ and ‘kitchen’ respectively (put-
back-book,
turn-off-monitor, put-milk-back-to-fridge and fetch-
food-from-microwave). We use a subset of the dataset to train the
model in each activity type separately. In each scenario, we ask 3
subjects to perform the activity twice. Therefore, we test 24 trials
in total. We evaluate three aspects. One is objective, the success

Fig. 11: Visualization of the learned topics using our model. For
better illustration, we decompose the segments with the same topic
into different modes (shown two) and divide a segment into three
stages in time. The clips from different segments in the same stage
are merged by scaling to the similar size of human skeletons.

rate (Succ-Rate): the laser spot lying within the object as correct.
The other two are subjective, the average Subjective Accuracy
Score (Subj-AccScore): we ask the participant if he thinks the
pointed object is correct; and the average Subjective Helpfulness
Score (Subj-HelpScore): we ask the participant if the output of the
robot is helpful. Both of them are in 1 − 5 scale, the higher the
better.

Table 3 gives the results of our robotic experiments. We can
see that our robot can achieve over 60% success rate and gives the
best performance. In most cases people think our robot is able to
help them understand what is forgotten. Fig. 10 gives an example
of our experiment, in which our robot observed what a human is
currently doing, realized he forgot to fetch food from microwave
and then correctly pointed out the microwave in the scene.

8 CONCLUSION AND FUTURE WORK
In this paper, we presented an algorithm that models the human
activities in a completely unsupervised setting. We showed that
to modeling the long-range relations between
it

is important

the actions. To achieve this, we considered the video as a se-
quence of human-words/object-words, and an activity as a set
of action-topics/object-topics. Then we modeled the word-topic
distributions, the topic correlations and the topic relative time
distributions. We then showed the effectiveness of our model
in the unsupervised action segmentation and clustering, as well
as the action patching. Moreover, we showed that our proposed
robot system using the action patching algorithm was able to
effectively remind people of forgotten actions in the real-world
robotic experiments. For evaluation, we also contributed a new
challenging RGB-D activity video dataset.

Though we showed the promising results and the interesting
applications of the purely unsupervised models in the paper, we
can see that the performance is not more than 50 percent on the
large-scale variant data, as we have no knowledge of the semantic
information. In the future, we plan to extend the model to the semi-
supervised approaches that can effectively use a small portion
of the annotated data for better learning, and improve on the
performance in the real-world applications.

APPENDIX

We give the detailed derivation of the posterior distribution of
znd (Eq. (2)) in this section. We begin with the joint dis-
tribution p(wh, wo, t, z(1), z(2)|π(1), π(2), β(1), β(12), θ), where
wh, wo, t, z(1), z(2), π(1), π(2) are all variables of the word
nd, the time stamp of a word tnd, the topic-assignment of a
nd,wo
wh
word z(1)
nd ,z(2)
kd in D documents
of K action topics and P object topics.

nd and the topic probability π(1)

kd ,π(2)

p(wh, wo, t, z(1), z(2)|π(1), π(2), β(1), β(12), θ)

= p(wh|z(1), β(1))p(wo|z(1), z(2), β(12))·

p(t|z(1), θ)p(z(1)|π(1))p(z(2)|π(2))

p(wh|z(1), φ(1))p(φ(1), β(1))dφ(1)·
p(wo|z(1), z(2), φ(12))p(φ(12), β(12))dφ(12)·

(cid:90)
(cid:90)

=

p(t|z(1), θ) · p(z(1)|π(1))p(z(2)|π(2)).

where the joint distribution is decided by the following ﬁve terms.

topic-word distributions:

p(wh|z(1), φ(1))p(φ(1), β(1))dφ(1)

Nd(cid:89)

n=1

1

φ(1)
z(1)
nd ,wh
nd

(cid:90) (cid:89)

K(cid:89)

k=1

(cid:89)

1

B(β(1))

w
w −1

φ(1)Nkw+β(1)

kw

dφ(1)
k

w −1

φ(1)β(1)

kw

dφ(1)
k

B(β(1))

w
B(Nk + β(1))

B(β(1))

B(Nkp + β(12))

B(β(12))

,

p(wo|z(1), z(2), φ(12))p(φ(12), β(12))dφ(12)

topic-pair relative time distribution:

p(t|z(1), θ) =

Nd(cid:89)
D(cid:89)
p(tnd|z(1)
Nd(cid:89)
Nd(cid:89)
D(cid:89)

n=1

d=1

:d , θ)

d=1

m=1

n=1

md,z(1)

nd

11

).

=

topic priors:

p(tmnd|θz(1)
Nd(cid:89)
D(cid:89)
Nd(cid:89)
D(cid:89)

π(1)
z(1)
nd ,d

n=1

d=1

π(2)
z(2)
nd ,d

.

n=1

p(z(1)|π(1)) =

p(z(2)|π(2)) =

d=1
Then for a certain assignment z(1)

nd , we give the posterior using

the above joint distribution:
:d , z(1)−nd, z(2)

nd |π(1)

nd , tnd)

p(z(1)
p(wh, wo, t, z(1), z(2)|π(1), π(2), β(1), β(12), θ)
p(wh, wo, t, z(1)−nd, z(2)|π(1), π(2), β(1), β(12), θ)

p(wh, wo, t, z(1), z(2)|π(1), π(2), β(1), β(12), θ)

=

∝

p(wh−nd, wo−nd, t−nd, z(1)−nd, z(2)|π(1), π(2), β(1), β(12), θ)
=π(1)
z(1)
nd ,d
where:

nd)p(tnd|z(1)

nd)ω(z(1)

nd , z(2)

nd , wh

nd , wo

:d , θ),

ω(z(1)

K(cid:89)

k=1

ω(z(1)

nd , wh

nd) =

ω(z(1)

nd , z(2)

nd , wo

nd) =

p(tnd|z(1)

:d , θ) =

=

+ β(1)

+ Nwhβ(1)

B(Nk + β(1))
B(N−nd
k + β(1))

K(cid:89)

P(cid:89)

=

N−nd
z(1)
nd ,wh
N−nd
z(1)
nd
B(Nkp + β(12))
B(N−nd
kp + β(12))
+ β(12)

nd ,wo

k=1

p=1

N−nd
z(1)
nd ,z(2)
N−nd
z(1)
nd ,z(2)
p(tmnd|θz(1)
Ω(tmnd|θz(1)

nd

=

(cid:89)
(cid:89)

m

m

+ Nwoβ(12)

)

nd ,z(1)

md

)p(tnmd|θz(1)
)Ω(tnmd|θz(1)

).

nd ,z(1)

md

md,z(1)

nd

md,z(1)

nd

Then assign z(1)
posterior Eq. (2):

nd with a speciﬁc topic k, we have the sampling

nd)p(tnd|z(1)

:d , θ),

p(z(1)
∝ π(1)

nd = k|π(1)
kd ω(k, wh

ω(k, wh

nd) =

ω(k, p, wo

nd) =

p(tnd|z(1)

:d , θ) =

:d , z(1)−nd, z(2)
nd , tnd)
nd)ω(k, z(2)
nd , wo
N−nd
kwh + β(1)
N−nd
k + Nwh β(1)
N−nd
kpwo + β(12)
N−nd
kp + Nwoβ(12)
Ω(tmnd|θz(1)

Nd(cid:89)

,

,

m

md,k)Ω(tnmd|θk,z(1)

md

),

d=1

(cid:90)
(cid:90) D(cid:89)
K(cid:89)
K(cid:89)
(cid:90)
K(cid:89)

k=1

k=1

P(cid:89)

k=1

p=1

=

=

=

=

where we denote the Beta function as B(β) =

Similarly we have:

(cid:81)K
Γ((cid:80)K

k=1 Γ(βk)
k=1 βk) .

p(z(2)

nd = p|π(2)

:d , z(2)−nd, z(1)

nd ) ∝ π(2)

pd ω(z(1)

nd , p, wo

nd).

12

(a) turn-on-monitor

(b) turn-off-monitor

(c) walk

(d) play-computer

(e) read

(f) fetch-book

(g) put-back-book

(h) take-item

(i) put-down-item

(j) leave-ofﬁce

(k) fetch-from-fridge

(l) put-back-to-fridge

(m) prepare-food

(n) microwave

(o) fetch-from-microwave

(p) pour

(q) drink

(r) leave-kitchen

(s) move-kettle

(t) ﬁll-kettle

(u) plug-in-kettle

Fig. 12: Examples of every action class in our dataset. The left is RGB frame and the right is depth frame with human skeleton (yellow).

REFERENCES

develop/.

[2] Adults

[1] Kinect v2 sensor.

http://www.microsoft.com/en-us/kinectforwindows/

[3]

three

forget

things

day,
http://www.telegraph.co.uk/news/uknews/5891701/
Adults-forget-three-things-a-day-research-ﬁnds.html,
Daily Telegraph.
J. Aggarwal and M. Ryoo. Human activity analysis: A review. ACM
Comput. Surv., 43(3):16:1–16:43, 2011.

research

2009.

ﬁnds.

The

a

[4] S. M. Assari, A. R. Zamir, and M. Shah. Video classiﬁcation using
semantic concept co-occurrences. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2014.

[5] S. Bhattacharya, M. M. Kalayeh, R. Sukthankar, and M. Shah. Recogni-
tion of complex events: Exploiting temporal dynamics between underly-
ing concepts. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2014.

[6] D. M. Blei and J. D. Lafferty. A correlated topic model of science. The

Annals of Applied Statistics, 1(1):17–35, 2007.

[7] D. M. Blei and J. D. Lafferty. Topic models. Text mining: classiﬁcation,

clustering, and applications, 10:71, 2009.

[8] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J.

Mach. Learn. Res., 3:993–1022, 2003.

[9] P. Bojanowski, R. Lajugie, F. Bach, I. Laptev, J. Ponce, C. Schmid, and
J. Sivic. Weakly supervised action labeling in videos under ordering
constraints. In European Conference on Computer Vision (ECCV), 2014.
[10] G. Chen, M. Giuliani, D. S. Clarke, A. K. Gaschler, and A. Knoll.
Action recognition using ensemble weighted multi-instance learning. In
International Conference on Robotics and Automation (ICRA), 2014.

[11] A. Chrungoo, S. Manimaran, and B. Ravindran. Activity recognition for
natural human robot interaction. In Social Robotics, volume 8755, pages
84–94. 2014.

[12] P. Doll´ar and C. L. Zitnick. Structured forests for fast edge detection. In

International Conference on Computer Vision (ICCV), 2013.

[13] O. Duchenne, I. Laptev, J. Sivic, F. Bach, and J. Ponce. Automatic
In European Conference on

annotation of human actions in video.
Computer Vision (ECCV), 2009.

[14] T. A. Faruquie, P. K. Kalra, and S. Banerjee. Time based activity
In British Machine Vision

inference using latent dirichlet allocation.
Conference (BMVC), 2009.

[15] A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B.

Rubin. Bayesian data analysis. CRC press, 2013.

13
TABLE 4: Action sequence examples in our dataset. The action ‘walk’ is omitted as it can be between any actions. Possible forgotten actions
are in the brackets.

ofﬁce:
read → leave-ofﬁce
fetch-book → read → [put-back-book] → leave-ofﬁce
put-down-item → read → [take-item] → leave-ofﬁce
put-down-item → fetch-book → read → [put-back-book] → [take-item] → leave-ofﬁce
put-down-item → fetch-book → read → [take-item] → [put-back-book] → leave-ofﬁce
fetch-book → put-down-item → read → [put-back-book] → [take-item] → leave-ofﬁce
fetch-book → put-down-item → read → [take-item] → [put-back-book] → leave-ofﬁce
turn-on-monitor → play-computer → [turn-off-monitor] → leave-ofﬁce
put-down-item → turn-on-monitor → play-computer → [turn-off-monitor] → [take-item] → leave-ofﬁce
put-down-item → turn-on-monitor → play-computer → [take-item] → [turn-off-monitor] → leave-ofﬁce
turn-on-monitor → put-down-item → play-computer → [turn-off-monitor] → [take-item] → leave-ofﬁce
turn-on-monitor → put-down-item → play-computer → [take-item] → [turn-off-monitor] → leave-ofﬁce
kitchen:
pour → drink
pour → [drink] → leave-kitchen
fetch-from-fridge → pour → put-back-to-fridge → drink
fetch-from-fridge → pour → [put-back-to-fridge] → [drink] → leave-kitchen
fetch-from-fridge → pour → drink → put-back-to-fridge
fetch-from-fridge → pour → [drink] → [put-back-to-fridge] → leave-kitchen
fetch-from-fridge → prepare-food → [put-back-to-fridge] → microwave → [fetch-from-microwave] → leave-kitchen
fetch-from-fridge → prepare-food → microwave → [put-back-to-fridge] → [fetch-from-microwave] → leave-kitchen
fetch-from-fridge → prepare-food → microwave → [fetch-from-microwave] → [put-back-to-fridge] → leave-kitchen
move-kettle → ﬁll-kettle → move-kettle → [plug-in-kettle]

[16] M. Hoai, Z. zhong Lan, and F. De la Torre.

classiﬁcation of human actions in video.
Computer Vision and Pattern Recognition (CVPR), 2011.

Joint segmentation and
In The IEEE Conference on

[17] J.-F. Hu, W.-S. Zheng, J. Lai, and J. Zhang. Jointly learning heteroge-
neous features for rgb-d activity recognition. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015.

[18] N. Hu, Z. Lou, G. Englebienne, and B. Krse. Learning to recognize
In Proceedings of Robotics:

human activities from soft labeled data.
Science and Systems (RSS), 2014.

[19] A. Jain, A. Gupta, M. Rodriguez, and L. Davis. Representing videos
In The IEEE conference on

using mid-level discriminative patches.
Computer Vision and Pattern Recognition (CVPR), 2013.

[20] S. Jones and L. Shao. Unsupervised spectral dual assignment clustering
In The IEEE Conference on Computer

of human actions in context.
Vision and Pattern Recognition (CVPR), 2014.

[21] V. Kantorov and I. Laptev. Efﬁcient feature extraction, encoding and clas-
siﬁcation for action recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2014.

[22] Y. Ke, R. Sukthankar, and M. Hebert. Event detection in crowded videos.

In European Conference on Computer Vision (ECCV), 2007.

[23] D. I. Kim and E. B. Sudderth. The doubly correlated nonparametric topic
model. In Advances in Neural Information Processing Systems (NIPS),
2011.

[24] A. Kl¨aser, M. Marszałek, C. Schmid, and A. Zisserman. Human focused
action localization in video. In International Workshop on Sign, Gesture,
and Activity (SGA) in Conjunction with ECCV, 2010.

[25] H. S. Koppula, R. Gupta, and A. Saxena. Learning human activities and
object affordances from RGB-D videos. I. J. Robotic Res., 32(8):951–
970, 2013.

[26] H. S. Koppula and A. Saxena. Anticipating human activities using
In Robotics: Science

object affordances for reactive robotic response.
and Systems (RSS), 2013.

[27] H. S. Koppula and A. Saxena. Learning spatio-temporal structure
In

from RGB-D videos for human activity detection and anticipation.
International Conference on Machine Learning (ICML), 2013.

[28] H. Kuehne, A. Arslan, and T. Serre. The language of actions: Recovering
the syntax and semantics of goal-directed human activities. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
[29] I. Laptev and P. Perez. Retrieving actions in movies. In International

Conference on Computer Vision (ICCV), 2007.

[30] Y.-Y. Lin, J.-H. Hua, N. C. Tang, M.-H. Chen, and H.-Y. Mark Liao.
Depth and skeleton associated action recognition without online acces-
sible rgb-d cameras. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2014.

[31] J. Liu, B. Kuipers, and S. Savarese. Recognizing human actions by
In The IEEE Conference on Computer Vision and Pattern

attributes.
Recognition (CVPR), 2011.

[32] M. Losch, S. Schmidt-Rohr, S. Knoop, S. Vacek, and R. Dillmann. Fea-
ture set selection and optimal classiﬁer for human activity recognition.
In Robot and Human interactive Communication, 2007.

[33] S. Ma, L. Sigal, and S. Sclaroff. Space-time tree ensemble for action

recognition. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2015.

[34] S. Mathe and C. Sminchisescu. Actions in the Eye: Dynamic Gaze
IEEE

Datasets and Learnt Saliency Models for Visual Recognition.
Trans. Pattern Anal. Mach. Intell. (TPAMI), 2014.

[35] S. Narayan and K. R. Ramakrishnan. A cause and effect analysis of
In The IEEE Conference on

motion trajectories for modeling actions.
Computer Vision and Pattern Recognition (CVPR), 2014.

[36] H. Nguyen, A. Jain, C. D. Anderson, and C. C. Kemp. A clickable world:
Behavior selection through pointing and context for mobile manipulation.
In International Conference on Intelligent Robots and Systems, 2008.

[37] B. Ni, V. R. Paramathayalan, and P. Moulin. Multiple granularity analysis
for ﬁne-grained action detection. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2014.

[38] J. C. Niebles, C.-W. Chen, and L. Fei-Fei. Modeling temporal structure
of decomposable motion segments for activity classiﬁcation. In European
Conference on Computer Vision (ECCV), 2010.

[39] H. Pirsiavash and D. Ramanan. Parsing videos of actions with segmental
In The IEEE Conference on Computer Vision and Pattern

grammars.
Recognition (CVPR), 2014.

[40] L. Piyathilaka and S. Kodagoda. Human activity recognition for domestic
robots. In Field and Service Robotics, volume 105, pages 395–408, 2015.
[41] X. Ren, L. Bo, and D. Fox. Rgb-(d) scene labeling: Features and
In The IEEE Conference on Computer Vision and Pattern

algorithms.
Recognition (CVPR), 2012.

[42] S. Sadanand and J. J. Corso. Action bank: A high-level representation
of activity in video. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2012.

[43] B. Schiele. A database for ﬁne grained activity detection of cooking
In The IEEE Conference on Computer Vision and Pattern

activities.
Recognition (CVPR), 2012.

[44] J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica

Sinica, 4:639–650, 1994.

[45] Q. Shi, L. Cheng, L. Wang, and A. Smola. Human action segmentation
and recognition using discriminative semi-markov models. International
Journal of Computer Vision (IJCV), 93(1):22–32, 2011.

[46] F. Souza, S. Sarkar, A. Srivastava, and J. Su. Temporally coherent
In The IEEE
interpretations for long videos using pattern theory.
Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
[47] C. Stauffer and W. Grimson. Adaptive background mixture models for
In The IEEE Conference on Computer Vision and

real-time tracking.
Pattern Recognition (CVPR), 1999.

[48] J. Sung, C. Ponce, B. Selman, and A. Saxena. Unstructured human
In International Conference on

activity detection from rgbd images.
Robotics and Automation (ICRA), 2012.

[49] K. Tang, L. Fei-Fei, and D. Koller. Learning latent temporal structure for
complex event detection. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2012.

[50] Y. Tian, R. Sukthankar, and M. Shah. Spatiotemporal deformable part
models for action detection. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2013.

[51] R. Vemulapalli, F. Arrate, and R. Chellappa. Human action recognition

14

In The IEEE
by representing 3d skeletons as points in a lie group.
Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
[52] N. N. Vo and A. F. Bobick. From stochastic grammar to bayes network:
In The IEEE Conference on

Probabilistic parsing of complex activity.
Computer Vision and Pattern Recognition (CVPR), 2014.

[53] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Action Recognition by
Dense Trajectories. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2011.

[54] X. Wang and Q. Ji. A hierarchical context model for event recognition
in surveillance video. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2014.

[55] X. Wang and A. McCallum. Topics over time: A non-markov continuous-
time model of topical trends. In ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD), 2006.

[56] C. Wu, I. Lenz, and A. Saxena. Hierarchical semantic labeling for task-
relevant rgb-d perception. In Robotics: Science and Systems (RSS), 2014.
[57] C. Wu, J. Zhang, S. Savarese, and A. Saxena. Watch-n-patch: Unsuper-
vised understanding of actions and relations. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015.

[58] C. Wu, J. Zhang, B. Selman, S. Savarese, and A. Saxena. Watch-bot:
In

Unsupervised learning for reminding humans of forgotten actions.
International Conference on Robotics and Automation (ICRA), 2016.

[59] D. Wu and L. Shao. Leveraging hierarchical parametric networks for
skeletal joints based action segmentation and recognition. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
[60] S. Yang, C. Yuan, B. Wu, W. Hu, and F. Wang. Multi-feature max-
margin hierarchical bayesian model for action recognition. In The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
learning
manipulation action plans by watching unconstrained videos from the
world wide web. In AAAI, 2015.

[61] Y. Yang, Y. Li, C. Fermuller, and Y. Aloimonos. Robot

[62] Y. Yang, I. Saleemi, and M. Shah. Discovering motion primitives for
unsupervised grouping and one-shot learning of human actions, gestures,
IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI),
and expressions.
35(7):1635–1648, 2013.

