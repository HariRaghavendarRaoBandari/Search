Capacity of Systems with Queue-Length Dependent

Service Quality

Avhishek Chatterjee, Daewon Seo, and Lav R. Varshney

1

6
1
0
2

 
r
a

M
5

 

 
 
]
T
I
.
s
c
[
 
 

1
v
5
7
6
1
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

We study the information-theoretic limit of reliable information processing by a server with queue-length
dependent quality of service. We deﬁne the capacity for such a system as the number of bits reliably processed per
unit time, and characterize it in terms of queuing system parameters. We also characterize the distributions of the
arrival and service processes that maximize and minimize the capacity of such systems in a discrete-time setting. For
arrival processes with at most one arrival per time slot, we observed a minimum around the memoryless distribution.
We also studied the case of multiple arrivals per time slot, and observed that burstiness in arrival has adverse effects
on the system. The problem is theoretically motivated by an effort to incorporate the notion of reliability in queueing
systems, and is applicable in the contexts of crowdsourcing, multimedia communication, and stream computing.

channel capacity, quality of service, queuing

Index Terms

I. INTRODUCTION

Consider the following abstraction of a centrally-controlled system of jobs and servers: a job requester places a
request to a central controller for a server to process a large group of jobs. The central controller, considering factors
such as availability of servers and commitment to other customers, chooses an appropriate server. Then it gradually
dispatches jobs to that server. The server processes the jobs as they arrive, following a ﬁrst-in ﬁrst-out queuing
discipline. This models many systems with jobs and servers, such as in crowdsourcing, multimedia communication,
and stream computing.

In addition to the queuing discipline, we consider the server to be imperfect. The quality of service received
by a job depends on the number of jobs waiting in the queue at that time: longer queues lead to more noise in
information processing. The overall performance of the server on the group of jobs, therefore, depends on the state
of the queue as it evolves over the entire service duration. Hence, in turn, it depends on the service requirements of
the jobs, their arrivals to the server or the dispatch time from the controller, and the relation between service quality
and queue state. If the job requester knows the dispatch and service distributions, it can increase overall reliability
by designing the group of jobs, either with added redundancy or combining them appropriately. In this work, we
formally characterize the maximum rate at which jobs can be reliably processed in such a requester-dispatcher-
server system and study various queuing systems and their parameters, e.g., arrival and service distributions, to
achieve optimal rates.

Fig. 1 presents a schematic of the system under study. As shown, there is an equivalence between our system
and a communication channel. A large job is equivalent to a message in the communication setting. This large job
is broken into a group of jobs, which is equivalent to a codeword of symbols. The random errors made by the
server are equivalent to channel noise corrupting codeword symbols. Finally, the processed jobs (likely erroneous)
are combined to complete the large job reliably, equivalent to decoding the original message from a received
noisy codeword. Note that in contrast to many treatments of queuing systems, we are concerned with forward
error correction rather than feedback-based repeat requests. We study the limiting rate at which large jobs can be
processed with arbitrarily small error probability, and try to understand the best level at which to load the servers.
This work lies at the intersection of information theory and queuing theory [1], bringing together notions of
burstiness and unreliability. Indeed, incorporating the information-theoretic notion of reliable job processing in a

The authors are with the Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA (e-mail:
{avhishek, dseo9, varshney}@illinois.edu). This work was supported in part by the National Science Foundation under grant CCF-1623821.

2

Fig. 1. Schematic of the system.

queueing system is of general theoretical interest. This problem also arises in many practical scenarios of growing
prominence.

• Crowdsourcing and human computation: Unlike machines or computers, the quality of service delivered by
a human worker depends on his/her workload. Overloading a person with work often negatively impacts their
quality of work [2], as in our model. Studying such scenarios is vital to understand optimal allocation of jobs
in crowdsourcing. In this context, the encoder in Fig. 1 is the organization (e.g., Visipedia [3]) that submits
jobs to a crowdsourcing platform (e.g., Samasource [4]). The platform is the dispatcher and the crowd worker
to whom jobs are assigned is the server. Note that the transmitter and receiver are the same, and the code
symbols represent completed work. Error-correcting codes can be developed for difﬁcult human computations,
as described in [5].

• Multimedia communication: When a user is in a live video or VoIP call over a multiple-access network, the
access point—e.g., WiFi router or base station—has to contend for wireless resources to send the information
packets. This results in an accumulation of packets at the MAC buffer of the access point. When the buffer
is close to overﬂow, the access point either drops them [6], sends their corresponding low-quality versions
[7] (assuming multiresolution coding [8]), or packs multiple MAC packets in the available time slot using
higher coding/modulation. All of these scenarios can be modeled by queue-dependent service quality. We are
interested in the maximum rate for reliable data transmission in this system. Here the application provider (e.g.
Google Voice or Skype) is the encoder, the core network of the internet is the dispatcher, and the wireless
access node is the server.

• Stream computing: A queue-length based service quality model is also suitable for large-scale online learning
with limited memory. In these settings often only a sketch of the data can be stored. As more data arrives,
the quality of the sketches has to be reduced due to the memory constraint. This leads to a tradeoff between
service quality and load on the system. With emerging in-sensor computing devices, some basic error-control
coding operations may be implemented before the data network dispatches information to the ﬁnal server that
performs appropriate computations.

A. Related Literature

Anantharam and Verd´u introduced the notion of timing channels for servers or queues [9]. Information is encoded
in the times between consecutive information packets, and these packets are subsequently processed by a server
according to some queueing discipline. Due to randomness in the sojourn times of packets through servers, the
encoded timing information is distorted, which the receiver must decode. Later, a discrete-time version of the
problem was studied [10]; further insight into this problem was obtained by studying the entropy of arrival and
departure processes of a queue [11]. Timing channels in queues have further been investigated for suitable decoding
schemes [12], zero-rate reliability [13], connections to game-theoretic settings [14], information leakage [15], [16],
and models of information overload in microblogging [17]. Although we use some related proof techniques as these
works, we are not concerned with information encoded in the timing between packets, only in the information in
the symbols.

The study of information-theoretic limits of queuing multiple-access channels was pioneered by Telatar [18], and
further explored in [19], [20]. This line of work is essentially concerned with the reliable transmission of bursty

CHANNEL'''''''JOB''DESIGNING'''''''QUEUE''''''JOB'COMBINING'ENCODER'DECODER''''''''JOB''DISPATCHING''t!t!t!SubmittedJobSequenceDispatchedJobSequenceProcessedJob(Corrupted)Sequence3

sources [21], as we are here. In multiple-access settings, however, the main constraint beyond noise is interference
among users. The present work has a single user, but performance does degrade with greater burstiness, a form of
self-interference as it were. A recent study of microbial communication also had a kind of self-interference called
channel clogging [22].

Queue-length dependent service times have been studied in operations research, see e.g. [23] and references
thereto, but queue-length dependent service quality that we investigate here has remained unstudied. In fact
traditional queuing theory does not deal with issues of noise and quality, giving the present work theoretical
novelty that may be applicable broadly in engineering theory.

B. Organization

The remainder of the paper is organized as follows. Sec. II formalizes the system model, for two kinds of arrival
processes we call Type I and Type II. Sec. III deﬁnes the operational notion of system capacity and proves coding
theorems that give an equivalent informational deﬁnition. Sec. IV studies Type I capacities of well-known queueing
systems and their relations with arrival and service processes, whereas Sec. V studies Type II capacities. Before
the paper concludes, Sec. VI considers the extension where timestamps are not available.

II. SYSTEM MODEL

A transmitter and a receiver a priori agree on a set of possible sequences of symbols (or codebook). The
transmitter sends a sequence of symbols corresponding to a message to the dispatcher. The dispatcher sends these
symbols to a server according to some stochastic process. The server services these symbols which are then received
by the receiver. The receiver then tries to decode the message based on the received symbols.

The server works like a single ﬁrst in ﬁrst out (FIFO) queue with i.i.d. service requirements for each job. Jobs
correspond to symbols from a ﬁnite ﬁeld F. In the model, servicing a job involves reading the symbol and outputting
it. The server may make random errors during these steps and send out erroneous symbols.

We are interested in the information capacity of such a system which we refer to as a queue-channel.

A. Queuing Discipline

i.e. with values in Z+. The service time of the ith job is denoted Si and has a distribution pS.

We consider a discrete-time system, t ∈ {0, 1, 2, . . .}. Service requirement for jobs are i.i.d. and strictly positive,
We use the following convention. Arrivals at time t, if any, happen at the beginning of time slot t. Departures
from the queue at time slot t, if any, happen at the end of the time slot. This implies that a job arriving at time
slot t may receive and possibly ﬁnish its service at time t.

Let Q(t) be the number of jobs in the queue at the end of time slot t and Qi be the number of jobs in the system

when the ith job departs. As Si ≥ 1 for all i, at a time slot t, at most one job can depart.
We consider two basic types of arrivals processes (also called dispatch processes) into the queue: Type I and
Type II. In a Type I process, there is at most one arrival in any time slot and the times between two consecutive
arrivals are i.i.d. with distribution pA on {1, 2, . . .}. In a Type II process, the numbers of arrivals A(t) in time slot
t ≥ 1 are i.i.d. with distribution mA on {0, 1, 2, . . .}. The service rate and arrival rate are µ and λ, respectively,
satisfying EpS [S] = 1/µ and EpA[A] = 1/λ or EmA[A] = λ, respectively. We assume pS, pA, and mA have ﬁnite
second moments. For Type I systems, we assume either pA or pS has a support that spans Z+. For Type II systems,
we assume mA(1) > 0.

B. Service Noise

Transmission of symbols from a ﬁnite ﬁeld F over the queue-channel happens in two stages. Mapping the
message, the transmitter sends symbols {Xi ∈ F : 1 ≤ i ≤ n} to a dispatcher, which in turn sends the symbols to
the server according to a stochastic process of arrival rate λ. For stability of the queue, we assume λ < µ.
The symbol corresponding to the ith symbol is Xi ∈ F, and the output symbol corresponding to the ith symbol is
Yi ∈ F. They are related through the additive noise variable Zi ∈ F representing work error, such that Yi = Xi + Zi.
The distribution of the errors {Zi} depends on {Qi}. For any i, given Qi, Zi is independent of any other processes
or variables, and has a distribution ψq (on F) for Qi = q.

4

An n-length transmission over the queue-channel is denoted as follows. Inputs are {Xi : 1 ≤ i ≤ n}, channel
realizations are {Zi : 1 ≤ i ≤ n}, and outputs are {Yi : 1 ≤ i ≤ n}. Throughout, a k-dimensional random vector
is denoted by U k = (U1, U2, . . . , Uk).

All logarithms in the paper have base 2 so that information is measured in bits.

We are interested in the information capacity of unreliable server systems, i.e. the queue-channel described above.

In this section, we present results that are generic, i.e. are true for both Type I and II arrivals.

III. CAPACITY OF QUEUE-CHANNEL

A. Deﬁnition

Deﬁnition 1: An (n,(cid:101)R, T ) code consists of the encoding function X n = f (M ) and the decoding function
Let M, ˆM ∈ M be the message to be transmitted and decoded, respectively.
ˆM = g(X n, An, Dn), where the cardinality of the message set |M| = 2n(cid:101)R, and for each codeword, the expected
-achievable. For any 0 <  < 1, if there exists an -achievable code (n,(cid:101)R, T ), the rate R = (cid:101)R

Deﬁnition 2: If the decoder chooses ˆM with average probability of error less than , that code is said to be
T is said to be

total time for all symbols to reach the receiver is less than T .

achievable.

Deﬁnition 3: For an arrival process with distribution pA (Type I) or mA (Type II), the information capacity of
the queue-channel is deﬁned as the supremum over all achievable rates, which is denoted by C(pA) or C(mA) in
bits per unit time.
Since the transmitter sends symbols to the dispatcher ﬁrst, we assume the transmitter knows the arrival process
statistics, but not the realizations. Contrarily, the receiver knows the realized arrival and departure times of each
job.

B. Coding Theorem

Let Ai ∈ {1, . . .} and Di ∈ {1, . . .} be the time of arrival into the queue and the time of departure from the
queue of the ith symbol. The transmitter does not observe {Ai, Di}, whereas the receiver observes these. Thus
the queue-channel has inputs {Xi} and outputs {Yi, Ai, Di}. As dispatch is independent of job-design, the channel
transition probability factors as

P(Y n, An, Dn|X n) = P(An, Dn)P(Y n|X n, An, Dn).

The transmitter chooses {Xi} and hence, can choose any joint distribution for the codebook described by {Xi}.
Note {Yi, Ai, Di} depends on {Xi}, as well as on the arrival and service processes. In general, {Yi, Ai, Di} may
not be a stationary process. This means that the queue-channel is not necessarily an information-stable channel
[24], but the capacity formula can nevertheless be found using the information spectrum approach [25], [26]. Let
the information density be i(·), the normalized information density be

1
n

i(X n; Y n, An, Dn) =

log

1
n

P(Y n, An, Dn|X n)
P(Y n, An, Dn)

,

and the inf-information rate I(X; Y, A, D) be the lim-inf in probability of the normalized information density, i.e.
the largest α ∈ R ∪ {±∞} such that for all  > 0,

(cid:21)
i(X n; Y n, An, Dn) ≤ α − 
Then, capacity in bits per unit time of the queue-channel is given by

(cid:20) 1

n→∞ P
lim

= 0.

n

C(pA) (and C(mA)) = λ sup
P(X)

I(X; Y, A, D),

(1)

where λ is the arrival rate deﬁned in Sec. II, and the supremum is over all input processes X = (X1, X2, . . .).

This capacity expression is not easy to handle due to the various possibilities of (An, Dn) that can arise, however,
the next proposition allows us to characterize the distribution of i(·) (and hence, I) in a simpler form, in terms of
the distributions of X n, Y n, and Qn.

Proposition 1: The capacity expression (1) can be represented by using Qn,

C(pA) (and C(mA)) = λ sup
P(X)

I(X; Y|Q).

Proof: It sufﬁces to show that

i(X n; Y n, An, Dn) = i(X n; Y n|Qn).

5

(2)

Note that the additive noise Zn depends only on Qn = φn(An, Dn), where φn(·) is a function that computes
the number of symbols in the queue. Hence, P(Y n|An, Dn, X n) = P(Y n|Qn, X n). Also, X n is independent of
(An, Dn).

P(An, Dn|X n)P(Y n|An, Dn, X n)

P(An, Dn)P(Y n|An, Dn)
P(Y n|Qn, X n)
P(Y n|An, Dn)

=

=

P(Y n, An, Dn|X n)
=
P(Y n, An, Dn)
P(Y n|An, Dn, X n)
(cid:80)
P(Y n|An, Dn)
P(Y n|Qn, X n)
(cid:80)
X n P(Y n, X n|An, Dn)
P(Y n|Qn, X n)
X n P(X n|Qn)P(Y n|Qn, X n)

(cid:80)

=

=

=

P(Y n|Qn, X n)

X n P(X n|An, Dn)P(Y n|An, Dn, X n)
=

.

P(Y n|Qn, X n)
P(Y n|Qn)

Taking logarithm and normalizing yields i(X n; Y n, An, Dn) = i(X n; Y n|Qn).
Thus, it follows that the distribution of i(·) depends only on the joint distribution of (X n, Y n, Qn).
Based on this, we can give a single-letter characterization of the capacity of the queue-channel. In the proof
of the forthcoming coding theorem, the converse part is essentially due to Fano’s inequality and basic properties
of information quantities [27]. The direct part follows by choosing an appropriate input processes X to lower
bound supP(X) I(X; Y|Q). In this regard, this proof is structurally similar to earlier work that applied information
spectrum techniques, e.g. [10], [28].
The proof of the coding theorem also implicitly depends on the following lemma which characterizes the process

{Qi}.
(cid:80)n
Lemma 1: Under the assumptions in Sec. II and λ < µ < 1, there exists a unique distribution π such that if
Q1 ∼ π, then Qi ∼ π for all i ≥ 1, and the process {Qi} is ergodic, i.e. for any f : {0, 1, . . .} → R with ﬁnite
i=1 f (Qi) → Eπf as n → ∞. Moreover, for any initial distribution of Q1, Qi converges
Eπf, almost surely 1
n
to π in distribution and π(q) > 0 for all q ∈ {0, 1, . . .}.

Proof: See Appendix A.
Now the capacity theorem.
Theorem 1: For a given arrival process distribution pA (or mA) with λ < µ < 1 which follows the assumption
in Sec. II, there exists a distribution π such that π(q) > 0 for all q ∈ {0, 1, . . .} and P(Qn) → π as n → ∞. The
capacity of this queue-channel is λ(log |F| −
q π(q)H(ψq)), where H(ψq) is the entropy of a distribution ψq(Z)
on any ﬁnite set of size |F|.
Proof: First, we prove the converse. Let ˜R be the rate in bits per symbol. Let M and ˆM be transmitted and
decoded messages, respectively, such that ˆM = g(Y n, An, Dn), where g(·) is a decoding function. Note that a
Markov chain M − X n − (Y n, An, Dn) − ˆM holds.

(cid:80)

6

n ˜R = H(M ) = I(M ; ˆM ) + H(M| ˆM )

≤ I(M ; ˆM ) + n
≤ I(X n; Y n, An, Dn) + n
= I(X n; Y n|Qn) + n
n(cid:88)
n(cid:88)
= H(Y n|Qn) − H(Y n|X n, Qn) + n
n(cid:88)
=

H(Yi|Qn, Y i−1) −
n(cid:88)

i=1

i=1

≤

i=1

H(Yi|Qi) −

i=1

H(Yi|Xi, Qi) + n.

H(Yi|X n, Qn, Y i−1) + n

(3)

(4)

The ﬁrst two inequalities are Fano’s inequality and the data processing inequality. I(X n; Qn) = 0 since X n is
chosen independently of the dispatch and departure processes (An, Dn). Eq. (3) follows since I(X n; Y n, An, Dn) =
E[ni(X n; Y n, An, Dn)] and (2). The last inequality follows since removing conditioning only increases entropy
and Yi depends only on Xi, Qi.

Note that

with equality attained by assuming Xi is uniformly distributed over F. Let pi be the distribution of the queue-length
seen by the ith departure. Then, (4) can be further upper bounded as:

n ˜R ≤

i=1

n(cid:88)

H(Yi|Qi) −

(cid:32)(cid:88)

H(Yi|q) − H(Yi|Xi, q) ≤ log |F| − H(ψq),
n(cid:88)
n(cid:88)
(cid:32)
log |F| −
≤ n
= n log |F| − n

pi(q)(H(Yi|q) − H(Yi|Xi, q))

n(cid:88)
(cid:88)

H(Yi|Xi, Qi) + n

πqH(ψq) + δn + n,

(cid:88)

pi(q)H(ψq)

(cid:33)

(cid:33)

1
n

i=1

q

i=1

i=1

q

=

+ n

q

+ n

where δn → 0 as n → ∞. The last step follows because the queue-length process is ergodic (Lemma 1), and hence,
the Cesaro mean of expectations at different times converges to the expectation with respect to the stationary
distribution. Finally multiplying by λ concludes the converse.
To show achievability, we pick X n i.i.d. uniformly at random from F and show that the inf-information rate in
Proposition 1 for this process is log |F| −
Thus,

Note that as F is a ﬁeld, for any element yi ∈ F, yi−Xi spans all elements in F. Hence,(cid:80)

X∈F ψQi(Yi−Xi) = 1.

q πqH(ψq).

P(Yi, Xi|Qi) =

P(Xi|Qi)P(Yi|Xi, Qi) =

ψQi(Yi − Xi) =

(cid:81)
Since the {Xi} are i.i.d. and given Qi, Yi depends only on Xi, we get the following product form: P(yn|xn, qn) =

.

1

|F|

(cid:80)
(cid:88)

Xi∈F

(cid:88)

Xi∈F

1

|F|

(cid:88)
i P(yi|xi, qi), P(yn|qn) =(cid:81)

P(Yi|Qi) =

Xi∈F

1
n

i(X n; Y n|Qn) =

(cid:81)
(cid:81)

log

i P(yi|qi). This implies:
P(Y n|X n, Qn)
1
=
P(Y n|Qn)
n
1
n

n(cid:88)

= log |F| +

i=1

log ψQi(Zi).

1
n

log

(cid:81)
(cid:81)

i P(Yi|Xi, Qi)
i P(Yi|Qi)

=

1
n

log

i ψQi(Zi)
i 1/|F|

7

Fig. 2. Capacity of geo/geo/1 queue is plotted against arrival rate (for different service rates) for F = {0, 1} and noise distribution
P(Z = 1) = 0.1 for q = 0, otherwise P(Z = 1) = 0.4.

Now we use Lemma 1 to take a limit.

1
n

i(X n; Y n|Qn) → log |F| + EπQ,Z[log ψQ(Z)] almost surely as n → ∞

(cid:88)

q

= log |F| −

πqH(ψq).

With the coding theorems developed in this section in hand, Secs. IV and V study the capacity of a few interesting
classes of discrete-time queues. This results in insights regarding the dispatch and service processes that result in
best and worst information processing rates.

C. Comments

Before studying speciﬁc classes of queuing systems, we comment on the relation between the maximum packet
throughput and the maximum information throughput (the notion of capacity deﬁned here) of a queuing system.
Packet throughput of a queuing system is the maximum rate of packet arrivals that can be served without instability;
hence the packet throughput increases with λ on [0, µ). Though the expression for capacity (information throughput)
has λ as a multiplicative factor, this does not mean that information throughput increases with λ. In typical queuing
systems, the survival function corresponding to the stationary probability is increasing in λ. Thus, an increase in
λ also has a negative impact on the terms involving π. Hence, in typical queuing systems, there is an optimal
λ ∈ (0, µ) that maximizes information throughput. Fig. 2 shows an example.
IV. QUEUES WITH TYPE I ARRIVAL

This section is devoted to understanding the capacity of a queue with a Type I arrival process and its dependence
on the distribution of service times and inter-arrival times. First, we ﬁnd the capacity of a queue with geometric
service time and arbitrary arrival process, and characterize the capacity-optimizing arrival distributions. Then, we
study the capacity of a queue with geometric inter-arrival time and ﬁnd the capacity-optimizing service time
distribution. Capacity has a saddle point behavior around the geometric distribution.

In the application scenarios discussed in Sec. I, server performance deteriorates with increasing queue-length.
Deterioration of server performance with increasing queue-length is captured by a {ψq} whose entropy is non-
decreasing with q. A {ψq} of practical interest is a threshold behavior of the error-entropy with increasing queue-
length: H(ψq) = h0 for q ≤ b and H(ψq) = hb+1 for q ≥ b + 1, for some b ∈ {0, 1, . . .}.

00.20.40.60.800.050.10.150.2Arrival rate(λ)Capacity [bits/s]  µ=0.3µ=0.5µ=0.78

Threshold behavior captures a state of server panic based on workload, suitable for human servers and wireless
access points with small MAC buffer. The special case of b = 0 describes a human server that is distracted by any
waiting job or a bufferless MAC. The special case of b = 1 corresponds to a human server being distracted if more
than one job is waiting.

A. Discrete-time G/geo/1 queue

For a G/geo/1 queue, the service time distribution is geometric with an expected service time 1

µ, µ < 1. The
arrival process is Type I, with the inter-arrival times distributed as pA and the expected time between arrivals 1
λ,
λ < µ. Since this queueing system satisﬁes the assumptions in Sec. II, its capacity can be obtained from Theorem
1. For any arrival distribution, the capacity of G/geo/1 queue is given by the following theorem.

Theorem 2: The capacity of the G/geo/1 queue-channel is λ(log |F| − (1 − σ)(cid:80)
unique solution of the equation x =(cid:80)∞

q σqH(ψq)), where σ is the

n=0 pA(n)(1 − µ + xµ)n in (0, 1).

Proof: See Appendix B.

Proof of this theorem involves obtaining the steady-state distribution π of the queue-lengths seen by the departures.
Towards this, techniques similar to that in the analysis of continuous-time GI/M/1 queues [29] are extended to
the discrete-time setting. The closed-form expressions here differ to some extent from that in GI/M/1. Also, note
that some of the intermediate steps in the proof of Theorem 2 are used to prove some later results.

Based on the capacity characterization of the G/geo/1 queue, we explore the space of arrival distributions. This
leads to the following result about the best and worst (in terms of capacity) arrival distribution for a G/geo/1
queue.

λ ∈ Z+.

Proposition 2: For G/geo/1 queue with thresholded noise such that H(ψ0) = ··· = H(ψb) < H(ψb+1) = ···
Proof: Proof of this result builds on the property of the ﬁxed point equation x =(cid:80)∞
for some b ∈ {0, 1, . . .}, deterministic inter-arrival time maximizes capacity among all arrival distributions with the
same λ, for 1
(cid:80)
t=0(1 − µ + µx)tpA(t),
maximized when (1−σ)(cid:80)
and uses an intermediate result in the proof of Theorem 2.
First see that for any arrival distribution pA, π(q) = (1− σ)σq, and capacity is log |F|−
q π(q)H(ψq), which is
q σqH(ψq) is minimized. Since noise is thresholded at b, i.e. h0 = H(ψ0) = ··· = H(ψb)
(cid:88)
and hb+1 = H(ψb+1) = ··· , then the latter term may be written as

σqH(ψq) = h0(1 − σb+1) + hb+1(1 − (1 − σb+1)) = h0 + (hb+1 − h0)σb+1.

(1 − σ)

q

Next, note that the curves ˜A(σ) =(cid:80)∞

Hence, for a given {ψq}, capacity is maximized when σ is minimized.
t=1 pA(t)(1 − µ + µσ)t are convex and increasing with σ, and ˜A(0) > 0
(see Lemma 3 in Appendix). Also, there is a unique ﬁxed point in (0, 1). Thus, for these classes of curves, the
curve that lower bounds a set of curves crosses the line y = σ at the smallest value of σ among that set of curves.
Similarly, the curve that upper bounds a set of curves crosses the line y = σ at the largest value of σ.

λ,
For any 0 < α < 1 and any distribution pA with mean 1

∞(cid:88)

t=0

1

λ ,

αtpA(t) ≥ α
∞(cid:88)
(1 − µ + µσ)tpA(t)

t=0

˜A(pA, σ) =

by Jensen’s inequality, as αt is convex. Thus for any σ ∈ (0, 1) and pA with mean 1
λ,

≥ (1 − µ + µσ)
= ˜A(det, σ),

1
λ

where the equality can be attained by a deterministic inter-arrival time. This implies that the curve ˜A(det, σ) is a
lower-bounding curve for all other curves corresponding to different pA.

Proposition 3: For the G/geo/1 queue with {ψq} such that H(ψ0) = ··· = H(ψb) < H(ψb+1) = ··· for some

b ∈ {0, 1, . . .}, ˜pA(t, ) asymptotically minimizes the capacity among all arrival processes as  → 0, where

(cid:40)

˜pA(t, ) =

1 − ,
,

t = 1
t = N (),

9

for  > 0 and N () is chosen to satisfy the mean constraint 1/λ.

Proof: It is sufﬁcient to show that ˜A(pA, σ) is asymptotically maximized by ˜pA(t, ) as  → 0.

Consider developing an upper bound of ˜A(pA, σ) ﬁrst. Using the fact that for α ∈ (0, 1), αt is decreasing,

∞(cid:88)
(1 − µ + µσ)tpA(t) ≤

t=0

∞(cid:88)

t=0

˜A(pA, σ) =

(1 − µ + µσ)pA(t) = (1 − µ + µσ).

On the other hand, ˜A(pA, σ) evaluated at ˜pA(t, ) is:

˜A(˜pA(t, ), σ) = (1 − µ + µσ)(1 − ) + (1 − µ + µσ)N ,

which approaches the upper bound as  → 0, but has a ﬁxed-point solution in (0, 1). The pmf ˜pA(t, ) asymptotically
maximizes the ﬁxed-point solution as  → 0, thus minimizing the capacity.
The results of Propositions 2 and 3 agree with our intuition. Deterministic arrivals in Proposition 2 give enough
time to the server with a given service rate, so that each job sees the lowest queue length behind it on average.
On the other hand, a typical realization of ˜pA(t, ) is that jobs arrive every time slot (corresponding to t = 1) for
some time interval but then the next job arrives a very long time later corresponding to t = N (). The server will
be busiest during the ﬁrst interval, but will be almost idle until the next job. It yields the worst performance.

In crowdsourcing, it is common for the arrival process to come from some kind of job pre-processing. Since
this pre-processing system itself could be serial or parallel chains of servers with exponentially-distributed random
delays, we are interested in classes of arrival processes that are certain geometric families of distributions.

Let {Ai, 1 ≤ i ≤ I} be independent geometric random variables with means 1

sum-of-geometric random variable and to be As the set of such probability distributions with mean 1

λi

. Then deﬁne As to be a

λ, i.e.

(cid:88)
(cid:26)

i

Ai

As =

As =

pAs : E[As] =

(cid:26)

Am = Ai with probability ci,
Am =

pAm : E[Am] =

1
λ

.

(cid:27)

.

1
λ

(cid:27)

Also deﬁne Am to be a mixture of geometric random variables such that Am = Ai with probability mass {ci}
whose support is {1 ≤ i ≤ I}, with Am as the set of such probability distributions with mean 1

λ, i.e.

Then the next lemma follows.
Lemma 2: For any pAs ∈ As,

On the other hand, for any pAm ∈ Am.

Proof: See Appendix C.

˜A(pAs, σ) ≤ ˜A(geo, σ).

˜A(pAm, σ) ≥ ˜A(geo, σ).

Proposition 4: For any G/geo/1 queue-channel with thresholded noise at b ∈ {0, 1, . . .}, geometric inter-arrival
times minimize and maximize capacity among all arrival distributions of As and Am, respectively.
Proof: Following the arguments in the proof of Proposition 2, we only need to show that geometric inter-arrival
times maximizes (resp. minimize) σ for a given λ among As (resp. Am). Then the proposition follows from Lemma
2.

10

There is an important takeaway from this result in the context of job pre-processing for crowdsourcing. In
crowdsourcing systems all jobs are pre-processed to make them suitable for crowd workers and the inter-arrival
(inter-dispatch) time in our model corresponds to this pre-processing time. The above theorem implies it is best to
have a deterministic pre-processing time. However, if pre-processing times are highly variable due to some system
issues (geometric is the most entropic), then instead of having a single pre-processing step it is better to have a
series of sub-steps (corresponding to sum-of-geometric) for pre-processing.

A corollary is the capacity extrema representation of the G/geo/1 queue-channel among the class of sum-

(resp. mixture-) of-geometric distributions.

Corollary 1: For a given arrival rate λ and given service rate µ, the minimum (resp. maximum) capacity
of G/geo/1 queue-channel among the class of sum- (resp. mixture-) of-geometric inter-arrival distributions is

q σ∗qH(ψq)), where σ∗ = λ(1−µ)
µ(1−λ).

Proof: From the proof of Proposition 4, we know that geometric arrival achieves capacity extrema for arrival

λ(log |F| − (1 − σ∗)(cid:80)

rate λ,

∞(cid:88)

t=0

αtpA(t) =

α
1−α
λ + α
1−α

1

.

Letting α = 1 − µ + µσ and solving the ﬁxed point equation
1−µ+µσ
µ−µσ
λ + 1−µ+µσ
µ−µσ

˜A(geo, σ) =

1

= σ,

we have the unique solution σ∗ = λ(1−µ)
µ(1−λ).

B. Discrete-time geo/G/1 queue

inter-arrival times are geometric, but the service times have a general distribution.

In this section we consider another important class of queues, for which the arrival process is Bernoulli, i.e.

Deﬁne(cid:0)n
(cid:1) = 0 if n < k. By characterizing the stationary distribution seen by departures, we prove the following
(cid:1)(1 − λ)t−jλjpS(t), and for all complex z with |z| < 1,
Theorem 3: For j ∈ {0, 1, . . .}, let kj = (cid:80)∞
K(z) = (cid:80)∞
Π(z)−(cid:80)k−1

(cid:80)
q πqH(ψq)) for π0 = 1 − λ

j=0 zjkj, then capacity of this system is λ(log |F| −

µ, and πk =

capacity result.

k

(cid:0)t
µ ) (z−1)K(z)
z−K(z)

t=0

j

.

limz→0

j=0 πjzj
zk

, where Π(z) = (1 − λ

Proof: See Appendix D.

Derivation of the stationary distribution here follows similar steps as the derivation of the stationary distribution of
M/G/1 queue [29].

Next, we investigate the service time distributions that respectively maximize or minimize the capacity of a
geo/G/1 queue-channel. First, we consider the case of threshold error-entropy behavior for threshold b = 0. The
following corollary is a direct consequence of Theorem 3.

is the same for all pS.

Corollary 2: If {ψq} are such that H(ψ0) < H(ψ1) = H(ψ2) = ··· , then the capacity of the geo/G/1 queue
Proof: The capacity in this case depends on π only through π0, but π0 is the same for all service distributions

For the case with threshold b = 1, it can be shown that different service time distributions result in different

with mean 1

µ, as π0 = 1 − λ
µ.

capacities.

maximized by a deterministic service time (for 1
where

Proposition 5: If H(ψ0) = H(ψ1) < H(ψ2) = H(ψ3) = ··· , then the capacity of the geo/G/1 queue is
µ ∈ Z+) and is asymptotically minimized by ˜pS(t, ) as  → 0,
(cid:40)

˜pS(t, ) =

1 − ,
,

t = 1
t = N (),

for  > 0 and N () is chosen to satisfy the mean constraint 1/µ. Among the class of sum-of-geometric random
variables, capacity is minimized by the geometric service time distribution.

11

Proof: Let h0 = H(ψ0) and h2 = H(ψ2), h0 < h2. Then, by Theorem 1, the capacity of the system is

λ(log |F| − h0(π0 + π1) − h2(1 − π0 − π1)).
It is clear from the capacity expression, that it is maximized (resp. minimized) when π0 + π1 is maximized
(resp. minimized). Hence, it is enough to prove that deterministic service time maximizes π0 + π1, and geometric
service time minimizes π0 + π1 among the class of sum-of-geometric random variables.

Note that

π1 = lim
z→0

Π(z) − π0

z

,

which, after a few steps of algebra using the expression for Π(z) and the fact that π0 = 1 − λ

µ, gives

Thus, π0 + π1 = (1− λ
when k0 is maximized and vice-versa. After decomposing k0 for all t,

K(0). Using the deﬁnition that K(0) = k0 = P(no arrivals in S), the capacity is minimized

µ ) 1

1 − K(0)
K(0)

.

π1 = (1 − λ
µ )
∞(cid:88)

k0 =

(1 − λ)tpS(t).

t=1

The conclusion follows from the proofs of Lemma 2 and Proposition 3: the deterministic arrival with mass at
µ ∈ Z+ minimizes k0 by Jensen’s inequality, and ˜pS(t, ) asymptotically maximizes k0 as  → 0. In addition, k0
1
is maximized by geometric distribution among the class of sum-of-geometric random variables by Lemma 2.
Proposition 5 says that handling works with regularity in time yields the least queue length on average; cramming
and staying idle is the worst. For thresholded noise behavior we observe the following. For a geometric service
time, the worst dispatch process among the sum-of-geometric distributions is geometric. On the other hand, for a
geometric arrival process, the geometric service time is the worst among the sum-of-geometric distributions. Thus,
if we visualize the capacity function of a single server queue for a given arrival and service rate plotted against
arrival and service distributions (restricted to sum-of-geometric), there is a minimum where both distributions are
geometric.

In the context of crowdsourcing, this means it is always better to split highly-variable pre-processing (correspond-
ing to the dispatch process) or human work (corresponding to the service process) steps into a series of sub-steps.
That is, it is always better to take a job by parts (if coordination costs are not too high [30]).

V. QUEUES WITH TYPE II ARRIVALS

In this section, we study the queue-capacity of systems with Type II arrivals. An equivalent capacity expression
holds for Type II arrivals, i.e. possibly multiple arrivals in a time slot. Let Ni be a random variable counting the
(cid:80)
number of arrivals at time i. Thus, the {Ni} are i.i.d. with distribution mA.
Theorem 4: The queue-channel capacity of a queue with Type II arrivals distributed as mA, and service time
, kj =(cid:80)∞
t=1 P((cid:80)t
distributed as pS is given by λ(log |F| −
q πqH(ψq)), for π0 = 1 − λ
, where
Proof: The probability of j arrivals within a service time is (cid:80)∞
i=1 Ni = j)pS(t).
Π(z) = (1 − λ
the proof follows the same approach as the proof of Theorem 3.

i=1 Ni = j)pS(t). The remainder of

t=1 P((cid:80)t

µ, and πk = limz→0

Π(z)−(cid:80)k−1

µ ) (z−1)K(z)
z−K(z)

j=0 πjzj
zk

A. Effects of Service Processes

First, we characterize the effect of different service processes on the capacity, for a given arrival process. As the

following results show, deterministic service is best and bursty service is worst, as in Proposition 5.

Proposition 6: Suppose that H(ψ0) = H(ψ1) < H(ψ2) = ··· . For a given Type II arrival process mA, the
maximum capacity is achieved by deterministic service time over all service time distributions. The minimum
capacity is asymptotically achieved by ˜pS(t, ) as  → ∞, where

(cid:40)

˜pS(t, ) =

1 − ,
,

t = 1
t = N (),

12

for  > 0 and N () is chosen to satisfy the mean constraint 1/µ. In addition, the minimum capacity among the
class of sum-of-geometric random variables is achieved by geometric service time distribution.

Proof: Following the proof of Proposition 5, we only need to prove that k0 is minimized and asymptotically
maximized by deterministic service time and ˜pS(t, ), respectively. Further, k0 needs to be maximized by geometric
service time among the class of sum-of-geometric random variables.

Note that k0 =(cid:80)∞

t=1 P((cid:80)t

i=1 Ni = 0)pS(t) =(cid:80)∞

t=1(mA(0))tpS(t), where 0 < mA(0) < 1. Hence, the results

follow from the proof of Proposition 5.

B. Effects of Arrival Processes

Next, we are interested in understanding the effect of arrival processes on the capacity for the worst service time
distribution. Speciﬁcally, we are interested in ﬁnding the arrival processes that maximize and minimize the capacity.

Analogous to Corollary 2 and Proposition 5 for Type I systems, we have the following results.
Corollary 3: Consider the queue with given arrival rate λ and service distribution pS. If H(ψ0) < H(ψ1) =

H(ψ2) = ··· , the capacity of the queue with Type II arrival is the same for all arrival distributions.
Proposition 7: For H(ψ0) = H(ψ1) < H(ψ2) = H(ψ3) = ··· , for a given arrival rate λ and a service
distribution pS, the capacity of the queue-channel over all Type II arrival processes with ﬁnite support {0, 1, . . . , B}
Bλ )tpS(t).
is lower-bounded by CL = λ
Proof: By an argument similar to the proof of Proposition 5, the minimum is obtained when k0 is maximized.

log |F| + (H(ψ2) − H(ψ0))(1 − λ

, where k0 =(cid:80)

k0 − H(ψ2)

t(1 − 1

µ ) 1

(cid:16)

(cid:17)

Hence, it is sufﬁcient to show the maximum value of k0, thus the maximum of mA(0).

Towards this, we ﬁrst show that the distribution

(cid:40)
1 − 1
Bλ ,
1
Bλ ,

t = 0
t = B

∗
m
A(t) =

maximizes mA(0) among all discrete distributions with bounded support {0, . . . , B} and mean 1/λ.
This can be proved by contradiction. Suppose there is another distribution m(cid:48)
m∗
A(0). Now

A with mean 1/λ and m(cid:48)

A(0) >

B(cid:88)

t=0

Em(cid:48)

A[X] =

(cid:48)
A(t) ≤ B
tm

B(cid:88)
∗
(cid:48)
(cid:48)
A(0)) < B(1 − m
A(t) = B(1 − m
A(0)) =
m
A has expectation 1/λ. Hence, there exists no m(cid:48)

t=1

1
λ

,

A(0).

which contradicts the assumption that m(cid:48)
A on {0, . . . , B} with
m(cid:48)
A(0) > m∗
Although the maximal k0, k∗
A(t), the induced Markov chain Q is not irreducible because
m∗
A(1) = 0. Instead, we use the approximate probability mass function ˜mA(t), which has a nonzero mass at t = 1.
Deﬁne

0, is attained by m∗

1 − 1
Bλ − (cid:0)1 − 1

,
1
Bλ − 
Bλ ,

Bλ

t = 0
t = 1
t = B.

(cid:1) ,
0. Note that k0 =(cid:80)
(cid:19)

λ
µ

)

1
k0 − H(ψ2)

,

˜mA(t) =

(cid:18)

Then, we need to show ˜mA(t) approximates k0 arbitrarily close to k∗
a continuous function of mA(0). Thus, the conclusion follows.

Finally, the lower bound of capacity is computed as in the proof of Proposition 5,

t(mA(0))tpS(t), which is

where k0 =(cid:80)

CL = λ

log |F| + (H(ψ2) − H(ψ0))(1 −

t(1 − 1

Bλ )tpS(t).

(cid:17)
Proposition 8: For H(ψ0) = H(ψ1) < H(ψ2) = H(ψ3) = ··· , for a given arrival rate λ and a service
distribution pS, the maximum capacity of the queue-channel over all Type II arrival processes with ﬁnite support
λ )tpS(t), attained
{0, 1, . . . , B} is CU = λ
k0 − H(ψ2)
by the Bernoulli arrival process, i.e. mA(0) = 1 − 1/λ and mA(1) = 1/λ.

log |F| + (H(ψ2) − H(ψ0))(1 − λ

, where k0 =(cid:80)

t(1− 1

µ ) 1

(cid:16)

Proof: By a similar argument as above, the maximum is obtained when k0 is minimized. This is reached when

mA(0) is minimum. Hence, it is sufﬁcient to prove that among all discrete distributions, Bernoulli achieves it.

(cid:80)
Again, the proof is by contradiction. Let us assume there is another distribution m(cid:48)
t tm(cid:48)

A with the same mean, i.e.

A(0) < mA(0).
(cid:48)
(cid:48)
(cid:48)
A(0)) > (1 − mA(0)) =
A(t) = (1 − m
A(t) ≥
m

tm

1
λ

,

(cid:88)

t≥1

(cid:88)
A(t) = 1/λ for which m(cid:48)

which is a contradiction.

t

13

The capacity expression follows by substituting in the expression for k0 for Bernoulli arrival.

This proposition implies that having at most one arrival per time slot is better. In other words, burstiness in the
arrival process hurts performance.

VI. WITHOUT TIMING INFORMATION

So far, we have assumed that the received or processed jobs have timestamps on dispatch time and completion
time. Though this assumption is valid in many wireless settings (MAC timestamps are part of the protocols) and
crowdsourcing scenarios (e.g., Samasource maintains timestamps), this information may not always be available.
In this section we study the setting where the decoder does not have knowledge of An, Dn.

Here, the decoder no longer observes (Y n, An, Dn), but only observes Y n. Using the information spectrum

technique it immediately follows that the capacity is

C(pA) = λ sup
P(X)

I(X; Y).

The following theorem characterizes the capacity of the system based on the queue parameters and noise distribu-
tions. Proof follows similar steps as the proof of Theorem 1.

n → ∞. The capacity of this queue-channel is λ(log |F| − H((cid:80)

Theorem 5: For a given arrival (dispatch) process distribution pA (or mA) with λ < µ < 1 which follows the
assumption in Sec. II, there exists a distribution π such that π(q) > 0 for all q ∈ {0, 1, . . .} and P(Qn) → π as
q πqψq)), where πqψq is a mixture of distributions
{ψq}.
using standard information-theoretic inequalities it follows that

Proof: The converse is due to Fano’s inequality. As the system satisﬁes the Markov relation M −X n−Y n− ˆM,

n(cid:101)R ≤

n(cid:88)

i=1

(H(Xi) − H(Yi|Xi)) + n.

Note that Yi = Xi + Zi, and Zi is independent of everything else given Qi. Thus, the distribution of Zi only
depends on the distribution of Qi. Also, note that Xi is independent of the dispatch and service processes, and so
is independent of Qi. This implies the distribution of Zi is independent of Xi. So,

(cid:88)
(cid:80)
q P(Qi = q)ψq. So, H(Zi) = H((cid:80)

H(Yi|Xi) = H(Xi + Zi|Xi) =

x∈F

P(Xi = x)H(Zi + x|Xi = x) = H(Zi).

q P(Qi = q)ψq). The remainder of the proof follows since

Note that Zi ∼
P(Qi) → π and {Qi} is ergodic.
For achievability, like the proof of Theorem 1 we pick a uniform and i.i.d. P (X n) and show that I(X; Y) is
equal to the expression in the theorem. Ergodicity of {Qi} implies that of {Zi} which is used to take the limit
(almost surely) to evaluate I(X; Y).
Next, we consider some queuing systems to ﬁnd the best dispatch and service processes in this setting. In the
case of thresholded noise behavior the following result hold for a G/geo/1 system.
Proposition 9: For a G/geo/1 system with no timestamps, F = {0, 1}, P(Zi = 1|q) ≤ 0.5 for all q, and
H(ψ0) = ··· = H(ψb) < H(ψb+1) = ··· for some b ∈ {0, 1, . . .}, for a given λ < µ the queue-channel capacity
λ ∈ Z+) and is minimized by geometric inter-arrival among the
is maximized by deterministic inter-arrival (for 1
class of sum-of-geometric random variables.

(cid:33)

= H

πqψq

(cid:33)

(cid:32)(cid:88)
(cid:32)(cid:88)

q

q

πq + ψb+1

 .
ψ0
(cid:88)
(cid:88)
ψ0(1 − σb+1) + ψb+1σb+1(cid:17)
(cid:16)

q≥b+1

q≤b

πq

.

14

Proof: In this system

H

Note that πq = (1 − σ)σq where σ is the ﬁxed-point solution in Theorem 2. Hence,

H

πqψq

= H

maximized (minimized) when(cid:80)

Now, as P(Zi = 1|q) ≤ 0.5, by monotonicity of binary entropy over [0, 0.5], it follows that the above expression is
q≥b+1 πq is maximized (minimized), which in turn happens when σ is maximized
(minimized).

The remainder of the argument follows as in the proof of Proposition 2 and 4, because for G/geo/1, deterministic
arrival minimizes σ, while geometric arrival maximizes among the class of sum-of-geometric random variables.
Proposition 10: For a geo/G/1 system with no timestamps, F = {0, 1}, P(Zi = 1|q) ≤ 0.5 for all q, and
H(ψ0) = H(ψ1) < H(ψ2) = H(ψ3) = ··· , for a given λ < µ the queue-channel capacity is maximized by
λ ∈ Z+) and is minimized by geometric service time among the class of
a deterministic service time and (for 1
sum-of-geometric random variables.
Proof: By the same argument as in proof of Proposition 9, the maximum is achieved when π0+π1 is maximized.

The remaining argument follows the proof of Proposition 5.

VII. CONCLUSION

Inspired by several engineering applications, we studied the performance of servers with queue-length dependent
service quality using an information-theoretic approach. We deﬁned the capacity of such queuing systems to be the
maximum rate at which jobs can be processed with arbitrarily small error probability, and characterized it in terms
of queuing parameters.

We studied Type I and Type II arrivals separately for technical reasons. In Type I arrivals with some assumptions,
for the G/geo/1 queue, jobs arriving deterministically maximize capacity while bursty arrivals minimize capacity.
Similarly, for the geo/G/1 queue, deterministic service maximizes capacity, but bursty service minimizes capacity.
Type II arrivals give similar conclusions except that Bernoulli arrivals maximize capacity for the G/geo/1 queue.

APPENDIX

A. Proof of Lemma 1

the type.

First consider Type II processes. For these, λ < 1 implies(cid:80)

We need separate approaches for Type I and Type II arrival processes, as the nature of {Qi} process depends on
k kmA(k) < 1. If mA(0) = 0, the mean arrival rate
must be equal or greater than 1, contradicting the assumption. Hence, mA(0) > 0. Also from the assumption in
Sec. II, mA(1) > 0.

Under the assumptions, we show {Qi} is an irreducible and aperiodic Markov chain by proving P(Qi+1 =
Qi + 1|Qi), P(Qi+1 = max(Qi − 1, 0)|Qi), P(Qi+1 = Qi|Qi) > 0 for all Qi. If this is true then any state can be
reached from any other state, since states are in Z+. Notice the enumerated probabilities are probabilities of the
events corresponding to two, one, and no arrivals, respectively, during a service time.

By the above result and assumption, mA(0), mA(1) > 0 and there exists an s > 1 such that pS(s) > 0. Note the

probability of exactly two arrivals in a service time is lower bounded by
(mA(1))2(mA(0))s−2pS(s)

for any s > 1. As there exists an s > 1 such that pS(s) > 0 and mA(a), mA(1) > 0, this bound is strictly positive.
Probability of exactly one arrival in a service time is lower bounded by mA(1)(mA(0))s−1pS(s) for any s > 0,
which again is strictly positive. Probability of no arrival is lower bounded by pS(s)(mA(0))s, which is also strictly
positive.

15

q(cid:48)

Note that as P(Qi+1 = Qi|Qi) > 0, this Markov chain is also aperiodic. Due to the self-loop if P(Qi+k =
|Qi = q) is positive, then so is P(Qi+k+1 = q(cid:48)
Positive recurrence follows by considering queue-length to be the Lyapunov function, because λ < µ. Hence,
For Type I, {Qi} is not a Markov chain, and we take a different approach. First note that as µ < 1,(cid:80)
the result follows for Type II processes due to the existence of a unique stationary distribution for an irreducible
and aperiodic positive recurrent Markov chain. Hence, {Qi} is ergodic.
s spS(s) > 1.
This implies there exists an s > 1 such that pS(s) > 0. Note that by assumption λ < µ. Then, for Type I arrival
processes this implies there exists an a > 1 such that pA(a) > 0.

|Qi = q).

Consider the process {Wi}, the sojourn time for jobs. We ﬁrst claim that under the assumption, this is an
irreducible, aperiodic, and positive recurrent Markov chain. It is known in queuing theory that for i.i.d. inter-
arrival and service times, {Wi} is a Markov chain. Next, we show irreducibility and aperiodicity by showing that
P(Wi+1 = Wi + 1|Wi), P(Wi+1 = Wi|Wi), and P(Wi+1 = max(Wi − 1, 0), Wi) > 0.
First, we consider the case when pA has a support that spans Z+. As µ < 1, there exists an s > 1 such that
pS(s) > 0. Consider a possible path from Wi to Wi+1 = max(Wi + b, 0), b ∈ {0,±1}. This can happen as follows:
the (i + 1)th job brings a service time requirement of s, and it reaches the system s− b time after the ith job. As the
service times and inter-arrival times are independent, probability of this sample path event is exactly pS(s)pA(s−b),
which is strictly positive.
Next, we consider the case when pS has a support spanning Z+. As λ < 1, there exists an a > 1 such that
pA(a) > 0. Then a possible path for the events is as follows: the (i + 1)th job comes a time after ith job and brings
with it a service requirement of a + b. The rest follows by evaluating the probability of this event.

Note that {Wi} is an irreducible and aperiodic Markov chain. Note that given Wi, Qi is independent of anything

else because given Wi, it only depends on the number of arrivals in the time Wi:

(cid:32) q(cid:88)

i=1

(cid:33)

q+1(cid:88)

i=1

P(Qi = q) = P

Ai ≤ Wi <

Ai

.

As the Ai are i.i.d., this also implies that given a distribution of Wi, the distribution of Qi is ﬁxed.

It follows from queuing theory that {Wi} is positive recurrent for λ < µ. Hence, {Wi} converges in distribution
to a stationary distribution, and by the above argument, so does {Qi}. Ergodicity of Qi follows from the ergodicity
of Wi.

(cid:4)

B. Proof of Theorem 2

Let { ˆQi} be queue-lengths seen by the arrivals, then the stationary distribution of ˆQi is the same as that of Qi.
Note that there is only one arrival and one departure at a time. Since the queue-length is stable, the fraction of
time the queue-length increases by 1 from a value q is the same as the fraction of time the queue-length decreases
by 1 from q, for all q. Since increase corresponds to arrival and decrease corresponds to departure, the fraction
of arrivals and departures that see a queue-length q is the same. Thus it is sufﬁcient to show that the stationary

distribution of { ˆQ} is πk = (1 − σ)σk, where σ solves x =(cid:80)∞

n=0 pA(n)(1 − µ + xµ)n in (0, 1).

chain, and then derive the stationary distribution.

We shall ﬁrst show the uniqueness of the stationary distribution from the fact that { ˆQi} is an irreducible Markov
Consider the transition probability

As at most one arrival is possible, the probability is 0 for q(cid:48)

− q > 1. For q(cid:48)

− q ≤ 1,

P( ˆQi+1 = q

(cid:48)

| ˆQi = q, ˆQi−1, . . .).

(cid:48)

P( ˆQi+1 = q
= P(there are q − q

(cid:48)

| ˆQi = q, ˆQi−1, . . .)

+ 1 departures between i and i + 1 arrival | ˆQi = q, ˆQi−1, . . .).

As service time is geometric with mean 1
µ and hence memoryless, starting from any time, the time to the next
departure is geometric with the same mean, if there is a job in the queue. After any arrival, there is always at
least one job in the queue, and hence, time to the next departure is geometric. Thus the probability that there are

16

q − q(cid:48) + 1 departures given the past is nothing but the probability that the sum of q − q(cid:48) + 1 geometric random
variables is less than a realization of pA. Thus,

P( ˆQi+1 = q

(cid:48)

| ˆQi = q, ˆQi−1, . . .) =

=

=

t=0

∞(cid:88)
∞(cid:88)
∞(cid:88)

t=0

t=0

(cid:32)q−q(cid:48)+1(cid:88)

i=1

(cid:33)

q−q(cid:48)(cid:88)

pA(t)P

Si ≤ t ≤

Si

pA(t)P(Bin(t, µ) = q − q

(cid:18)

(cid:19)

pA(t)

t

q − q(cid:48) + 1

(1 − µ)t−q+q(cid:48)−1µq−q(cid:48)+1.

i=1

(cid:48)

+ 1)

(5)

Eq. (5) follows because the service times are geometric, meaning each time a job in service gets completed
according to a Bernoulli random variable, and the sum of Bernoulli random variables is binomial. This derivation
implies the transition depends only on q and q(cid:48), further implying the process is Markov.

Thus the probability of the q → 0 transition is
pA(t)

∞(cid:88)

t=0

(cid:18) t

(cid:19)

q + 1

(1 − µ)t−q−1µq+1.

Note that the transitions can be written as the amount of change in the queue-length, meaning a q → q(cid:48) transition
is a q − q(cid:48) change, and is nothing but the probability of having q − q(cid:48) + 1 departures before an arrival.
For k ≥ 0, let βk denote the probability that the sum of k geometric random variables is less than the time
between two arrivals. Then, for q(cid:48) > 0,

and for q(cid:48) = 0,

P( ˆQi+1 = q

(cid:48)

| ˆQi = q) = βq−q(cid:48)+1,
q(cid:88)

P( ˆQi+1 = 0| ˆQi = q) = 1 −

βk.

k=0

(cid:80)1
Also, as β0, β1, β2 > 0, the Markov chain is irreducible and aperiodic. Thus there exists a unique stationary
distribution π which solves π = π[P ], where [P ] is the probability transition matrix. The transition matrix [P ] is
written as a matrix whose ﬁrst column is (1−β0, 1−
k=0 βk, . . .)T and other columns are (0, . . . , β0, β1, β2, . . .)T ,
where β0 is the (i, i + 1)th entry.
(cid:32)
∞(cid:88)
From π = π[P ] it follows that
∞(cid:88)

i(cid:88)

(cid:33)

1 −

π0 =

πi,

k=0

βk

i=0

πk =

πk−1+iβi

for k > 0.

i=0

Like in the analysis of GI/M/1 queue [29], we guess a solution πk = π0σk for some σ < 1. Next, we check if
this solution satisﬁes π = π[P ] for a unique σ < 1.

It follows from π = π[P ], as above, that σ must satisfy

i=0

∞(cid:88)
∞(cid:88)
∞(cid:88)

t=0

t=0

σ =

=

=

(cid:18)t
(cid:19)

i

(1 − µ)t−iµi

∞(cid:88)
(cid:19)

t=0

σi

∞(cid:88)
(cid:18)t
t(cid:88)

i=0

i

i=0

σiβi =

pA(t)

pA(t)

(1 − µ)t−i(σµ)i

pA(t)(1 − µ + σµ)t.

(6)

(7)

complete the proof. (cid:4)

Eq. (6) follows by interchanging the two sums, as per the Fubini-Tonelli theorem since terms are non-negative.
Eq. (7) follows using the binomial theorem.

To show that the distribution π is unique, we show that x =(cid:80)∞
in 0 < x < 1. Towards this we characterize (cid:80)∞
t=0 pA(t)(1 − µ + xµ)t has a unique solution
Lemma 3: For any pA on Z+ and µ ∈ (0, 1),(cid:80)∞
t=0 pA(t)(1 − µ + xµ)t, in Lemmas 3 and 4 given below, which
t=0 pA(t)(1 − µ + xµ)t is an increasing function of x in (0, 1),
∞(cid:88)

and strictly convex in (0, 1).

Proof: Deﬁne

17

It is sufﬁcient to show that f(cid:48)(x), f(cid:48)(cid:48)(x) are both strictly positive in x ∈ (0, 1).
Let the partial sum up to T in f (x) be fT (x), i.e.

f (x) =

fT (x) =

t=0

pA(t)(1 − µ + xµ)t.
T(cid:88)
T(cid:88)

pA(t)(1 − µ + xµ)t,

t=0

t=0

µtpA(t)(1 − µ + xµ)t.
T(cid:88)

T(cid:88)

t=0

(cid:48)
f
T (x) =

and then

(cid:48)
f
T (x) =

It is easy to see that f(cid:48)

T (x) is increasing as 0 < 1 − µ + xµ < 1. In addition, f(cid:48)

T (x) is bounded since

T (x) is increasing and bounded, limT→∞ f(cid:48)

Since f(cid:48)
Next, note that for any x ∈ (0, 1), the difference between f(cid:48)

tpA(t) < ∞.

t=0

µtpA(t)(1 − µ + xµ)t ≤ µ
∞(cid:88)

T (x) exists for all x ∈ (0, 1).
T (x) and f(cid:48)(x) is
∞(cid:88)

µtpA(t)(1 − µ + xµ)t−1 ≤ µ

tpA(t) → 0

t=T +1

(cid:48)

f

(cid:48)
T (x) =

(x) − f

t=T +1

as T → ∞, where the inequality follows from 0 < 1 − µ + xµ < 1 and the limit follows from the condition of
ﬁxed mean. Then, limT→∞ f(cid:48)

T (x) = f(cid:48)(x) uniformly in (0, 1). That f(cid:48)(x) > 0 follows from

0 < 1 − µ + xµ < 1.

point in (0, 1).

Lemma 4: The equation x =(cid:80)∞
Again, f (x) =(cid:80)∞
exists δ > 0 such that µ(cid:80)

Similarly, we can show the existence and strict positivity of f(cid:48)(cid:48)(x), which completes the proof.

Proof: Note that x = 1 is a solution to this ﬁxed-point equation. First, we show that there is at least one ﬁxed

t=0 pA(t)(1 − µ + xµ)t has a unique solution in (0, 1).

that f (x) is strictly greater than x in (0, 1).

t=0 pA(t)(1 − µ + xµ)t > 0 for x = 0. Hence, if there is no ﬁxed point in (0, 1) this implies
t t(1− δ)tpA(t) = µ ˆA(1− δ), where ˆA(α) = EpAαA.
λ > 1, there

We know that generating function ˆA is continuous around 1. Hence, as δ → 0, ˆA(1 − δ) → 1

Now, consider the derivative of f (x) at 1− δ

t t(1 − δ)tpA(t) > 1. This means that the derivative of f (x) at x = 1 − δ is > 1.

µ, This is µ(cid:80)

If f (x) > x for all x ∈ (0, 1), then the following is true. From convexity of f,

λ. As µ

f (1) ≥ f (1 − δ) + δf

(1 − δ) > 1 − δ + δf

(1 − δ) > 1 − δ + δ = 1.

(cid:48)

(cid:48)

This is a contradiction. So, there exists a ﬁxed point in (0, 1).

Let us assume there are more than one ﬁxed points in (0, 1). By Lemma 3, f (x) is convex in (0, 1). A convex
function can intersect a line at most twice. As f (x) crosses y = x at x = 1, there can be only one ﬁxed point in
[0, 1), but 0 is not a ﬁxed point.

Note that for a geometric random variable Ai with mean 1/λi and letting α = (1 − µ − σµ) ∈ (0, 1),

˜A(pAi, σ) =

=

αtpAi(t) =

t=1

t=1

αλi

(1 − α) + αλi

αt(1 − λi)t−1λi = αλi
α
1−α
+ α
1−α

.

1
λi

(α(1 − λi))t

(8)

Consider the sum-of-geometric random variables As ﬁrst. Then for any sum-of-geometric random variable pAs ∈

∞(cid:88)

t=0

As,

∞(cid:88)

∞(cid:88)
I(cid:89)

t=1

∞(cid:88)

∞(cid:88)

=

∞(cid:88)
I(cid:89)

18

C. Proof of Lemma 2

˜A(pAs, σ) =

=

αtpAs(t) =

ti=1, 1≤i≤I

αtipAi(ti) =

αt1+t2+···+tI pA1(t1)··· pAI (tI )
α
1−α
+ α
1−α

.

1
λi

i=1
The last equality follows from (8). Note that the inequality

i=1

ti=1

(cid:89)

i

(cid:88)

i

(1 + xi) ≥ 1 +

xi

holds for any xi > 0. Hence, inverting both sides of this inequality and scaling both numerator and denominator
by α

1−α,

˜A(pAs, σ) =

I(cid:89)

α
1−α
+ α
= ˜A(geo, σ).

1
λi

i=1

1−α ≤

(cid:80)I

α
1−α
1
λi

+ α
1−α

=

α
1−α
λ + α
1−α

1

i=1

Next since Am is mixed, for any pAm ∈ Am,

˜A(pAm, σ) =

αtpAm(t) =

I(cid:88)

ci

∞(cid:88)

i=1

ti=1

αtipAi(ti) =

I(cid:88)

i=1

α
1−α
+ α
1−α

.

ci

1
λi

The last expression is convex in 1/λi. Hence by Jensen’s inequality

˜A(pAm, σ) =

α
1−α
+ α

1−α ≥

ci

1
λi

α
1−α
1
λi

=

α
1−α
λ + α
1−α

1

+ α
1−α

i=1 ci

(cid:80)I

∞(cid:88)

t=1

I(cid:88)

i=1

= ˜A(geo, σ).

D. Proof of Theorem 3

Consider the following transition probability for q > 0.

|Qi = q, Qi−1, . . .)

P(Qi+1 = q
= P(there are q(cid:48)
= P(sum of q(cid:48)

− q + 1 arrivals between departures i − 1 and i | Qi = q, Qi−1, . . .)
− q + 1 geometric times ≤ interdeparture time between i − 1 and i)

(cid:48)

(cid:18)

∞(cid:88)
∞(cid:88)

t=0

t=0

=

=

= kq(cid:48)−q+1

pS(t)P(Bin(t, λ) = q

(cid:48)

− q + 1)

(cid:19)

pS(t)

q(cid:48)

t

− q + 1

(1 − µ)t−q(cid:48)+q−1µq(cid:48)−q+1

(9)

(10)

19

Eq. (9) follows because geometric random variables are memoryless. Geometric inter-arrival is the same as Bernoulli
arrival per time slot, and the sum of Bernoulli variables is binomial, which leads to (10).
When Qi = 0, note that just before the (i + 1)th arrival, the queue-length is 0, and it is 1 just after the (i + 1)th
arrival. Then the probability that Qi+1 = q(cid:48) is equal to the probability that there are exactly q(cid:48) arrivals during the
service time of the (i + 1)th job. From above, this is equal to kq(cid:48).

This proves {Qi} is Markov; irreducibility and aperiodicity follows since P(Qi+1 = Qi + δ|Qi) > 0 for
From π = π[P ] for this Markov chain it follows that

δ ∈ {0,±1}.

π0k0 + π1k0 = π0
π0k1 + π1k1 + π2k0 = π1

...

Multiplying the ﬁrst equation by z0, the second by z, the third by z2, and so on, and then summing all of them
we get

which, after some algebra, gives

π0K(z) + K(z)(π1 + π2z + ··· ) = Π(z),

Π(z) =

π0(z − 1)K(z)

.

z − K(z)

K(1) =(cid:80)
Note that K(cid:48)(z) =(cid:80)

We know that Π(1) = 1, then the left side must also be 1 for z = 1. But it is 0

0 when evaluated at z = 1, as

j kj = 1. Thus using l’Hˆopital’s rule we get

1 − K(cid:48)(1)
K(1)
j jkj, i.e. K(cid:48)(1) is the expected number of arrivals in a
time distributed as pS. As arrivals are Bernoulli and are independent from service times, from Wald’s lemma we
get

j jkjzj which gives K(cid:48)(1) =(cid:80)

π0 =

.

(cid:48)

K

(1) =

λ
µ

,

µ.
which in turn gives π0 = 1 − λ
evaluating the limit of Π(z)−(cid:80)k−1
From Π(z) we can obtain π1 by evaluating Π(z)−π0

z

j=0 πjzj
zk

as z → 0.

as z → 0. By repeating the procedure we can obtain πk by

REFERENCES

[1] A. Ephremides and B. Hajek, “Information theory and communication networks: An unconsummated union,” IEEE Trans. Inf. Theory,

vol. 44, no. 6, pp. 2416–2434, Oct. 1998.

[2] B. Schwartz, “Queues, priorities, and social process,” Social Psychology, vol. 41, no. 1, pp. 3–12, Mar. 1978.
[3] S. Branson, G. Van Horn, C. Wah, P. Perona, and S. Belongie, “The ignorant led by the blind: A hybrid human-machine vision system

for ﬁne-grained categorization,” Int. J. Comput. Vis., vol. 108, no. 1-2, pp. 3–29, May 2014.

[4] M. Borokhovich, A. Chatterjee, J. Rogers, L. R. Varshney, and S. Vishwanath, “Improving impact sourcing via efﬁcient global service

[5] A. Vempaty, L. R. Varshney, and P. K. Varshney, “Reliable crowdsourcing for multi-class labeling using coding theory,” IEEE J. Sel.

delivery,” in Proc. Data for Good Exchange (D4GX), Sep. 2015.

Topics Signal Process., vol. 8, no. 4, pp. 667–679, Aug. 2014.

[6] K. Sriram and D. M. Lucantoni, “Trafﬁc smoothing effects of bit dropping in a packet voice multiplexer,” IEEE Trans. Commun.,

[7] S. C. Draper, M. D. Trott, and G. W. Wornell, “A universal approach to queuing with distortion control,” IEEE Trans. Autom. Control,

[8] V. K. Goyal, “Multiple description coding: Compression meets the network,” IEEE Signal Process. Mag., vol. 18, no. 5, pp. 74–93,

vol. 37, no. 7, pp. 703–712, Jul. 1989.

vol. 50, no. 4, pp. 532–537, Apr. 2005.

Sep. 2001.

pp. 446–461, Mar. 1998.

[9] V. Anantharam and S. Verd´u, “Bits through queues,” IEEE Trans. Inf. Theory, vol. 42, no. 1, pp. 4–18, Jan. 1996.
[10] A. S. Bedekar and M. Azizo˜glu, “The information-theoretic capacity of discrete-time queues,” IEEE Trans. Inf. Theory, vol. 44, no. 2,

20

357–370, Feb. 2003.

pp. 705–709, Mar. 2000.

no. 2, pp. 447–465, Feb. 2005.

pp. 2455–2477, Sep. 2002.

[11] B. Prabhakar and R. Gallager, “Entropy and the timing capacity of discrete queues,” IEEE Trans. Inf. Theory, vol. 49, no. 2, pp.

[12] R. Sundaresan and S. Verd´u, “Sequential decoding for the exponential server timing channel,” IEEE Trans. Inf. Theory, vol. 46, no. 2,

[13] A. B. Wagner and V. Anantharam, “Zero-rate reliability of the exponential-server timing channel,” IEEE Trans. Inf. Theory, vol. 51,

[14] J. Giles and B. Hajek, “An information-theoretic and game-theoretic study of timing channels,” IEEE Trans. Inf. Theory, vol. 48, no. 9,

[15] X. Gong, N. Kiyavash, and P. Venkitasubramaniam, “Information theoretic analysis of side channel information leakage in FCFS

schedulers,” in Proc. 2011 IEEE Int. Symp. Inf. Theory, Jul. 2011, pp. 1255–1259.

[16] S. K. Gorantla, S. Kadloor, N. Kiyavash, T. P. Coleman, I. S. Moskowitz, and M. H. Kang, “Characterizing the efﬁcacy of the NRL

network pump in mitigating covert timing channels,” IEEE Trans. Inf. Forensics Security, vol. 7, no. 1, pp. 64–75, Feb. 2012.

[17] M. Tavan, R. D. Yates, and W. U. Bajwa, “Bits through bufferless queues,” in Proc. 51st Annu. Allerton Conf. Commun. Control

[18] ˙I. E. Telatar, “Multi-access communications with decision feedback decoding,” Ph.D. thesis, Massachusetts Institute of Technology,

[19] ˙I. E. Telatar and R. G. Gallager, “Combining queueing theory with information theory for multiaccess,” IEEE J. Sel. Areas Commun.,

Comput., Oct. 2013, pp. 755–762.

Cambridge, MA, May 1992.

vol. 13, no. 6, pp. 963–969, Aug. 1995.

[20] S. Raj, E. Telatar, and D. Tse, “Job scheduling and multiple access,” in Advances in Network Information Theory, P. Gupta, G. Kramer,

and A. J. van Wijngaarden, Eds. Providence: DIMACS, American Mathematical Society, 2004, pp. 127–137.

[21] S. Musy and E. Telatar, “On the transmission of bursty sources,” in Proc. 2006 IEEE Int. Symp. Inf. Theory, Jul. 2006, pp. 2899–2903.
[22] N. Michelusi, J. Boedicker, M. Y. El-Naggar, and U. Mitra, “Queuing models for abstracting interactions in bacterial communities,”

arXiv:1508.00942 [cs.ET]., Aug. 2015.

[23] C. M. Harris, “Queues with state-dependent stochastic service rates,” Oper. Res., vol. 15, no. 1, pp. 117–130, Jan.-Feb. 1967.
[24] M. S. Pinsker, Information and Information Stability of Random Variables and Processes. San Francisco: Holden-Day, 1964.
[25] S. Verd´u and T. S. Han, “A general formula for channel capacity,” IEEE Trans. Inf. Theory, vol. 40, no. 4, pp. 1147–1157, Jul. 1994.
[26] T. S. Han, Information-Spectrum Methods in Information Theory. Berlin: Springer, 2003.
[27] T. M. Cover and J. A. Thomas, Elements of Information Theory. New York: John Wiley & Sons, 1991.
[28] G. Caire and S. Shamai (Shitz), “On the capacity of some channels with channel state information,” IEEE Trans. Inf. Theory, vol. 45,

no. 6, pp. 2007–2019, Sep. 1999.

[29] L. Kleinrock, Queuing Systems, Volume I: Theory.
[30] A. Chatterjee, L. R. Varshney, and S. Vishwananth, “Work capacity of freelance markets: Fundamental limits and decentralized schemes,”

John Wiley & Sons, Inc., 1975.

in Proc. 2015 IEEE INFOCOM, Apr. 2015, pp. 1769–1777.

