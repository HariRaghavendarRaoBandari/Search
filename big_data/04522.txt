Learning Optimal Social Dependency for Recommendation

Yong Liu, Peilin Zhao, Xin Liu, Min Wu, Xiao-Li Li
Institute for Infocomm Research, A*STAR, Singapore
{liuyo, zhaop, liu-x, wumin, xlli}@i2r.a-star.edu.sg

6
1
0
2

 
r
a

M
 
5
1

 
 
]

R

I
.
s
c
[
 
 

1
v
2
2
5
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Social recommender systems exploit users’ social
relationships to improve the recommendation ac-
curacy.
Intuitively, a user tends to trust different
subsets of her social friends, regarding with differ-
ent scenarios. Therefore, the main challenge of so-
cial recommendation is to exploit the optimal social
dependency between users for a speciﬁc recom-
mendation task. In this paper, we propose a novel
recommendation method, named probabilistic rela-
tional matrix factorization (PRMF), which aims to
learn the optimal social dependency between users
to improve the recommendation accuracy, with or
without users’ social relationships. Speciﬁcally,
in PRMF, the latent features of users are assumed
to follow a matrix variate normal (MVN) distri-
bution. The positive and negative dependency be-
tween users are modeled by the row precision ma-
trix of the MVN distribution. Moreover, we have
also proposed an efﬁcient alternating algorithm to
solve the optimization problem of PRMF. The ex-
perimental results on real datasets demonstrate that
the proposed PRMF method outperforms state-of-
the-art social recommendation approaches, in terms
of root mean square error (RMSE) and mean abso-
lute error (MAE).

1 Introduction
Recommender systems have been widely used in our lives to
help us discover useful information from a large amount of
data. For example, on the popular e-commerce websites such
as Amazon, the recommender systems predict users’ prefer-
ences on products based on their past purchasing behaviors
and then recommend a user a list of interesting products she
may prefer [Linden et al., 2003]. For a recommender system,
the most critical factor is the prediction accuracy of users’
preferences. In practice, the most successful prediction meth-
ods are collaborative ﬁltering based approaches, especially
the matrix factorization models [Su and Khoshgoftaar, 2009].
The recent rapid developments of online social net-
working services (e.g., Facebook and Twitter) motivate
the emergences of social recommender systems that ex-
ploit users’ online social
recommenda-

friendships for

tion [Yang et al., 2014]. The social recommender systems
usually assume that a user has similar interests with her so-
cial friends. Although the recommendation accuracy can usu-
ally be improved, there do not exist strong connections be-
tween the online friendship and the similarity of users’ in-
terests [Ma, 2014]. Because there are different categories
of social networking friends in online social networks, e.g.,
school friends, work-related friends, friends sharing same in-
terest/activities, and neighborly friends [Zhang et al., 2013].
The tastes of a user’s online friends usually vary signiﬁcantly.
In different scenarios, a user tends to trust the recommen-
dations from different subsets of her online social friends.
Hence, the key to success for a social recommender system
is to exploit the most appropriate social dependency between
users for recommendation.

e.g.,

trust

relationships

and

social

online

Moreover, existing social

recommender systems are
usually developed based on users’ explicit social relation-
[Jamali and Ester, 2009;
ships,
Jamali and Ester, 2010]
friend-
ships [Ma et al., 2011; Yang et al., 2012; Tang et al., 2013].
They ignore the underlying implicit social relationships
between users that have most similar or dissimilar rating
behaviors.
This implicit social relationships have been
found to be beneﬁcial for improving the recommendation
accuracy [Ma, 2013].
In addition, users’ explicit social
relationships may be unavailable in many application sce-
narios. This also limits the application of traditional social
recommendation approaches.

In previous studies, users’ social dependency adopted for
recommendation are predeﬁned based on users’ explicit or
implicit social relationships [Ma, 2013; Yang et al., 2014].
Differing from previous work, this paper proposes a novel so-
cial recommendation method, namely probabilistic relational
matrix factorization (PRMF), which aims to learn the opti-
mal social dependency between users to improve the recom-
mendation accuracy. The proposed method can be applied
to the recommendation scenarios with or without users’ ex-
plicit social relationships. In PRMF, the user latent features
are assumed to follow a matrix variate normal (MVN) dis-
tribution. The positive and negative social dependency be-
tween users are modeled by the row precision matrix of the
MVN distribution. This is motivated by the success of us-
ing the MVN distribution to model the task relationships
for multi-task learning [Zhang and Yeung, 2010]. To solve

the optimization problem of PRMF, we propose a novel al-
ternating algorithm based on the stochastic gradient descent
(SGD) [Koren et al., 2009] and alternating direction method
of multipliers (ADMM) [Boyd et al., 2011] methods. More-
over, we extensively evaluated the performances of PRMF
on four public datasets. Empirical experiments showed that
PRMF outperformed the state-of-the-art social recommenda-
tion methods, in terms of root-mean-square error (RMSE) and
mean absolute error (MAE).

2 Related Work and Background

this

ﬁrst
probabilistic

section, we
about

some
back-
In
ground
factorization
(PMF) [Mnih and Salakhutdinov, 2007], one of the most
popular matrix factorization models. Then, we review the
state-of-the-art social recommendation methods.

introduce
matrix

2.1 Probabilistic Matrix Factorization
i=1 and
For the recommendation problem with m users {ui}m
j=1, the matrix factorization models map both
n items {vj}n
users and items into a shared latent space with a low dimen-
sionality d ≪ min(m, n). For each user ui, her latent fea-
tures are represented by a latent vector Ui ∈ R1×d. Similarly,
the latent features of the item vj are described by a latent vec-
tor Vj ∈ R1×d. In the PMF model, users’ ratings on items are
assumed to follow a Gaussian distribution as follows:

p(R|U, V, σ2) =

mYi=1

nYj=1(cid:2)N (Rij|UiV ⊤j , σ2)(cid:3)Wij ,

(1)

where R ∈ Rm×n is the matrix denoting users’ ratings on
items; U ∈ Rm×d and V ∈ Rn×ddenote the latent features
of all users and items, respectively; σ2 is the variance of the
Gaussian distribution; Wij is an indicator variable. If ui has
rated vj, Wij = 1, otherwise, Wij = 0. In addition, we also
place zero-mean spherical Gaussian priors on U and V as:

p(U|σ2

u) =

mYi=1

N (Ui|0, σ2

uI), p(V |σ2

v) =

nYj=1

vI).

N (Vj|0, σ2
(2)
v are the variance

u and σ2

where I is the identity matrix, σ2
parameters. Through the Bayesian inference, we have
p(U, V |R, σ2, σ2
(3)
The model parameters (i.e., U and V ) can be learned via max-
imizing the log-posterior in Eq. (3), which is equivalent to
solving the following problem:

v) ∝ p(R|U, V, σ2)p(U|σ2

u)p(V |σ2
v).

u, σ2

min
U,V

1
2kW ⊙ (R − U V ⊤)k2

F +

λu
2 kUk2

F +

λv
2 kV k2

F , (4)

where W ∈ Rm×n is the indicator matrix, and Wij is the
(i, j) element of W .
In Eq. (4), ⊙ denotes the Hadamard
product of two matrices, λu = σ2/σ2
v, and
k · kF denotes the Frobenius norm of a matrix.

u, λv = σ2/σ2

2.2 Social Recommendation Methods

In recent years, lots of social recommendation approaches
have been proposed [Yang et al., 2014]. The Social Regu-
larization (SR) [Ma et al., 2011] is one of the most represen-
tative methods. The SR method was implemented by ma-
trix factorization framework. The basic idea was that a user
may have similar interests with her social friends, and thus
the learned latent features of a user and that of her social
friends should be similar. The user similarity can be cal-
culated using the Cosine similarity and Pearson correlation
coefﬁcient [Su and Khoshgoftaar, 2009]. To improve the per-
formance of SR, [Yu et al., 2011] proposed an adaptive social
similarity function. Moreover, as a user tends to trust differ-
ent subsets of her social friends regarding with different do-
mains, [Yang et al., 2012] introduced the circle-based recom-
mendation (CircleCon) models, which considered domain-
speciﬁc trust circles for recommendation. [Tang et al., 2013]
exploited both the local and global social context for recom-
[Ma, 2013] extended the SR model to exploit
mendation.
users’ implicit social relationships for recommendation. The
implicit social relationships were deﬁned between a user and
other users that had most similar or dissimilar rating behav-
iors with her. In addition, [Li et al., 2015] extended SR to ex-
ploit the community structure of users’ social networks to im-
prove the recommendation accuracy. [Guo et al., 2015] de-
veloped the TrustSVD model, which extended the SVD++
model [Koren, 2008] to incorporate the explicit and implicit
inﬂuences from rated items and trusted users for recommen-
dation. [Hu et al., 2015] proposed a recommendation frame-
work named MR3, which jointly modeled users’ rating be-
haviors, social relationships, and review comments.

3 The Proposed Recommendation model

This section presents the details of the proposed PRMF model
that extends PMF to jointly learn the user preferences and the
social dependency between users.

3.1 Probabilistic Relational Matrix Factorization

In Section 2.1, the PMF model assumes the users are inde-
pendent of each other (see Eq.(2)). Thus, it ignores the social
dependency between users. However, in practice, users are
usually connected with each other through different types of
social relationships, e.g., trust relationships and online social
relationships.
In this work, to exploit users’ social depen-
dency for recommendation, we place the following priors on
the user latent features:

p(U|σ2

u) ∝(cid:0) mYi=1

N (Ui|0, σ2

uI)(cid:1)q(U ),

(5)

where the ﬁrst term of the priors is used to penalize the com-
plexity of the latent features of each user, and the second
term q(U ) is used to model the dependency between different
users. Speciﬁcally, we deﬁne

q(U ) = MN m×d(cid:0)0, Θ−1, I(cid:1), s.t. Θ ≻ 0,

(6)

where MN a×b(M, A, B) denotes the matrix variate normal
(MVN) distribution1 with mean M ∈ Ra×b, row covariance
A ∈ Ra×a, and column covariance B ∈ Rb×b; Θ ≻ 0 in-
dicates Θ is a positive deﬁnite matrix. In Eq. (6), Θ is the
row precision matrix (i.e., the inverse of the row covariance
matrix) that models the relationships between different rows
of U . In other words, Θ describes the social dependency be-
tween different users. Thus, Θ is called the social dependency
matrix. In this work, for simplicity, we set the column covari-
ance matrix of the MVN distribution as I, which indicates the
user latent features in different dimensions are independent.
Moreover, we can also rewrite Eq. (5) as follows:

p(U|Θ, σ2

u) = MN m×d(cid:0)0, (Θ +

1
σ2
u

I)−1, I(cid:1).

In practice, a user is usually only correlated with a small
fraction of other users. Thus, it is reasonable to assume the
social dependency matrix Θ is sparse. To achieve this objec-
tive, we introduce a sparsity-inducing prior for Θ as follows:

(7)

p(Θ|γ) ∝ exp(−

γ
2kΘk1),

(8)

where γ is a positive constant used to control the sparsity of
Θ, and k · k1 is the ℓ1-norm of a matrix. In addition, the spar-
sity of Θ can also help improve the computation efﬁciency of
the proposed model (see the discussions in Section 3.2).

Moreover, we also assume users’ ratings follow the Gaus-
sian distribution in Eq. (1), and add Gaussian priors on the
item latent features as in Eq. (5). Through the Bayesian infer-
ence, we have

p(U, V, Θ|R, σ2, σ2
u, σ2
∝p(R|U, V, σ2)p(U|Θ, σ2

v, γ)
u)p(Θ|γ)p(V |σ2
v).

(9)

Then, the model parameters (i.e., U , V , and Θ) can be ob-
tained by solving the following problem:

min

U,V,Θ≻0
1

+

1
2σ2kW ⊙ (R − U V ⊤)k2
2(cid:2)tr(U⊤ΘU ) − d log |Θ +

F +

1
σ2
u

1
2σ2

F +

ukUk2
I| + γkΘk1(cid:3),

1
2σ2

F

v kV k2
(10)

where | · | denotes the determinant of a matrix. In Eq. (10),
the model parameters are learned without prior information
about users’ social relationships.

Incorporating Prior Social Information
Previous studies have demonstrated that user’ explicit so-
cial relationships [Yang et al., 2014] and implicit social re-
lationships [Ma, 2013] can help improve the recommenda-
tion accuracy. These prior social information (i.e., explicit
and implicit social relationships) contains prior knowledge
about users’ social dependency in a recommendation task.

1 The density function for a random matrix X following the

MVN distribution MN a×b(M, A, B) is

p(X) =

exp(− 1

2 tr(cid:2)B−1(X − M )⊤A−1(X − M )(cid:3))

(2π)ab/2|B|a/2|A|b/2

.

Let p0(U ) denote the MVN distribution of the user latent fea-
tures U derived from the prior information as follows:

p0(U ) = MN m×d(cid:0)0, Σ, I(cid:1),

(11)

where Σ ∈ Rm×m is the prior row covariance matrix.
If
users’ explicit social relationships are available, the elements
of Σ can be deﬁned as follows:
Σik =(cid:26) cov(Ri∗, Rk∗)
where Ri∗ is the ith row in R; cov(x1, x2) denotes the covari-
ance between two observations; F (ui) denotes the set of ui’s
social friends. Moreover, if users’ explicit social relationships
are unavailable, we deﬁne the elements of Σ as follows:

if uk ∈ F(ui) or ui ∈ F(uk) or i=k,
otherwise,

0

Σik = cov(Ri∗, Rk∗).

(12)

min

To exploit the prior information for recommendation, we
aims to minimize the distance between the learned MVN dis-
u) and the MVN distribution p0(U ) de-
tribution p(U|Θ, σ2
rived from prior information. This is achieved by minimiz-
ing the differential relative entropy between p(U|Θ, σ2
u) and
p0(U ) as follows:
Θ≻0Z p0(U ) log
tr(cid:2)(Θ +
tr(cid:0)ΘΣ(cid:1) −

p0(U )
p(U|Θ, σ2
u)
d
2

log(cid:12)(cid:12)(Θ +

The differential relative entropy in Eq. (13) can be obtained
following the ideas in [Dhillon, 2007].

I)Σ(cid:12)(cid:12) −

I)Σ(cid:3) −

log |(Θ +

1
σ2
u
1
2

md
2

d
2
1
2

1
σ2
u

1
σ2
u

=

∝

I)|.

(13)

A Uniﬁed Model
Considering both the constraints in Eq. (10) and Eq. (13), we
formulate the ﬁnal objective function of the proposed PRMF
model as follows:

min

U,V,Θ≻0
α

1
2kW ⊙ (R − U V ⊤)k2

F +

λu
2 kUk2

F +

λv
2 kV k2

F

+

2(cid:8)tr(cid:2)Θ(U U⊤ + βΣ)(cid:3) − (d + β) log(cid:12)(cid:12)Θ +

λu
α

I(cid:12)(cid:12) + γkΘk1(cid:9),

(14)

where λu = σ2/σ2
v, and α = σ2; β is the pa-
rameter that controls the contribution from prior information.

u, λv = σ2/σ2

3.2 Optimization Algorithm
The optimization problem in Eq. (14) can be solved by using
the following alternating algorithm.
Optimize U and V : By ﬁxing Θ, the optimization problem
in Eq. (14) becomes:

min
U,V

1
2kW ⊙ (R − U V ⊤)k2

F +

λu
2 kUk2

F +

λv
2 kV k2

F

+

α
2

tr(U⊤ΘU ).

(15)

Algorithm 1: PRMF Optimization Algorithm

Input
Output: U , V , Θ

: D, Σ, d, λu, λv, α, β, γ, θ, ρ

X ← SV D(Σ, d);

1 if β > 0 then
2
3 Initialize U and V randomly, and set Θ = I;
4 for iter = 1, 2, . . . , max iter do
5
6
7

for t = 1, 2, . . . , T do

else

U ; τ = γ
d ;

if β = 0 then

foreach (ui, vj, Rij) ∈ D do
Ui ← Ui + θ(cid:0)∆ij Vj − λuUi − αΘi∗U(cid:1);
Vj ← Vj + θ(cid:0)∆ij Ui − λvVj(cid:1);
bU = 1√d
bU = [
α bUbU⊤;
Z 0 = Θ; Y 0 = 0; E = I − λu
ρbU⊤bU )−1bU⊤;
P = I − 1
for t = 0, 1, . . . , K − 1 do
ρ(cid:3);
Θt+1 = sof t(cid:2)Z t − Y t, τ
Z t+1 = P ( 1
ρ E + Θt+1 + Y t);
Y t+1 = Y t + Θt+1 − Z t+1;

ρbU (I + 1

√β
√d+β X]; τ = γ

1√d+β U,

d+β ;

Θ = ΘK;

8

9

10

11

12

13

14

15
16

17

18

19

where Y is a scaled dual variable and ρ > 0. We can obtain
the following ADMM updates:

(20)

Θt+1 = arg min

Z t+1 = arg min

ρ
2kΘ − Z t + Y tk2
ρ
2kΘt+1 − Z + Y tk2

F + τkΘk1,

F + tr(Z⊤CZ) − tr(EZ),

(24)

(25)
(26)

(27)

(28)

The optimization problem in Eq. (15) can be solved using the
SGD algorithm [Koren et al., 2009]. The updating rules used
to learn the latent features are as follows:

Ui ← Ui + θ(cid:0)∆ij Vj − λuUi − αΘi∗U(cid:1)
Vj ← Vj + θ(cid:0)∆ij Ui − λvVj(cid:1),

(16)
where ∆ij = Rij − UiV ⊤j , Θi∗ is the ith row of Θ, and θ is
the learning rate. Note the SGD updates are only performed
on the observed rating pairs D = {(ui, vj, Rij)|Wij > 0}.
Optimize Θ: By ﬁxing U and V , the optimization problem
with respect to Θ is as follows:

tr(cid:2)Θ(U U⊤ + βΣ)(cid:3) − (d + β) log |Θ +

I| + γkΘk1.
min
Θ≻0
(17)
The optimization problem in Eq. (17) is convex with respect

λu
α

1

λu
α

I)−1 −

to Θ. Therefore, the optimal solutionbΘ to Eq. (17) satisﬁes:
(bΘ +
where bG is the sub-gradient, and the elements of bG are in
[−1, 1]. Therefore, following the derivation of Dantzig esti-
mator [Candes and Tao, 2007], we drop the constraint Θ ≻ 0
and consider the following optimization problem:

d + βbG,

(U U⊤ + βΣ) =

d + β

(18)

γ

min kΘk1 s.t.
1

λu
α

(cid:13)(cid:13)(Θ +

By multiplying Θ + λu
following relaxation of Eq. (19):

γ

.

(19)

d + β
α I with the constraint, we obtain the

d + β

I)−1 −

(U U⊤ + βΣ)(cid:13)(cid:13)∞ ≤
minkΘk1 s.t. (cid:13)(cid:13)CΘ − E(cid:13)(cid:13)∞ ≤ τ,

1

d+β (U U⊤ + βΣ), E = I − λu

α C,
where C =
and τ = γ
d+β . This relaxation has been used in the
constrained ℓ1-minimization for inverse matrix estimation
(CLIME) [Cai et al., 2011].
the optimization in
Eq. (20) is equivalent to the following optimization problem:

Indeed,

min

Θ

tr(cid:0)Θ⊤CΘ(cid:1) − tr(EΘ) + τkΘk1.

symmetric. We use the following symmetrization step to ob-

Let eΘ be the solution of Eq. (21), which is not necessarily
tain the ﬁnalbΘ,
bΘik = bΘki = eΘik I(|eΘik| ≤ |eΘki|) +eΘkiI(|eΘik| > |eΘki|),
(22)
where I(·) is an indicator function, which is equal to 1 if the
condition is satisﬁed, and otherwise 0. In addition, although
constraint Θ ≻ 0 is not imposed in Eq. (21), the symmetrized
bΘ is positive deﬁnite with high probability and converge to
the solution of Eq. (17) under the spectral norm, guaranteed
by Remark 1 and Theorem 1 in [Liu and Luo, 2014].

The optimization problem in Eq. (21) can be solved using
the ADMM algorithm [Boyd et al., 2011]. The augmented
Lagrangian of Eq. (21) is as follows:

Lρ(Θ, Z, Y ) =tr(cid:0)Z⊤CZ(cid:1) − tr(EZ) + τkΘk1 + hY, Θ − Zi

(23)

+

ρ
2kΘ − Zk2
F ,

(21)

Y t+1 = Y t + Θt+1 − Z t+1.
The closed solution to Eq. (24) is as follows:

where

τ

Θt+1 = sof t(cid:2)Z − Y,

ρ(cid:3),
sof t(cid:2)A, λ(cid:3) =( Aij − λ if Aij > λ,

Aij + λ if Aij < −λ,
otherwise.

0

The solution to Eq. (25) is as follows:

Z = (

1
ρ

C + I)−1(

1
ρ

E + Θ + Y ).

(29)

The time complexity of the inverse operation in Eq. (29) is
O(m3). As Σ is symmetric, to improve the computation efﬁ-
ciency, we consider the following approximation Σ ≈ XX⊤,
where X ∈ Rm×d. In this paper, we obtain X by factorizing
Σ using SVD and choosing the singular vectors with respect
to the top d largest singular values (see line 2 in Algorithm 1).

1

1

(

1
ρ

1√d+β U,

ρbU (I +

C + I)−1 ≈ I −

√β
Let bU = [
√d+β X] ∈ Rm×2d. When the prior co-
variance matrix Σ is not available, we set bU = 1√d
U . Then,
C ≈ bUbU⊤. Following Woodbury matrix identity, we have
ρbU⊤bU )−1bU⊤.

Using this matrix operation, the time complexity of the up-
date of Z in Eq. (29) becomes O(dm2). The details of the
proposed optimization algorithm are summarized in Algo-
rithm 1. At each iteration, the time complexity of the SGD
updates is O(T · |D| · ¯m · d), where ¯m denotes the average
number of nonzero elements in each row of Θ. Thus, the
sparsity of Θ can help improve the computation efﬁciency of
the proposed method. The time complexity of the ADMM
updates is O(K · d · m2). Empirically, we set T = 30 and
K = 30 in our experiments.

(30)

4 Experiments

In this section, we conduct empirical experiments on real
datasets to demonstrate the effectiveness of the proposed rec-
ommendation method.

4.1 Experimental Setting

Dataset Description
The experiments are performed on four public datasets:
MovieLens-100K2, MovieLens-1M3, Ciao, and Epinions4.
MovieLens-100K contains 100,000 ratings given by 943
users to 1,682 movies. MovieLens-1M consists of 1,000,209
ratings given by 6,040 users to 3,706 movies. We denote these
two datasets by ML-100K and ML-1M, respectively. For the
Ciao and Epinions datasets, we remove the items that have
less than 3 ratings. Finally, on Ciao dataset, we have 185,042
ratings given by 7,340 users to 21,881 items. On Epinions
dataset, there are 642,104 ratings given by 22,112 users to
59,104 items. The densities of the rating matrices on these
datasets are 6.30% for ML-100K, 4.47% for ML-1M, 0.12%
for Ciao, and 0.05% for Epinions. Moreover, on Ciao and
Epinions datasets, we have observed 111,527 and 353,419 so-
cial relationships between users. The densities of the social
relation matrices are 0.21% for Ciao and 0.07% for Epinions.
Table 1 summarizes the details of the experimental datasets.
For each dataset, 80% of the observed ratings are randomly
sampled for training, and the remaining 20% observed ratings
are used for testing.

Evaluation Metrics
The performances of the recommendation algorithms are
evaluated by two most popular metrics: mean absolute error
(MAE) and root mean square error (RMSE). The deﬁnitions

2http://grouplens.org/datasets/movielens/100k/
3http://grouplens.org/datasets/movielens/1m/
4http://www.public.asu.edu/ jtang20/datasetcode/truststudy.htm

Table 1: The statistics of the experimental datasets.

# Users
# Items
# Ratings
Rating Density
# Social Rel.
Social density

ML-100K ML-1M
6,040
3,706

943
1682

100,000
6.30%
N.A.
N.A.

1,000,209

4.47%
N.A.
N.A.

Ciao
7,340
21,881
185,042
0.12%
111,527
0.21%

Epinions
22,112
59,104
642,104
0.05%
353,419
0.07%

Table 2: Performance comparisons on datasets without users’
explicit social relationships.

Dataset

Method

RMSE

MAE

ML-100K

ML-1M

PMF
SRimp
PRMF
PRMFimp
PMF
SRimp
PRMF
PRMFimp

0.9251±0.0021
0.9205±0.0013
0.9157±0.0006
0.9132±0.0003
0.8728±0.0029
0.8621±0.0008
0.8592±0.0009
0.8572±0.0009

0.7314±0.0017
0.7290±0.0008
0.7226±0.0005
0.7210±0.0005
0.6807±0.0023
0.6739±0.0006
0.6738±0.0010
0.6727±0.0010

of MAE and RMSE are as follows:

|Rij − ˆRij|,

(31)

(Rij − ˆRij)2, (32)

M AE =

RM SE = vuut

1

|Dtest| X(ui,vj)∈Dtest
|Dtest| X(ui,vj )∈Dtest

1

where Rij denotes the observed rating in the testing data, ˆRij
is the predicted rating, and |Dtest| is the number of tested
ratings. From the deﬁnitions, we can notice that lower MAE
and RMSE values indicate better recommendation accuracy.
Evaluated Recommendation Methods
We compare the following recommendation methods:
(1)
PMF: This is the probabilistic matrix factorization model in-
troduced in Section 2.1; (2) SRimp: This is the SR method
that exploits users’ implicit social relationships for recom-
mendation [Ma, 2013]; (3) SRexp: This is the SR method
that exploits users’ explicit social relationships for recom-
mendation [Ma et al., 2011]; (4) LOCABAL: This is the so-
cial recommendation model proposed in [Tang et al., 2013],
which exploits both local and global social context for rec-
ommendation; (5) eSMF: This is the extended social ma-
trix factorization model that extends LOCABL to consider
the graph structure of social neighbors for recommenda-
tion [Hu et al., 2015]; (6) PRMF: This method learns users’
social dependency without prior information about users’ so-
cial relationships. The objective function is Eq. (10); (7)
PRMFimp: This is the proposed method that exploits users’
implicit social relationships as the prior knowledge used to
learn users’ social dependency; (8) PRMFexp: This is the
proposed method that exploits users’ explicit social relation-
ships to learn the social dependency between users.
Parameter Settings
We adopt cross-validation to choose the parameters for the
evaluated algorithms. The validation data is constructed by

0.94

0.93

E
S
M
R

0.92

0.91

0.90

0
0

1

0.5

Θ

f
o

y
t
i
s
r
a
p
S

E
S
M
R

1.036

1.034

1.032

1.030

1.028

0
10
10

1.026

0
0

1

0.8

0.6

0.4

0.2

Θ

f
o

y
t
i
s
r
a
p
S

0
10
10

RMSE
Sparsity

10−4 10−3 10−2 10−1 0.3
10−4 10−3 10−2 10−1 0.3
γ

0.5
0.5

0.7
0.7

0.9
0.9

1.0
1.0

(b) Ciao

RMSE
Sparsity

10−4 10−3 10−2 10−1 0.3
10−4 10−3 10−2 10−1 0.3
γ

0.5
0.5

0.7
0.7

0.9
0.9

1.0
1.0

(a) ML-100K

Figure 1: Performance trend of PRMFimp on the ML-100K and Ciao datasets measured by RMSE with different settings of γ.

Table 3: Performance comparisons on datasets with users’
explicit social relationships.

Dataset

Method

RMSE

MAE

Ciao

Epinions

PMF
SRimp
SRexp
LOCABAL
eSMF
PRMF
PRMFimp
PRMFexp
PMF
SRimp
SRexp
LOCABAL
eSMF
PRMF
PRMFimp
PRMFexp

1.1031±0.0045
1.0597±0.0042
1.0286±0.0029
1.0777±0.0039
1.0592±0.0043
1.0326±0.0030
1.0305±0.0030
1.0258±0.0035
1.1613±0.0022
1.1321±0.0054
1.1263±0.0027
1.1289±0.0008
1.1306±0.0025
1.1097±0.0024
1.1081±0.0023
1.1085±0.0024

0.8439±0.0034
0.8234±0.0036
0.7951±0.0020
0.8330±0.0022
0.8122±0.0010
0.8006±0.0023
0.7993±0.0023
0.7980±0.0025
0.8932±0.0019
0.8874±0.0016
0.8795±0.0023
0.8734±0.0010
0.8749±0.0025
0.8703±0.0022
0.8691±0.0022
0.8695±0.0022

randomly chosen 10% of the ratings in the training data.
For matrix factorization methods, we set the dimensional-
ity of the latent space d to 10. The latent features of users
and items are randomly initialized by a Gaussian distribu-
tion with mean 0 and standard deviation 1/√d. Moreover,
we set the regularization parameters λu = λv and choose
the parameters from {10−5, 10−4,··· , 10−1}. For PRMF,
α is chosen from {2−5, 2−4,··· , 2−1}, θ is chosen from
{2−5, 2−4,··· , 2−1}. Moreover, we set γ = 10−4, β = 10,
and ρ = 100. For SR methods, the regularization parameter
α is chosen from {2−7, 2−6,··· , 20}. The user similarity is
computed using Pearson correlation coefﬁcient, and we set
the threshold of the user similarity at 0.75 and N = 10, fol-
lowing [Ma, 2013]. The parameters of LOCABAL and eSMF
are set following [Tang et al., 2013] and [Hu et al., 2015].

4.2 Summary of Experiments
Table 2 summarizes the experimental results on the datasets
without users explicit social relationships, and Table 3 sum-
marizes the results on the datasets with users’ explicit social
relationships. We make the following observations:
• On all datasets, PRMF outperforms PMF by 0.94% on
ML-100K, 1.36% on ML-1M, 7.05% on Ciao, and 5.16%

on Epinions, in terms of RMSE. This indicates the recom-
mendation accuracy can be improved by jointly learning
users’ preferences and users’ social dependency.

• Compared with SRimp, PRMFimp achieves better results
on all datasets. For example, in terms of RMSE, PRMFimp
outperforms SRimp by 0.73%, 0.49%, 2.92%, and 2.40%,
respectively. This shows that users’ social dependency
learned from the rating data is more effective than the user
dependency predeﬁned based on users’ implicit social re-
lationships, for improving the recommendation accuracy.
• The proposed PRMFexp method outperforms the state-of-
the-art social recommendation methods that exploits users’
explicit social relationships for recommendation. On the
Ciao and Epinions datasets, the average improvements of
PRMFexp over SRexp, LOCABAL, and eSMF, in terms of
RMSE, are 1.03%, 3.60%, 2.77%, respectively. This again
demonstrates the effectiveness of the proposed method.

• PRMFimp and PRMFexp outperforms PRMF on all
datasets. This indicates the prior information (e.g., users’
explicit and implicit social relationships) are beneﬁcial in
improving recommendation accuracy. However, the im-
provements are not very signiﬁcant. One potential reason
is the SVD factorization used in Algorithm 1 (line 2) may
not accurately approximate the prior covariance matrix.

In addition, we also study the impact of the sparsity of
the learned social dependency matrix Θ on the recommen-
dation accuracy. We choose the regularization parameter
γ from {0, 10−4, 10−3, 10−2, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 10}.
Figure 1 shows the performance trend of PRMFimp on the
ML-100K and Ciao datasets, in terms of RMSE. Observed
that the sparsity of Θ increases with the increase of γ. As
shown in Figure 1(a), denser social dependency matrix gen-
erally achieves better recommendation accuracy. However,
denser social dependency matrix cause more computation
time used to learn the user latent features. Indeed, there ex-
ists some balance between the computation efﬁciency and the
recommendation accuracy. For example, on the ML-100K
dataset, by setting γ to 0.3, the sparsity of the learned Θ is
65.21%, and the RMSE value is 0.9149, which is 0.56% bet-
ter than the best competitor SRimp. Moreover, Figure 1(b)
also indicates that better recommendation accuracy may be
achieved by learning a sparse Θ. For example, on the Ciao
dataset, the best recommendation accuracy is achieved by set-

ting γ to 0.1, and the sparsity of the learned Θ is 79.74%.
On the ML-1M and Epinions datasets, we have similar ob-
servations with that on the ML-100K dataset. Due to space
limitation, we do not report those results here.

5 Conclusion and Future Work
In this paper, we propose a novel social recommendation
method, named probabilistic relational matrix factorization
(PRMF). For a speciﬁc recommendation task, the proposed
approach jointly learns users’ preferences and the optimal so-
cial dependency between users, to improve the recommenda-
tion accuracy. Empirical results on real datasets demonstrate
the effectiveness of PRMF, in comparison with start-of-the-
art social recommendation algorithms.

The future work will focus on the following potential direc-
tions. First, we would like to develop more efﬁcient optimiza-
tion algorithms for PRMF, based on the parallel optimization
framework proposed in [Wang et al., 2013]. Second, we are
also interested in extending PRMF to solve the top-N item
recommendation problems.

References
[Boyd et al., 2011] Stephen Boyd, Neal Parikh, Eric Chu,
Borja Peleato, and Jonathan Eckstein. Distributed opti-
mization and statistical learning via the alternating direc-
tion method of multipliers. Foundations and Trends R(cid:13) in
Machine Learning, 3(1):1–122, 2011.
[Cai et al., 2011] Tony Cai, Weidong Liu, and Xi Luo. A
constrained ℓ1 minimization approach to sparse precision
matrix estimation. Journal of the American Statistical As-
sociation, 106(494):594–607, 2011.

[Candes and Tao, 2007] Emmanuel Candes and Terence Tao.
The dantzig selector: statistical estimation when p is much
larger than n. The Annals of Statistics, pages 2313–2351,
2007.

[Dhillon, 2007] JVDI Dhillon. Differential entropic cluster-

ing of multivariate gaussians. In NIPS’07, 2007.

[Guo et al., 2015] Guibing Guo, Jie Zhang, and Neil Yorke-
Smith. Trustsvd: Collaborative ﬁltering with both the ex-
plicit and implicit inﬂuence of user trust and of item rat-
ings. In AAAI’15, 2015.

[Hu et al., 2015] Guang-Neng Hu, Xin-Yu Dai, Yunya Song,
Shu-Jian Huang, and Jia-Jun Chen. A synthetic approach
for recommendation: combining ratings, social relations,
and reviews. In IJCAI’15, pages 1756–1762. AAAI Press,
2015.

[Jamali and Ester, 2009] Mohsen Jamali and Martin Ester.
Trustwalker: a random walk model for combining trust-
based and item-based recommendation. In KDD’09, pages
397–406. ACM, 2009.

[Jamali and Ester, 2010] Mohsen Jamali and Martin Ester. A
matrix factorization technique with trust propagation for
recommendation in social networks. In RecSyS’10, pages
135–142. ACM, 2010.

[Koren et al., 2009] Yehuda Koren, Robert Bell, and Chris
Volinsky. Matrix factorization techniques for recom-
mender systems. Computer, (8):30–37, 2009.

[Koren, 2008] Yehuda Koren. Factorization meets the neigh-
borhood: a multifaceted collaborative ﬁltering model. In
KDD’08, pages 426–434. ACM, 2008.

[Li et al., 2015] Hui Li, Dingming Wu, Wenbin Tang, and
Nikos Mamoulis. Overlapping community regularization
for rating prediction in social recommender systems.
In
RecSyS’15, pages 27–34. ACM, 2015.

[Linden et al., 2003] Greg Linden, Brent Smith, and Jeremy
York. Amazon. com recommendations: Item-to-item col-
laborative ﬁltering.
Internet Computing, IEEE, 7(1):76–
80, 2003.

[Liu and Luo, 2014] Weidong Liu and Xi Luo.

High-
dimensional sparse precision matrix estimation via sparse
column inverse operator. Journal of Multivariate Analysis,
2014.

[Ma et al., 2011] Hao Ma, Dengyong Zhou, Chao Liu,
Michael R Lyu, and Irwin King. Recommender systems
with social regularization. In WSDM’11, pages 287–296.
ACM, 2011.

[Ma, 2013] Hao Ma. An experimental study on implicit so-
cial recommendation. In SIGIR’13, pages 73–82. ACM,
2013.

[Ma, 2014] Hao Ma. On measuring social friend interest
similarities in recommender systems. In SIGIR’14, pages
465–474. ACM, 2014.

[Mnih and Salakhutdinov, 2007] Andriy Mnih and Ruslan
In

Probabilistic matrix factorization.

Salakhutdinov.
NIPS’07, pages 1257–1264, 2007.

[Su and Khoshgoftaar, 2009] Xiaoyuan Su and Taghi M
Khoshgoftaar. A survey of collaborative ﬁltering tech-
niques. Advances in Artiﬁcial Intelligence, 2009:4, 2009.
[Tang et al., 2013] Jiliang Tang, Xia Hu, Huiji Gao, and
Huan Liu. Exploiting local and global social context for
recommendation. In IJCAI’13, pages 2712–2718. AAAI
Press, 2013.

[Wang et al., 2013] Huahua Wang, Arindam Banerjee, Cho-
Jui Hsieh, Pradeep K Ravikumar, and Inderjit S Dhillon.
Large scale distributed sparse precision estimation.
In
NIPS’13, pages 584–592, 2013.

[Yang et al., 2012] Xiwang Yang, Harald Steck, and Yong
Liu. Circle-based recommendation in online social net-
works. In KDD’12, pages 1267–1275. ACM, 2012.

[Yang et al., 2014] Xiwang Yang, Yang Guo, Yong Liu, and
Harald Steck. A survey of collaborative ﬁltering based
social recommender systems. Computer Communications,
41:1–10, 2014.

[Yu et al., 2011] Le Yu, Rong Pan, and Zhangfeng Li. Adap-
tive social similarities for recommender systems. In Rec-
SyS’11, pages 257–260. ACM, 2011.

[Zhang and Yeung, 2010] Yu Zhang and Dit-Yan Yeung. A
convex formulation for learning task relationships in
multi-task learning. In UAI’10, pages 733–742, 2010.

[Zhang et al., 2013] Xingang Zhang, Qijie Gao, Christo-
pher S.G. Khoo, and Amos Wu. Categories of friends
on social networking sites: An exploratory study.
In A-
LIEP’13, 2013.

