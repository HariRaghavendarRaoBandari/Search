Estimating Non-Simpliﬁed Vine Copulas Using

Penalized Splines

Christian Schellhase∗

Fabian Spanhel†

7th March 2016

Abstract

Vine copulas (or pair-copula constructions) have become an important tool
for high-dimensional dependence modeling. Typically, so called simpliﬁed vine
copula models are estimated where bivariate conditional copulas are approxi-
mated by bivariate unconditional copulas. We present the ﬁrst non-parametric
estimator of a non-simpliﬁed vine copula that allows for varying conditional copu-
las. To overcome the curse of dimensionality, we approximate conditional copulas
with more than one conditioning argument by a conditional copula with the ﬁrst
principal component as conditioning argument. Using penalized hierarchical B-
splines we can directly estimate conditional copulas by setting linear restrictions
on the spline coeﬃcients. An extensive simulation study is conducted, showing a
substantial improvement in the out-of-sample Kullback-Leibler divergence if the
variation in the conditional copula is not negligible. An application to telescope
data further demonstrates the potential beneﬁt that can be achieved when con-
ditional copulas are modeled.

Keywords: Vine, Pair-copula, Simplifying Assumption, Conditional Copula,

Penalized Spline.

1

Introduction

6
1
0
2

 
r
a

M
4

 

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
4
2
4
1
0

.

3
0
6
1
:
v
i
X
r
a

Simpliﬁed vine copulas, or pair-copula constructions, have become a very active ﬁeld of
research over the last decade (Bedford and Cooke (2002); Aas et al. (2009); Kurowicka and Joe
(2011); Kauermann and Schellhase (2014); Brechmann et al. (2012); Spanhel and Kurz
(2015a)). Their popularity stems from their simplicity and ﬂexibility. Simpliﬁed vine

∗Centre for Statistics, Bielefeld University, Department of Business Administration and Economics,

Germany. E-mail: cschellhase@wiwi.uni-bielefeld.de

†Department of Statistics, Ludwig-Maximilians-Universit¨at M¨unchen, Germany.

E-mail:

spanhel@stat.uni-muenchen.de

1

copula models decompose the complex modeling of a high-dimensional copula into the
hierarchical modeling of several bivariate unconditional copulas. These bivariate uncon-
ditional copulas are often called pair-copulas and can be chosen arbitrarily. Due to this
fact, simpliﬁed vine copula models give rise to very ﬂexible multivariate copula models
which are often found to be superior to other multivariate copula models (Aas and Berg,
2009; Fischer et al., 2009). Moreover, if the data generating process (dgp) satisﬁes the
simplifying assumption (Hobæk Haﬀ et al., 2010), the dgp can actually be represented
by a simpliﬁed vine copula.

However, in general, a simpliﬁed vine copula model is only an approximation of
a multivariate copula. That is because bivariate conditional copulas are typically re-
quired as building blocks to express an arbitrary copula by a vine copula. A simpliﬁed
vine copula model then approximates the conditional copulas by bivariate unconditional
copulas, see Spanhel and Kurz (2015a) for a detailed investigation of such approxima-
tions. Depending on the dgp, the goodness of such a approximation can be adequate
or insuﬃcient. For theses reasons, the modeling of conditional copulas might improve
the modeling of high-dimensional copulas.

The modeling of a conditional copula, without reference to the vine copula frame-
work, has been investigated by several authors. Acar et al. (2011); Abegaz et al. (2012)
and Vatter and Chavez-Demoulin (2015), investigate the semi-parametric estimation
of a conditional copula. That is, a parametric copula family is speciﬁed and its de-
pendence parameter is treated as a non-parametric function of the conditioning ar-
gument(s). Gijbels et al. (2011); Veraverbeke et al. (2011); Gijbels et al. (2012) and
Fermanian and Wegkamp (2012), apply kernel-methods to extract a conditional copula
from a conditional distribution function. The literature on estimating conditional cop-
ulas within the vine copula framework is less developed. In the three-dimensional case,
Acar et al. (2012) propose a local likelihood estimator for a parametric conditional cop-
ula with one conditioning argument. Lopez-Paz et al. (2013) utilize a Gaussian process
to model the parametric conditional copulas of a higher-dimensional vine copula. The
approaches of Acar et al. (2012) and Lopez-Paz et al. (2013) are both semi-parametric
and require the choice of a parametric copula family. The selection of the copula fam-
ily is computationally expensive because one has to ﬁt the conditional copula for each
copula family in order to choose the best family. Moreover, it is also possible that
the conditional copula can not be approximated by a parametric copula family with a
varying dependence parameter.

The objective of this paper is to construct a non-parametric non-simpliﬁed vine
copula estimator to improve the approximation of a multivariate distribution. For
this purpose, we apply penalized B-splines to model the conditional copulas of a non-
simpliﬁed vine copula. We show how penalized B-splines can be used to directly es-
timate a conditional copula so that there is no need to extract a conditional copula
from a previously ﬁtted conditional distribution as it is the case in the kernel-based

2

approaches of Gijbels et al. (2011); Veraverbeke et al. (2011); Gijbels et al. (2012) and
Fermanian and Wegkamp (2012). A direct model of the conditional copula is beneﬁ-
cial when it comes to simulations, because the extraction of a conditional copula is
computationally expensive if one wants to simulate from the ﬁtted vine copula model.
In order to tackle the curse of dimensions, we approximate a bivariate conditional
copula with a vector of conditioning arguments by a bivariate conditional copula with
the ﬁrst principal component of the conditioning variables as the conditioning argu-
ment. Simulation studies show that our approach can outperform parametric and non-
parametric simpliﬁed vine copula estimators if the variation in the conditional copulas is
not negligible. Moreover, an application to telescope data reveals a major improvement
if conditional copulas are modeled. To the best of our knowledge, this paper presents the
ﬁrst eﬀort to model a non-simpliﬁed vine copula using non-parametric methods. The
rest of the paper is structured as follows. Section 2 discusses (simpliﬁed) vine copulas
and conditional copulas. In Section 3 we construct an estimator for conditional copula
densities using penalized B-splines. We describe the estimation of non-simpliﬁed D-vine
copulas using conditional copulas in Section 4. Simulation results and an application
are presented in Section 5. A discussion in Section 6 closes the article.

2 Background on (simpliﬁed) vine copulas and con-

ditional copulas

Consider a continuous p-dimensional random vector X = (X1, . . . , Xp). Following
Sklar’s 1959 theorem, we can write the distribution of X as

F1:p(x1, . . . , xp) = C1:p{F1(x1), . . . , Fp(xp)},

where C1:p is the copula of X and Fi is the marginal distribution of Xi, i = 1, . . . , p.
Assuming that X has a density f1:p, we can express the density f1:p using the copula
density c1:p and the marginal densities fi, i = 1, . . . , p, by

f1:p(x1, . . . , xp) = c1:p {F1(x1), . . . , Fp(xp)}

fj(xj).

p

Yj=1

Regular vine copulas (Bedford and Cooke, 2002) provide a functional decomposition of
a copula into a sequence of bivariate conditionals copulas. For simplicity, we consider
in the following D-vine copulas which are subset of regular vine copulas. In this case
the density f1:p of X is represented by

f1:p(x1, . . . , xp) =

p−1

p−j

Yj=1

Yi=1

ci,i+j|sij{ui|sij , ui+j|sij|xsij }

fj(xj),

p

Yj=1

3

where sij = i + 1, . . . , i + j − 1, uk|sij = Fk|sij (xk|xsij ) for k = i, i + j, and ci,i+j|sij = ci,i+1
for j = 1. D-vine copulas can be considered as an ordered sequence of trees, where
j refers to the number of the tree and a bivariate conditional copula Ci,i+j|sij (Patton
(2006)) is assigned to each of the K − j edges of tree j. The conditional copula Ci,i+j|sij
is deﬁned by Ci,i+j|sij(a, b|xsij ) := P(Fi|sij (Xi|Xsij ) ≤ a, Fi+j|sij(Xi+j|Xsij ) ≤ b|Xsij =
xsij ), where (a, b, xsij ) ∈ [0, 1]2 × Rj−1. Note that each bivariate conditional copula
Ci,i+j|sij is in general a function of j + 1 variables. Due to their general form, D-vine
copulas do not provide a feasible model framework without further assumptions. For
this reason, one commonly assumes that the dgp satisﬁes the simplifying assumption.
That is, one assumes that Ci,i+j|sij(·, ·|xsij) does not depend on xsij for all j = 1, . . . , p−
1, i = 1, . . . , d − j. As it has been shown by Kurz and Spanhel (2016), this is equivalent
to assuming that Ci,i+j|sij is equal to the corresponding j-th order partial copula C PVC
i,i+j; sij
that is associated with the partial vine copula of X. Thus, if the simplifying assumption
is true, the joint density of a p-dimensional copula collapses to a product of p(p − 1)/2
unconditional bivariate copula densities, i.e.,

p−1

p−j

p

f1:p(x1, . . . , xp) =

cPVC
i,i+j; sij{uPVC

i|sij , uPVC

i+j|sij}

fj(xj),

(1)

Yj=1

Yi=1

Yj=1

i,i+j; sij is a bivariate unconditional copula density and uPVC
k|sij

where cPVC
Fk|sij (xk|xsij ) for k = i, i + j, and cPVC
density in (1) is then the density of the partial vine copula of X.

i,i+j|sij

(xk|xsij ) =
= ci,i+1 for j = 1. The corresponding copula

= F PVC
k|sij

The validity of the simplifying assumption is true for the multivariate Gaussian,
Student-t and Clayton copula. However, it is not true in general (Hobæk Haﬀ et al.,
2010; St¨ober et al., 2013). If the simplifying assumption does not hold for the dgp, a
pair-copula construction that is based on bivariate unconditional copulas only approx-
imates the underlying copula. This might be a problem when the variation in the con-
ditional copulas is not negligible. Moreover, spurious (un)conditional (in)dependencies
may emerge, meaning that two random variables seem to be (un)conditionally indepen-
dent when actually they are not (Spanhel and Kurz, 2015a). Therefore, we intend to
weaken the simplifying assumption by modeling the conditional copulas of a vine copula.

Overcoming the curse of dimensions

It is not useful to model all conditional copulas without imposing any restrictions.
Without any constraints, a vine copula density decomposes the copula density into p−j
functions of j + 1 variables, where j = 1, . . . , p −1. Thus, the conditional copula density
in the last tree has the same dimensionality as c1:p and it would be more sensible to
directly estimate the p-dimensional copula density c1:p in this case. Moreover, if one
tries to consistently estimate each conditional copula Ci,i+j|sij, then one ultimately runs
into the curse of dimensions. Consequently, we have to impose constraints that are
weaker than the simplifying assumption but still yield a modeling framework which
overcomes the curse of dimensions.

4

We propose to approximate a conditional copula Ci,i+j|sij(·, ·|xsij) with j − 1 condi-
tioning arguments xsij by a conditional copula ˜Ci,i+j|uij(·, ·|vij) with one conditioning
argument vij, where vij is a function of xsij . We follow this approach because non-
parametric estimators of a bivariate conditional copula suﬀer greatly from the curse of
dimensions if the number of conditioning arguments is larger than one (Scott, 2008;
Nagler and Czado, 2015). Once the conditional copula density has more than one con-
ditioning argument, we use the ﬁrst principal component of the j − 1 conditioning
arguments xsij as the scalar conditioning argument vij.

and n ≥ k, let uk,l = (n+1)−1Pn

To ﬁx ideas, for k ∈ sij and l = 1, . . . , n, where n is the size of the observed sample
i=1 11xk,i≤xk,l be the standardized rank of observation xk,l.
Moreover, let ¯uk be the sample mean of uk = (uk,l)l=1,...,n so that ˜uk = uk − ¯uk has zero
mean. Applying a principal component analysis to ˜usij = (˜uk)k∈sij gives the values of
the ﬁrst principal component (vij,l)l=1,...,n. The standardized ranks of the ﬁrst principal
component (vij,l)l=1,...,n are used to approximate Ci,i+j|sij(·, ·|xsij,l) by ˜Ci,i+j|sij(·, ·|vij,l)
for l = 1, . . . , n. Note that if j = 2 then vij = usij . Using the ﬁrst principal component
as the scalar conditioning variable for the conditional copula is an ad-hoc approach and
not derived from theoretical considerations. The main appeal of this approach is that it
is computationally cheap, a property which can be crucial for high-dimensional appli-
cations. Moreover, it performs satisfactorily well in our simulations and applications.
Alternative approaches that reduce the dimension of the conditioning vector to a scalar
are deliberately left open for further research.

3 Penalized B-Spline Estimation of a Conditional

Copula Density

The estimation of p-dimensional unconditional copula densities by means of penalized
(hierarchical) B-splines has been investigated in Kauermann et al. (2013). We now
extend this approach to a bivariate conditional copula density with one conditioning
argument. For this purpose, we consider exemplary the conditional copula C12|3 of F12|3,
where F12|3 is the conditional cdf of (X1, X2) given X3. Recall that the conditional
copula density c12|3 has to satisfy the following constraints

∀(u1, u2, u3) ∈ (0, 1)3 : c12|3(u1, u2|u3) ≥ 0,

∀(u1, u2, u3) ∈ (0, 1)3 : Z 1

0

The ﬁrst constraint ensures that the conditional copula density is non-negative while the
second constraint guarantees that each marginal density is the density of the uniform
distribution. Note that the second constraint implies that c12|3 integrates to one, i.e.,

c12|3(u1, u2|u3) du1 =Z 1

0

c12|3(u1, u2|u3) du2 = 1.

(2)

(3)

∀u3 ∈ (0, 1) : Z[0,1]2

c12|3(u1, u2|u3) du1 du2 = 1.

5

The constraint given in (3) can be imposed by extracting the conditional copula density
from the underlying conditional density using

c12|3(u1, u2|u3) =

f12|3(F −1

1|3 (u1|u3), F −1

2|3 (u2|u3)|u3)

f1|3(F −1

1|3 (u1|u3)|u3)f2|3(F −1

2|3 (u2|u3)|u3)

.

1|3 and F −1

1|3 and F −1

For the extraction of a conditional copula we have to estimate the conditional densi-
ties (f1|3, f2|3, f12|3) and compute the quantile functions F −1
2|3 . Note that the
quantile functions F −1
2|3 must be consistent with the conditional densities f1|3
and f2|3, i.e., they can not be estimated separately. Therefore, the quantile functions
are evaluated by inverting the conditional cdfs F1|3 and F2|3. In general, the inversion
of these conditional cdfs can only be accomplished by numerical inversion methods.
Thus, a repeated evaluation of the conditional copula density which is extracted from
a conditional density can computationally be rather expensive. For this reason, we
aim at modeling the conditional copula c12|3 directly using a mixture of B-spline basis
densities.

3.1 Sparse B-Spline Density Basis

Let uj = (uj,1, . . . , uj,n)′, τ = (τk)k=1,...,K be a tuple of equidistant knots with τ1 = 0
and τK = 1, and φτk(x), x ∈ [0, 1] be a regular linear univariate B-spline normalized

to be a density, i.e., R φτk(x) dx = 1. The K-dimensional univariate B-spline density

basis for uj is given by

B(τ )(uj) :=


φτ1(uj,1)

...

. . . φτK (uj,1)

...

φτ1(uj,n)

. . . φτK (uj,n)

.




The three-dimensional full tensor product for (u1, u2, u3) follows as

ΦK(u1, u2, u3) :=

B(τ )(uj).

3

Oj=1

and the corresponding approximation of the conditional copula density using B-splines
is given by

c12|3(u1, u2|u3; b) := ΦK(u1, u2, u3)b = X1≤k1,k2,k3≤K

bk1,k2,k3

φτkj

(uj),

(4)

3

Yj=1

where b := (bk1,k2,k3 : 1 ≤ k1, k2, k3 ≤ K) are the spline basis coeﬃcients. The goodness
of the approximation depends on K which determines the dimension of the basis ΦK and
therefore the number of coeﬃcients in b. Obviously, K can not be chosen too large as
the increasing amount of coeﬃcients boosts the computational demand dramatically.1

1 For instance, choosing K = 17 in (4) results in 4,913 coeﬃcients (see Table 2).

6

In order to handle the increasing amount of coeﬃcients for increasing K, we follow
Kauermann et al. (2013) and make use of Zenger’s so called sparse grids (Zenger, 1991)
to obtain a reduced basis to establish numerical feasibility.

To implement the approach, let the linear univariate B-spline density basis B(τ (d)) be

built upon 2d+1 equidistant knots which are collected in the tuple τ (d) = (τi(d))i=1,...,2d+1 =
(k2−d)k=0,...,2d. Note that the basis B(τ (d)) has dimension K = 2d + 1, i.e., the number
of knots depend on d. The full tensor product of the corresponding B-spline basis is
given by

Φ(d)(u1, u2, u3) =

3

Oj=1

B(τ (d))(uj).

(5)

We now transform B(τ (d))(uj) into its hierarchical representation (see Forsey and Bartels
1988, 1995). Deﬁne the hierarchical index sets I0 = {1, 2} and Il = {2j | j ∈ N, 1 ≤ j ≤
2l−1}, l = 1, . . . , d. Let B(τ (l))
(uj) denote the columns Il of Bτ (l)(uj).2 The univariate
hierarchical B-spline basis of degree d is deﬁned as

Il

˜B(τ (d))(uj) =(cid:16)B(τ (0))

I0

(uj), B(τ (1))

I1

(uj), . . . , B(τ (d))

Id

(uj)(cid:17) .

Figure 1 presents the univariate B-spline basis B(τ (d))(uj) and the building parts of the
corresponding hierarchical basis ˜B(τ (d))(uj) for d = 2. Let ˜e be a K-dimensional row
vector such that its k-th element is given by ˜ek := min{l = 0, . . . , d : k ≤ |τ (l)|}, e.g.,
˜e = (0, 0, 1, 2, 2) if d = 2. The vector ˜e denotes the hierarchical level of ˜B(τ (d))(uj) and
its k-th element identiﬁes the hierarchical level of the k-th column of ˜B(τ (d))(uj). By
construction, B(τ (d))(uj) and ˜B(τ (d))(uj) have full rank, i.e. B(τ (d))(uj) ˜A = ˜B(τ (d))(uj)
for some invertible K × K matrix ˜A, so that both univariate bases ˜B(τ (d))(uj) and
B(τ (d))(uj) span the same space. The three-dimensional hierarchical B-spline basis
follows as

(d)

˜Φ

(u1, u2, u3) :=

˜B(τ (d))(uj) =

B(τ (d))(uj) ˜A

(6)

3

Oj=1

3

Oj=1

and the corresponding approximation of the conditional copula density is given by

c12|3(u1, u2|u3; ˜b(d)) = ˜Φ

(d)

(u1, u2, u3)˜b(d),

˜A)−1b.

where ˜b(d) = (N3

j=1

To overcome the exponential increase in the number of spline coeﬃcients, we use
a three-dimensional sparse B-spline basis which reduces the dimension by deleting the

2 For l = 2, we get I2 = {2, 4}, τ (2) = (0, 0.25, 0.5, 0.75, 1), and

B(τ (2))

I2

(uj) = B(τ (2))(uj) 0

0

0 0
1 0

1 0

0 0!⊤

φ0.25(uj,1) φ0.75(uj,1)

...

...

φ0.25(uj,n) φ0.75(uj,n)

=


.




7

q ⌉ + βl−q(⌈ l

columns from the full tensor product basis whose cumulated hierarchy level exceeds D,
where d ≤ D ≤ 3d. The cumulated hierarchy level of the full tensor product basis is
deﬁned as follows. For α ∈ R1×n and β ∈ R1×q, deﬁne the l-th element of (α⊕β) ∈ R1×nq
by (α ⊕ β)l = α⌈ l
q ⌉−1), where ⌈·⌉ is the ceil function. Note that the operation
⊕ is associative. Recall that the k-th element of ˜e identiﬁes the hierarchy level of
the k-th column of ˜B(τ (d))(uj), so that the l-th element of ǫ = ˜e ⊕ ˜e ⊕ ˜e ∈ R1×K 3
denotes the cumulated hierarchy level of the l-th column of the hierarchical B-spline
basis ˜Φ
(u1, u2, u3). Deﬁne OD = {j = 1, . . . , K3 : ǫj ≤ D}, i.e., OD contains the
position of the columns of ˜Φ
(u1, u2, u3) whose cumulated hierarchy level does not
exceed D. Let OD(j) be the j-th smallest element of OD and deﬁne the orthogonal
matrix E(OD) ∈ RK 3×|OD| such that its (OD(j), j)-entry is one for j = 1, . . . , |OD|, and
the other entries are zero. The three-dimensional sparse B-spline basis follows as

(d)

(d)

(d,D)

˜Φ

(u1, u2, u3) =" 3
Oj=1

˜B(τ (d))(uj)# E(OD),

(7)

where the lower index d is the degree of the univariate hierarchical B-spline basis
and the upper index D, d ≤ D ≤ 3d, refers to the maximum cumulated hierarchy
level. Only the columns in the hierarchical B-spline basis ˜Φ(d)(u1, u2, u3) whose cu-
mulated hierarchy level does not exceed D constitute the three-dimensional sparse
B-spline basis ˜Φ
(u1, u2, u3). The corresponding spline coeﬃcients are given by
˜b(d,D) = E(OD)⊤ ˜b(d).

(d,D)

The reduction of the basis decreases the number of spline coeﬃcients and thereby
the numerical eﬀort as can be seen in Table 2 where the dimension of ˜Φ(d) and ˜Φ(d,D)
is shown for various values of d and D. Figure 2 shows the construction principle for
a bivariate sparse B-spline basis with hierarchy level d = 2. Note that ˜Φ(d,3d) = ˜Φ(d).
The approximation of the conditional copula density using the sparse B-spline basis in
(7) is given by

c12|3(u1, u2|u3; ˜b

(d,D)

) := ˜Φ(d,D)(u1, u2, u3)˜b(d,D) = X1≤k1,k2,k3≤K

P3

i=1 eki ≤D

˜b(d)

k1,k2,k3

3

Yj=1

˜B(τ (d))

kj

(uj),

(8)

kj

where ˜B(τ (d))
density, some constraints on the coeﬃcients ˜b
discuss later on.

(d,D)

(uj) is the kj-th column of ˜B(τ (d))(uj). To achieve a conditional copula

have to be formulated, which we

3.2 Estimation and Penalization

To construct the likelihood for the spline coeﬃcients b, and later on for ˜b
, assume
we observe an iid random sample xi = (xi1, xi2, xi3), i = 1, . . . , n, from which we obtain

(d,D)

8

ui = (ui1, ui2, ui3) using uij = ˆFj(xij), where ˆFj(·) is a (consistent) estimate of the
marginal distribution function. Let 1n = (1 . . . , 1)′ ∈ Rn×1. The log likelihood for b is
then

l(b) = 1⊤

n log{ΦK(u1, u2, u3)b} =

n

Xi=1

log( X1≤k1,k2,k3≤K

bk1,k2,k3

φτkj

(uj,i)) ,

3

Yj=1

(9)

which needs to be maximized subject to the constraints (2) and (3). The accuracy of the
spline approximation in (4) improves for large K, but the corresponding ﬁt will suﬀer
from estimation variability due to over-parameterization of the data. We brieﬂy present
the concept of penalization used in Kauermann et al. (2013), who impose a penalty on
the spline coeﬃcients b to achieve a smooth ﬁt. Eilers and Marx (1996) suggest to
penalize r-th order diﬀerences for the B-spline coeﬃcients, which easily extends to
the multivariate setting as shown in Marx and Eilers (2005). Let L ∈ R(K−r)×K be a
diﬀerence matrix of order r, e.g. for r = 1 we get

0

1 −1
0
...
0

· · ·
0
1 −1 · · ·
0
...
...
. . .
1 −1
0

...
0

L =




,




and let W = diag(w1, . . . , wK) be the weight matrix linking a regular B-spline basis to
a B-spline density basis, i.e. wl is the integral from 0 to 1 of the l-th regular B-spline.
By means of L we can now penalize diﬀerences in neighbouring spline coeﬃcients and
deﬁne the penalty matrix P = W L⊤LW , see also Wand and Ormerod (2008) and
Ruppert et al. (2003). This penalty applies only to a single dimension. To achieve
smoothness of the ﬁtted copula density for all variables, we use the Kronecker product
yielding the entire penalty matrix

P(λ) =

λjPj.

3

Xj=1

where λ = (λ1, λ2, λ3), Pj =(cid:16)Nj−1
by-component tensor product (with N0

l=1 is the component-
l=3+1 IK). The coeﬃcient λj is
the penalty parameter for the j-th variable which needs to be selected in a data driven
manner, as discussed later. Incorporating the penalty into the log-likelihood gives the
penalized log-likelihood

l=j+1 IK(cid:17), andNj−1

l=1 IK(cid:17)⊗P ⊗(cid:16)N3

l=1 IK = 1 = N3

lp(b, λ) = l(b) −

b⊤P(λ)b,

1
2

which is maximized for given λ with respect to b. Note that λ determines the amount
of smoothness for the ﬁtted coeﬃcients and setting λ = 0 gives the unpenalized ML
estimate.

9

The penalized log-likelihood (9) is now reformulated by replacing the B-spline basis
in (4) with their hierarchical representation in (6). Recall that ˜b(d) = (⊗3
b de-
notes the corresponding spline coeﬃcient vector for basis ˜Φ(d)(u1, u2, u3). The penalized
log-likelihood (10) in terms of ˜b(d) takes the form

˜A)

j=1

−1

˜lp(˜b(d), λ) = ˜l(˜b(d)) −

˜b(d)⊤ ˜P(λ)˜b(d),

1
2

where the log-likelihood is deﬁned as ˜l(˜b(d)) = 1⊤

n log{ ˜Φ(d)(u1, u2, u3)˜b(d)}. Further-

more, we deﬁne the penalty matrix ˜P(λ) =P3

j=1 λj ˜Pj with

˜Pj =  j−1
Ol=1

˜I(d)! ⊗ { ˜A⊤P ˜A} ⊗  3
Ol=j+1

˜I(d)! ,

and ˜I(d) = (W ˜A)⊤(W ˜A). Finally, the penalized log-likelihood using a sparse B-spline
basis equals

˜l(D)

p

(˜b(d,D), λ) = ˜l(D)(˜b(d,D)) −

˜b(d,D)⊤ ˜P(D)(λ)˜b(d,D)

(10)

1
2

with obvious deﬁnition for ˜l(D)(˜b(d,D)) and ˜P(D)(λ) = E(OD)⊤ ˜P(λ)E(OD). Note that
since ˜b(d,3d) = ˜b(d) we have ˜l(3d) = ˜l.

3.3 Constraints on the Parameters

We have to formulate constraints on the coeﬃcient vector ˜b(d,D) such that c12|3(u1, u2|u3; ˜b(d,D))
in (8) becomes a conditional copula density. The ﬁrst marginal density of c12|3(u1, u2|u3)
(see (3)) follows as

Z ˜Φ(d,D)(u1, u2, u3)˜b(d,D)du1 = X1≤k2,k3≤K

˜b(d)

k2,k3

3

Yj=2

˜B(τ (d))

kj

(uj)

where

˜b(d,D)

k2,k3 = X1≤k1≤K

P3

i=1 eki ≤D

˜b(d)
k1,k2,k3.

To guarantee that the ﬁrst marginal density is one, we impose the following constraint
on the spline coeﬃcients evaluated at the knots τ (d)

˜b(d)

k2,k3

X1≤k2,k3≤K

3

Yj=2

˜B(τ (d))

kj

(τ (d)) = 1K.

(11)

Note that (11) yields linear constraints for ˜b(d,D) which are easy to implement. Identical
calculations are done for the second marginal density of c12|3(u1, u2|u3) which is given by

R ˜Φ(d,D)(u1, u2, u3)˜b(d,D)du2. We need two further constraints so that c12|3(u1, u2|u3; ˜b(d,D))

10

is a density. First, c12|3(u1, u2|u3; ˜b(d,D)) should integrate to one. Since all columns of
the sparse basis ˜Φ
are B-spline densities over u1, u2, u3, we guarantee that the sum
of the elements in ˜b(d,D) equals 1, i.e.,

(d,D)

1⊤ ˜b(d,D) = 1.

To ensure that the density is non-negative we impose that

c12|3(u1, u2|u3; ˜b(d,D)) ≥ 0

holds at the knots locations of the sparse B-spline density basis.

3.4 Selection of the Penalty Parameter

(12)

(13)

To limit the numerical eﬀort, we set λ1 = λ2 = λ3 and use a direct estimation approach
for the penalty parameter following the idea presented in Kauermann and Schellhase
(2014). Let ˜U ˜Λ ˜U⊤ be the singular value decomposition of ˜P(D). Starting with some
suitable λ, we construct the estimating equation for a new ˆλ for the diﬀerence penalty
using

ˆ˜b(d,D)

ˆλ−1 =

where the smoothing matrix S(λ) is

⊤

˜P(D)(λ)
tr(S(λ))

ˆ˜b(d,D)

(14)

ˆ˜b(d,D), λ = 0) ˜U + λ ˜Λ)−1 ˜U⊤H(d,D)

p

(

(

ˆ˜b(d,D), λ = 0) ˜Uo

ˆ˜b(d,D), λ) denotes the second-order partial derivative of (10) with respect to

(

S(λ) :=n( ˜U⊤H(d,D)

p

and H(d,D)
ˆ˜b(d,D), i.e.,

p

H(d,D)

p

(ˆ˜b(d,D), λ) = −

n

Xi=1

(d,D)

˜Φ

(u1,i, u2,i, u3,i) ˜Φ

(d,D)⊤

c12|3(u1,i, u2,i|u3,i;

(u1,i, u2,i, u3,i)
ˆ˜b(d,D))

− ˜P(D)(λ).

Apparently, both sides of equation (14) depend on λ but an iterative solution is possible
by ﬁxing λ on the right hand side in (14), update λ on the left hand side and iterate
this step by updating the right hand side of (14). This estimation scheme yields the so
called REML estimate and has been suggested in generalized linear mixed models by
Schall (1991). For penalized spline smoothing Wood (2011) shows that the selection of
smoothing parameter λ based in the mixed model approach behaves superior to AIC
selected values.

3.5 Practical Implementation

The constraints (11), (12) and (13) can be accommodated as side conditions in a
quadratic programming optimization routine to maximize the likelihood given in (10).

11

For this purpose, we make use of the quadprog package in R. The starting values for
˜b(d,D) are chosen such that the resulting copula density is the density of the inde-
In each step
pendence copula and the initial value of λ is set to a moderate size.
ˆ˜b(d,D), keeping λ ﬁxed and then reﬁt λ using (14). This
we estimate new coeﬃcients
estimation scheme is repeated until convergence.

4 Estimation of D-Vine Copulas

First, we select the tree structure of the D-vine copula which is determined by the

ﬁrst tree (Aas et al. (2009)). We estimate all (cid:0)p

Cij, 1 ≤ i < j ≤ p, using the procedure proposed in Kauermann and Schellhase
(2014). The resulting maximized likelihood value for each pair (i, j) is denoted by
ˆ˜b(d,D), ˆλ). For each pair, we calculate the corrected Akaike information criterion
˜l(D)
(
p
(cAIC) (Hurvich and Tsai (1989))

2(cid:1) bivariate unconditional copulas

AICc(λ) = −2˜l(

ˆ˜b(d,D), λ) + 2df(λ) +

2df(λ)(df(λ) + 1)

n − df(λ) − 1

,

(15)

where df(λ) is the degree of the model deﬁned by

df(λ) = tr(cid:20)n ˜H(D)

p

(ˆ˜b(d,D), λ)o−1

˜H(D)

p (cid:16)ˆ˜b(d,D), λ = 0(cid:17)(cid:21) .

The pairs are ordered with respect to the cAIC in (15) and the ﬁrst tree is chosen such
that the sum of the cAIC values is minimized. From the second tree on, we consider
three diﬀerent non-parametric estimators for the building blocks of the vine copula.
The ﬁrst estimator ’SimpA’ has been proposed in Kauermann and Schellhase (2014)
for estimating p-dimensional copulas and uses unconditional copulas as building blocks
for a simpliﬁed vine copula model, i.e.,

ˆcSimpA
1:p

(u1, . . . , up) =

p−1

p−j

Yj=1

Yi=1

ˆci,i+j; sij{ui|sij , ui+j|sij},

(16)

where ˆci,i+j; sij is an estimator for the density of the j-th order partial copula of Fi,i+j|sij
(see Spanhel and Kurz, 2015a). Thus, this estimator estimates the density of the partial
vine copula of C1:p. The second estimator ’Cond’ is given by

ˆcCond
1:p

(x1, . . . , xp) =

p−1

p−j

Yj=1

Yi=1

ˆci,i+j|vij {ui|sij , ui+j|sij|vij},

(17)

where ˆci,i+j|vij is an estimator for the bivariate conditional copula with the standardized
ranks of the ﬁrst principal component of Usij as conditioning variable (see Section
2). The third estimator ’Test’ only estimates a conditional copula if the simplifying

12

assumption is rejected for a particular edge of the vine copula. That is, from the second
tree on, we test whether the simplifying assumption is adequate for a conditional copula.
In particular, the hypotheses for the i-th copula in the j-th tree, j ≥ 2, are given by

H0 : P{Ci,i+j|sij(u, v|Xsij) = Ci,i+j; sij(u, v)} = 1, for all (u, v) ∈ (0, 1)2,

vs H1 : P{Ci,i+j|sij(u, v|Xsij) = C p

i,i+j; sij(u, v)} 6= 1, for some (u, v) ∈ (0, 1)2,

where C p
i,i+j; sij(u, v) := P(Fi|sij (Xi|Xsij ) ≤ a, Fi+j|sij (Xi+j|Xsij ) ≤ b) is the partial
copula of Ci,i+j|sij (Spanhel and Kurz, 2015b). Note that the non-simpliﬁed vine copula
estimator provides estimates for the unknown conditional cdfs Fi|sij and Fi+j|sij so that
pseudo-observations from the i-th copula in the j-th tree are available. In order to test
whether the conditional copula Ci,i+j|sij collapses to its partial copula C p
i,i+j; sij, we apply
the testing procedure proposed by Kurz and Spanhel (2016) for which an R-Package
pacotest is in preparation. Consequently, the estimator ’Test’ approximates a general
D-vine copula by

ˆcTest
1:p (u1, . . . , up) =

p−1

p−j

Yj=1

Yi=1

˜ci,i+j|vij{ui|sij , ui+j|sij|vij}

where ˜ci,i+j|vij = ˆci,i+j|vij (see (17)) if the simplifying assumption is rejected and ˜ci,i+j|vij =
ˆci,i+j; sij (see (16)) if the simplifying assumption can not be rejected. The entire pro-
cedure is implemented in the R package pencopulaCond to be provided on the CRAN
server (http://cran.r-project.org/ ).

5 Simulations and Application

In order to investigate the performance of the presented non-parametric estimators
for non-simpliﬁed D-vine copulas, we conduct an extensive simulation study. For that
purpose, we generate N = 100 data sets of sample size n = 500 and n = 2000 for three-
and ﬁve-dimensional distributions, which are later explained in Section 5.1. Throughout
the simulation study, we estimate copula densities with a varying degree d, which
determines the amount of univariate knots, and diﬀerent maximum cumulated hierarchy
levels, which determines the amount of sparsity. In the following, D2 and D3 refer to
the maximum cumulated hierarchy level that is used for the estimation of a bivariate
unconditional copula density (two-dimensional function) and a bivariate conditional
copula density (three-dimensional function), respectively. For instance, d = 2, D2 =
4, D3 = 6 refers to an estimation where the degree of the univariate hierarchical B-
spline basis is two (d = 2) and the maximum cumulated hierarchy level is four for
the unconditional copula densities (D2 = 4) and six for the conditional copula densities
(D3 = 6). In this case, we estimate 25 basis coeﬃcients for a bivariate copula density and
125 basis coeﬃcients for a conditional copula density with one conditioning argument.

13

Changing the values of d, D2, and D3, varies the basis size, which is increased throughout
the simulation study up to d = 4, D2 = 8, D3 = 6, which corresponds to 289 basis
coeﬃcients for a two-dimensional density estimation and 881 basis coeﬃcients for a
three-dimensional density estimation. See also Table 1 and Table 2 for the number of
basis coeﬃcients used for diﬀerent conﬁgurations in the simulation study. We use the
three non-parametric vine copula estimators ’Test’, ’Cond’, and ’SimpA’, deﬁned in
Section 4 and the R-package CDVine (Schepsmeier and Brechmann (2011)) to estimate
a parametric vine copula. The function CDVineCopSelect is used to select the vine
structure and the parametric copula families.

To evaluate the performance of the four estimators, we use the cAIC, log-likelihood
and Kullback-Leibler divergence (KL) as in-sample measures. Note that the AIC value
of the parametric vine copula estimator does not account for the selection of the copula
family and is therefore negatively biased.
In order to assess the out-of-sample per-
formance, we simulate n new observations from the underlying p-dimensional copula
family and evaluate the out-of-sample log-likelihood and the average out-of-sample KL
divergence which is given by

KL (c1:p, ˆc1:p) =

1
n

2n

Xi=n+1

ln

c1:p(u1,i, . . . , up,i)
ˆc1:p(u1,i, . . . , up,i)

,

where c1:p is the true copula density and ˆc1:p is the density that was estimated using
the observations (u1,i, . . . , up,i)i=1,...,n.

5.1 Description of the Examples

In three dimensions, we consider Frank and Gumbel copulas with Kendall’s τ ∈ {0.25, 0.5}
and the equally weighted mixture of two normal distributions with cdf given by

F1:3(x; µ1, µ2, Σ1, Σ2) = 0.5Φ1:3(x; µ1, Σ1) + 0.5Φ1:3(x; µ2, Σ2),

where Φ1:d(·, µ, Σ) denotes the cdf of the d-dimensional normal distribution with mean
µ and covariance matrix Σ and

µ1 = 13, Σ1 = −

2
5

131′

3 +

7
5

I3, µ2 = −13, Σ2 =

2
5

131′

3 +

3
5

I3.

Secondly, we analyze the performance for ﬁve-dimensional copulas. We consider Frank
and Gumbel copulas with Kendall’s τ ∈ {0.25, 0.5} and two equally weighted mixtures
of two normal distributions with cdf given by

F (i)
1:5(x; µ1, Σ1, µ(i)

2 , Σ(i)

2 ) = 0.5Φ1:5(x; µ1, Σ1) + 0.5Φ1:5(x; µ(i)

2 , Σ(i)
2 ),

i = 1, 2,

14

where

2
5

µ1 = −15, Σ1 =

µ(1)
2 = 15, Σ(1)

151′
1
5
µ(2)
2 = 15, vech(Σ(2)

2 = −

5 +

I5,

3
5

151′

5 +

I5,

6
5
2
5

2 ) =(cid:16)1, −

, −

2
5

, −

1
5

, −

1
10

, 1, −

2
5

, −

1
5

, −

1
10

, 1, −

1
5

, −

1
10

, 1, −

1
10

, 1(cid:17).

Note that all dgps are exchangeable, except the second mixture of two normal dis-
tributions in ﬁve dimensions. Moreover, although the Clayton copula is the only
Archimedean copula that can be represented as a simpliﬁed vine copula (St¨ober et al.,
2013), Archimedean copulas are rather close to simpliﬁed vine copulas because the
variation of their conditional copulas is strongly limited (Mesﬁoui and Quessy, 2008).
Thus, we expect that in these cases the modeling of conditional copulas will only result
in a slight improvement. Because of this, we also consider mixtures of normal distri-
butions where the variation of the conditional copulas can be more pronounced.
In
general, if the pairwise correlations of diﬀerent components are not equal, the resulting
normal mixture is not a simpliﬁed vine copula.

5.2 Results for the three-dimensional Examples

Table 3-7 show the simulation results for the three-dimensional examples. Moreover,
the box plots of the diﬀerences KLnon−par − KLpar, where KLnon−par is the out-of-
sample KL divergence of a non-parametric estimation (’SimpA’, ’Cond’ or ’Test’) and
KLpar is the out-of-sample KL divergence of the parametric estimation ’CDVine’, are
plotted for the Gumbel and Frank copula in Figure 3 and for the normal mixture in the
top row of Figure 5. The length of the whiskers is chosen such that the whiskers cover
95% of the diﬀerences, i.e., if the value zero is not contained within the whiskers, the
diﬀerence between the out-of-sample KL divergences is signiﬁcant.

As expected, the modeling of a conditional copula does not pay oﬀ for the Gumbel
and Frank copula if the sample size is rather low (n = 500). The average out-of-sample
KL divergences of the non-parametric simpliﬁed vine copula estimator (’SimpA’) and
the proposed non-parametric non-simpliﬁed vine copula estimators (’Cond’ and ’Test’)
are not indistinguishable at a 5% signiﬁcance level. Moreover, the simplifying assump-
tion is not always rejected, indicating that the minor variation of the conditional copula
is hard to detect. According to the out-of-sample KL divergence the non-parametric
estimators are statistically not distinguishable from the parametric simpliﬁed vine cop-
ula estimator ’CDVine’ if Kendall’s τ = 0.25. However, the non-parametric vine copula
estimators perform signiﬁcantly worse than the parametric model if Kendall’s τ = 0.5.
For n = 2000 observations, the violation of the simplifying assumption is always
detected by the testing procedure of Kurz and Spanhel (2016). Consequently, the esti-
mators ’Cond’ and ’Test’ are identical. In addition, there are now signiﬁcant diﬀerences

15

between the non-parametric simpliﬁed vine copula estimator and the non-parametric
non-simpliﬁed vine copula estimators. While ’SimpA’ performs signiﬁcantly worse than
the parametric simpliﬁed vine copula estimator for the Frank copula, this does not hold
for ’Cond’ and ’Test’ which are statistically indistinguishable from the parametric sim-
pliﬁed vine copula estimator ’CDVine’. For the Clayton copula the estimators ’Cond’
and ’Test’ also perform better than ’SimpA’. However, the parametric approach ’CD-
Vine’ often performs here signiﬁcantly better than any non-parametric estimator.

The results for the three-dimensional mixture of normal distributions are reported in
Table 7 and plotted in the top row of Figure 5. The non-parametric estimator ’SimpA’,
which approximates conditional copulas by unconditional copulas, performs similarly
to the parametric estimator ’CDVine’ in terms of average out-of-sample KL divergence.
Modeling the conditional copula in the second tree of the three-dimensional normal
mixture (’Cond’ and ’Test’) greatly reduces the average out-of-sample KL divergence
and results in a signiﬁcantly better model even if the sample size is n = 500. Note that
the simplifying assumption is always rejected in the second tree of this example so that
the estimators ’Cond’ and ’Test’ are identical. Increasing the sample size to n = 2000
only slightly improves the performance of the non-parametric simpliﬁed vine copula
estimator ’SimpA’ as compared to ’CDVine’. In contrast, the average out-of-sample KL
divergence is much more reduced by applying the non-parametric estimators ’Cond’ and
’Test’. Thus, if the variation in the conditional copula is not negligible, our approach
may result in a substantial improvement. Additionally, we observe decreasing values of
KLnon−par with a larger basis size, which is in line with the expected performance. The
estimated conditional copula density is illustrated in Figure 6 and an animated picture
is provided in the supplementary material.

5.3 Results for the ﬁve-dimensional examples

Table 8-13 show the simulation results for the ﬁve-dimensional examples. The corre-
sponding box plots of the KL diﬀerences are plotted for the Gumbel and Frank copula
in Figure 4 and for the two normal mixtures at the bottom of Figure 5. The results for
the Frank and Gumbel copula in ﬁve dimensions are similar to the three-dimensional
case. For a small sample size (n = 500) and Kendall’s τ = 0.25, it is still diﬃcult to
detect the slight variation in the conditional copula. The estimator ’Cond’ performs
slightly better than the other non-parametric estimators ’Test’ and ’SimpA’, although
this is not signiﬁcant. For Kendall’s τ = 0.5 the performance of all non-parametric
estimators is again signiﬁcantly worse in comparison to the parametric simpliﬁed vine
copula estimator ’CDVine’. If the number of observations is increased to n = 2000,
the diﬀerences between ’SimpA’ and ’Cond’ or ’Test’ become signiﬁcant for Kendall’s
τ = 0.25 and τ = 0.5. For Kendall’s τ = 0.25, ’Cond’ and ’Test’ are even statistically in-
distinguishable from the parametric simpliﬁed vine copula model ’CDVine’. While the
modeling of conditional copulas also decreases the KL divergence for Kendall’s τ = 0.5,

16

the estimators ’Test’ and ’Cond’ still perform signiﬁcantly worse than ’CDVine’ in this
case. Thus, modeling the slight variation of the conditional copulas does not compen-
sate for the slower convergence rate of the non-parametric non-simpliﬁed vine copula
estimators.

The box plots at the bottom in Figure 5 show that the parametric and the non-
parametric estimator of the simpliﬁed vine copula model perform equally well for both
ﬁve-dimensional normal mixtures and both sample sizes. There is no signiﬁcant diﬀer-
ence in the average out-of-sample KL divergences for n = 500. Increasing the sample
size to n = 2000 does also not result in a signiﬁcant diﬀerence in the KL divergences of
’SimpA’ and ’CDVine’. On the contrary, the estimators ’Cond’ and ’Test’ both yield
a substantially lower out-of-sample KL divergence which is highly signiﬁcant even for
small sizes (n = 500). Since the simplifying assumption is also almost always rejected
for each conditional copula, this is consistent with the fact that the variation in the
conditional copulas is not negligible in both normal mixtures. Increasing the sample
size to n = 2000 observations further increases the substantial diﬀerence between the
average out-of-sample KL divergence of ’Cond’ or ’Test’ and the parametric simpliﬁed
vine copula model. This diﬀerence also becomes larger for the second ﬁve-dimensional
mixture of normal distributions if the basis size is increased (bottom right in Figure
5). Consequently, the consideration of conditional copulas may result in a remark-
able performance improvement if the partial vine copula does not yield an adequate
approximation.

5.4

Illustrating Application

We illustrate the potential gains of non-simpliﬁed vine copula density estimation using
the ten-dimensional MAGIC data set from the UCI Machine Learning Repository web
page (url: https://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope).
The synthetic data set reproduces measurements taken on images from the MAGIC
(Major Atmospheric Gamma-ray Imaging Cherenkov) Telescopes and has been previ-
ously analyzed with a kernel-based simpliﬁed vine copula estimator by Nagler and Czado
(2015). For a detailed description of the data set we refer to Bock et al. (2004) and the
UCI web page.

We restrict the analysis to signals corresponding to primary gamma signals and
the ﬁrst n = 12000 observations. We use standardized ranks to model the marginal
distributions and apply the estimator ’Test’ for the setup d = 4, D2 = 8, D3 = 6 and
the estimator ’SimpA’ for the setup d = 4, D2 = 8. The ﬁrst tree of these D-vine
copula estimates is determined by minimizing the sum of (c)AIC values (see Section
4). Since the estimators ’SimpA’ and ’Cond’ use the same estimator for the ﬁrst tree
of the D-Vine copula, both estimated D-vine copulas exhibit the same order for the
ﬁrst tree and therefore the same tree structure. Based on this order, we also estimate
a parametric D-vine copula using the R-package CDVine.

17

We divide the data set into 12 disjoint subsamples y1, . . . , y12, each with 1000 obser-
vations, and estimate the D-vine copulas for the odd-numbered data sets. To evaluate
the performance of the estimator, we compute the out-of-sample log-likelihood of the
even-numbered data sets. In particular, the log-likelihood of an even-numbered data set
y2i is computed on the basis of the model that has been ﬁtted to the odd-numbered data
set y2i−1, for i = 1, . . . , 6. The in-sample (out-of-sample) log-likelihoods of the estima-
tors for the odd-numbered (even-numbered) data sets are presented in Table 14. The
non-parametric simpliﬁed vine copula estimator ‘SimpA’ performs considerably better,
in terms of in-sample and out-of-sample log-likelihood, than the parametric simpliﬁed
vine copula estimator ’CDVine’. In addition, the modeling of conditional copulas fur-
ther improves the performance, with the estimator ’Test’ having the largest in- and and
out-of-sample log-likelihood.

6 Discussion

We propose the ﬁrst non-parametric estimator of a non-simpliﬁed vine copula that
features varying conditional copulas. The use of a reduced hierarchical B-spline basis
allows us to maintain numerical feasibility and to directly estimate conditional copu-
las. Since non-parametric estimators suﬀer greatly from the curse of dimensionality, we
approximate the conditioning vector by a monotone function of its ﬁrst principal compo-
nent. This approach results in a computationally fast approximation of the conditional
copula and performs satisfactorily well in the considered simulations. The simulation
study shows that, if the variation in the conditional copulas is not negligible, the mod-
eling of conditional copulas can result in a substantial improvement in comparison to
(non-)parametric simpliﬁed vine copula estimators. All in all, the paper presents the
ﬁrst step in estimating a non-simpliﬁed vine copula model non-parametrically and il-
lustrates potential gains that can be achieved by the modeling of conditional copulas.
More sophisticated approaches along the lines of Hall and Yao (2005) that reduce the
dimension of the conditioning vector are left open for further research.

Acknowledgment

We thank G¨oran Kauermann (LMU Munich) for intensive discussions at the begin-

ning of this project.

References

Aas, K. and D. Berg (2009). Models for construction of multivariate dependence – a

comparison study. The European Journal of Finance 15 (7-8), 639–659.

18

Aas, K., C. Czado, A. Frigessi, and H. Bakken (2009). Pair-copula constructions of

multiple dependence. Insur. Math. Econ. 44 (2), 182–198.

Abegaz, F., I. Gijbels, and N. Veraverbeke (2012). Semiparametric estimation of con-
ditional copulas. Journal of Multivariate Analysis 110, 43 – 73. Special Issue on
Copula Modeling and Dependence.

Acar, E. F., R. V. Craiu, and F. Yao (2011). Dependence calibration in conditional

copulas: A nonparametric approach. Biometrics 67 (2), 445–453.

Acar, E. F., C. Genest, and J. Neˇslehov´a (2012). Beyond simpliﬁed pair-copula con-

structions. Journal of Multivariate Analysis 110, 74–90.

Bedford, T. and R. M. Cooke (2002). Vines: A New Graphical Model for Dependent

Random Variables. The Annals of Statistics 30 (4), 1031–1068.

Brechmann, E. C., C. Czado, and K. Aas (2012). Truncated regular vines in high
dimensions with application to ﬁnancial data. Canadian Journal of Statistics 40 (1),
68–85.

Eilers, P. H. C. and B. D. Marx (1996). Flexible smoothing with B-splines and penalties.

Statist. Sci. 11 (2), 89–121.

Fermanian, J.-D. and M. H. Wegkamp (2012). Time-dependent copulas. Journal of
Multivariate Analysis 110, 19 – 29. Special Issue on Copula Modeling and Depen-
dence.

Fischer, M., C. K¨ock, S. Schl¨uter, and F. Weigert (2009). An empirical analysis of

multivariate copula models. Quantitative Finance 9 (7), 839–854.

Forsey, D. R. and R. H. Bartels (1988). Hierarchical B-spline reﬁnement. In SIGGRAPH
’88: Proceedings of the 15th annual conference on Computer graphics and interactive
techniques, New York, NY, USA, pp. 205–212. ACM.

Forsey, D. R. and R. H. Bartels (1995). Surface ﬁtting with hierarchical splines. ACM

Transactions on Graphics 14 (2), 134–161.

Gijbels, I., M. Omelka, and N. Veraverbeke (2012). Multivariate and functional covari-

ates and conditional copulas. Electronic Journal of Statistics 6 (0), 1273–1306.

Gijbels, I., N. Veraverbeke, and M. Omelka (2011). Conditional copulas, association
measures and their applications. Computational Statistics & Data Analysis 55 (5),
1919–1932.

Hall, P. and Q. Yao (2005, 06). Approximating conditional distribution functions using

dimension reduction. Ann. Statist. 33 (3), 1404–1421.

19

Hobæk Haﬀ, I., K. Aas, and A. Frigessi (2010). On the simpliﬁed pair-copula con-
struction – Simply useful or too simplistic? Journal of Multivariate Analysis 101 (5),
1296–1310.

Hurvich, C. M. and C.-L. Tsai (1989). Regression and time series model selection in

small samples. Biometrika 76 (2), 297–307.

Kauermann, G. and C. Schellhase (2014). Flexible Pair-Copula Estimation in D-vines

with Penalized Splines. Stat. Comput. 24 (6), 1081–1100.

Kauermann, G., C. Schellhase, and D. Ruppert (2013). Flexible Copula Density Esti-

mation with Penalized Hierarchical B-Splines. Scand. J. Stat. 40 (4), 685–705.

Kurowicka, D. and H. Joe (Eds.) (2011). Dependence modeling. Singapore: World

Scientiﬁc.

Kurz, M. S. and F. Spanhel (2016). Testing the simplifying assumption in high-

dimensional vine copulas. In preparation.

Lopez-Paz, D., J. Hernandez-Lobato, and Z. Ghahramani (2013). Gaussian process
vine copulas for multivariate dependence. In Proceedings of the 30th International
Conference on Machine Learning, W&CP 28(2), pp. 10–18. JMLR.

Marx, B. and P. H. C. Eilers (2005). Multidimensional penalized signal regression.

Technometrics 47, 13–22.

Mesﬁoui, M. and J.-F. Quessy (2008). Dependence structure of conditional archimedean

copulas. Journal of Multivariate Analysis 99 (3), 372 – 385.

Nagler, T. and C. Czado (2015). Evading the curse of dimensionality in multivariate

kernel density estimation with simpliﬁed vines. ArXiv e-prints.

Patton, A. J. (2006). Modelling Asymmetric Exchange Rate Dependence. International

Economic Review 47 (2), 527–556.

Ruppert, D., M. Wand, and R. Carroll (2003). Semiparametric Regression. Cambridge

University Press, Cambridge.

Schall, R. (1991). Estimation in generalized linear models with random eﬀects.

Biometrika 78 (4), 719–727.

Schepsmeier, U. and E. C. Brechmann (2011). CDVine: Statistical inference of C- and

D-vine copulas. R package version 1.1-4.

Scott, D. W. (2008). The Curse of Dimensionality and Dimension Reduction, pp. 195–

217. John Wiley & Sons, Inc.

20

Sklar, A. (1959). Fonctions de r´epartition `a n dimensions et leurs marges. Publ. Inst.

Statist. Univ. Paris 8, 229–231.

Spanhel, F. and M. S. Kurz (2015a). Simpliﬁed vine copula models: Approximations

based on the simplifying assumption. ArXiv e-prints.

Spanhel, F. and M. S. Kurz (2015b). The partial copula: Properties and associated

dependence measures. ArXiv e-prints.

St¨ober, J., H. Joe, and C. Czado (2013). Simpliﬁed pair copula constructions—

Limitations and extensions. Journal of Multivariate Analysis 119, 101–118.

Vatter, T. and V. Chavez-Demoulin (2015). Generalized additive models for conditional

dependence structures. Journal of Multivariate Analysis 141, 147 – 167.

Veraverbeke, N., M. Omelka, and I. Gijbels (2011). Estimation of a Conditional Copula

and Association Measures. Scandinavian Journal of Statistics 38 (4), 766–780.

Wand, M. P. and J. T. Ormerod (2008). On semiparametric regression with O’Sullivan

penalised splines. Aust. N. Z. J. Stat. 50 (2), 179–198.

Wood, S. (2011). Fast stable restricted maximum likelihood and marginal likelihood
estimation of semiparametric generalized linear models. J. Roy. Stat. Soc. B 73 (1),
3–36.

Zenger, C. (1991). Sparse grids. Notes Numer. Fluid Mech. Multidiscip. Des. 31, 241–

251.

21

Figure 1: (a) B-spline density basis B(τ (2))(uj) and corresponding building blocks of
the univariate hierarchical B-spline density basis B(τ (0))
(uj)
as graphics (b), (c) and (d).

(uj) and B(τ (2))

(uj), B(τ (1))

I1

I2

I0

8

6

4

2

0

4

3

2

1

0

(a)

(b) (hierarchy level d=0)

4

3

2

1

0

0=t 1(2)

0.25=t 2(2) 0.50=t 3(2) 0.75=t 4(2)

1=t 5(2)

0=t 1(0)

1=t 2(0)

uj

uj

(c) (hierarchy level d=1)

(d) (hierarchy level d=2)

4

3

2

1

0

0=t 1(1)

0.5=t 2(1)

uj

1=t 3(1)

0=t 1(2)

0.25=t 2(2) 0.5=t 3(2) 0.75=t 4(2)

1=t 5(2)

uj

Figure 2: ˜Φ(2,4)(u1, u2) is the full tensor product of two univariate B-spline bases, each
margin consists of B(τ (0))
(uj) for j = 1, 2. The construction
principle of the sparse B-spline basis ˜Φ(2,2)(u1, u2) is to remove columns from the full
tensor product, reducing the number of spline bases from 25 to 17 in this bivariate
example for d = 2.

(uj) and B(τ (2))

(uj), B(τ (1))

I0

I1

I2

I0

(u1) B(τ (0))
(u1) B(τ (1))
(u1) B(τ (2))

I1

I2

I0

(u2)

B(τ (0))
(u1) ⊗ B(τ (0))
(u1) ⊗ B(τ (0))
(u1) ⊗ B(τ (0))

I0

I0

I0

I0

B(τ (0))
B(τ (1))
B(τ (2))

I1

I2

I0

(u2) B(τ (0))
(u2) B(τ (1))
(u2)

I1

22

I1

(u2)

B(τ (1))
(u1) ⊗ B(τ (1))
(u1) ⊗ B(τ (1))
I1
removed

I1

(u2) B(τ (0))
(u2)

I0

I2

(u2)

B(τ (2))
(u1) ⊗ B(τ (2))
I2
removed
removed

(u2)

Frank copula,t =0.25, p=3

Frank copula,t =0.50, p=3

n=500

n=2000

n=500

n=2000

e
c
n
e
r
e

f
f
i

 

D
L
K

0.02

0.00

−0.02

e
c
n
e
r
e

f
f
i

 

D
L
K

0.25

0.20

0.15

0.10

0.05

0.00

d=2, D 2=4, D 3=6
d=2, D 2=4, D 3=4

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=6
d=2, D 2=4, D 3=4

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=4
d=4, D 2=8, D 3=6

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

Basis

Basis

Gumbel copula,t =0.25, p=3

Gumbel copula,t =0.50, p=3

n=500

n=2000

n=500

n=2000

0.10

0.05

e
c
n
e
r
e
f
f
i

D
 
L
K

0.00

e
c
n
e
r
e
f
f
i

D
 
L
K

0.25

0.20

0.15

0.10

0.05

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=6
d=2, D 2=4, D 3=4

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=4
d=4, D 2=8, D 3=6

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

Basis

Basis

Figure 3: Box plots of the diﬀerences KLnon−par − KLpar for three-dimensional Frank
and Gumbel copulas. KLnon−par is the out-of-sample KL divergence of a non-parametric
estimation (’SimpA’, ’Cond’ or ’Test’) and KLpar is the out-of-sample KL divergence
of the parametric estimation using the CDVine package. Blue refers to the estimator
’Test’, pink corresponds to the estimator ’Cond’ and green refers to the estimator
’SimpA’. The whiskers cover 95% of the data.

23

Frank copula,t =0.25, p=5

Frank copula,t =0.5, p=5

n=500

n=2000

n=500

n=2000

0.05

0.00

−0.05

e
c
n
e
r
e

f
f
i

 

D
L
K

e
c
n
e
r
e

f
f
i

 

D
L
K

0.4

0.3

0.2

0.1

0.0

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=4
d=4, D 2=8, D 3=6

d=2, D 2=4, D 3=6
d=2, D 2=4, D 3=4

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=3
d=3, D 2=6, D 3=6

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

Basis

Basis

Gumbel copula,t =0.25, p=5

Gumbel copula,t =0.5, p=5

n=500

n=2000

n=500

n=2000

e
c
n
e
r
e

f
f
i

 

D
L
K

0.20

0.15

0.10

0.05

0.00

−0.05

e
c
n
e
r
e

f
f
i

 

D
L
K

0.5

0.4

0.3

0.2

0.1

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=4
d=4, D 2=8, D 3=6

d=2, D 2=4, D 3=6
d=2, D 2=4, D 3=4

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=3
d=3, D 2=6, D 3=6

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

Basis

Basis

Figure 4: Box plots of the diﬀerences KLnon−par −KLpar for ﬁve-dimensional Frank and
Gumbel copulas. KLnon−par is the out-of-sample KL divergence of a non-parametric
estimation (’SimpA’, ’Cond’ or ’Test’) and KLpar is the out-of-sample KL divergence
of the parametric estimation using the CDVine package. Blue refers to the estimator
’Test’, pink corresponds to the estimator ’Cond’ and green refers to the estimator
’SimpA’. The whiskers cover 95% of the data.

24

Normalmixture, p=3

n=500

n=2000

0.1

0.0

−0.1

−0.2

e
c
n
e
r
e

f
f
i

 

D
L
K

d=2, D 2=4, D 3=6
d=2, D 2=4, D 3=4

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

Basis

Normalmixture 1, p=5

Normalmixture 2, p=5

n=500

n=2000

n=500

n=2000

e
c
n
e
r
e

f
f
i

 

D
L
K

0.3

0.2

0.1

0.0

−0.1

−0.2

−0.3

0.2

0.0

−0.2

−0.4

e
c
n
e
r
e

f
f
i

 

D
L
K

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=3
d=3, D 2=6, D 3=6

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

d=2, D 2=4, D 3=4
d=2, D 2=4, D 3=6

d=3, D 2=6, D 3=6
d=3, D 2=6, D 3=3

d=4, D 2=8, D 3=6
d=4, D 2=8, D 3=4

Basis

Basis

Figure 5: Box plots of the diﬀerences KLnon−par −KLpar for three- and ﬁve-dimensional
mixtures of normal distributions. KLnon−par is the out-of-sample KL divergence of a
non-parametric estimation (’SimpA’, ’Cond’ or ’Test’) and KLpar is the out-of-sample
KL divergence of the parametric estimation using the CDVine package. Blue refers to
the estimator ’Test’, pink corresponds to the estimator ’Cond’ and green refers to the
estimator ’SimpA’. The whiskers cover 95% of the data.

25

3.0

2.5

3.0

2.5

density

2.0
1.5
1.0
0.5
0.0
1.0

0.8

1.0

0.8

0.6

0.4

0.6

0.4

0.2

0.2

0.0

0.0

density

2.0
1.5
1.0
0.5
0.0
1.0

0.8

1.0

0.8

0.6

0.4

0.6

0.4

0.2

0.2

0.0

0.0

log like=83.635, pen. log like= 68.46, AIC=−114.591, alpha=0

log like=83.635, pen. log like= 68.46, AIC=−114.591, alpha=0

3.0

2.5

3.0

2.5

density

2.0
1.5
1.0
0.5
0.0
1.0

0.8

1.0

0.8

0.6

0.4

0.6

0.4

0.2

0.2

0.0

0.0

density

2.0
1.5
1.0
0.5
0.0
1.0

0.8

1.0

0.8

0.6

0.4

0.6

0.4

0.2

0.2

0.0

0.0

log like=83.635, pen. log like= 68.46, AIC=−114.591, alpha=0

log like=83.635, pen. log like= 68.46, AIC=−114.591, alpha=0

3.0

2.5

3.0

2.5

density

2.0
1.5
1.0
0.5
0.0
1.0

0.8

1.0

0.8

0.6

0.4

0.6

0.4

0.2

0.2

0.0

0.0

density

2.0
1.5
1.0
0.5
0.0
1.0

0.8

1.0

0.8

0.6

0.4

0.6

0.4

0.2

0.2

0.0

0.0

log like=83.635, pen. log like= 68.46, AIC=−114.591, alpha=0

log like=83.635, pen. log like= 68.46, AIC=−114.591, alpha=0

3.0

2.5

3.0

2.5

density

2.0
1.5
1.0
0.5
0.0
1.0

0.8

1.0

0.8

0.6

0.4

0.6

0.4

0.2

0.2

0.0

0.0

density

2.0
1.5
1.0
0.5
0.0
1.0

0.8

1.0

0.8

0.6

0.4

0.6

0.4

0.2

0.2

0.0

0.0

log like=83.635, pen. log like= 68.46, AIC=−114.591, alpha=0

log like=83.635, pen. log like= 68.46, AIC=−114.591, alpha=0

Figure 6: Conditional copula densities of the three-dimensional mixture of normal dis-
tributions for conditional arguments 0.01, 0.15, 0.29, 0.43, 0.57, 0.71, 0.85, 0.99 (top left
to bottom right).

26

Table 1: Dimension of tensor product basis and sparse hierarchical basis of linear B-splines for a
bivariate unconditional bivariate copula density. K = 2d + 1 is the number of marginal equidistant
knots, d is the degree of the univariate hierarchical B-spline basis and D = D2 is the maximum
cumulated hierarchy level.

d

2 (K = 5)
2
3 (K = 9)
3
4 (K = 17)
4
5 (K = 33)
5

D = D2
4
2
6
3
8
4
10
5

basis

number of coeﬃcients

tensor prod. (D = 2d)

sparse (D = d)

tensor prod. (D = 2d)

sparse (D = d)

tensor prod. (D = 2d)

sparse (D = d)

tensor prod. (D = 2d)

sparse (D = d)

25
17
81
37
289
81

1,089
177

Table 2: Dimension of tensor product basis and sparse hierarchical basis of linear B-splines for a
bivariate conditional copula density. K = 2d + 1 is the number of marginal equidistant knots, d
is the degree of the univariate hierarchical B-spline basis and D = D3 is the maximum cumulated
hierarchy level.

d

2 (K = 5)
2
2
3 (K = 9)
3
3
4 (K = 17)
4
4
4
5 (K = 33)
5

D = D3
6
4
2
9
6
3
12
8
6
4
15
5

basis

number of coeﬃcients

tensor prod. (D = 3d)

sparse (D = 2d)
sparse (D = d)

tensor prod. (D = 3d)

sparse (D = 2d)
sparse (D = d)

tensor prod. (D = 3d)

sparse (D = 2d)
sparse (D = 1.5d)

sparse (D = d)

tensor prod. (D = 3d)

sparse (D = d)

125
105
50
729
473
123
4,913
2,225
881
297

35,937

705

27

Table 3: Trivariate Frank copula, p = 3, τ = 0.25. We report for N = 100 simulations the means
(empirical standard deviation) of cAIC, log-likelihood and Kullback-Leibler divergence for in- and
out-of-sample.

n
500

2000

basis
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

estimator

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

cAIC
-151.612 (23.890)
-149.077 (23.670)
-151.612 (23.890)
-149.077 (23.673)
-151.451 (24.214)
-152.254 (23.809)
-149.702 (23.602)
-152.286 (23.807)
-149.787 (23.606)
-152.328 (24.388)
-153.544 (24.417)
-150.969 (24.141)
-153.591 (24.393)
-151.174 (24.085)
-153.655 (24.956)
-184.928 (27.502)

-680.829 (59.554)
-680.800 (59.537)
-680.819 (59.542)
-680.790 (59.525)
-667.264 (59.287)
-689.958 (60.885)
-689.922 (60.865)
-690.536 (60.476)
-690.503 (60.457)
-678.148 (60.332)
-694.749 (60.360)
-694.749 (60.360)
-695.379 (60.123)
-695.381 (60.075)
-681.991 (59.470)
-733.009 (61.106)

in-sample

log-likelihood
89.394 (11.953)
90.815 (11.838)
89.394 (11.953)
90.815 (11.840)
87.916 (12.170)
89.995 (11.942)
91.401 (11.857)
90.019 (11.945)
91.471 (11.859)
88.747 (12.477)
91.455 (12.649)
92.688 (12.500)
91.481 (12.638)
92.860 (12.455)
90.194 (13.219)
96.244 (13.833)

359.805 (33.237)
359.831 (33.246)
359.804 (33.228)
359.830 (33.237)
349.119 (32.448)
369.014 (34.752)
369.036 (34.759)
369.514 (34.502)
369.538 (34.510)
360.280 (33.712)
373.406 (34.023)
373.406 (34.023)
373.882 (34.043)
373.970 (33.972)
364.060 (32.624)
370.735 (30.591)

KL
0.018 (0.009)
0.016 (0.009)
0.018 (0.009)
0.016 (0.009)
0.021 (0.010)
0.017 (0.010)
0.014 (0.009)
0.017 (0.010)
0.014 (0.009)
0.020 (0.011)
0.014 (0.010)
0.012 (0.009)
0.014 (0.010)
0.011 (0.009)
0.017 (0.010)
0.005 (0.007)

0.014 (0.005)
0.014 (0.005)
0.014 (0.005)
0.014 (0.005)
0.019 (0.005)
0.009 (0.005)
0.009 (0.005)
0.009 (0.005)
0.009 (0.005)
0.014 (0.005)
0.007 (0.004)
0.007 (0.004)
0.007 (0.004)
0.007 (0.004)
0.012 (0.004)
0.009 (0.003)

out-of-sample

log-likelihood
83.863 (12.841)
85.546 (12.605)
83.863 (12.841)
85.545 (12.605)
82.925 (13.102)
83.776 (13.111)
85.468 (12.862)
83.786 (13.111)
85.500 (12.862)
82.798 (13.382)
84.067 (13.368)
85.761 (13.148)
84.067 (13.364)
85.838 (13.148)
83.095 (13.643)
86.674 (14.407)

353.522 (25.203)
353.616 (25.189)
353.518 (25.206)
353.611 (25.191)
342.833 (26.329)
358.899 (26.950)
358.990 (26.938)
359.120 (26.672)
359.213 (26.659)
349.297 (27.455)
362.028 (26.633)
362.028 (26.633)
362.089 (26.655)
362.292 (26.515)
352.100 (27.120)
367.722 (26.856)

KL
0.023 (0.012)
0.020 (0.010)
0.023 (0.012)
0.020 (0.010)
0.025 (0.012)
0.023 (0.012)
0.020 (0.010)
0.023 (0.012)
0.020 (0.010)
0.025 (0.012)
0.023 (0.012)
0.019 (0.010)
0.023 (0.012)
0.019 (0.010)
0.025 (0.012)
0.018 (0.009)

0.018 (0.005)
0.018 (0.005)
0.018 (0.005)
0.018 (0.005)
0.024 (0.005)
0.016 (0.005)
0.016 (0.005)
0.016 (0.005)
0.016 (0.005)
0.021 (0.006)
0.014 (0.005)
0.014 (0.005)
0.014 (0.005)
0.014 (0.005)
0.019 (0.005)
0.011 (0.004)

28

Table 4: Trivariate Frank copula, p = 3, τ = 0.50. We report for N = 100 simulations the means
(empirical standard deviation) of cAIC, log-likelihood and Kullback-Leibler divergence for in- and
out-of-sample.

n
500

2000

basis
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

estimator

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

cAIC
-637.525 (46.107)
-632.129 (44.628)
-637.531 (46.107)
-632.135 (44.629)
-641.760 (45.194)
-651.205 (46.366)
-643.115 (44.849)
-652.535 (45.963)
-643.628 (45.044)
-657.030 (45.388)
-658.658 (46.358)
-648.681 (45.664)
-658.161 (47.045)
-648.928 (45.679)
-663.655 (45.722)
-728.498 (43.594)

in-sample

log-likelihood
352.922 (23.353)
352.137 (22.773)
352.929 (23.351)
352.143 (22.771)
353.571 (23.425)
366.915 (23.861)
363.530 (23.196)
367.916 (23.769)
363.982 (23.424)
369.136 (23.853)
375.036 (24.317)
369.950 (24.300)
374.825 (24.778)
370.202 (24.444)
376.979 (24.454)
367.939 (21.820)

KL
0.043 (0.016)
0.045 (0.016)
0.043 (0.016)
0.045 (0.016)
0.042 (0.017)
0.015 (0.018)
0.022 (0.016)
0.013 (0.018)
0.021 (0.016)
0.011 (0.018)
0.001 (0.023)
0.009 (0.017)
0.001 (0.024)
0.008 (0.017)
0.006 (0.020)
0.013 (0.009)

-2794.905 (93.815)
-2794.905 (93.815)
-2795.007 (93.866)
-2795.007 (93.866)
-2782.344 (95.866)
-2800.856 (95.168)
-2800.856 (95.168)
-2801.072 (96.357)
-2801.072 (96.357)
-2716.498 (146.026)
-2810.944 (95.886)
-2810.944 (95.886)
-2819.688 (95.389)
-2819.688 (95.389)
-2806.867 (97.833)
-2916.777 (96.880)

0.014 (0.005)
1468.369 (46.917)
0.014 (0.005)
1468.369 (46.917)
0.013 (0.005)
1469.119 (46.958)
0.013 (0.005)
1469.119 (46.958)
0.024 (0.005)
1448.228 (48.159)
0.003 (0.005)
1489.134 (47.991)
1489.134 (47.991)
0.003 (0.005)
1494.604 (49.147) < 0.001 (0.010)
1494.604 (49.147) < 0.001 (0.010)
0.032 (0.040)
1431.297 (80.861)
1504.544 (49.436)
0.006 (0.014)
0.006 (0.014)
1504.544 (49.436)
0.012 (0.006)
1513.741 (48.543)
0.012 (0.006)
1513.741 (48.543)
1490.831 (50.091)
0.002 (0.006)
0.017 (0.004)
1462.268 (48.359)

out-of-sample

log-likelihood
334.929 (28.929)
335.211 (28.756)
334.933 (28.928)
335.214 (28.756)
335.129 (29.350)
328.300 (34.220)
327.303 (34.409)
328.857 (34.527)
327.483 (34.604)
329.217 (35.144)
327.721 (37.698)
326.012 (37.588)
327.681 (37.773)
325.932 (37.615)
328.497 (38.139)
360.171 (28.052)

1434.508 (53.502)
1434.508 (53.502)
1434.535 (53.502)
1434.535 (53.502)
1417.014 (53.417)
1424.476 (59.885)
1424.476 (59.885)
1424.806 (61.085)
1424.806 (61.085)
1376.885 (87.095)
1409.653 (64.202)
1409.653 (64.202)
1412.297 (62.410)
1412.297 (62.410)
1394.477 (61.742)
1457.651 (53.031)

KL
0.077 (0.031)
0.077 (0.031)
0.077 (0.031)
0.077 (0.031)
0.077 (0.032)
0.091 (0.049)
0.093 (0.049)
0.089 (0.049)
0.092 (0.049)
0.089 (0.050)
0.092 (0.051)
0.095 (0.050)
0.092 (0.050)
0.095 (0.050)
0.090 (0.051)
0.027 (0.015)

0.031 (0.006)
0.031 (0.006)
0.031 (0.006)
0.031 (0.006)
0.040 (0.007)
0.036 (0.013)
0.036 (0.013)
0.036 (0.015)
0.036 (0.015)
0.060 (0.032)
0.043 (0.017)
0.043 (0.017)
0.042 (0.015)
0.042 (0.015)
0.051 (0.015)
0.019 (0.005)

29

Table 5: Trivariate Gumbel copula, p = 3, τ = 0.25. We report for N = 100 simulations the means
(empirical standard deviation) of cAIC, log-likelihood and Kullback-Leibler divergence for in- and
out-of-sample.

n
500

2000

basis
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

estimator

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

cAIC
-147.195 (27.371)
-146.537 (27.226)
-147.194 (27.371)
-146.536 (27.227)
-146.016 (27.320)
-149.839 (28.889)
-149.182 (28.656)
-150.313 (29.233)
-149.672 (29.012)
-148.610 (28.825)
-154.806 (30.799)
-154.158 (30.556)
-155.481 (30.750)
-154.908 (30.686)
-153.415 (30.738)
-224.412 (36.552)

-761.547 (82.910)
-761.547 (82.910)
-761.651 (82.956)
-761.651 (82.956)
-718.244 (76.833)
-770.216 (78.765)
-770.216 (78.765)
-783.480 (78.304)
-783.480 (78.304)
-730.662 (72.858)
-782.115 (78.733)
-782.115 (78.733)
-791.251 (77.241)
-791.251 (77.241)
-736.749 (73.268)
-893.159 (87.841)

in-sample

log-likelihood
89.446 (14.321)
89.897 (14.057)
89.445 (14.321)
89.897 (14.057)
85.528 (14.042)
92.055 (16.167)
92.507 (15.919)
92.422 (16.431)
92.886 (16.175)
88.115 (15.972)
97.143 (18.554)
97.594 (18.299)
97.548 (18.576)
98.123 (18.345)
93.209 (18.388)
115.866 (18.265)

422.548 (48.518)
422.548 (48.518)
422.698 (48.586)
422.698 (48.586)
391.534 (42.128)
435.850 (46.514)
435.850 (46.514)
448.044 (45.831)
448.044 (45.831)
408.142 (39.982)
448.372 (47.167)
448.372 (47.167)
455.775 (45.033)
455.775 (45.033)
415.303 (40.859)
450.510 (44.006)

KL
0.067 (0.020)
0.066 (0.021)
0.067 (0.020)
0.066 (0.021)
0.075 (0.022)
0.062 (0.021)
0.061 (0.021)
0.061 (0.020)
0.060 (0.020)
0.070 (0.022)
0.052 (0.021)
0.051 (0.021)
0.051 (0.020)
0.050 (0.020)
0.059 (0.022)
0.014 (0.009)

0.034 (0.010)
0.034 (0.010)
0.034 (0.010)
0.034 (0.010)
0.049 (0.010)
0.027 (0.009)
0.027 (0.009)
0.021 (0.007)
0.021 (0.007)
0.041 (0.008)
0.021 (0.007)
0.021 (0.007)
0.017 (0.007)
0.017 (0.007)
0.038 (0.008)
0.020 (0.004)

out-of-sample

log-likelihood
85.434 (14.434)
86.291 (14.237)
85.434 (14.433)
86.291 (14.236)
82.117 (14.646)
85.914 (14.625)
86.773 (14.426)
86.105 (14.757)
86.974 (14.549)
82.552 (14.822)
87.982 (15.552)
88.878 (15.304)
88.111 (15.494)
89.129 (15.379)
84.483 (15.679)
106.217 (20.505)

408.281 (37.261)
408.281 (37.261)
408.366 (37.251)
408.366 (37.251)
381.952 (35.233)
414.927 (34.569)
414.927 (34.569)
424.282 (34.560)
424.282 (34.560)
390.718 (32.940)
422.422 (34.800)
422.422 (34.800)
428.360 (34.462)
428.360 (34.462)
393.809 (33.373)
453.995 (38.735)

KL
0.072 (0.024)
0.071 (0.024)
0.072 (0.024)
0.071 (0.024)
0.079 (0.025)
0.071 (0.023)
0.070 (0.023)
0.071 (0.023)
0.069 (0.023)
0.078 (0.024)
0.067 (0.023)
0.065 (0.023)
0.067 (0.023)
0.065 (0.022)
0.074 (0.024)
0.031 (0.014)

0.046 (0.012)
0.046 (0.012)
0.046 (0.012)
0.046 (0.012)
0.060 (0.012)
0.043 (0.011)
0.043 (0.011)
0.038 (0.010)
0.038 (0.010)
0.055 (0.010)
0.039 (0.010)
0.039 (0.010)
0.036 (0.009)
0.036 (0.009)
0.054 (0.010)
0.024 (0.005)

30

Table 6: Trivariate Gumbel copula, p = 3, τ = 0.50. We report for N = 100 simulations the means
(empirical standard deviation) of cAIC, log-likelihood and Kullback-Leibler divergence for in- and
out-of-sample.

n
500

2000

basis
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

estimator

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

cAIC
-659.197 (53.244)
-658.133 (53.333)
-659.211 (53.259)
-658.153 (53.347)
-661.702 (51.396)
-677.170 (52.628)
-675.179 (53.433)
-685.294 (56.021)
-683.717 (56.860)
-685.525 (52.449)
-693.680 (55.923)
-691.761 (56.626)
-701.486 (57.282)
-700.560 (57.442)
-699.663 (54.048)
-847.645 (63.320)

-3012.948 (125.196)
-3012.948 (125.196)
-3014.778 (125.265)
-3014.778 (125.265)
-2951.788 (124.321)
-3031.039 (123.799)
-3031.039 (123.799)
-3050.155 (124.847)
-3050.155 (124.847)
-2986.284 (123.659)
-3079.853 (128.652)
-3079.853 (128.652)
-3092.061 (128.879)
-3092.061 (128.879)
-3033.724 (128.377)
-3396.309 (146.820)

in-sample

log-likelihood
368.146 (29.264)
367.958 (29.073)
368.214 (29.326)
368.033 (29.135)
364.904 (27.330)
385.400 (28.450)
384.564 (28.892)
392.226 (31.744)
391.737 (32.128)
387.260 (27.962)
402.712 (31.672)
401.936 (32.071)
409.585 (32.860)
409.746 (32.860)
403.130 (29.575)
427.772 (31.673)

1581.701 (63.189)
1581.701 (63.189)
1583.846 (63.230)
1583.846 (63.230)
1533.681 (62.478)
1619.727 (63.213)
1619.727 (63.213)
1638.653 (64.368)
1638.653 (64.368)
1584.786 (63.148)
1676.067 (67.965)
1676.067 (67.965)
1688.036 (68.081)
1688.036 (68.081)
1637.568 (67.325)
1701.935 (73.438)

KL
0.136 (0.027)
0.136 (0.026)
0.136 (0.027)
0.136 (0.026)
0.142 (0.031)
0.101 (0.029)
0.103 (0.028)
0.087 (0.026)
0.088 (0.026)
0.097 (0.030)
0.067 (0.027)
0.068 (0.026)
0.053 (0.024)
0.052 (0.023)
0.066 (0.027)
0.016 (0.010)

0.082 (0.011)
0.082 (0.011)
0.081 (0.011)
0.081 (0.011)
0.106 (0.013)
0.063 (0.011)
0.063 (0.011)
0.053 (0.010)
0.053 (0.010)
0.080 (0.012)
0.035 (0.010)
0.035 (0.010)
0.029 (0.009)
0.029 (0.009)
0.054 (0.011)
0.022 (0.004)

out-of-sample

log-likelihood
353.548 (26.280)
353.551 (26.297)
353.549 (26.259)
353.558 (26.277)
350.669 (25.049)
351.673 (30.028)
351.356 (29.938)
356.428 (31.371)
356.379 (31.342)
351.833 (30.086)
353.030 (36.120)
352.784 (36.083)
357.342 (36.705)
358.111 (36.596)
351.701 (35.799)
420.164 (33.119)

1561.742 (53.370)
1561.742 (53.370)
1562.656 (53.415)
1562.656 (53.415)
1518.208 (53.407)
1562.078 (56.205)
1562.078 (56.205)
1572.140 (56.674)
1572.140 (56.674)
1526.890 (56.566)
1545.135 (69.034)
1545.135 (69.034)
1550.006 (70.147)
1550.006 (70.147)
1507.346 (69.447)
1711.379 (67.371)

KL
0.164 (0.037)
0.164 (0.037)
0.164 (0.037)
0.164 (0.037)
0.170 (0.035)
0.168 (0.043)
0.168 (0.042)
0.158 (0.044)
0.158 (0.044)
0.167 (0.043)
0.165 (0.050)
0.165 (0.050)
0.156 (0.050)
0.155 (0.050)
0.167 (0.049)
0.031 (0.015)

0.099 (0.013)
0.099 (0.013)
0.099 (0.013)
0.099 (0.013)
0.121 (0.014)
0.099 (0.018)
0.099 (0.018)
0.094 (0.017)
0.094 (0.017)
0.117 (0.018)
0.108 (0.023)
0.108 (0.023)
0.105 (0.023)
0.105 (0.023)
0.126 (0.024)
0.024 (0.005)

31

Table 7: Normalmixture, p = 3. We report for N = 100 simulations the means (empirical standard
deviation) of cAIC, log-likelihood and Kullback-Leibler divergence for in- and out-of-sample.

n
500

2000

basis
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

estimator

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

cAIC
-487.889 (40.062)
-487.889 (40.062)
-489.252 (39.494)
-489.252 (39.494)
-416.286 (40.299)
-508.751 (40.233)
-508.751 (40.233)
-518.734 (40.712)
-518.734 (40.712)
-427.416 (41.451)
-518.474 (41.060)
-518.474 (41.060)
-527.472 (40.957)
-527.472 (40.957)
-434.271 (41.676)
-408.325 (35.669)

-2359.515 (84.576)
-2359.515 (84.576)
-2389.938 (85.040)
-2389.938 (85.040)
-1910.837 (79.523)
-2361.550 (84.324)
-2361.550 (84.324)
-2443.693 (83.453)
-2443.693 (83.453)
-1921.824 (80.242)
-2414.786 (84.305)
-2414.786 (84.305)
-2457.987 (84.153)
-2457.987 (84.153)
-1930.509 (80.940)
-1653.806 (70.992)

in-sample

log-likelihood
286.505 (22.088)
286.505 (22.088)
287.531 (21.620)
287.531 (21.620)
238.645 (21.052)
309.694 (22.290)
309.694 (22.290)
318.987 (23.624)
318.987 (23.624)
253.484 (22.580)
322.801 (24.098)
322.801 (24.098)
332.766 (25.337)
332.766 (25.337)
262.938 (23.087)
209.463 (17.865)

1267.379 (43.322)
1267.379 (43.322)
1285.942 (43.855)
1285.942 (43.855)
1012.215 (40.456)
1287.654 (43.218)
1287.654 (43.218)
1352.862 (42.978)
1352.862 (42.978)
1040.222 (41.262)
1333.853 (43.476)
1333.853 (43.476)
1376.487 (43.920)
1376.487 (43.920)
1052.997 (42.105)
832.603 (35.562)

KL
0.147 (0.024)
0.147 (0.024)
0.144 (0.023)
0.144 (0.023)
0.242 (0.028)
0.100 (0.023)
0.100 (0.023)
0.082 (0.026)
0.082 (0.026)
0.213 (0.032)
0.074 (0.024)
0.074 (0.024)
0.054 (0.027)
0.054 (0.027)
0.194 (0.031)
0.301 (0.028)

0.089 (0.008)
0.089 (0.008)
0.080 (0.008)
0.080 (0.008)
0.217 (0.013)
0.079 (0.009)
0.079 (0.009)
0.047 (0.007)
0.047 (0.007)
0.203 (0.013)
0.056 (0.008)
0.056 (0.008)
0.035 (0.007)
0.035 (0.007)
0.197 (0.013)
0.307 (0.014)

out-of-sample

log-likelihood
273.037 (23.599)
273.037 (23.599)
273.774 (23.546)
273.774 (23.546)
229.762 (21.218)
277.480 (31.307)
277.480 (31.307)
282.630 (32.173)
282.630 (32.173)
225.219 (32.407)
282.275 (31.501)
282.275 (31.501)
287.695 (30.598)
287.695 (30.598)
227.271 (32.636)
206.850 (17.904)

1240.274 (43.080)
1240.274 (43.080)
1255.668 (43.845)
1255.668 (43.845)
995.030 (41.295)
1241.183 (44.690)
1241.183 (44.690)
1284.864 (46.432)
1284.864 (46.432)
999.808 (42.547)
1258.275 (51.410)
1258.275 (51.410)
1281.802 (52.317)
1281.802 (52.317)
990.418 (49.155)
831.485 (35.389)

KL
0.186 (0.033)
0.186 (0.033)
0.185 (0.033)
0.185 (0.033)
0.273 (0.031)
0.177 (0.047)
0.177 (0.047)
0.167 (0.047)
0.167 (0.047)
0.282 (0.052)
0.168 (0.044)
0.168 (0.044)
0.157 (0.042)
0.157 (0.042)
0.278 (0.050)
0.318 (0.030)

0.111 (0.009)
0.111 (0.009)
0.103 (0.009)
0.103 (0.009)
0.234 (0.012)
0.110 (0.011)
0.110 (0.011)
0.089 (0.011)
0.089 (0.011)
0.231 (0.013)
0.102 (0.013)
0.102 (0.013)
0.090 (0.014)
0.090 (0.014)
0.236 (0.015)
0.315 (0.013)

32

Table 8: Frank copula, p = 5, τ = 0.25. We report for N = 100 simulations the means (empirical
standard deviation) of cAIC, log-likelihood and Kullback-Leibler divergence for in- and out-of-
sample.

n
500

2000

basis
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

estimator

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

cAIC
-360.594 (47.557)
-336.993 (46.727)
-360.592 (47.556)
-336.988 (46.725)
-362.036 (47.667)
-361.061 (47.876)
-337.168 (46.881)
-361.127 (47.875)
-337.662 (47.032)
-362.499 (47.957)
-366.338 (48.977)
-341.160 (47.923)
-366.349 (49.066)
-341.965 (47.879)
-367.705 (49.361)
-469.840 (53.084)

-1727.807 (92.879)
-1720.633 (91.510)
-1722.397 (92.582)
-1720.614 (91.516)
-1691.259 (94.119)
-1739.756 (92.898)
-1733.853 (92.062)
-1741.132 (93.639)
-1735.821 (92.138)
-1711.962 (94.793)
-1748.914 (92.869)
-1744.690 (91.708)
-1750.886 (93.121)
-1747.276 (92.079)
-1721.190 (94.364)
-1879.555 (100.400)

in-sample

log-likelihood
224.868 (23.846)
233.577 (23.367)
224.866 (23.844)
233.567 (23.367)
221.505 (23.796)
225.293 (24.109)
233.809 (23.581)
225.348 (24.116)
234.309 (23.768)
221.983 (24.124)
230.803 (25.642)
238.346 (24.998)
231.052 (25.705)
239.007 (24.951)
227.811 (26.033)
247.880 (26.665)

936.185 (51.995)
934.948 (50.871)
930.301 (51.590)
934.990 (50.917)
901.497 (51.929)
950.156 (52.507)
950.490 (51.813)
951.715 (53.451)
952.657 (52.148)
924.991 (53.247)
958.886 (52.157)
961.124 (51.420)
960.299 (52.724)
963.413 (51.967)
934.453 (52.719)
955.707 (50.308)

KL
0.069 (0.015)
0.052 (0.014)
0.069 (0.015)
0.052 (0.014)
0.076 (0.016)
0.069 (0.016)
0.052 (0.014)
0.068 (0.016)
0.051 (0.014)
0.075 (0.017)
0.058 (0.017)
0.042 (0.014)
0.057 (0.017)
0.041 (0.014)
0.064 (0.018)
0.023 (0.014)

0.052 (0.008)
0.053 (0.008)
0.055 (0.009)
0.053 (0.008)
0.069 (0.010)
0.045 (0.008)
0.045 (0.008)
0.044 (0.009)
0.044 (0.008)
0.058 (0.010)
0.041 (0.008)
0.040 (0.008)
0.040 (0.008)
0.038 (0.008)
0.053 (0.010)
0.042 (0.007)

out-of-sample

log-likelihood
212.119 (22.292)
221.752 (21.518)
212.120 (22.291)
221.737 (21.503)
210.406 (21.950)
212.141 (22.408)
221.781 (21.581)
212.163 (22.417)
221.811 (21.625)
210.424 (22.072)
213.022 (22.468)
222.502 (21.792)
213.180 (22.605)
222.693 (21.808)
211.298 (22.376)
218.439 (24.503)

899.959 (53.512)
906.592 (53.084)
896.854 (53.064)
906.579 (53.085)
866.834 (52.893)
906.636 (54.404)
913.782 (53.967)
907.165 (53.882)
914.726 (54.163)
879.074 (53.093)
912.768 (54.216)
921.004 (54.418)
912.788 (54.254)
921.845 (54.563)
885.683 (53.229)
928.607 (54.600)

KL
0.092 (0.018)
0.072 (0.018)
0.092 (0.018)
0.072 (0.018)
0.095 (0.019)
0.091 (0.018)
0.072 (0.018)
0.091 (0.018)
0.072 (0.018)
0.095 (0.019)
0.090 (0.018)
0.071 (0.017)
0.089 (0.018)
0.070 (0.017)
0.093 (0.019)
0.079 (0.020)

0.068 (0.009)
0.065 (0.008)
0.070 (0.009)
0.065 (0.008)
0.085 (0.010)
0.065 (0.009)
0.061 (0.008)
0.065 (0.009)
0.061 (0.008)
0.079 (0.010)
0.062 (0.009)
0.058 (0.008)
0.062 (0.009)
0.057 (0.008)
0.075 (0.010)
0.054 (0.008)

33

Table 9: Frank copula, p = 5, τ = 0.5. We report for N = 100 simulations the means (empirical
standard deviation) of cAIC, log-likelihood and Kullback-Leibler divergence for in- and out-of-
sample.

n
500

2000

basis
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

estimator

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

cAIC
-1415.513 (95.173)
-1372.865 (95.691)
-1416.118 (94.703)
-1370.644 (96.073)
-1419.062 (95.797)
-1438.452 (96.167)
-1390.308 (96.280)
-1439.945 (96.677)
-1392.470 (96.900)
-1452.237 (95.511)
-1452.808 (97.149)
-1401.967 (97.648)
-1455.269 (95.506)
-1406.557 (97.444)
-1467.662 (95.402)
-1661.810 (93.456)

-6222.256 (165.004)
-6177.791 (168.010)
-6221.234 (165.740)
-6177.608 (167.987)
-6211.233 (163.204)
-6071.770 (202.385)
-6075.451 (189.843)
-6086.887 (204.297)
-6086.669 (189.615)
-6067.189 (210.334)
-6270.886 (163.767)
-6174.067 (180.823)
-6283.721 (164.016)
-6186.519 (180.786)
-6270.180 (162.441)
-6618.882 (165.047)

in-sample

log-likelihood
800.063 (48.913)
792.971 (49.631)
800.322 (49.056)
791.086 (49.908)
793.366 (49.600)
825.137 (49.707)
812.807 (50.297)
826.623 (50.331)
814.878 (51.101)
828.727 (49.563)
842.794 (51.149)
826.987 (52.302)
845.763 (50.339)
831.350 (52.405)
847.936 (49.922)
844.025 (46.726)

3309.341 (83.010)
3286.849 (84.769)
3310.947 (83.762)
3288.924 (84.600)
3263.370 (82.100)
3261.303 (111.370)
3264.731 (102.870)
3285.990 (111.272)
3288.436 (102.737)
3226.330 (117.193)
3398.083 (83.165)
3346.958 (95.450)
3418.422 (84.066)
3367.909 (95.576)
3362.512 (82.924)
3324.391 (82.514)

KL
0.150 (0.035)
0.164 (0.029)
0.149 (0.035)
0.167 (0.030)
0.163 (0.030)
0.099 (0.036)
0.124 (0.030)
0.096 (0.037)
0.120 (0.032)
0.092 (0.032)
0.064 (0.036)
0.096 (0.032)
0.058 (0.034)
0.087 (0.032)
0.054 (0.031)
0.062 (0.022)

0.083 (0.012)
0.094 (0.012)
0.082 (0.012)
0.093 (0.012)
0.105 (0.011)
0.107 (0.055)
0.105 (0.046)
0.094 (0.054)
0.093 (0.045)
0.124 (0.058)
0.038 (0.013)
0.064 (0.030)
0.028 (0.013)
0.053 (0.030)
0.056 (0.013)
0.075 (0.009)

out-of-sample

log-likelihood
748.560 (41.769)
747.230 (40.779)
748.935 (41.464)
746.180 (40.554)
745.712 (41.223)
725.712 (54.884)
721.823 (54.673)
726.322 (55.141)
722.745 (54.833)
727.487 (55.800)
726.197 (56.726)
721.275 (56.418)
727.886 (57.056)
723.540 (56.827)
729.856 (57.133)
809.351 (36.341)

3231.323 (76.852)
3218.406 (76.112)
3231.008 (77.007)
3218.269 (76.114)
3192.099 (73.018)
3145.953 (126.053)
3154.046 (113.485)
3151.995 (125.877)
3158.853 (112.910)
3107.450 (126.870)
3199.437 (86.854)
3166.198 (95.442)
3204.696 (86.936)
3171.260 (95.821)
3163.574 (82.997)
3312.967 (70.573)

KL
0.239 (0.044)
0.242 (0.041)
0.238 (0.043)
0.244 (0.040)
0.245 (0.043)
0.285 (0.080)
0.292 (0.080)
0.283 (0.081)
0.290 (0.081)
0.281 (0.081)
0.284 (0.080)
0.293 (0.079)
0.280 (0.081)
0.289 (0.080)
0.276 (0.081)
0.117 (0.031)

0.129 (0.014)
0.136 (0.014)
0.130 (0.014)
0.136 (0.014)
0.149 (0.014)
0.172 (0.050)
0.168 (0.044)
0.169 (0.050)
0.166 (0.043)
0.191 (0.052)
0.145 (0.024)
0.162 (0.033)
0.143 (0.024)
0.159 (0.033)
0.163 (0.024)
0.089 (0.011)

34

Table 10: Gumbel copula, p = 5, τ = 0.25. We report for N = 100 simulations the means
(empirical standard deviation) of cAIC, log-likelihood and Kullback-Leibler divergence for in- and
out-of-sample.

n
500

2000

basis
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

estimator

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

cAIC
-367.847 (60.734)
-361.198 (62.095)
-367.843 (60.735)
-361.186 (62.095)
-366.691 (60.551)
-373.242 (62.916)
-366.182 (64.289)
-374.599 (64.021)
-367.637 (65.288)
-372.832 (63.079)
-380.495 (64.812)
-373.109 (65.654)
-382.382 (65.863)
-375.193 (66.532)
-379.890 (64.596)
-538.979 (82.601)

-1979.901 (145.589)
-1982.145 (145.548)
-1980.551 (145.720)
-1982.896 (145.764)
-1825.095 (128.620)
-1984.058 (142.105)
-1985.913 (141.570)
-2010.155 (141.340)
-2012.243 (141.283)
-1844.893 (128.348)
-2005.082 (141.581)
-2007.020 (141.659)
-2026.301 (141.446)
-2028.460 (141.500)
-1857.461 (128.865)
-2177.701 (149.111)

in-sample

log-likelihood
238.669 (32.406)
246.569 (31.965)
238.666 (32.410)
246.553 (31.967)
224.523 (31.081)
245.235 (35.918)
253.141 (35.641)
246.491 (36.902)
254.485 (36.596)
232.023 (34.981)
254.134 (38.512)
261.902 (37.864)
255.701 (39.067)
263.416 (38.615)
240.973 (37.290)
282.070 (41.125)

1119.031 (85.577)
1121.598 (85.200)
1120.131 (85.848)
1122.868 (85.523)
999.244 (69.658)
1137.515 (85.298)
1139.842 (84.505)
1164.074 (82.853)
1167.049 (82.475)
1032.288 (71.834)
1163.419 (85.480)
1166.131 (85.188)
1181.070 (83.519)
1184.299 (83.210)
1048.558 (73.402)
1103.760 (74.748)

KL
0.145 (0.035)
0.129 (0.036)
0.145 (0.035)
0.129 (0.036)
0.173 (0.041)
0.132 (0.030)
0.116 (0.031)
0.129 (0.029)
0.113 (0.030)
0.158 (0.034)
0.114 (0.026)
0.099 (0.028)
0.111 (0.026)
0.096 (0.028)
0.140 (0.031)
0.058 (0.020)

0.072 (0.013)
0.071 (0.013)
0.072 (0.013)
0.070 (0.013)
0.132 (0.015)
0.063 (0.013)
0.062 (0.013)
0.050 (0.011)
0.048 (0.011)
0.116 (0.013)
0.050 (0.012)
0.049 (0.012)
0.041 (0.011)
0.040 (0.011)
0.108 (0.013)
0.080 (0.009)

out-of-sample

log-likelihood
224.234 (30.019)
234.837 (29.316)
224.227 (30.020)
234.826 (29.318)
212.649 (29.778)
225.734 (31.273)
236.257 (30.424)
226.244 (31.313)
236.700 (30.481)
214.465 (30.968)
229.380 (31.874)
239.789 (31.119)
230.340 (31.590)
240.482 (31.220)
217.953 (31.562)
255.378 (41.273)

1068.321 (70.459)
1071.926 (70.307)
1068.546 (70.190)
1072.419 (70.444)
959.817 (62.321)
1071.170 (68.721)
1074.841 (69.208)
1088.866 (68.687)
1093.625 (69.387)
971.440 (62.643)
1084.108 (69.158)
1088.208 (69.818)
1096.128 (69.299)
1100.977 (69.956)
977.864 (62.918)
1088.326 (78.662)

KL
0.182 (0.040)
0.161 (0.038)
0.182 (0.040)
0.161 (0.038)
0.205 (0.041)
0.179 (0.039)
0.158 (0.037)
0.178 (0.040)
0.157 (0.037)
0.202 (0.040)
0.172 (0.038)
0.151 (0.036)
0.170 (0.038)
0.150 (0.036)
0.195 (0.039)
0.120 (0.023)

0.104 (0.020)
0.102 (0.020)
0.104 (0.021)
0.102 (0.020)
0.158 (0.022)
0.103 (0.019)
0.101 (0.018)
0.094 (0.018)
0.091 (0.017)
0.152 (0.021)
0.096 (0.018)
0.094 (0.017)
0.090 (0.018)
0.088 (0.017)
0.149 (0.020)
0.094 (0.010)

35

Table 11: Gumbel copula, p = 5, τ = 0.5. We report for N = 100 simulations the means (empirical
standard deviation) of cAIC, log-likelihood and Kullback-Leibler divergence for in- and out-of-
sample.

n
500

2000

basis
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

estimator

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

cAIC
-1485.072 (113.763)
-1465.888 (113.899)
-1485.151 (114.105)
-1465.816 (113.946)
-1491.261 (112.666)
-1517.451 (114.778)
-1489.424 (113.322)
-1538.338 (118.950)
-1512.448 (119.049)
-1540.986 (114.346)
-1550.921 (118.098)
-1521.033 (117.350)
-1568.030 (118.774)
-1540.780 (118.664)
-1566.464 (115.817)
-1910.363 (135.463)

-6879.390 (208.350)
-6843.269 (210.515)
-6883.637 (209.743)
-6845.770 (210.858)
-6750.065 (203.877)
-6880.660 (215.780)
-6844.694 (216.807)
-6933.951 (216.414)
-6891.965 (218.149)
-6790.248 (213.877)
-6879.085 (285.348)
-6843.225 (284.344)
-6913.524 (286.597)
-6875.456 (283.830)
-6781.977 (282.808)
-7667.143 (229.977)

in-sample

log-likelihood
847.851 (62.292)
850.324 (62.069)
848.255 (62.257)
850.539 (62.240)
835.817 (60.183)
882.460 (61.782)
876.409 (60.651)
900.822 (66.060)
897.671 (66.502)
885.910 (60.926)
919.972 (66.118)
913.465 (65.850)
934.758 (66.869)
931.546 (67.241)
917.358 (63.401)
969.001 (67.497)

3657.316 (105.782)
3637.606 (108.178)
3662.647 (107.050)
3641.869 (108.475)
3542.265 (102.959)
3721.623 (113.520)
3700.933 (114.389)
3775.597 (113.565)
3753.305 (115.288)
3639.543 (111.852)
3782.239 (157.943)
3763.084 (157.898)
3816.549 (158.275)
3797.862 (156.688)
3688.166 (156.064)
3847.911 (114.868)

KL
0.314 (0.042)
0.309 (0.040)
0.313 (0.041)
0.308 (0.040)
0.338 (0.042)
0.245 (0.040)
0.257 (0.040)
0.208 (0.039)
0.214 (0.039)
0.238 (0.040)
0.169 (0.036)
0.183 (0.038)
0.140 (0.036)
0.146 (0.038)
0.175 (0.038)
0.071 (0.021)

0.185 (0.016)
0.195 (0.015)
0.182 (0.017)
0.193 (0.015)
0.243 (0.018)
0.153 (0.028)
0.163 (0.027)
0.126 (0.026)
0.137 (0.025)
0.194 (0.027)
0.123 (0.065)
0.132 (0.065)
0.106 (0.065)
0.115 (0.064)
0.170 (0.065)
0.090 (0.011)

out-of-sample

log-likelihood
808.139 (56.881)
812.849 (56.876)
808.032 (56.628)
812.763 (56.699)
797.148 (55.278)
795.374 (66.447)
795.805 (66.638)
808.077 (67.064)
810.100 (67.352)
794.039 (66.322)
803.286 (67.423)
803.765 (67.472)
813.422 (67.767)
815.547 (68.045)
797.707 (66.256)
943.318 (67.724)

3567.255 (96.307)
3558.065 (96.651)
3568.515 (95.583)
3558.688 (96.586)
3464.231 (95.815)
3550.782 (110.529)
3541.510 (110.478)
3577.307 (112.693)
3565.394 (111.907)
3466.049 (110.651)
3467.350 (136.065)
3458.433 (137.442)
3483.017 (137.058)
3472.879 (136.632)
3378.433 (133.242)
3829.235 (119.823)

KL
0.402 (0.059)
0.392 (0.055)
0.402 (0.059)
0.392 (0.056)
0.424 (0.059)
0.427 (0.083)
0.426 (0.081)
0.402 (0.081)
0.398 (0.079)
0.430 (0.078)
0.411 (0.092)
0.410 (0.091)
0.391 (0.091)
0.387 (0.090)
0.422 (0.088)
0.131 (0.026)

0.235 (0.024)
0.239 (0.024)
0.234 (0.024)
0.239 (0.023)
0.286 (0.026)
0.243 (0.033)
0.247 (0.032)
0.230 (0.033)
0.235 (0.032)
0.285 (0.034)
0.284 (0.058)
0.289 (0.059)
0.277 (0.058)
0.282 (0.058)
0.329 (0.057)
0.104 (0.010)

36

Table 12: Normalmixture, p = 5, mixture 1. We report for N = 100 simulations the means
(empirical standard deviation) of cAIC, log-likelihood and Kullback-Leibler divergence for in- and
out-of-sample.

n
500

2000

basis
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

estimator

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

cAIC
-946.498 (62.089)
-946.841 (62.115)
-951.412 (62.240)
-951.412 (62.240)
-933.419 (62.322)
-975.252 (59.970)
-974.980 (60.127)
-978.510 (59.585)
-978.237 (59.738)
-940.798 (61.633)
-988.965 (60.413)
-988.997 (60.327)
-990.162 (59.479)
-990.544 (59.742)
-944.512 (62.023)
-1055.554 (62.442)

-4829.079 (125.027)
-4829.079 (125.027)
-4831.460 (124.625)
-4831.460 (124.625)
-4114.635 (124.486)
-4862.455 (130.212)
-4862.455 (130.212)
-4898.140 (130.319)
-4898.140 (130.319)
-4145.790 (129.886)
-4925.202 (122.919)
-4925.202 (122.919)
-4943.523 (122.813)
-4943.523 (122.813)
-4164.624 (126.240)
-4272.203 (127.817)

in-sample

log-likelihood
572.186 (32.640)
573.023 (32.645)
576.240 (33.180)
576.240 (33.180)
540.623 (32.120)
602.744 (32.273)
603.101 (32.215)
606.610 (32.492)
606.845 (32.410)
556.042 (32.313)
617.907 (33.218)
618.250 (32.989)
619.900 (33.023)
620.640 (32.988)
564.178 (33.078)
543.297 (31.252)

2649.381 (66.224)
2649.381 (66.224)
2654.539 (66.114)
2654.539 (66.114)
2165.662 (65.108)
2705.137 (70.886)
2705.137 (70.886)
2765.277 (72.222)
2765.277 (72.222)
2219.477 (70.023)
2780.762 (65.781)
2780.762 (65.781)
2822.814 (66.177)
2822.814 (66.177)
2245.752 (67.584)
2153.382 (64.125)

KL
0.385 (0.036)
0.383 (0.037)
0.377 (0.038)
0.377 (0.038)
0.448 (0.039)
0.324 (0.039)
0.323 (0.039)
0.316 (0.041)
0.316 (0.040)
0.417 (0.039)
0.294 (0.039)
0.293 (0.039)
0.290 (0.042)
0.288 (0.042)
0.401 (0.040)
0.443 (0.036)

0.213 (0.017)
0.213 (0.017)
0.210 (0.017)
0.210 (0.017)
0.454 (0.020)
0.185 (0.019)
0.185 (0.019)
0.155 (0.020)
0.155 (0.020)
0.428 (0.022)
0.147 (0.016)
0.147 (0.016)
0.126 (0.016)
0.126 (0.016)
0.414 (0.021)
0.461 (0.020)

out-of-sample

log-likelihood
529.639 (35.256)
530.666 (35.011)
532.308 (35.455)
532.308 (35.455)
497.309 (35.402)
527.131 (48.172)
527.936 (47.615)
528.763 (48.230)
529.397 (47.654)
483.170 (47.361)
534.937 (44.817)
535.565 (44.341)
534.505 (46.040)
535.466 (44.886)
484.633 (44.070)
-497.755 (35.896)

2494.534 (69.278)
2494.534 (69.278)
2495.738 (69.417)
2495.738 (69.417)
2050.639 (63.536)
2487.624 (83.008)
2487.624 (83.008)
2508.181 (82.241)
2508.181 (82.241)
2036.933 (76.982)
2505.953 (78.068)
2505.953 (78.068)
2514.654 (77.940)
2514.654 (77.940)
2025.888 (75.157)
2057.809 (60.883)

KL
0.436 (0.047)
0.434 (0.047)
0.430 (0.049)
0.430 (0.049)
0.500 (0.049)
0.441 (0.074)
0.439 (0.073)
0.438 (0.075)
0.436 (0.074)
0.529 (0.075)
0.425 (0.061)
0.424 (0.060)
0.426 (0.064)
0.424 (0.062)
0.526 (0.061)
0.500 (0.040)

0.258 (0.017)
0.258 (0.017)
0.258 (0.017)
0.258 (0.017)
0.480 (0.020)
0.262 (0.030)
0.262 (0.030)
0.251 (0.030)
0.251 (0.030)
0.487 (0.031)
0.253 (0.027)
0.253 (0.027)
0.248 (0.026)
0.248 (0.026)
0.493 (0.028)
0.477 (0.019)

37

Table 13: Normalmixture, p = 5, mixture 2. We report for N = 100 simulations the means
(empirical standard deviation) of cAIC, log-likelihood and Kullback-Leibler divergence for in- and
out-of-sample.

n
500

2000

basis
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 4
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4, D3 = 6
d = 2, D2 = 4
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 3
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6, D3 = 6
d = 3, D2 = 6
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 4
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8, D3 = 6
d = 4, D2 = 8

estimator

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA

Test
Cond
Test
Cond
SimpA
CDVine

cAIC
-1003.752 (64.303)
-1003.542 (64.232)
-1007.686 (63.925)
-1007.686 (63.925)
-945.217 (61.189)
-1037.104 (64.044)
-1036.961 (64.013)
-1050.334 (65.538)
-1051.900 (66.289)
-956.991 (62.098)
-1058.971 (64.695)
-1062.101 (65.635)
-1091.920 (66.493)
-1093.184 (66.538)
-963.694 (61.741)
-1054.885 (62.046)

-5405.236 (141.170)
-5405.236 (141.170)
-5449.218 (139.625)
-5449.218 (139.625)
-4229.033 (126.277)
-5454.495 (134.315)
-5454.495 (134.315)
-5637.733 (130.667)
-5637.733 (130.667)
-4265.989 (127.044)
-5614.852 (129.186)
-5614.852 (129.186)
-5695.225 (131.684)
-5695.225 (131.684)
-4278.977 (123.207)
-4279.110 (125.550)

in-sample

log-likelihood
610.460 (34.902)
611.619 (35.097)
613.798 (34.615)
613.798 (34.615)
548.191 (31.951)
646.464 (35.786)
647.615 (35.898)
659.045 (37.481)
661.377 (38.071)
570.287 (33.655)
670.710 (37.249)
674.470 (37.977)
701.950 (39.641)
704.232 (39.857)
583.089 (33.850)
543.033 (31.055)

2971.796 (78.796)
2971.796 (78.796)
3002.132 (77.335)
3002.132 (77.335)
2233.408 (65.014)
3048.628 (71.814)
3048.628 (71.814)
3219.036 (69.253)
3219.007 (69.253)
2297.557 (66.638)
3195.264 (69.526)
3195.264 (69.526)
3296.468 (72.345)
3296.468 (72.345)
2321.777 (65.706)
2156.685 (62.843)

KL
0.725 (0.048)
0.723 (0.048)
0.718 (0.046)
0.718 (0.046)
0.850 (0.052)
0.653 (0.045)
0.651 (0.045)
0.628 (0.050)
0.623 (0.051)
0.805 (0.052)
0.605 (0.048)
0.597 (0.050)
0.542 (0.048)
0.537 (0.049)
0.780 (0.051)
0.860 (0.051)

0.466 (0.024)
0.466 (0.024)
0.451 (0.024)
0.451 (0.024)
0.835 (0.026)
0.428 (0.020)
0.428 (0.020)
0.342 (0.019)
0.342 (0.019)
0.803 (0.027)
0.354 (0.021)
0.354 (0.021)
0.304 (0.021)
0.304 (0.021)
0.791 (0.027)
0.874 (0.027)

out-of-sample

log-likelihood
567.684 (35.749)
569.345 (35.514)
570.361 (35.702)
570.361 (35.702)
508.091 (34.241)
568.860 (44.237)
570.205 (44.955)
575.799 (45.620)
578.350 (46.022)
494.364 (45.943)
583.868 (42.711)
587.103 (41.795)
603.192 (44.220)
605.514 (43.829)
498.921 (41.440)
498.360 (36.658)

2813.785 (78.037)
2813.785 (78.037)
2834.962 (76.885)
2834.962 (76.885)
2114.739 (66.771)
2829.096 (76.145)
2829.096 (76.145)
2924.024 (77.216)
2924.024 (77.216)
2107.085 (73.003)
2902.696 (79.594)
2902.696 (79.594)
2939.544 (82.600)
2939.544 (82.600)
2092.310 (76.083)
2068.477 (62.685)

KL
0.779 (0.056)
0.775 (0.057)
0.773 (0.055)
0.773 (0.055)
0.898 (0.052)
0.776 (0.078)
0.774 (0.080)
0.762 (0.081)
0.757 (0.082)
0.925 (0.082)
0.746 (0.070)
0.740 (0.068)
0.708 (0.070)
0.703 (0.068)
0.916 (0.067)
0.917 (0.057)

0.518 (0.026)
0.518 (0.026)
0.508 (0.026)
0.508 (0.026)
0.868 (0.027)
0.511 (0.028)
0.511 (0.028)
0.463 (0.028)
0.463 (0.028)
0.872 (0.032)
0.474 (0.027)
0.474 (0.027)
0.456 (0.028)
0.456 (0.028)
0.879 (0.032)
0.891 (0.027)

Table 14: Log-likelihood values of ﬁtted D-vine copulas for the MAGIC data set using (a) the
estimator ’Test’ with basis d = 4, D2 = 8, D3 = 6, (b) the estimator ’SimpA’ with basis d =
4, D2 = 8 and (c) the parametric estimator ’CDVine’.

data set
in
out

y2
y1
y4
y3
y6
y5
y8
y7
y10
y9
y12
y11
mean (sd)

(a) d = 4, D2 = 8, D3 = 6
in-sample
out-of-sample

(b) d = 4, D2 = 8

(c) CDVine

in-sample

out-of-sample

in-sample

out-of-sample

6340
6548
6661
6474
6414
6478

5775
6105
5702
5830
5832
5668

6248
6450
6494
6328
6268
6408

5680
6006
5594
5686
5766
5592

5346
5328
5554
5532
5340
5340

5273
5363
5268
5248
5333
5333

6486 (111)

5819 (155)

6366 (100)

5721 (154)

5446 (119)

5318 (67)

38

