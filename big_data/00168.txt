6
1
0
2

 
r
a

M
1

 

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
8
6
1
0
0

.

3
0
6
1
:
v
i
X
r
a

New optimized Schwarz algorithms for one dimensional

Schr¨odinger equation with general potential

F. Xing ∗

March 2, 2016

Abstract

The aim of this paper is to develop new optimized Schwarz algorithms for the one
dimensional Schr¨odinger equation with linear or nonlinear potential. After present-
ing the classical algorithm which is an iterative process, we propose a new algorithm
for the Schr¨odinger equation with time-independent linear potential. Thanks to two
main ingredients (constructing explicitly the interface problem and using a direct
method on the interface problem), the new algorithm turns to be a direct process.
Thus, it is free to choose the transmission condition. Concerning the case of time-
dependent linear potential or nonlinear potential, we propose to use a pre-processed
linear operator as preconditioner which leads to a preconditioned algorithm. Nu-
merically, the convergence is also independent of the transmission condition.
In
addition, both of these new algorithms implemented in parallel cluster are robust,
scalable up to 256 sub domains (MPI process) and take much less computation time
than the classical one, especially for the nonlinear case.

1

Introduction

This paper deals with the optimized Schwarz method without overlap for the one dimen-
sional Schr¨odinger equation deﬁned on a bounded spatial domain Ω = (a0, b0), a0, b0 ∈ R
and t ∈ (0, T ), which reads

(cid:26) L u := i∂tu + ∂xxu + V u = 0, (t, x) ∈ (0, T ) × (a0, b0),

u(0, x) = u0(x), x ∈ (a0, b0),

(1)
where L is the Schr¨odinger operator, the initial value u0 ∈ L2(R) and the general real
potential V consists of a linear and a nonlinear part

V = V (t, x) + f (t, x, u).

∗Laboratoire de Math´ematiques J.A. Dieudonn´e, UMR 7351 CNRS, University Nice Sophia Antipolis,
Team COFFEE, INRIA Sophia Antipolis M´editerran´ee, Parc Valrose 06108 Nice Cedex 02, France, BRGM
Orl´eans France

1

We complete the equation with homogeneous Neumann boundary conditions on the bound-
ary ∂nu(t, x) = 0, x = a0, b0 where ∂n denotes the normal directive, n being the outward
unit vector on the boundary.

Among the various domain decomposition methods, we focus our attention on the
optimized Schwarz method [10], which has been widely studied over the past years for
many diﬀerent equations, like the Poisson equation [14], the Helmholtz equation [8, 12]
and the convection-diﬀusion equation [15].
In order to perform the optimized Schwarz method, the equation (1) is ﬁrst semi-
discretized in time on the entire domain (0, T ) × (a0, b0). The time interval (0, T ) is
discretized uniformly with NT intervals of length ∆t. We denote by un (resp. Vn) an
approximation of the solution u (resp. V ) at time tn = n∆t. The usual semi-discrete in
time scheme developed by Dur´an and Sanz-Serna [9] applied to (1) reads as

un − un−1

∆t

i

+ ∂xx

un + un−1

Vn + Vn−1

un + un−1

2

+

2
un + un−1

+ f (tn+1/2, x,

2

2
un + un−1

2

= 0, 1 (cid:54) n (cid:54) NT .

)

By introducing new variables vn = (un + un−1)/2 with v0 = u0 and Wn = (Vn + Vn−1)/2,
we get a stationary equation deﬁned on Ω with unknown vn

Lxvn :=

2i
∆t

vn + ∂xxvn + Wnvn + f (tn+1/2, x, vn)vn =

2i
∆t

un−1,

(2)

The original unknown un is recovered by un = 2vn − un−1.

The equation (2) is stationary for any 1 (cid:54) n (cid:54) NT . We can therefore apply the
optimized Schwarz method. Let us decompose the spatial domain Ω into N sub domains
Ωj = (aj, bj), j = 1, 2, ..., N without overlap as shown in Figure 1 for N = 3. The classical
Schwarz algorithm is an iterative process and we identify the iteration number thanks
to label k. We denote by vk
j,n the solution on sub domain Ωj at iteration k = 1, 2, ... of
the Schwarz algorithm (resp uk
j,n). Assuming that u0,n−1 is known, the optimized Schwarz
algorithm for (2) consists in applying the following sequence of iterations for j = 1, 2, ..., N



Lxvk
∂nj vk
∂nj vk

2i
j,n =
∆t
j,n + Sjvk
j,n + Sjvk

uj,n−1, (x, y) ∈ Ωj,
j,n = ∂nj vk−1
j,n = ∂nj vk−1

j−1,n + Sjvk−1
j+1,n + Sjvk−1

j−1,n, x = aj,
j+1,n, x = bj,

(3)

with a special treatment for the two extreme sub domains Ω1 and ΩN since the boundary
conditions are imposed on {x = a1} and {x = bN}

∂n1vk

1,n = 0, x = a1,

∂nN vk

N,n = 0, x = bN ,

where Sj is the transmission operator.

2

pade : Sjv = −i
Sm

(cid:114) 2i
≈(cid:16) − i
m(cid:88)
4m )(cid:1), dm
s = eiθ/2/(cid:0)m cos2( (2s−1)π

∆t

am
s + i

s=0

m(cid:88)

s=1

+ V + f (v)v,

s )−1(cid:17)

+ V + f (v) + dm

(5)

v,

s dm
am
s (

2i
∆t

Ω1

Ω2

Ω3

a0 = a1

b1 = a2

b2 = a3

x

b2 = b0

Figure 1: Domain decomposition without overlap, N = 3.

The transmission condition is an important issue for this method. In recent years, many
works contribute to look for good transmission conditions to obtain faster convergence in
the context of the Schwarz waveform relaxation method and the optimized Schwarz method
for the Schr¨odinger equation. The widely used Robin transmission condition

Robin : Sjv = −ip · v, p ∈ R+ is parameter,

(4)

the optimal transmission condition for constant potential etc. are considered in [13]. The
authors of [2, 7, 6] introduce the idea using some absorbing boundary operators [3, 4] as
the transmission operator. For example,

s = eiθ tan2( (2s−1)π

where am
4m ), s = 0, 1, ..., m are constant
coeﬃcients associate with the Pad´e approximation of order m. These operators are con-
structed by using some pseudo diﬀerential techniques and could be seen as diﬀerent ap-
proximate accesses to the optimal operator. However, numerically there are always some
parameters in these transmission conditions that need to be optimized for each potential
V , each size of time step ∆t, each mesh and each number of sub domains N . This leads
to huge extra tests before launching the simulation.

The parallel scalability is one of the key objectives of this method. The authors of
[7] introduced some new eﬃcient and robust algorithms in the framework of the Schwarz
waveform relaxation algorithm for the one dimensional Schr¨odinger equation. Then, the
new algorithms are extended to the two dimensional linear case in the context of the
optimized Schwarz algorithm [6]. These new algorithms are much more scalable and
eﬃcient than the classical one. However, choosing the transmission condition and the
parameters in the transmission condition remain very inconvenient.

The objective of this article is to develop scalable optimized Schwarz algorithms on
parallel computers. We propose some new algorithms based on the studies of the classical
algorithm. These new algorithms have following main features theoretically or numerically:
the convergence is independent of the transmission condition, the size of time step ∆t, the
mesh and the number of sub domains. In addition, the computation time is scalable and
much less compared with the classical one.

3

This paper is organized as follows. In section 2, we study the classical algorithm and
construct the interface problem. In Section 3 and 4, we present the new algorithm for
the Schr¨odinger equation with time-independent linear potential and the preconditioned
algorithm for the Schr¨odinger equation with general potential. Some numerical results are
shown in Section 5. Finally, we draw a conclusion.

Remark 1.1. To simplify the presentation, we only consider the Robin transmission con-
dition (4) in the following paper. Applying other more complex transmission conditions in
[13, 2, 7] such as (5) is direct, however they are not useful in our new algorithms.

2 Classical optimized Schwarz algorithm

Let us introduce the ﬂuxes lk

j,n, j = 1, 2, ..., N deﬁned as

j,n and rk
j,n(aj) + Sjvk

lk
j,n = ∂nj vk

j,n(aj),

rk
j,n = ∂nj vk

j,n(bj) + Sjvk
lk
1,n = rk

j,n(bj),

with a special deﬁnition for the two extreme sub domains:
algorithm (3) can be splitted into local problems on sub domain Ωj, j = 1, 2, ..., N

N,n = 0. Thus, the



Lxvk
∂nj vk
∂nj vk

2i
j,n =
∆t
j,n + Sjvk
j,n + Sjvk

uj,n−1,
j,n = lk
j,n = rk

j,n, x = aj,
j,n, x = bj,

(6)

(7)

and ﬂux problems(cid:40) lk+1

j,n = −rk
j,n = −lk
rk+1

j−1,n + 2Sjvk
j+1,n + 2Sjvk

j−1,n(bj−1), j = 2, 3, ..., N,
j+1,n(aj+1), j = 1, 2, ..., N − 1.

The spatial approximation is realized by the standard P1 ﬁnite element method. If f =
j,n) the nodal interpolation
j,n) with Nj nodes, Mj the mass matrix, Sj the stiﬀness matrix, Mj,Wn
Wnvφdx. Then, the matrix formulation

the generalized mass matrix with respect to(cid:82) bj

0, then the system (6) is linear. Let us denote by vk
vector of vk

j,n (resp. uk

j,n (resp. uk

of the N local problems is given by

(8)
Mj − Sj + Mj,Wn and ”·(cid:62)” is the standard notation of the transpose of a
where Aj,n = 2i
matrix or a vector. The boundary matrix MΓj and the restriction matrix Qj are deﬁned
as

j,n =

∆t

,

j,n
rk
j,n

j,n−1 − MΓj Q(cid:62)

j

(cid:18)lk

(cid:19)

aj

vk

Mjuk

2i
∆t

(cid:16)Aj,n + ip · MΓj
1 0 ··· 0

(cid:17)
 ∈ CNj×Nj , Qj =

0 0 ··· 0
...
...
0 0 ··· 1

...

4

(cid:18)1 0 0 ··· 0 0

0 0 0 ··· 0 1

(cid:19)

∈ C2×Nj .

MΓj =

q, q = 1, 2, ... 
to(cid:82) bj

Let us denote by ζq
f (tn+1/2, x, ζ q

aj

(cid:17)

ζ q
j =

2i
∆t

(cid:16) 2i
+ ∂xx + Wn
∆t
j − ip · ζ q
∂nζ q
j = lk
j − ip · ζ q
∂nζ q
j = rk
j the nodal interpolation of ζ q
(cid:17)
j )ζ q

j,n, x = aj,
j,n, x = bj.

(cid:16)Aj,n + ip · MΓj
(cid:40) lk+1

j and bf (tn+1/2,x,ζq

j ) the vector with respect

j φdx. Thus, the matrix formulation of (9) is
j,n−1 − bf (tn+1/2,x,ζq−1

Mjuk

ζq
j =

j

2i
∆t

) − Q(cid:62)

j

(cid:18)lk

(cid:19)

n
rk
n

In addition, the discrete formulation of the ﬂux problems (7) reads
j−1,n, j = 1, 2, ..., N − 1,
j+1,n, j = 2, 3, ..., N.

j−1,n − 2ip · Qj−1,rvk
j+1,n − 2ip · Qj+1,lvk

j,n = −rk
j,n = −lk
rk+1

.

(10)

(11)

If f (cid:54)= 0, then the equation (6) is nonlinear. Assuming that uj,n−1 is known, the
j = uj,n−1 and
j,n as the limit of the iterative procedure with respect to the label

j,n is accomplished by a ﬁxed point procedure. We take ζ 0

computation of vk
compute the solution vk

uj,n−1 − f (tn+1/2, x, ζ q−1

j

)ζ q−1

j

,

(9)

where Qj,l = (1, 0,··· , 0, 0) ∈ CNj , Qj,r = (0, 0,··· , 0, 1) ∈ CNj .
The classical algorithm is initialized by an initial guess of l0

j,n, j = 1, 2, ..., N .
The boundary conditions for any sub domain Ωj at iteration k + 1 involve the knowledge
of the values of the functions on adjacent sub domains Ωj−1 and Ωj+1 at prior iteration k.
Thanks to the initial guess, we can solve the Schr¨odinger equation on each sub domain,
allowing to build the new boundary conditions for the next step, communicating them
to other sub domains. This procedure is summarized in (12) for N = 3 sub domains at

j,n and r0

iteration k + 1.rk

 Solve−−−→

1,n
lk
2,n
rk
2,n
rk
3,n

−→

vk

vk
vk

1,n

2,n

3,n

−rk

1,n − 2ip · Q1,rvk
2,n − 2ip · Q2,lvk
−lk
−rk
2,n − 2ip · Q2,rvk
−lk
3,n − 2ip · Q3,lvk

1,n

j,n

j,n

N,n

 Comm.

−−−−→

 .

rk+1

1,n
lk+1
2,n
rk+1
2,n
lk+1
3,n

(12)

Let us deﬁne the discrete interface vector by
1,n,··· , lk

gk
n = (rk

j,n, rk

j,n,··· , lk

N,n)(cid:62).

Thanks to this deﬁnition, we give a new interpretation to the algorithm which can be
written as

(13)
where I is identity operator and Rh,n is an operator. The solution to this iteration process
is given as the solution to the discrete interface problem

n = Rh,ngk
gk+1

n = I − (I − Rh,n)gk
n.

(I − Rh,n)gn = 0.

5



Lh,n =

(cid:122)(cid:125)(cid:124)(cid:123)

MPI 0

(cid:122)

(cid:125)(cid:124)

(cid:123)

(cid:122)

(cid:125)(cid:124)

(cid:123)

MPI 1

MPI 2

(cid:122)

(cid:125)(cid:124)

MPI N−2

(cid:123) MPI N−1
(cid:122)(cid:125)(cid:124)(cid:123)

x2,1
n

x2,2
n

x1,4
n

x2,3
n

x2,4
n

x3,1
n

x3,2
n

x3,3
n

x3,4
n

···

···

xN−1,1

n

xN−1,2

n

xN−1,3

n

xN−1,4

n

xN,1

n



3 New algorithm for time-independent linear poten-

tial

The ﬁrst aim of this section is to show that if the potential is linear f = 0, then the
discrete interface problem can be written as

(14)
where dn is a vector and Lh,n ∈ C(2N−2)×(2N−2) is a matrix (the notation “MPI j” above
the columns of the matrix will be used after in this section)

(I − Lh,n)gn = dn,

.

(15)

Especially, if the linear potential is time-independent V = V (x), then

Lh,1 = Lh,2 = ... = Lh,NT .

(16)

Secondly, based on these observations, we propose a new Schwarz algorithm for the

Schr¨odinger equation with time-independent linear potential.
Proposition 1. Assuming that f = 0, then the operator Rh,n is linear

Rh,ngk

n = Lh,ngk

n + dn

where Lh,n is a matrix as shown by (15).
independent V = V (x), then the interface matrix Lh,n satisﬁes (16).
Proof. Firstly, by some straight forward calculations using (8) and (11), we can verify that

In addition, if the linear potential is time-

n = −I − 2ip · Qj,l(Aj,n + ip · MΓj )−1MΓj Q(cid:62)
xj,1
j,l,
n = −2ip · Qj,l(Aj,n + ip · MΓj )−1MΓj Q(cid:62)
xj,2
j,r,
n = −2ip · Qj,r(Aj,n + ip · MΓj )−1MΓj Q(cid:62)
xj,3
j,l,
n = −I − 2ip · Qj,r(Aj,n + ip · MΓj )−1MΓj Q(cid:62)
xj,4
j,r,

6

(17)

and dn = (dn,1,r, ..., dn,j,l, dn,j,r, ..., dn,N,r)(cid:62) with

dn,j,l = 2ip · 2i
∆t
dn,j,r = 2ip · 2i
∆t

· Qj−1,r(Aj−1,n + ip · MΓj−1)−1Mj−1uj−1,n, j = 2, 3, ..., N,
· Qj+1,l(Aj+1,n + ip · MΓj+1)−1Mj+1uj+1,n, j = 1, 2, ..., N − 1.

(18)

Secondly, since V = V (x), then for j = 1, 2, ..., N , we have
Mj,W1 = Mj,W2 = ... = Mj,WNT

,

⇒ Aj,1 = Aj,2 = ... = Aj,NT =

Thus, the elements of Lh,n satisfy

Mj − Sj + Mj,Wn.

2i
∆t

for j = 1, 2, ..., N , s = 1, 2, 3, 4.

xj,s
1 = xj,s

2 = ... = xj,s
NT

,

j,l (resp. MΓj Q(cid:62)

j,r). In total, to know the four elements xj,1

Thanks to the analysis yielded in the previous proposition, we can build explicitly Ln,h
with not much computation. According to (17), to know the elements xj,1 and xj,3 (resp.
xj,2 and xj,4), it is necessary to compute one time the application of (Aj,n + ip · MΓj )−1 to
vector MΓj Q(cid:62)
n and xj,4
n ,
it is enough to compute two times the application of (Aj,n + ip· MΓj )−1 to vector. In other
words, this amounts to solve the Schr¨odinger equation on each sub domain two times to
build the matrix Lh,n. In addition, let us note that Lh,1 = Lh,2 = ... = Lh,NT if V = V (x).
The construction of the vector dn is similar. According to (18), one needs to apply
(Aj,n + ip · MΓj )−1 to vector Mjuj,n for j = 1, 2, ..., N .
In other words, we solve the
Schr¨odinger equation on each sub domain one time. The vector is stored in a distributed
manner using the PETSc library [5]

n , xj,2

n , xj,3

dn =(cid:0) dn,1,r
(cid:124)(cid:123)(cid:122)(cid:125)

MPI 0

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

, dn,2,l, dn,2,r

,··· , dn,j,l, dn,j,r

,··· , dn,N,l

MPI 1

MPI j-1

(cid:1)(cid:62)

.

(cid:124)(cid:123)(cid:122)(cid:125)

MPI N-1

Let us now present the new algorithm for the Schr¨odinger equation with time-independent

linear potential.

Algorithm 1: New algorithm, time-independent linear potential
1: Build the interface matrix Lh,n explicitly.
2: The initial datum is u0.
for n = 1, 2, ..., NT do

2.1: Build the vector dn on time step n,
2.2: Solve the linear system

(I − Lh,n)gn = dn.

(19)

2.3: Solve the Schr¨odinger equation on time step n on each sub domain
using the ﬂux from step 2.2 to compute un.

7

Concerning the computational phase 2.2 in the new algorithm and the storage of the

matrix Lh,n, there are some choices.
Choice 1. The transpose of Lh,n is stored in a distributed manner using the library
PETSc. As shown by (17), the ﬁrst column of Lh,n lies in MPI process 0. The second
and third columns are in MPI process 1, and so on for other processes. The linear system
(19) is solved by any iterative methods, such as the ﬁxed point method and the Krylov
methods (GMRES, BiCGStab) [16]. Note that if the ﬁxed point method is used, then
formally we recover the classical algorithm (13).
Choice 2. The transpose of Lh,n is stored in a distributed manner using the library
PETSc. Unlike the ﬁrst choice, we solve the linear system (19) by a parallel LU direct
method (MPI). One obvious beneﬁt is that the algorithm has no convergence problem.
Thus, it is possible to use any p ∈ R+ in the Robin transmission condition without looking
for the optimal one which makes the algorithm converge with less iterations for each size
of time step, each mesh and each number of sub domains.
The size of Lh,n is 2N − 2, which is small for a LU direct method even when N is large,
such as N = 256. Solving the linear system needs few arithmetic operations. However,
two points are not negligible. It is well known that the LU direct method is inherently
sequential. In addition, one MPI process contains only two rows of L(cid:62)
h,n (four elements).
Thus, the parallel LU direct method could suﬀer considerable communication costs.
Choice 3. The matrix Lh,n is stored locally in one MPI process after construction. For
n = 1, 2, ..., NT , the distributed vector dn is ﬁrstly gathered to one MPI process, then the
linear system (19) is solved by a sequential LU direct method. Finally, the solution gn is
broadcased to all MPI processes. This choice is a variation of the second one. It can avoid
communications from the parallel implementation of the LU method.

Remark 3.1. In the new algorithm with the choice 2 or 3, it is enough to do one time
the LU factorization of I − Lh,n.

There are two main novelties in the context of the optimized Schwarz method for the
one dimensional Schr¨odinger equation. We construct explicitly the matrix Lh,n, while
in the classical algorithm, Lh,n remains an abstract operator.
It is not usual to build
explicitly such huge operator, but as we have seen, its computation is not costly. We
use the direct method on the interface problem, thus the algorithm is independent of the
transmission condition since the convergence properties of I − Lh,h has no inﬂuence on
direct linear solver.

4 Preconditioned algorithm for general potential

In the previous sections, we have interpreted the classical algorithm for the Schr¨odinger
equation as (13). However, if f (cid:54)= 0, then the operator Rh,n is nonlinear. If f = 0 and

8

V = V (t, x), it is necessary to construct the interface matrix Lh,n for each time step n.
Thus, the new algorithm is not suitable here. Instead, to reduce the number of iterations
required for convergence, inspired from [7], we add a preconditioner P −1 (P is a non
singular matrix) in (13) or (14) which leads to the preconditioned algorithm:
• for V = V (t, x),

• for V = V (t, x) + f (t, x, u),

n = I − P −1(I − Rh,n),
gk+1
P −1(I − Lh,n)gn = P −1dn,

n = I − P −1(I − Rh,n)gk
gk+1
n.

(20)
(21)

(22)

We now turn to explain which preconditioner is used. The interface problem for the

free Schr¨odinger equation (V = 0, f = 0) is

(I − L0)gn = dn,

where the symbol L0 is used to highlight here the potential is zero. The transmission
condition is the same as for (1). We propose for time-dependent linear or nonlinear
potential the preconditioner as

P = I − L0.

We have three reasons to believe that this is a good choice.

1. Intuitively, the Schr¨odinger operator without potential is a roughly approximating of

the Schr¨odinger operator with potential:

i∂tu + ∂xxu ≈ i∂tu + ∂xxu + V u + f (t, x, u)u.

In other words, V u + f (t, x, u)u is a perturbation of the free Schr¨odinger operator.
If V = 0, then I−L0 = I−Lh,n = I−(Rh,n−Rh,n·0) where 0 is the zero vector. Thus,
the matrix L0 could be a good approximation of the matrix Lh,n and of the nonlinear
operator Rh,n − Rh,n · 0:

P = I − L0 ≈ I − Lh,n, P = I − L0 ≈ I − (Rh,n − Rh,n · 0),

(23)

2. The matrix L0 can be constructed easily as explained in the previous section.
3. The implementation of the preconditioner and the storage of P are same as that of
(19), which are discussed in the previous section by the three choices. In eﬀect, for any
vector y, the vector z := P −1y is computed by solving the linear system

P z = (I − L0)z = y,

(24)

as the inverse of a matrix numerically is too expensive. Note that using the choice 2 or
3 ensures that the application of the preconditioner is robust.

9

5 Numerical results
The physical domain (a0, b0) = (−16, 16) is decomposed into N equal sub domains without
overlap. The ﬁnal time is ﬁxed to be T = 1.0. The initial data is

u0(x) = e−(x+1)2+i(x+1).

Let us consider three diﬀerent potentials in the following three sub sections

• time-independent linear potential: V = −x2,
• time-dependent linear potential: V = 5tx,
• nonlinear potential: V = x2

10 − |u|2,

which give rise to solutions that propagate to the right side and undergoes dispersion (see
Figure 2). This ﬁrst subsection devotes to the comparison of the classical algorithm and
the new algorithm. In section 5.2, we mainly study the spectral properties of the classical
algorithm and the preconditioned algorithms, i.e. the eigenvalues of the operators I −Lh,n
and P −1(I − Lh,n). The last one compares the classical algorithm and the preconditioned
algorithm for the general nonlinear potential.

Figure 2: Initial solution |u0| and numerical solutions of the Sch¨odinger equation with
diﬀerent potentials at ﬁnal time T = 1.0. The time step is ∆t = 0.001. The meshes are
∆x = 1.0 × 10−5 for V = −x2 and ∆x = 5.0 × 10−5 for the others.

All the numerical tests are implemented in a CPU cluster (16 cores/node, Intel Sandy
Bridge E5-2670, 64GB memory/node). We ﬁx always one core per MPI process, one MPI
process per sub domain and 16 MPI processes per node. The communications are handled
by OpenMPI 1.6.5 (GCC 4.8). The linear systems (8) and (10) related to the ﬁnite element
method are solved by the LU direct method using the MKL Pardiso library.

10

Remark 5.1. Without less of generality, we consider the number of iterations of the ﬁrst
time step.
In other words, we study the number of iterations of the evolution between
t0 → t1 to compute v1 with the optimized Schwarz method.
Remark 5.2. Using the zero vector as the initial guess vector g0 is normally more time
eﬃcient. While as mentioned in [11], it could give wrong conclusions associated with the
convergence. Thus, the zero guess vector is used when one wants to evaluate the compu-
tation time, while the random guess vector is used when study the number of iterations.

Remark 5.3. The theoretical optimal parameter p in the Robin transmission condition
is not at hand for us. We then seek for the best parameter numerically for each case if
necessary.

5.1 Time-independent linear potential

In this part, we are interested in observing the eﬃciency of our new algorithms for the
time-independent linear potential V = −x2. We denote by Tref the computation time to
solve the Schr¨odinger equation numerically on a single processor on the entire domain and
Tcls (resp. Tnew) the computation time of the classical (resp. new) algorithm for N sub
domains. We test the algorithms for N = 2, 4, 8, 16, 32, 64, 128, 256 sub domains.

The computation times are shown in Table 1 where the ﬁxed point method, two Krylov
methods (GMRES and BiCGStab) and the LU direct method (sequential implementation
and parallel implementation) are used on the interface problem (Step 2.2 in the algorithm
1). The time step and the mesh are ∆t = 0.001, ∆x = 10−5. Roughly speaking, in Table
1 we have

Tcls = Tsub × Niter × NT + ...,
Tnew = Tsub × 2 + (Tsub × 2 + TLd) × NT + ...,

where Tsub denotes the computation time for solving the equation on one sub domain
for one time step, TLd is the computation time for solving the interface problem, “...”
represent the negligible part of computation time such as the construction of matrices for
the ﬁnite element method. Firstly we can see that Tnew with the parallel LU method takes
much more times than others if N is large. As has been explained in section 3 (Choice
2), since the LU method is inherently sequential and the size of (19) is quite small, the
communication is costly in our case. From now on, we abandon our idea using a parallel
LU direct solver on the interface problem. For other choices, if the number of sub domains
N is not so large, then Tsub (cid:29) TLd and the minimum of Niter is 3 in all our tests (see Table
2). If the number of sub domains N is large, then TLd ∼ Tsub and Niter (cid:62) 3. It is for
this reason that the new algorithm takes less computation time and we do not observe
big diﬀerence when diﬀerent solvers used on the interface problem in the new algorithm.
However, we emphasize that the new algorithm with the sequential LU method on the
interface problem is the best choice. It allows to use any p ∈ R+. In other words, the

11

algorithm is independent of the transmission condition. Note that it is possible to use
other more complex transmission condition as presented in [13, 2, 7]. Normally, this leads
to better convergence properties of the classical algorithms. However, the new algorithm
with the direct method on the interface problem is a direct algorithm.

Table 1: Computation time (seconds) of the classical algorithm and the new algorithm
with the ﬁxed point method, two Krylov methods (GMRES and BiCGStab) and the LU
direct method (sequential implementation and parallel implementation) where V = −x2,
∆t = 0.001, ∆x = 10−5 and p = 45.

2

4

8

64

128

256

32

16
594.0

N
Tref
Tcls 11102.0 5909.4 2948.7 1596.5 788.9 412.7 199.3 105.4
7.9
Tnew
28.1
Tcls
7.0
Tnew
31.2
Tcls
6.9
Tnew
6.3
Tnew
65.2
Tnew

101.5
13.4
423.3 220.0 116.4 55.2
101.9
12.7
489.4 235.3 115.8 50.7
101.4
12.7
12.1
101.0
102.9
31.4

185.1
730.0
611.4
1461.5
728.8
185.6
1807.2 1161.5 795.9
727.7
185.3
183.9
663.3
680.8
189.3

375.4
363.6
356.8

374.7
934.5
375.5

48.8
48.4
51.3

25.5
25.0
33.6

48.9

25.7

49.0

26.1

Fixed Point

GMRES

BiCGStab

LU-S
LU-P

LU-S: LU direct method (sequential implementation) in MKL Pardiso library.
LU-P: LU direct method (parallel implementation, MPI) in MUMPS library [1].

Table 2: Number of iterations with the iterative methods used on the interface problem
for some diﬀerent p where V = −x2, ∆t = 0.001 and ∆x = 10−5.

N = 2

N = 256

p Fixed point GMRES BiCGStab Fixed point GMRES BiCGStab
5
10
15
20
25
30
35
40
45
50

159
83
58
45
39
35
33
32
31
32

185
96
67
52
44
40
37
36
35
35

15
15
14
14
13
13
13
13
13
13

3
3
3
3
3
3
3
3
3
3

9
9
8
8
8
8
8
7
7
7

3
3
3
3
3
3
3
3
3
3

In conclusion, the new algorithm with the sequential LU direct method is robust, scal-
able and independent of the transmission condition. In addition, it takes less computation

12

time than the classical algorithm.

5.2 Time-dependent linear potential

In this subsection, we consider the preconditioned algorithm and the classical algorithm
for the time-dependent potential V = 5tx. According to our numerical investigations
in the previous subsection, the preconditioner (24) is implemented using the choice 3 in
section 3, which is robust and time eﬃcient.
Firstly, we are interested in the eﬃciency of the preconditioner. The explicit construc-
tion of the interface problem allows us to study the spectral properties of I − Lh,n and
P −1(I − Lh,n). Figure 3 and Figure 4 present their eigenvalues for some diﬀerent meshes
where N = 2 and N = 256 respectively. Without less of generality, we only consider
n = 1. It can be seen that the eigenvalues of the preconditioned linear system are really
close to 1+0i, which illustrates that P −1 is a good preconditioner.
Next, we show in Figure 5 and 6 the eigenvalues of I − Lh,n and P −1(I − Lh,n) for
some diﬀerent p and for N = 2, 256 respectively. It can be seen that the parameter p
has almost no inﬂuence on P −1(I − Lh,n). As for I − Lh,n, the distance between the
eigenvalues and the value 1+0i depends on p. Thus, if the ﬁxed point method is used
on the interface problem, then it is essential to choose the optimal parameter p in the
classical algorithm. However, as shown in Table 3 and 4 (number of iterations of the
classical algorithm and the preconditioned algorithm for N = 2 and N = 256), the use
of the Krylov methods (GMRES and BiCGStab) can also reduce the dependency on the
parameter p in the transmission condition.

Table 3: Number of iterations of the classical algorithm and the preconditioned algorithm
(+PC) for N = 2 where V = 5tx, ∆t = 0.001 and ∆x = 5 × 10−5.

p

Fixed point

Fixed point + PC

GMRES

GMRES+PC

BiCGStab

BiCGStab + PC

5
158
3
3
3
3
2

10
82
3
3
3
3
2

15
58
3
3
3
3
2

20
45
3
3
3
3
2

25
39
3
3
3
3
2

30
35
3
3
3
3
2

35
33
3
3
3
3
2

40
32
3
3
3
3
2

45
31
3
3
3
3
2

50
32
3
3
3
3
2

Finally, Table 5 presents the computation times of the classical algorithm and the
preconditioned algorithm where the ﬁxed point method, the GMRES method and the
BiCGStab method are used on the interface problem. The computation time of the pre-
conditioned algorithm Tpc consists of three major parts: the construction of the precon-
ditioner (denoted by T1), the application of Rh,n to vectors (denoted by T2), and the
application of preconditioner (denoted by T3). Then we have
Tpc ≈ T1 + (T2 + T3) × Niter × NT ,

(25)

13

(a) ∆t = 0.01, ∆x = 0.01

(b) ∆t = 0.01, ∆x = 5 × 10−5

(c) ∆t = 0.001, ∆x = 0.01

(d) ∆t = 0.001, ∆x = 5 × 10−5

Figure 3: Eigenvalues of I − Lh,n and P −1(I − Lh,n) for N = 2 where n = 1 and p = 45.

Table 4: Number of iterations of the classical algorithm and the preconditioned algorithm
(+PC) for N = 256 where V = 5tx, ∆t = 0.001 and ∆x = 5 × 10−5.

p

Fixed point

Fixed point + PC

GMRES

GMRES+PC

BiCGStab

BiCGStab + PC

5
184
4
14
4
8
3

10
95
4
12
4
8
3

15
66
4
13
3
7
3

20
52
4
12
4
7
3

25
44
4
12
4
7
3

30
40
3
12
4
7
3

35
37
4
12
4
7
3

40
36
4
12
4
7
3

45
35
4
12
4
7
3

50
35
4
12
4
7
2

14

0.40.60.81.01.21.41.6Real part0.30.20.10.00.10.20.3Imaginary partI−Lh,nP−1(I−Lh,n)0.40.60.81.01.21.41.6Real part0.30.20.10.00.10.20.3Imaginary partI−Lh,nP−1(I−Lh,n)0.9900.9951.0001.0051.010Real part0.40.20.00.20.4Imaginary partI−Lh,nP−1(I−Lh,n)0.9900.9951.0001.0051.010Real part0.40.20.00.20.4Imaginary partI−Lh,nP−1(I−Lh,n)(a) ∆t = 0.01, ∆x = 5 × 10−5

(b) ∆t = 0.001, ∆x = 5 × 10−5

Figure 4: Eigenvalues of I −Lh,n and P −1(I −Lh,n) for N = 256 where n = 1 and p = 45.

Figure 5: Eigenvalues of I − Lh,n (left) and P −1(I − Lh,n) (right) for diﬀerent p where
N = 2 and ∆t = 0.001, ∆x = 5 × 10−5.

15

0.20.40.60.81.01.21.41.61.8Real part0.60.40.20.00.20.40.6Imaginary partI−Lh,nP−1(I−Lh,n)0.960.981.001.021.04Real part0.40.20.00.20.4Imaginary partI−Lh,nP−1(I−Lh,n)0.00.51.01.52.0Real part0.40.20.00.20.4Imaginary partp=5p=10p=15p=20p=25p=30p=35p=40p=45p=50p=55p=60p=65p=700.00.51.01.52.0Real part0.40.20.00.20.4Imaginary partp=5p=10p=15p=20p=25p=30p=35p=40p=45p=50p=55p=60p=65p=70Figure 6: Eigenvalues of I − Lh,n (left) and P −1(I − Lh,n) (right) for diﬀerent p where
N = 256 and ∆t = 0.001, ∆x = 5 × 10−5.

where T2 (cid:29) T3, T1 ≈ 2T2. As can be seen from Table 3 and Table 4, if the ﬁxed point
method is used on the interface problem, the preconditioner allows to reduce signiﬁcantly
the number of iterations required for convergence. Thus, Tpc < Tcls.

Table 5: Computation time (seconds) of the classical algorithm and the preconditioned
algorithm (+PC) where V = 5tx, ∆t = 0.001 and ∆x = 5 × 10−5.

N
Tref

Fixed point

GMRES

Fixed point+PC 1400.3
1250.0
1246.1
1235.1
BiCGStab+PC 1326.7

GMRES+PC

BiCGStab

2

4

8

16

32

64

128

256

2100.4

3438.5 1732.8 840.0 413.9 193.7 80.1 45.9 24.4
28.8 16.5
9.9
34.2 19.3 10.4
28.4 16.2 10.0
36.9 17.7 10.7
31.1 15.2 10.8

332.1 159.7
376.8 188.0
331.9 157.7
345.8 179.2
287.7 147.9

692.6
712.5
664.9
710.8
595.0

71.6
85.3
70.6
81.1
68.1

Remark 5.4. Let us read Table 4 and Table 5 together for N = 256 and p = 45 with the
ﬁxed point method used on the interface problem. The reduction of computation time is
not proportional to that of number of iterations. In fact, the number of iterations of the
preconditioned algorithm is about 10 times less than that of the classical one. However,
its computation time is only reduced by about 2.5 times. There are three reasons:
• For each time step n, the linear systems (8) or (10) are solved multiple times. However,
it is enough to do one time the LU factorization of the matrix Aj,n +ip·MΓj , of which the
computation times are same for the classical algorithm and the preconditioned algorithm.
• As explained in remark 5.2, the initial vector used in Table 4 is a random vector, while

it is the zero vector in Table 5.

16

0.00.51.01.52.0Real part0.40.20.00.20.4Imaginary partp=5p=10p=15p=20p=25p=30p=35p=40p=45p=50p=55p=60p=65p=700.00.51.01.52.0Real part0.40.20.00.20.4Imaginary partp=5p=10p=15p=20p=25p=30p=35p=40p=45p=50p=55p=60p=65p=70• The implementation of the preconditioner takes a little computation time.

Before turning to consider the Schr¨odinger equation with a general nonlinear potential,
we can conclude that P −1 is a very eﬃcient preconditioner. In addition, the use of Krylov
methods on the interface problem allows to reduce both of the number of iterations and
the computation time in the framework of the classical algorithm.

5.3 Nonlinear potential

The objective of the last subsection is to investigate the performance of the preconditioned
algorithm for the nonlinear potential V = x2/10−|u|2. Unlike the linear case, the interface
problem (13) is nonlinear here. Thus, it is not possible to use Krylov methods to accelerate
the convergence. Table 6 presents the computation time of the classical algorithm (Tcls)
and the preconditioned algorithm (Tpc) for N = 2, 4, 8, 16, 32, 64, 128, 256 where the time
step is ∆t = 0.001, the mesh is ∆x = 5 × 10−5 and p = 45. We can see that both of the
algorithms are scalable and the preconditioner can signiﬁcantly reduce the computation
time.

Table 6: Computation time (seconds) of the classical algorithm and the preconditioned
algorithm where V = x2/10 − |u|2, ∆t = 0.001, ∆x = 5 × 10−5 and p = 45.

N
Tref
Tcls
Tpc

2

4

8

11152.7
2593.4

6147.9
1362.3

3074.4
648.3

16
2423.9
1615.2
318.2

32

64

128

256

767.0
153.7

416.6
81.0

202.8
39.1

98.0
18.7

We show in Table 7 the number of iterations of the classical algorithm (Ncls) and the
preconditioned algorithm (Npc) with diﬀerent parameter p in the transmission condition
for N = 2 and N = 256. As the linear case, the preconditioned algorithm is almost
independent of the parameter p. In addition, the preconditioned algorithm needs much
less iterations to converge. Note that it is for the same reasons as the linear case that
the reduction of computation time is not proportional to that of number of iterations (see
remark 5.4).

In conclusion, both of the classical algorithm and the preconditioned algorithm are
robust. The preconditioned algorithm takes less number of iterations and less computation
times than the classical one. In addition, the preconditioned algorithm is not sensible to
the transmission condition.

6 Conclusion

We proposed in this paper a new optimized Schwarz algorithm for the one dimensional
Schr¨odinger equation with time-independent linear potential and a preconditioned algo-
rithm for the Schr¨odinger equation with general potential. These algorithms are robust

17

Table 7: Number of iterations of the classical algorithm and the preconditioned algorithm
for some diﬀerent p where V = x2/10 − |u|2, ∆t = 0.001 and ∆x = 5 × 10−5.

N = 2

N = 256
p Ncls Npc Ncls Npc
3
5
3
10
15
4
4
20
4
25
4
30
35
4
4
40
4
45
50
4

170
90
63
50
43
39
36
35
34
34

147
79
55
44
38
34
32
31
30
31

3
3
3
3
3
3
3
3
3
3

and scalable. They could reduce signiﬁcantly both the number of iterations and the com-
putation time. In addition, by using the sequential LU direct method on the interface
problem in the context of the new algorithm, the convergence is independent of the trans-
mission condition.

Acknowledgments

This work was granted access to the HPC and visualization resources of “Centre de Calcul
Interactif” hosted by University Nice Sophia Antipolis.

References

[1] MUMPS library, http://mumps.enseeiht.fr/.

[2] X. Antoine, E. Lorin, and A.D. Bandrauk. Domain decomposition method and high-
order absorbing boundary conditions for the numerical simulation of the time depen-
dent schr¨odinger equation with ionization and recombination by intense electric ﬁeld.
J. Sci. Comput., pages 1–27, 2014.

[3] Xavier Antoine, Christophe Besse, and Pauline Klein. Absorbing boundary conditions
for the one-dimensional Schr¨odinger equation with an exterior repulsive potential. J.
Comput. Phys., 228(2):312–335, February 2009.

[4] Xavier Antoine, Christophe Besse, and Pauline Klein. Absorbing Boundary Con-
SIAM J. Sci. Comput.,

ditions for General Nonlinear Schr¨odinger Equations.
33(2):1008–1033, 2011.

18

[5] Satish Balay, Mark˜F. Adams, Jed Brown, Peter Brune, Kris Buschelman, Victor
Eijkhout, William˜D. Gropp, Dinesh Kaushik, Matthew˜G. Knepley, Lois Curfman
McInnes, Karl Rupp, Barry˜F. Smith, and Hong Zhang. PETSc Users Manual.
Technical Report ANL-95/11 - Revision 3.4, Argonne National Laboratory, 2013.

[6] Christophe Besse and Feng Xing. Domain decomposition algorithms for two dimen-

sional linear Schr¨odinger equation. Preprint, arXiv:1506.05639, 2015.

[7] Christophe Besse and Feng Xing. Schwarz waveform relaxation method for one di-
mensional Schr¨odinger equation with general potential. Preprint, arXiv:1503.02564,
2015.

[8] Yassine Boubendir, Xavier Antoine, and Christophe Geuzaine. A quasi-optimal non-
overlapping domain decomposition algorithm for the Helmholtz equation. J. Comput.
Phys., 231(2):262–280, 2012.

[9] A. Dur´an and J.M. Sanz-Serna. The numerical integration of relative equilibrium
solutions. The nonlinear Schrodinger equation. IMA J. Numer. Anal., 20(2):235–261,
April 2000.

[10] Martin J Gander. Optimized Schwarz Methods. SIAM J. Numer. Anal., 44(2):699–

731, January 2006.

[11] Martin J Gander. Schwarz methods over the course of time. Electron. Trans. Numer.

Anal., 31:228–255, 2008.

[12] Martin J Gander, Fr´ed´eric Magoules, and Fr´ed´eric Nataf. Optimized Schwarz methods
without overlap for the Helmholtz equation. SIAM J. Sci. Comput., 24(1):38–60,
2002.

[13] Laurence Halpern and J´er´emie Szeftel. Optimized and quasi-optimal Schwarz wave-
form relaxation for the one dimensional Schr¨odinger equation. Math. Model. Methods
Appl. Sci., 20(12):2167–2199, December 2010.

[14] Pierre-Louis Lions. On the Schwarz alternating method. III: a variant for nonover-
lapping subdomains. Third Int. Symp. domain Decompos. methods Partial Diﬀer.
equations, 6:202–223, 1990.

[15] Fr´ed´eric Nataf and Francois Rogier. Factorization of the convection-diﬀusion operator
and the Schwarz algorithm. Math. Model. Methods Appl. Sci., 05(01):67–93, February
1995.

[16] Yousef Saad. Iterative methods for sparse linear systems. Society for Industrial and

Applied Mathematics, 2nd edition, 2003.

19

