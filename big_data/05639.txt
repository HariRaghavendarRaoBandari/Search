6
1
0
2

 
r
a

 

M
7
1

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
9
3
6
5
0

.

3
0
6
1
:
v
i
X
r
a

Sensitivity of mixing times in Eulerian digraphs

Lucas Boczkowski ∗

Yuval Peres†

Perla Sousi‡

Abstract

Let X be a lazy random walk on a graph G. If G is undirected, then the mixing time is
upper bounded by the maximum hitting time of the graph. This fails for directed chains, as the
biased random walk on the cycle Zn shows. However, we establish that for Eulerian digraphs,
the mixing time is O(mn), where m is the number of edges and n is the number of vertices. In
the reversible case, the mixing time is robust to the change of the laziness parameter. Surpris-
ingly, in the directed setting the mixing time can be sensitive to such changes. We also study
exploration and cover times for random walks on Eulerian digraphs and prove universal upper
bounds in analogy to the undirected case.

Keywords and phrases. Random walk, mixing time, Eulerian digraph.
MSC 2010 subject classiﬁcations. Primary 60J10.

1

Introduction

Random walks on graphs have been thoroughly studied, but many results are only known in the
undirected setting, where spectral methods and the connection with electrical networks is available.
As noted in Aldous and Fill [1], many properties of random walk on undirected graphs extend to
Eulerian digraphs. In this paper we show this holds true for basic bounds on mixing and exploration
times, but fails for robustness of mixing times under modiﬁcation of laziness parameters.

Deﬁnition 1.1. Let G = (V, E) be a directed graph, where V is the set of vertices and E the
set of directed edges. We write dout(x) and din(x) for the outdegree and indegree of the vertex x
respectively. The graph G is connected if for all vertices x and y there is a path from x to y
ignoring directions. It is called d-regular if dout(x) = din(x) = d for all x ∈ V (G). Finally it is
called Eulerian if dout(x) = din(x) for all x ∈ V (G).
Let X be a Markov chain on the ﬁnite state space E with transition matrix P and stationary

distribution π. We write P t(x, y) = Px(Xt = y) for all x, y ∈ E. The chain X is called lazy
if P (x, x) ≥ 1/2 for all x ∈ E.
Let G = (V, E) be a directed graph. Then a lazy simple random on G is a Markov chain X with
transition probabilities given by

P (v, v) =

1
2

and P (v, w) =

1

2dout(v)

1((v, w) ∈ E) ∀ v, w ∈ V.

∗Universit´e Paris Diderot, Paris, France; lucasboczko@gmail.com
†Microsoft Research, Redmond, Washington, USA; peres@microsoft.com
‡University of Cambridge, Cambridge, UK; p.sousi@statslab.cam.ac.uk

1

For convenience, when we consider Eulerian graphs, we drop the subscript “out”. For all ε > 0 we
deﬁne the ε-total variation mixing time of the transition probability matrix P via

tmix(ε) = min{t ≥ 0 : max

x

and the ε − L∞ mixing via

tunif (ε) = min

(cid:26)

t ≥ 0 : max

x,y

(cid:13)(cid:13)P t(x,·) − π(cid:13)(cid:13)TV ≤ ε}
(cid:12)(cid:12)(cid:12)(cid:12) P t(x, y)
(cid:12)(cid:12)(cid:12)(cid:12) ≤ ε
(cid:27)

π(y) − 1

.

Following the convention, we write tmix = tmix(1/4) and tunif = tunif (1/4).

If X is reversible and lazy, then it is known ([10, Chapter 10] or [11]) that

tunif ≤ 4 max
x,y∈E

Ex[τy] + 1,

where τy = inf{t ≥ 0 : Xt = y} for all y ∈ E.
However, if X is not reversible, then this upper bound fails. Indeed, if X is a lazy biased walk
on Zn with P (i, i) = 1/2 and P (i, i + 1) = 1/3 = 1/2− P (i, i− 1), where all the operations are done
modulo n, then tunif is of order n2, while maxx,y Ex[τy] is of order n. This chain can be realised as
a random walk on an Eulerian multi-graph.

It is well-known that if X is a lazy simple random walk on an undirected connected graph G, then
there is a positive constant c so that

max
x,y

Ex[τy] ≤ c · mn,

where m is the number of edges and n the number of vertices of G. Therefore, when G is undirected
then

tunif ≤ c(cid:48)

· mn,

for a positive constant c(cid:48). It turns out that the same upper bound is true in the Eulerian directed
case as the following theorem shows.

Recall that an Eulerian digraph is strongly connected if and only if it is connected. Therefore, a
random walk on a connected Eulerian graph is always irreducible.

Theorem 1.2. There exists a positive constant c1 so that for all lazy walks on connected Eulerian
digraphs on n vertices and m directed edges the following bound holds

tunif ≤ c1 · mn.
More precisely, there exists a constant c2 so that for all a

(cid:18)

(cid:19)

.

tunif (a) ≤

c2
a

(mn) ∧

m2
a

Moreover, if G is regular, then there exists a constant c3 so that for all a

tunif (a) ≤ c3 ·

n2
a2

.

2

Remark 1.3. We note that for a < 1, the bounds can be improved using sub-multiplicativity to

for general Eulerian digraphs, and

in the regular case.

tunif (a) ≤ c1mn · log(1/a)

tunif (a) ≤ c3n2 · log(1/a)

Montenegro and Tetali in [12, Example 6.4] prove a bound of O(m2) on the total variation
and L2 mixing time for random walk on connected Eulerian graphs under an expansion hypothesis.
Our bound O(mn) of Theorem 1.2 improves on their bound considerably in the lazy case especially
when the degrees are high.

We prove Theorem 1.2 in Section 2, where we also derive an upper bound on hitting times of moving
targets.

We now turn to a property of undirected graphs that does not transfer to the directed Eulerian
setting. K. Burdzy asked whether the mixing time of a chain is robust to modiﬁcations of laziness
parameters (explained formally below). A positive answer for reversible chains was given in [13]:

so that the following holds. Let P be an irreducible reversible transition matrix on the state space E

Theorem 1.4. [13, Corollary 9.5] For all c1, c2 ∈ (0, 1), there exist two positive constants c and c(cid:48)
and let (cid:101)P = (P + I)/2 have mixing time tmix. Suppose that (a(x, x))x∈E satisfy c1 ≤ a(x, x) ≤ c2

for all x ∈ E. Let Q be the transition matrix of the Markov chain with transitions: when at x it
stays at x with probability a(x, x). Otherwise, with probability 1 − a(x, x) it jumps to state y ∈ E
with probability P (x, y). We then have

where tmix(Q) is the mixing time of the transition matrix Q.

ctmix ≤ tmix(Q) ≤ c(cid:48)tmix,

Surprisingly, in the directed case the mixing time is sensitive to the change of the laziness parameter
as the following theorem demonstrates.

Theorem 1.5. Let X be a random walk on the graph G of Figure 1, which is obtained by gluing
two cycles Zn at 0. At sites on the left cycle which are at distance in [n/4, 3n/4] from 0, the random
walk stays in place with probability α = 2/(√5 + 1) and everywhere else with probability 1/2. With
the remaining probability it jumps to a neighbour according to a biased walk, i.e. with probability 2/3
clockwise and 1/3 counter-clockwise, except when at 0 it additionally chooses with equal probability
on which cycle to start walking. Then there exist two positive constants c1 and c2 so that for all n

c1n3/2 ≤ tmix ≤ tunif ≤ c2n3/2.

This contrasts with the case α = 1/2, where tmix, tunif (cid:16) n2.
We prove Theorem 1.5 in Section 3. We now turn to properties of random walks on undirected
graphs that also extend to the directed case. Before stating the results we introduce some notation.

Deﬁnition 1.6. Let G be a graph and X a simple random walk on G. For k ∈ N the k-exploration

time, Tk, is the ﬁrst time that X has visited k distinct vertices of G. When k = n, then Tn = τcov
is called the cover time of the graph.

3

Figure 1: Biased random walk on the cycles with diﬀerent laziness parameters

When the graph is undirected, Barnes and Feige [3] showed that Ev[Tk] ≤ Ck3 for a universal
In the case of regular graphs, the bound Ev[Tk] ≤ Ck2

constant C and all starting vertices v.
follows from standard results (see for instance [1, Chapter 6]). The following results extend these
bounds to the directed Eulerian setting. We prove them in Section 4.

Theorem 1.7. There exists a positive constant c1 so that if G is a regular connected Eulerian
digraph on n vertices, then for all starting vertices v and all k ≤ n we have

Ev[Tk] ≤ c1k2.

Theorem 1.8. There exists a positive constant c2 so that if G = (V, E) is a connected Eulerian
digraph on n vertices, then for all starting vertices v and all k ≤ n we have

Ev[Tk] ≤ c2k3.

Notation. For functions f, g we will write f (n) (cid:46) g(n) if there exists a constant c > 0 such that
f (n) ≤ cg(n) for all n. We write f (n) (cid:38) g(n) if g(n) (cid:46) f (n). Finally, we write f (n) (cid:16) g(n) if both
f (n) (cid:46) g(n) and f (n) (cid:38) g(n).

2 Bounds on mixing times

In this section we prove Theorem 1.2, which gives an upper bound for the L∞ mixing time. There
are several well developed tools for analysing mixing times, most of which are not eﬀective in the
directed case. Our bounds rely on the spectral proﬁle technique introduced in [6]. We stress that
no other technique we are aware of yields results of this precision.

We start by recalling the spectral proﬁle technique of [6].

Let X be a Markov chain on V with transition matrix P and stationary distribution π. The

Dirichlet energy of a function f : S → R is deﬁned via

(cid:88)

v,w∈V

EP (f, f ) :=

1
2

(f (v) − f (w))2 π(v)P (v, w).

4

023231313laziness=αlaziness=12laziness=12For f : S → R and g : S → R we deﬁne their inner product with respect to π via

(cid:88)

x

(cid:104)f, g(cid:105)π =

f (x)g(x)π(x).

Note that for a reversible chain we have

For a subset ∅ (cid:54)= S ⊆ V we deﬁne λ(S) via

EP (f, f ) = (cid:104)f, (I − P )f(cid:105).

(2.1)

where supp(f ) = {x : f (x) (cid:54)= 0} and Varπ(f ) = Eπ
The spectral proﬁle Λ : [π∗,∞) → R is deﬁned by

λ(S) =

inf

f≥0, supp(f )⊆S

,

EP (f, f )
Varπ(f )

(cid:2)(f − Eπ[f ])2(cid:3).

Λ(r) =

inf

π∗≤π(S)≤r

λ(S),

where π∗ = minx π(x).

We can now state the main result of [6] that we will use.

Theorem 2.1 ([6]). Let P be an irreducible transition matrix with stationary distribution π satis-
fying P (x, x) ≥ δ > 0 for all x. Then for all a > 0 we have
dr

(cid:38)(cid:90) 4/a

(cid:39)

tunif (a) ≤ 2

4π∗

δrΛ(r)

.

The following lemma is standard. We will use it in the proof of Theorem 1.2, so we include the
proof here for completeness.

Lemma 2.2. For all r ≤ 1/2 the spectral proﬁle satisﬁes

inf

f≥0, π(supp(f ))≤r

EP (f, f )
Eπ[f 2] ≤ Λ(r) ≤

inf

f≥0, π(supp(f ))≤r

2EP (f, f )
Eπ[f 2]

(2.2)

Proof. For a function f and a set A the Cauchy-Schwartz inequality gives

(E[f1(A)])2 ≤ E(cid:2)f 21(A)(cid:3) π(A).
(cid:2)f 2(cid:3) ,

Varπ (f ) ≥ (1 − π(A))Eπ

Taking A = supp(f ) we get

and this proves the lemma.

We are now ready to give the proof of Theorem 1.2.

Proof of Theorem 1.2. We will use Theorem 2.1, and hence we need to ﬁnd a lower bound on
the spectral proﬁle Λ. Consider a set S with π(S) ≤ r.

Let (cid:98)P the matrix of the reversed chain, i.e. the matrix deﬁned by

π(v)(cid:98)P (v, u) = π(u)P (u, v)

for all u, v.

5

Since the graph is Eulerian, taking the reversal amounts to reversing the orientation of the edges.
Let also

P + (cid:98)P

.

2

Q =

Then the chain with matrix Q is a reversible chain and can be seen as a walk on a weighted graph.
An edge appearing in the original graph in both directions has weight 2, other edges have weight 1.
Using Lemma 2.2 shows that we can replace the variance in the deﬁnition of Λ by an L2 norm.
It is immediate to check by rearranging the terms in the deﬁnition of the Dirichlet energy that

Because of this, we can work with the reversible chain Q = (P + (cid:98)P )/2. Therefore, using (2.1) the

(2.3)

EP (f, f ) = E(cid:98)P (f, f ) = EQ(f, f ).

ratio we are interested in lower bounding becomes

(cid:104)f, (I − Q)f(cid:105)π

Eπ[f 2]

=: 1 − ρ.

λ(S) =

inf

f≥0, supp(f )⊆S

A function f minimising the above expression is a left eigenfunction of the restriction (cid:101)Q of Q to
S, for some eigenvalue ρ. The matrix (cid:101)Q is an |S| × |S| strictly sub-stochastic matrix which implies
that ρ < 1 and f is strictly positive by the Perron Frobenius theorem. We now set (cid:101)f (x) = π(x)f (x)
for all x. Then the reversibility of Q yields that (cid:101)f is a right eigenvector of (cid:101)Q.
Deﬁne ν = (cid:101)f
(cid:107)(cid:101)f(cid:107)1

the L1-renormalisation of (cid:101)f . By deﬁnition
νQ = ν(cid:101)Q + ξ = ρν + ξ,

for some vector ξ supported on Sc. This shows that starting with initial distribution ν on S, the

exit time from S is a geometric variable with mean (1 − ρ)−1. Therefore, we get

λ(S) =

1

Eν[τ (Sc)] ≥

1
Ev[τ (Sc)]

,

max
v∈S

where Ev is w.r.t the chain Q, which as explained above is a reversible chain. Hence taking the
inﬁmum over all S with π(S) ≤ r gives
Λ(r) ≥

Ev[τ (Sc)]

max

.

1
max
v∈S

S:π(S)≤r

It now remains to upper bound Ev[τ (Sc)], where v ∈ S, for the reversible chain with matrix Q.
The distance from v to Sc is at most |S|. Therefore using the commute time identity for reversible
chains [10, Proposition 10.6] we obtain

This now gives a lower bound on the spectral proﬁle

Ev[τ (Sc)] ≤ |S| · rm ≤ (rm)2 ∧ (rmn).

Plugging this in the integral of Theorem 2.1 gives for a positive constant c1 that

Λ(r) ≥

tunif (a) ≤

c1
a

(cid:0)(rm)2 ∧ (rmn)(cid:1)−1 .
(cid:19)
(cid:18)

m2
a

(mn) ∧

6

and this concludes the proof when G is an Eulerian digraph.

When G is a regular digraph, then the chain Q corresponds to a simple random walk on a regular
undirected graph. If S is a set with |S| ≤ rn, then we can apply [1, Proposition 6.16] to obtain

Ev[τ (Sc)] ≤ 4|S|2 ≤ 4(rn)2.

Similarly as above, Theorem 2.1 now gives

tunif (a) (cid:46) n2
a2

and this now ﬁnishes the proof.

2.1 Short-time bounds and moving target.

In this section we give an application of Theorem 1.2 to short time estimates for transition proba-
bilities and hitting times for moving targets.

Let u = (us)s≥0 be a deterministic trajectory in the graph, i.e. u : N → V (G). We deﬁne the ﬁrst
collision time τcol via

where X is a simple random walk on G.

τcol = inf{t ≥ 0 : Xt = ut},

Corollary 2.3. There exists a positive constant c so that the following holds. Let G be an Eulerian
digraph on n vertices and m edges and X a lazy simple random walk on it. Let (ut)t≥0 with
ut ∈ V (G) for all t be a moving target on G. Then

E[τcol] ≤ c · (mn) · (1 + log(m/n)).

Moreover, if G is regular, then

E[τcol] ≤ c · n2.

The following lemma will be crucial in the proof of the corollary above.

Lemma 2.4. There exists a positive constant C so that the following holds. Let G be an Eulerian
digraph on n vertices and m edges. If P is the transition matrix of a lazy simple random walk on G,
then we have

(2.4)

π(v) − 1

(cid:12)(cid:12)(cid:12)(cid:12) P t(u, v)

(cid:12)(cid:12)(cid:12)(cid:12) ≤
Moreover, if G is regular, then for all t(cid:12)(cid:12)(cid:12)(cid:12) P t(u, v)
(cid:12)(cid:12)(cid:12)(cid:12) P t(u, v)

π(v) − 1

Proof. For all a > 0, if t ≥ tunif (a), then it follows that

π(v) − 1

C m√
t
C mn
t

if t ≤ n2,
if t ≥ n2.

n
√t

.

(cid:12)(cid:12)(cid:12)(cid:12) ≤ C
(cid:12)(cid:12)(cid:12)(cid:12) ≤ a.

(cid:40)

7

Using Theorem 1.2 we have that in the general Eulerian case

(cid:18)

(cid:19)

.

tunif (a) ≤

c2
a

(mn) ∧

m2
a

Hence in order for this bound to be smaller than t, we need to take

a =

c2(mn)

t

∧

√c2 · m
√t

.

When G is regular, then tunif (a) ≤ c3n2/a2, and hence we need to take a = √c3 · n/√t.
Proof of Corollary 2.3. Let X be a random walk on G and u = (us)s≥0 a deterministic trajectory
in G. We set for all t

t(cid:88)

Zt(u) =

1(Xs = us)

.

π(us)

s=1

From Theorem 1.2 we know that tunif (1/4) ≤ Cmn for some constant C and tunif (1/4) ≤ Cn2 in
the regular case. Take t = 2Cmn and t = 2Cn2 in the regular case. Then

For the second moment of Zt(u) we have using the Markov property

E(cid:2)Z2
t (u)(cid:3) =

(cid:88)

E[Zt(u)] ≥ t − tunif (1/4) ≥ Cmn.
(cid:88)

P(Xs = us, Xr = ur)

s≤r≤t

π(us)π(ur)

E[Zt(f )]

P(Xs = us)

f

.

sup

(cid:33)

f

s≤t

π(us)

1 + sup

E[Zt(f )]

≤ E[Zt(u)] +
(cid:32)
(cid:17) (cid:46) (mn) · (1 + log(m/n)).
(cid:19)

≤ E[Zt(u)]
(cid:16)
Cnm(cid:88)
(cid:18)

1 +

s=n2

mn

s

1 +

n
√s

(cid:46) n2.

Applying Lemma 2.4 we obtain for all trajectories u

(cid:88)

P(Xs = us)

s≤t

π(us)

(cid:18)

n2(cid:88)

s=1

≤

(cid:19)

1 +

+

m
√s

In the regular case, Lemma 2.4 gives(cid:88)

P(Xs = us)

s≤t

π(us)

2Cn2(cid:88)

s=1

≤

Therefore, from this and the second moment method we deduce

P(Zt(u) > 0) ≥

1

1 + log(m/n)

(E[Zt(u)])2

E(cid:2)Z2
t (u)(cid:3) (cid:38)
P(Zt(u) > 0) ≥ c1.

and in the regular case

This shows that the ﬁrst collision time τcol is stochastically dominated by t× Geom(P (Zt > 0)), so
using the above and taking expectations we obtain

E[τcol] (cid:46) (mn) · (1 + log(m/n))
and E[τcol] (cid:46) n2 in the regular case and this completes the proof.
Question Can the logarithmic term in Corollary 2.3 be removed, i.e., is E[τcol] (cid:46) mn for all
Eulerian digraphs?

8

3 Sensitivity of mixing

In this section we prove Theorem 1.5. We start by recalling a classical result from diophantine
approximation that motivates the choice of the laziness parameter α.
For a ﬁnite sequence (ak)k≤n the gap is deﬁned as

k − a(cid:48)
k) is the sequence (ak) sorted in increasing order.

k≤n|a(cid:48)

Gap = sup

k+1|,

where (a(cid:48)

Lemma 3.1. Let ξ be an irrational with continued fraction coeﬃcients ai < B and for all k
let xk = (kξ) mod 1. Then there exists a constant c such that for all n the gap of the ﬁnite sequence
{xk : k ≤ n} is at most c/n. Moreover, for any interval J ⊆ [0, 1] of length |J| = 1/n, the number
among x1, ..., xn that fall in J is at most B + 2.

The key to the proof of Theorem 1.5 is the variability of the number of times that the walk goes
around the left cycle versus the right cycle. This variability has little or no eﬀect if the durations
of these two types of excursions are the same or rationally related. However, if the ratio is an
irrational such as the golden mean, then Lemma 3.1 together with this variability imply that the
distribution of return times to the origin is much less concentrated than in the rational case. This
in turn yields the mixing bounds.

A similar connection of mixing to diophantine approximation was found by Angel, Peres and Wilson
in [2], but the chains considered there are not related by a change of the laziness parameter.

Proof of Lemma 3.1. As in [8, page 88], let Dn be the discrepancy of the sequence x1, . . . , xn,
i.e.,

(cid:12)(cid:12)(cid:12)(cid:12)|k ≤ n : xk ∈ [α, β)|

n

(cid:12)(cid:12)(cid:12)(cid:12) .

− (β − α)

Dn = sup

0≤α≤β≤1

In particular Dn is at least the largest gap in the sequence. Let qi denote the denominators in the
expansion of ξ, see for instance [8, page 122, line 7]. So if ai < B for all i, then qi+1 ≤ Bqi for all i.
Given any n ﬁnd the largest i so that qi ≤ n < qi+1 ≤ Bqi. Now [8, inequality (3.17)] gives
Dqi < 2/qi < 2B/n, and hence the largest gap in x1, x2, . . . , xqi is at most 2B/n. This then also
applies to the largest gap in x1, . . . , xn.

To prove the second assertion of the lemma, using [8, inequality (3.17)] again we get Dqi+1 < 2/qi+1.
This means that the number of xj for j ≤ qi+1 that fall in J is at most 2 + qi+1|J| ≤ 2 + B. This
concludes the proof.

In this section we consider the graph G described in the Introduction and depicted in Figure 2
below. We call C1 the cycle on the left and C2 the cycle on the right.

Let X be a random walk on G with transition probabilities as explained in the statement of
Theorem 1.5. The key to proving Theorem 1.5 is the following.

50 , 10n3/2(cid:105)
(cid:104) n3/2

Lemma 3.2. For all t ∈

we have

P0(Xt = 0) (cid:16)

1
n

.

9

Figure 2: Biased random walk on the cycles with diﬀerent laziness parameters

In order to prove this lemma we will construct a coupling between the walk X and another walk Y
which is constructed by taking independent excursions in every cycle and every time it visits 0 it
chooses one of the two cycles equally likely. Then we will show that the coupling succeeds with
high probability.
Deﬁnition 3.3. Let (ξi)i≥1 be i.i.d. random variables taking values in {0, 1} equally likely. Let
i )i≥1 be i.i.d. random variables distributed as the commute time between 0 and a for a random
(T 1
i )i≥1 be i.i.d. random variables distributed as the commute
walk on the cycle C1. Similarly, let (T 2
time between 0 and b for a random walk on the cycle C2.
For all k ≥ 1 we let

i=1

Sk =

k(cid:88)
(cid:104) n3/2
100 , 10n3/2(cid:105)
(cid:88)

(ξiT 1

i + (1 − ξi)T 2
i ).

we have

P(Sk = t) (cid:16)

1
n

.

k

(cid:18) 1

(cid:46)

(cid:33)
(cid:32) k(cid:88)

i=1

(cid:18)
(cid:33)

k
2

ξi −

= x

1
√k

.

(cid:16)

10

The main ingredient in the proof of Lemma 3.2 is the following estimate.

Lemma 3.4. For all times t ∈

Before proving Lemma 3.4 we collect some standard estimates for sums of independent random
variables.

The ﬁrst one is a direct application of the Local CLT for the Bernoulli random variables (ξi).
Claim 3.5. There exists a positive constant c so that for all x ∈ {−k/2, . . . , k/2} we have

(cid:19)(cid:19)

(cid:16)

(cid:17)

k
2

ξi −

= x

exp

√k

cx2
k

−

∨ exp

−c√k

(cid:32) k(cid:88)

i=1

P

and for all x with |x| ≤ √k we have
P

(3.1)

(3.2)

0AA0aBB0b23231313laziness=αlaziness=12laziness=12Proof. The claim follows using [9, Theorem 2.3.11] for |x| ≤ k3/4 and the Azuma-Hoeﬀding in-
(cid:3) and D2
equality for |x| > k3/4.
Lemma 3.6. For all i we set D1
(cid:18)
constant c1 so that for all k, (cid:96) ≤ n and all y ∈ R we have
(cid:96)(cid:88)
(cid:112)

(cid:3). There exists a positive

i − E(cid:2)T 2
(cid:19)(cid:33)

−c1n1/3(cid:17)

(cid:32) k(cid:88)

∨ exp

i = T 2

i = T 1

(k + (cid:96))n

(k + (cid:96))n

(cid:32)

(cid:33)

−c1

i = y

(cid:16)

D1

i +

exp

D2

(cid:46)

y2

i=1

i=1

P

.

i

(k + (cid:96))n (in the right support) we have

Moreover, for all |y| ≤

i

i − E(cid:2)T 1
1(cid:112)
(cid:96)(cid:88)

D1

i +

D2

i = y

i=1

i=1

(cid:16)

.

(k + (cid:96))n

Proof. We ﬁrst note that it suﬃces to prove that for all y we have

(cid:33)

(cid:19)(cid:19)

1(cid:112)
(cid:16)

∨ exp

−c1(kn)1/3(cid:17)

and

(3.3)

(cid:32) k(cid:88)

P

(cid:33)
(cid:33)

(cid:32) k(cid:88)
(cid:32) k(cid:88)

i=1

i=1

P

P

D1

i = y

D1

i = y

(cid:18) 1

(cid:46)

(cid:18)

√kn

exp

−c1

y2
kn

1
√kn

(cid:16)

if |y| ≤

√kn

for any value of the laziness parameter α on the left half of the cycle. Then the same bound will
also hold for (D2
i ) by taking α = 1/2. From these two bounds and using that the convolution of
Gaussian densities is another Gaussian proves the lemma. So we now focus on proving (3.3).

i are independent.

into the time Si to go from 0 to a and the time S(cid:48)

We split the time T 1
i
that Si and S(cid:48)
We start by analysing Si. Consider a biased (2/3, 1/3) random walk Z on Z with laziness equal
to α in [n/4, n/2]∪ [−n/2,−n/4] and equal to 1/2 everywhere else. Then the time Si has the same
distribution as the ﬁrst hitting time of {−n/2, n/2} by Z. Let Y be another biased random walk
on Z with the same transitions as Z. We are going to couple Y and Z so that they are equal up
to the ﬁrst hitting time of {−n/4, n/2}. If they hit −n/4 before n/2, then we declare that the
coupling has failed. We then get that for a positive constant c we have

i to go from a to 0. Note

P(coupling has failed) ≤ e−cn.
exponentially small in n probability, we can replace the sum (cid:80)k

On the event that the coupling has succeeded, we get that T Z{−n/2,n/2} = T Y

i=1 Si by (cid:80)k

i.i.d. distributed as the ﬁrst hitting time τ of n/2 for the walk Y on Z, i.e.

(cid:32) k(cid:88)
(Si − E[Si]) = y

(cid:33)

P

where the last step follows from the fact that E(cid:104)

i=1

(cid:46) e−cn + P

Exactly in the same way as above, we get

(cid:32) k(cid:88)

i=1

P

(cid:33)

i − E(cid:2)S(cid:48)

i

(cid:3)) = y

(S(cid:48)

(cid:32) k(cid:88)

i=1

(cid:32) k(cid:88)

(cid:33)
(τi − E[τi]) = y − O(e−cn)
(cid:105)
= E(cid:104)
(cid:105)
+ O(e−cn).
(cid:3)) = y − O(e−cn)
i − E(cid:2)τ(cid:48)

T Y

(τ(cid:48)

n/2

i

(cid:33)

,

T Z{−n/2,n/2}

n/2. Therefore, up to
i=1 τi, where (τi) are

,

(3.4)

(cid:46) e−cn + P

i=1

11

pose τ as τ =(cid:80)n/2−1

i is now the ﬁrst hitting time of n/2 for a biased walk as above with the only diﬀerence

where τ(cid:48)
being that the laziness is equal to α in [−n/4, n/4] and equal to 1/2 everywhere else.
We now turn to bound the probability appearing on the right hand side of (3.4). First we decom-
j=1 Tj−1,j, where Tj−1,j is the time to hit j starting from j − 1. Note that these
times are independent, but not identically distributed for all j. However, for j ≤ n/8, up to an
exponentially small in n probability, we can couple as before Tj−1,j with i.i.d. random variables
each with the distribution of the ﬁrst hitting time of 1 starting from 0 for a biased (2/3, 1/3) ran-
dom walk on Z with laziness equal to 1/2 everywhere. Then the coupling fails only if the walk
starting from j − 1 hits −n/4 before j, which has exponentially small in n probability. Similarly
for j ∈ [3n/8, n/2] we can couple them to i.i.d. random variables each with the same distribution
as the ﬁrst hitting time of 1 starting from 0 by a biased random walk on Z with laziness equal to
α everywhere.
Finally, for j ∈ [n/8+1, 3n/8] the random variables Tj−1,j are independent and all have exponential
tails with the same uniform constant, i.e. there exist positive constants c1 and c2 so that for all j
in this range

P(Tj−1,j ≥ x) ≤ c1e−c2x.

Therefore, taking the sum over all i ≤ k we get by [14, Theorem 15] and Chernoﬀ’s bound for a
positive constant c1 and x > 0

(cid:19)

x2
kn

−c1

)

(T (i)

(T (i)

P

P

(cid:105)
(cid:105)

T (i)
j−1,j

)

T (i)
j−1,j

i=1

j=n/8+1

i=1

j=n/8+1

3n/8(cid:88)
3n/8(cid:88)

j−1,j − E(cid:104)
j−1,j − E(cid:104)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ x
 (cid:46) exp
(cid:18)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ x
 (cid:46) exp (−c1x)
 (cid:46) 1
(cid:18)
= y − O(e−cn)
For |y| ≤ √kn using again [9, Theorem 2.3.11] yields
(cid:105)(cid:17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) k(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) k(cid:88)
j−1,j − E(cid:104)
(cid:16)
n/8(cid:88)
 k(cid:88)

 k(cid:88)

j−1,j − E(cid:104)

= y − O(e−cn)

n/8(cid:88)

(cid:105)(cid:17)

T (i)
j−1,j

T (i)
j−1,j

√kn

exp

i=1

j=1

(cid:16)

T (i)

T (i)

P

P

i=1

j=1

cy2
kn

−

 (cid:16)

∨ exp

1
√kn

.

if x ≤ c1kn

(3.5)

if x > c1kn.

(3.6)

(cid:16)

−c(kn)1/3(cid:17)

.

Using the local CLT [9, Theorem 2.3.11] for |y| ≤ (kn)2/3 and [14, Theorem 15] for |y| > (kn)2/3
we obtain for a positive constant c

(cid:19)

(cid:105)(cid:17)

The same bound (with diﬀerent constants) holds for the sum over j ∈ [3n/8 + 1, n/2]. Using again
that the convolution of two Gaussian densities is Gaussian we get that

 k(cid:88)

P

n/8(cid:88)

(cid:16)

j−1,j − E(cid:104)

T (i)

k(cid:88)

(cid:105)(cid:17)
(cid:19)

+

T (i)
j−1,j

(cid:18)

j−1,j − E(cid:104)

(cid:16)
n/2(cid:88)
−c(kn)1/3(cid:17)

T (i)

(cid:16)

(cid:46) 1
√kn

exp

−

cy2
kn

∨ exp

i=1

j=1

i=1

j=3n/8+1



T (i)
j−1,j

= y − O(e−cn)

12

 (cid:16)

1
√kn

.

T (i)
j−1,j

(cid:105)(cid:17)
j−1,j − E(cid:104)

T (i)

= y − O(e−cn)
(cid:105)(cid:17)

T (i)
j−1,j

and

P(cid:0)Ak = y(cid:48)

− x(cid:1) P(Bk = x)

+

T (i)

T (i)

(cid:16)

i=1

j=1

i=1

j=1

Ak =

i=1

j=3n/8+1

T (i)
j−1,j

T (i)
j−1,j

(cid:105)(cid:17)

We now set

n/2(cid:88)

k(cid:88)

and for |y| ≤ √kn
(cid:16)
P
T (i)

 k(cid:88)

n/8(cid:88)

k(cid:88)
(cid:105)(cid:17)

j−1,j − E(cid:104)
(cid:16)
n/2(cid:88)

j−1,j − E(cid:104)
(cid:16)
k(cid:88)
n/8(cid:88)
k(cid:88)
3n/8(cid:88)
(cid:33)

(cid:105)(cid:17)
j−1,j − E(cid:104)
j−1,j − E(cid:104)
(cid:16)
(cid:88)
P(Bk = x) P(cid:0)Ak = y(cid:48)
− x(cid:1) +
(cid:18)
−c1(kn)1/3(cid:17)
(cid:16)
(cid:33)
where we set y(cid:48) = y − O(e−cn). For |y| ≤ √kn we get

Putting all these estimates together we get

(cid:32) k(cid:88)

(cid:46) 1
√kn

(cid:88)

x:|x|≥|y|/2

∨ exp

D1

i = y

T (i)
j−1,j

i=1

j=n/8+1

i=1

j=3n/8+1

y2
kn

(cid:19)
(cid:32) k(cid:88)

−c1

Bk =

P

i=1

T (i)

exp

=

+

.

x:|x|≤ |y|

2

,

P

D1

i = y

(cid:16)

1
√kn

and this completes the proof of (3.3).

i=1

To prove Lemma 3.4 we also need to know the ratio of E0
following lemma which is a standard result, but we include the proof for the reader’s convenience.

1

1

Lemma 3.7. For all n we have

(cid:3). This is the content of the

(cid:2)T 1

(cid:2)T 2
(cid:3) to E0
(cid:1)
· f (n) and Var(cid:0)T 1

1

(cid:16) n,

(cid:18)
(cid:3) =
(cid:3) = 4f (n)

2 +

(cid:2)T 1
(cid:2)T 2

1

1

E0
E0

1 − α

(cid:1)

1

(cid:19)
and Var(cid:0)T 2
−n/2 ·

1

3n
2 − 3n · 2

(cid:16) n,

2n/2 − 1
2n/2 − 2−n/2

.

where

f (n) =

Proof. It suﬃces to prove the statement for T 1
1 , since taking α = 1/2 proves the second assertion.
Recall that T 1
1 is the time it takes starting from 0 to hit a and come back to 0 afterwards. We
split the time T 1
1 into the time S1 to go from 0 to a and the time S2 to go from a to 0. Since
we are only interested in the expectation of T 1
1 we are going to couple S1 and S2 as follows: we
identify the upper part of the cycle [0, n/2] with the lower part [n/2, n], i.e. we identify i ∈ [0, n/2]
with n/2 + i ∈ [n/2, n]. Let S be the time it takes for a non-lazy walk to go from 0 to a. Then S
has the same law as the time to go from a to 0. Let Li be the local time at vertex i up to time S,
i.e. the number of visits of the non-lazy walk to i before hitting a starting from 0. Then this has

the same law as Ln/2+i for a random walk starting from a before it hits 0. Let (ξi,j) and ((cid:101)ξi,j)

13

be two independent families of i.i.d. geometric random variables with means 1 and 1/(1 − α) − 1
respectively. Using the identiﬁcation of the two parts of the cycle explained above, we can now
write

S1 = S +

S2 = S +

i=0

j=1

ξi,j +

Li(cid:88)
n/4(cid:88)
n/4(cid:88)
Li(cid:88)
(cid:101)ξi,j +
E[Li] E(cid:104)

j=1

i=0

(cid:101)ξi,j +

n/2(cid:88)
n/2(cid:88)

Li(cid:88)
Li(cid:88)

j=1

(cid:101)ξi,j +

Li(cid:88)
Li(cid:88)

j=1

n(cid:88)
n(cid:88)

i=n/4+1

i=n/2+1

i=3n/4+1

ξi,j +

ξi,j +

i=n/4+1

j=1

i=n/2+1

j=1

i=3n/4+1

3n/4(cid:88)
3n/4(cid:88)
(cid:18)

ξi,j

(cid:101)ξi,j,

j=1

Li(cid:88)
Li(cid:88)
(cid:18)

j=1

(cid:19)

(cid:19)

(cid:105)

i

1

1

1

1 +

2 +

E0

(cid:88)

E[S] .

E[S] =

= 2E[S] +

ξ1,1 +(cid:101)ξ1,1

(cid:3) = 2E[S] +

where n is identiﬁed with 0 in Zn. Taking expectation and adding these two equalities yields

(cid:2)T 1
variance. It suﬃces to show that Var (S1) (cid:16) n, since Var(cid:0)T 1
τa = τa ∧ τ−a + 1(τ−a < τa) ·(cid:101)τ ,

1 − α − 1
(cid:1) = 2Var (S1). Let Y be a biased
An elementary calculation shows that E[S] = f (n). We now turn to prove the asymptotic for the
(2/3, 1/3) random walk on Z with laziness equal to α in [a/2, a] and [−a,−a/2] and equal to 1/2
everywhere else. Then S1 has the same law as τa ∧ τ−a, where τx stands for the ﬁrst hitting time
of x by Y . We can then write

where (cid:101)τ is the time it takes to hit a starting from −a, and is independent of 1(τ−a < τa). Note
that we can write τa =(cid:80)a
Var ((cid:101)τ ) (cid:16) a. This together with the above decomposition of τa ﬁnishes the proof.

i=1 Ti−1,i, where Ti−1,i is the time it takes Y to hit i starting from i − 1.
Using that (Ti−1,i)i have exponential tails and Azuma-Hoeﬀding we obtain that Var (τa) (cid:16) a and

1 − α

1

Proof of Lemma 3.4. For all k we have

P(Sk = t) = P

(ξiT 1

i + (1 − ξi)T 2

i ) = t

=

ξi =

k
2

+ x,

(ξiT 1

(cid:33)

(cid:32) k(cid:88)

k/2(cid:88)

P

x=−k/2

i=1

(cid:33)

(cid:32) k(cid:88)

i=1

i=1

k(cid:88)
k/2+x(cid:88)

(cid:33)

(cid:33)
 .

i ) = t

.

i + (1 − ξi)T 2
k/2−x(cid:88)

T 1

i +

T 2
i = t

i=1

i=1

(cid:32) k(cid:88)

i=1

k(cid:88)

i=1

(cid:32) k(cid:88)

i=1

P

We set for all i

Since the i.i.d. families (T 1

i )i, (T 2

i )i and (ξi)i are independent, by renumbering it follows that

ξi =

k
2

+ x,

(ξiT 1

i + (1 − ξi)T 2

i ) = t

= P

ξi =

k
2

+ x

P

T 1
i = βf (n) + D1
i

and T 2

i = 4f (n) + D2
i ,

where β = (3 − 2α)/(1 − a). We now have

k/2+x(cid:88)

P

k/2−x(cid:88)

 =

(cid:88)

(cid:18) k

2

1

T 1

i +

T 2
i = t

i=1

i=1

y

f (n)(β + 4) + f (n)(β − 4)x + y = t

(cid:19)
 .

×

k/2+x(cid:88)

k/2−x(cid:88)

D1

i +

D2

i = y

i=1

i=1

P

14

Since E(cid:2)T 1

i

dence of (T 1
now yields for all y

i ) and (T 2

i

i
i ), it follows that the same holds for (D1

(cid:3) = βf (n) and E(cid:2)T 2
(cid:3) = 4f (n), it follows that E(cid:2)D1
k/2+x(cid:88)
(cid:18)
k/2−x(cid:88)
k/2+x(cid:88)
 (cid:16)

 (cid:46)

(cid:18) 1

k/2−x(cid:88)

√kn

−c1

i = y

i = y

i +

i +

exp

D2

D1

D1

D2

i=1

i=1

P

P

i=1

i=1

and for |y| ≤ √kn we have

Putting everything together we obtain

P(Sk = t) =

1

f (n)(β + 4) + f (n)(β − 4)x + y = t

2

(cid:18) k

(cid:88)

x,y

(cid:3) = E(cid:2)D2
(cid:19)(cid:19)

i

i ) and (D2

i ). Applying Lemma 3.6

(cid:3) = 0 and by the indepen-
−c1n1/3(cid:17)
(cid:16)

(3.7)

∨ exp

y2
kn

1
√kn

.

(cid:19)
k/2+x(cid:88)

P

P

(cid:32) k(cid:88)
k/2−x(cid:88)

ξi =

i=1

i=1

i=1

k
2

+ x

(cid:33)
 .

×

D1

i +

D2

i = y

(3.8)

(3.9)

To simplify notation we set g(k, x, y) for the expression appearing in the sum above. We start by
proving the lower bound, i.e.

(cid:88)

k

P(Sk = t) (cid:38) 1
n

.

(3.10)

To do this, we will choose speciﬁc values for k, x and y to satisfy the indicator. Dividing through
by f (n)(β + 4)/2 we want to ﬁnd k, x and y to satisfy the equation

k +

Consider the set

(cid:26)

2(β − 4)
β + 4

x +

2y

f (n)(β + 4)

=

2t

f (n)(β + 4)

.

|(cid:96)| ≤ n1/4
(cid:19)
Then by Lemma 3.1 there exists x with |x| ≤ n1/4 such that

(cid:96) ·

:

2(β − 4)
β + 4

(cid:18) 2(β − 4)

β + 4 · x −

f (n)(β + 4)

2t

mod 1 ≤ c · n−1/4,

(cid:27)

.

where c is the constant from Lemma 3.1 for the irrational number 2(β − 4)/(β + 4), which has
bounded continued fraction coeﬃcients. Now take k to be the closest integer to 2(β−4)
f (n)(β+4) .
Then the above inequality gives that

β+4 ·x−

2t

(cid:12)(cid:12)(cid:12)(cid:12) 2(β − 4)

β + 4 · x −

(cid:12)(cid:12)(cid:12)(cid:12) ≤ c · n−1/4.

2t

f (n)(β + 4) − k

From this, if we take y = t − kf (n)(β + 4)/2 − f (n)(β − 4)x we see that |y| ≤ c(cid:48)n3/4 for some
positive constant c(cid:48). Also we get that k (cid:16) √n. Plugging these values for k, x and y in (3.9) and

using (3.8) immediately proves (3.10).

15

It remains to prove the upper bound, i.e. using the notation introduced above

(cid:88)

(cid:88)

P(Sk = t) =

k

k,x,y

g(k, x, y) (cid:46) 1
n

.

(3.11)

We ﬁrst notice that since t (cid:16) n3/2 in the above sum we only take k ≤ c√n for a positive constant c.
We now split the sum into the following sets deﬁned for (cid:96), m ∈ Z

A(cid:96),m = {k ≤ c√n, (cid:96)n1/4 ≤ x ≤ ((cid:96) + 1)n1/4, mn3/4 ≤ y ≤ (m + 1)n3/4}.

We are going to use that if c2 is a positive constant, then for all n large enough we have

∀x : |x| ≤ n1/4, ∃ at most one k :

2(β − 4)
(β + 4)

x −

2t

f (n)(β + 4)

(3.12)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ c2n−1/4.

(cid:12)(cid:12)(cid:12)(cid:12)k +

For all x with |x| ≤ n1/4 from (3.12) there exists at most one k, and hence at most one y such that

k
2

f (n)(β + 4) + f (n)(β − 4)x + y = t.

(3.13)

Since |x| ≤ k/2, we get that |(cid:96)| ≤ c3n1/4 and the above equality gives that |m| ≤ c4n3/4, where c3
and c4 are two positive constants.

Fix now (cid:96), m as above and let (k, x, y), ((cid:101)k,(cid:101)x,(cid:101)y) ∈ A(cid:96),m both satisfy (3.13). Then
where c3 is a positive constant. Since |x −(cid:101)x| ≤ n1/4, Lemma 3.1 gives that the number of such
triples is of order 1. Note that for any a > 0 the function e−a/x
increasing for x > a. Therefore for k ≤ c√n we get that for all r ∈ Z

(cid:12)(cid:12)(cid:12)(cid:12) ≤ c5n−1/4,

(cid:12)(cid:12)(cid:12)(cid:12)(k −(cid:101)k) +

is decreasing for x < a and

(x −(cid:101)x)

2(β − 4)
β + 4

x

(cid:18)

cr2√n

1
k

exp

−

k

(cid:19)

1
√n

≤

e−cr2

.

(3.14)

Using this, Claim 3.5 and (3.7) we get

(cid:88)

(cid:88)

(cid:88)

|(cid:96)|≤c3n1/4

|m|≤c4n3/4

(k,x,y)∈A(cid:96),m

g(k, x, y) (cid:46)(cid:88)

(cid:96)∈Z

(cid:88)

m∈Z

exp(cid:0)

−c((cid:96)2 + m2)(cid:1) + exp

1
n

(cid:16)

−c6n1/3(cid:17)

1
n

(cid:16)

and this concludes the proof of the lemma.

The next result provides a coupling between the walk X and another walk Y on the graph G that
evolves as follows.

Deﬁnition 3.8. We deﬁne a round for X as the time elapsed between a visit to 0 and a return
to 0 after having ﬁrst hit either a or b. Recall that (ξi)i are i.i.d. Bernoulli random variables with
mean 1/2. At the beginning of the i-th round, if ξi = 0, then Y walks on C1 up until the time it
hits 0 after having ﬁrst visited a, i.e. Y spends time T 1
i on C1. If ξi = 1, then Y walks on C2 up
until hitting 0 having ﬁrst visited b, i.e. it spends time T 2
i on C2. Let σ : G → G be a mapping
such that every x ∈ C1 is mapped to the corresponding x ∈ C2 and vice versa.

16

For a Markov chain W we write τS(W ) for the ﬁrst hitting time of the set S, i.e.

τS(W ) = inf{t ≥ 0 : Wt ∈ S}.

If S = {x}, then we simply write τx(W ). If the chain under consideration is clear from the context,
then we simply write τS.

Lemma 3.9. There exists a coupling of X and Y so that

P(cid:0)

∀ t ≤ n2 : Xt = Yt or Xt = σ(Yt)(cid:1)

≥ 1 − ce−cn,

where c is a positive constant.

Proof. Let T0 be the ﬁrst time that X hits 0 after having ﬁrst hit either a or b. We will construct
a coupling of X and Y so that

P(∀ t ≤ T0 : Xt = Yt or Xt = σ(Yt)) ≥ 1 − ce−cn

(3.15)

for a positive constant c. If the coupling succeeds in the ﬁrst round, then we take an independent
copy of the same coupling in the second round and so on. Since by time n2, there are at most n
rounds, the union bound and (3.15) prove the lemma.
To lighten the notation we write τ = τ{A,A(cid:48),B,B(cid:48)}. Let ζ ∈ {0, 1} be a fair coin independent of X.
If ζ = 0, then for t ≤ T0 we set

From this construction it follows that Zτ ∈ {Xτ , σ(Xτ )}. So now we deﬁne Y by setting for t ≤ τ

It is easy to see that the event {Xτ = Zτ} and the random walk path X are independent. Indeed
for a set of paths F we have

P(Xτ = Zτ , (Xt) ∈ F ) = P(cid:0)Xτ , Zτ ∈ {A, A(cid:48)
= P(cid:0)Xτ ∈ {A, A(cid:48)

}, (Xt) ∈ F(cid:1) + P(cid:0)Xτ , Zτ ∈ {B, B(cid:48)
}, ζ = 0, (Xt) ∈ F(cid:1) + P(cid:0)Xτ ∈ {B, B(cid:48)

}, (Xt) ∈ F(cid:1)
}, ζ = 1, (Xt) ∈ F(cid:1)

=

1
2

P((Xt) ∈ F ) = P(Xτ = Zτ ) · P((Xt) ∈ F ) .

Given the independence, it is straightforward to check that Y has the same distribution as the
random walk path X on the two cycles up to time τ .
It also follows immediately from the construction that Yτ = Xτ and at each time t ≤ τ we have
either Yt = Xt or Yt = σ(Xt).
After time τ we continue X and Y together until they hit a if Yτ ∈ {A, A(cid:48)
Zτ ∈ {B, B(cid:48)

} or until they hit b if
}. If they hit 0 before hitting a or b, then we continue them independently afterwards

17

If ζ = 1, then for t ≤ T0 we set

Zt =

Zt =

Xt
σ(Xt)

Xt
σ(Xt)

(cid:40)

(cid:40)

(cid:40)

if Xt ∈ C1
if Xt ∈ C2.

if Xt ∈ C2
if Xt ∈ C1.

Yt =

Zt
σ(Zt)

if Xτ = Zτ
otherwise.

and we declare that the coupling has failed. After hitting either a or b we continue them together
until they hit 0. This completes a round. Note that the probability the coupling fails in one round
is by symmetry at most

P0(τA(cid:48) < τA) + PA(τ0 < τa) ≤ 2

−n/2+1

and this proves (3.15) and ﬁnishes the proof of the lemma.

Claim 3.10. There exists a positive constant c such that for all s ≥ 0 we have

P0(Xs = 0, τa ∧ τb > s) (cid:46) e−cs + e−cn.

Proof. Writing again τ = τ{A,A(cid:48),B,B(cid:48)}, we have

P0(Xs = 0, τa ∧ τb > s) =P0(Xs = 0, τa ∧ τb > s, τ > s)

+ P0(Xs = 0, τa ∧ τb > s, τ ≤ s) .

For the second probability appearing above using symmetry we immediately get

P0(Xs = 0, τa ∧ τb > s, τ ≤ s) ≤ 2(P0(τA(cid:48) < τA) + PA(τ0 < τa)) ≤ 2

−n/2+2.

Let π be the projection mapping, i.e. π(x) = x if x ∈ C1 and if y ∈ C2, then it maps it to the
corresponding point on C1. Then clearly Y = (π(Xt))t≤τ is a lazy (1/2) biased random walk on C1.
We now get

P0(Xs = 0, τa ∧ τb > s, τ > s) ≤ P0(Ys = 0, τ > s) .
Let (cid:101)Y be a lazy biased random walk on Z with pi,i+1 = 1 − pi,i−1 = 2/3. Then
(cid:12)(cid:12)(cid:12) ≥ s/6
(cid:17)
Since (cid:101)Ys − s/6 is a martingale with increments bounded by 7/6, applying Azuma-Hoeﬀding’s in-

(cid:16)(cid:12)(cid:12)(cid:12)(cid:101)Ys − s/6

P0(Ys = 0, τ > s) ≤ P0

(3.17)

(3.16)

.

(cid:16)(cid:101)Ys = 0
(cid:17)
≤ P0
(cid:12)(cid:12)(cid:12) ≥ s/6
(cid:16)(cid:12)(cid:12)(cid:12)(cid:101)Ys − s/6
(cid:17)

P0

≤ 2e−cs,

equality gives

where c is a positive constant. This together with (3.16) ﬁnishes the proof.

Proof of Lemma 3.2. Recall starting from 0 a round for X was deﬁned to be the time passed
until a return to 0 after having ﬁrst visited either a or b.

Let L be the last time before time t the walk completed a round. If no round has been completed
before time t, then we take L = 0. Hence in all cases XL = 0 and we obtain

(cid:0)L = t(cid:48), Xt = 0(cid:1) =

P0

(cid:88)

t(cid:48)<t

(cid:88)

t(cid:48)<t

P0

(cid:46) e−cn +

(cid:0)L = t(cid:48)(cid:1) P0
(cid:88)

(cid:0)Xt−t(cid:48) = 0, τa ∧ τb > t − t(cid:48)(cid:1)
(cid:0)L = t(cid:48)(cid:1) e−c(t−t(cid:48)),

P0

P0(Xt = 0) =

where for the last inequality we used Claim 3.10. Therefore we get

(cid:88)

t(cid:48)<t

(cid:0)L = t(cid:48)(cid:1) e−c(t−t(cid:48)) ≤

P0

(cid:88)

t(cid:48)<t

(cid:0)L = t(cid:48)(cid:1) e−c(t−t(cid:48)) +

P0

(cid:88)

(cid:0)L = t(cid:48)(cid:1) e−c(t−t(cid:48))

P0

t(cid:48)<t

t−t(cid:48)>2 log n/c

t(cid:48)<t

t−t(cid:48)≤2 log n/c

18

(cid:88)

t(cid:48)<t

t−t(cid:48)≤2 log n/c

1
n2 +

≤

(cid:0)L = t(cid:48)(cid:1) e−c(t−t(cid:48)).

P0

(3.18)

For the independent random variables (T 1

i ), (T 2

i ) and (ξi) from Deﬁnition 3.3, we now deﬁne

(cid:40)

(cid:0)ξiT 1

k(cid:88)

i=1

N = max

k ≥ 0 :

i + (1 − ξi)T 2

i

≤ t

(cid:41)

(cid:1)

and as in Deﬁnition 3.3 we write Sk for the sum appearing in the maximum above.
For the term P0(L = t(cid:48)) we have

(cid:0)L = t(cid:48), coupling failed(cid:1) ,

(cid:0)L = t(cid:48)(cid:1) = P0

P0

where by success we mean that {Xt = Yt or Xt = π(Yt), ∀t ≤ n2}. Therefore from Lemma 3.9 we
get

k+1 > t − t(cid:48)(cid:1)

k+1 + (1 − ξk+1)T 2

(cid:0)L = t(cid:48), coupling succeeded(cid:1) + P0
P(cid:0)SN = t(cid:48)(cid:1) (cid:46) P0
P(cid:0)N = k, Sk = t(cid:48)(cid:1) =
P(cid:0)Sk = t(cid:48)(cid:1) P(cid:0)ξk+1T 1

≤ P(cid:0)SN = t(cid:48)(cid:1) + ce−cn.
P(cid:0)Sk = t(cid:48), ξk+1T 1
k+1 > t − t(cid:48)(cid:1) .

(cid:0)L = t(cid:48)(cid:1)
(cid:88)

k+1 + (1 − ξk+1)T 2

k

P(cid:0)SN = t(cid:48)(cid:1) =

(cid:88)
(cid:88)

k

k

=

By the independence of the random variables N and Sk we obtain

By Chebyshev’s inequality and the concentration of T 1

i around their means by Lemma 3.7

we get for t − t(cid:48)

≤ 2 log n/c and n large enough

P(cid:0)ξk+1T 1
Therefore, this implies that for t(cid:48) with t − t(cid:48)
P(cid:0)SN = t(cid:48)(cid:1)
(cid:16)

(cid:88)

k

k+1 + (1 − ξk+1)T 2

i and T 2

k+1 > t − t(cid:48)(cid:1)
P(cid:0)Sk = t(cid:48)(cid:1)

1
n

,

(cid:16)

1
2

.

≥

≤ 2 log n/c and all n large enough

where the last equivalence follows from Lemma 3.4. Putting all estimates together in (3.18) yields

P0(Xt = 0) (cid:46) 1
n

.

Clearly we also have

P0(Xt = 0) ≥ P0(L = t) (cid:38) P(SN = t) (cid:16)

(cid:88)

k

P(Sk = t) (cid:16)

1
n

and this concludes the proof of the lemma.

Lemma 3.11. For all times t ∈

and all x ∈ C1 ∪ C2 we have

25 , 10n3/2(cid:105)
(cid:104) n3/2

P0(Xt = x) (cid:16)

1
n

.

19

P0

P0

t(cid:48)<t

t(cid:48)<t

t−t(cid:48)≥2E0[T0]

t−t(cid:48)<2E0[T0]

(cid:88)
(cid:0)L = t(cid:48), Xt = x(cid:1)
(cid:88)
(cid:0)L = t(cid:48)(cid:1) P0
(cid:104) n3/2
50 , 10n3/2(cid:105)
(cid:88)
P0(T0 > s, Xs = x) ≤
(cid:34) T0(cid:88)

s<2E0[T0]

(cid:35)

1
n

A1 (cid:16)

(cid:34) T0(cid:88)

s=1

E0

1
n

(cid:35)

1(Xs = x)

.

Proof. Let L be as in the proof of Lemma 3.2 and let T0 be the ﬁrst time that X completes a
round as in the proof of Lemma 3.9. Then we have

≤

(cid:88)
(cid:0)T0 > t − t(cid:48), Xt−t(cid:48) = x(cid:1) = A1 + A2.

P0(Xt(cid:48) = 0) P0

t(cid:48)<t

(cid:0)T0 > t − t(cid:48), Xt−t(cid:48) = x(cid:1)

P0(Xt = x) =

+

When t(cid:48) satisﬁes t − t(cid:48) < 2E0[T0], then using the range of values that t can take, we get that
for n large enough, t(cid:48)
, and hence Lemma 3.2 applies and yields P0(Xt(cid:48) = 0) (cid:16) 1/n.
Therefore,

∈

Since T0 is a stopping time satisfying P0(XT0 = 0) = 1, using [10, Lemma 10.5] we get

E0

1(Xs = x)

= π(x)E0[T0] = c1,

s=1

where c1 is a positive constant. We now claim that E[T0] (cid:16) n and Var (T0) (cid:16) n. Indeed, using the
coupling of Lemma 3.9 with probability 1 − ce−cn the time T0 is equal to T 1
1 equally likely.

1 or T 2

This together with Lemma 3.7 justiﬁes the claim.

Therefore, for the quantity A2 we have

(cid:88)

(cid:0)L = t(cid:48)(cid:1) P0(T0 ≥ 2E0[T0]) ≤ P0(T0 ≥ 2E0[T0]) ≤

P0

A2 ≤

t(cid:48)<t

t−t(cid:48)≥2E0[T0]

Var (T0)
(E0[T0])2 (cid:16)

1
n

,

where the last inequality follows from Chebyshev’s inequality.

We now turn to prove a lower bound. Using Lemma 3.2 again we have for n large enough

P0(Xt = x) ≥

P0(Xt(cid:48) = 0) P0

(cid:88)
T0∧3E0[T0](cid:88)

t(cid:48)<t

s=1

t−t(cid:48)<3E0[T0]

1
n

(cid:16)

E0

(cid:0)T0 > t − t(cid:48), Xt−t(cid:48) = x(cid:1)
 ≥

(cid:34) T0(cid:88)

E0

1
n

s=1

1(Xs = x)

1(Xs = x)1(T0 < 3E0[T0] , τx < T0)

(cid:35)

P0(T0 < 3E0[T0] , τx < T0) (cid:16)
For the last equivalence we used that as n → ∞

≥

1
n

1
n

.

P0(T0 < 3E0[T0]) ≥

2
3

and P0(τx < T0) ≥

1
2

(1 − o(1)),

where the ﬁrst inequality follows from Markov’s inequality and for the second one we used that
with probability 1/2 the walk goes around the cycle where x belongs and the probability that x is
not hit is at most 2−n/2. This completes the proof.

20

(cid:104) n3/2
10 , 10n3/2(cid:105)
Px(Xt = y) (cid:16)

1
n

.

Lemma 3.12. For all x, y and all times t ∈

we have

Proof. We have

Px(Xt = y) =

(cid:88)

s<t

Px(τ0 = s, Xt = y) + Px(τ0 > t, Xt = y) .

Note that Px(τ0 > t) ≤ 1/n2 by Chebyshev’s inequality. By the Markov property we now get

Px(τ0 = s, Xt = y) =

Px(τ0 = s) P0(Xt−s = y)

(cid:88)

s<t

Px(τ0 = s) P0(Xt−s = y) +

Px(τ0 = s) P0(Xt−s = y) .

(cid:88)

2Ex[τ0]<s<t

(cid:88)

s<t

=

(cid:88)

s<2Ex[τ0]

By Chebyshev’s inequality again we get

(cid:104) n3/2
10 , 10n3/2(cid:105)

Px(τ0 ≥ 2Ex[τ0]) (cid:46) 1
(cid:104) n3/2
25 , 10n3/2(cid:105)
and s < 2Ex[τ0] (cid:46) n, then t− s ∈

n

.

When t ∈
Lemma 3.11 gives for n large enough

and this completes the proof.

Px(Xt = y) (cid:16)

1
n

for n large enough. Hence

Lemma 3.13. Let P be a transition matrix on a ﬁnite state space with stationary distribution π.

∞ distance d1 the total variation distance, i.e.,

Let d∞ denote the L
d∞

(r) = max
x,y

(cid:12)(cid:12)(cid:12)(cid:12) P r(x, y)

π(y) − 1

(cid:12)(cid:12)(cid:12)(cid:12)

and d1(r) = max

x (cid:107)P r(x,·) − π(cid:107)TV.

Then for all s, t ∈ N we have

d∞

(s + t) ≤ d∞

(s)d1(t).

Proof. A standard duality argument (see for instance Chapter 1 of Saloﬀ-Coste [5]) implies that
for all p ∈ [1,∞], the Lp distance to stationarity at time t equals the operator norm (cid:107)P t(cid:107)q→∞.
Here P t acts on the functions that have mean zero w.r.t. π and 1/p + 1/q = 1. Clearly

Applying this with p = ∞ and q = 1 gives what we want.

(cid:107)P t+s(cid:107)q→∞ ≤ (cid:107)P t(cid:107)q→∞(cid:107)P s(cid:107)∞→∞.

Proof of Theorem 1.5. (upper bound)
Let t = n3/2. By Lemma 3.12 there exist two positive constants c1 < 1 and c2 > 1 so that for all x
and y we have

c1π(y) ≤ P t(x, y) ≤ c2π(y).

21

(3.19)

Therefore, for any two vertices x, x(cid:48) on the graph G we get

(cid:107)P t(x,·) − P t(x(cid:48),·)(cid:107)TV = 1 −

P t(x, y) ∧ P t(x(cid:48), y) ≤ 1 − c1.
Since ¯d(s) = maxx,x(cid:48) (cid:107)P s(x,·) − P s(x(cid:48),·)(cid:107)TV is sub-multiplicative, we obtain that

y

(cid:88)

So, choosing (cid:96) such that (1 − c1)(cid:96) ≤ 1/4, we get

¯d((cid:96)t) ≤ (1 − c1)(cid:96).

x (cid:107)P (cid:96)t(x,·) − π(cid:107)TV ≤ ¯d((cid:96)t) ≤
max

1
4

,

and hence this gives that

tmix (cid:46) n3/2.

(3.20)

Using (3.19) again also gives that for all x and y

(cid:12)(cid:12)(cid:12)(cid:12) P t(x, y)

π(y) − 1

(cid:12)(cid:12)(cid:12)(cid:12) ≤ (c2 − 1) ∨ (1 − c1),

and hence this implies that d∞(t) ≤ (c2 − 1) ∨ (1 − c1). Using (3.20) and Lemma 3.13 now proves
that tunif (cid:46) n3/2.

Before giving the proof of the lower bound on the mixing time, we state and prove a preliminary
lemma on the concentration of a biased random walk on Z when the laziness parameter is bounded
away from 1.

Lemma 3.14. Let δ ∈ (0, 1] and Y be a birth and death chain on Z starting from 0 with
= 1 − p(x, x) − p(x, x − 1).

p(x, x) ≤ 1 − δ

2(1 − p(x, x))

p(x, x + 1) =

and

3

Then for all t and all positive constants c < 1, there exists a positive constant C and an interval

It = [k − C√t, k + C√t] with
where k is such that t − E0[τk] > 0 is minimised.

P0(Yt ∈ It) ≥ c,

Proof. It is elementary to check that there exist two positive constants c1 and c2 so that for

all k ∈ N we have
Since E0[τk] increases by at most c2 when k increases by 1, for every given t we can ﬁnd the ﬁrst k
so that 0 ≤ E0[τk] − t ≤ c2. Let C be a positive constant to be determined. Since at every time

step Y moves by at most 1, we immediately get that

(cid:17)

c1k ≤ E0[τk] ≤ c2k and Var (τk) (cid:16) k.
(cid:17)
+ P(cid:16)
τk > t + C√t
≤ P(cid:16)
(cid:17)

τk − E[τk] > t − E[τk] + C√t

≤ P(cid:16)

Yt /∈ (k − C√t, k + C√t)

P(cid:16)

(cid:17)
τk > t + C√t

= P(cid:16)

(cid:17)
τk < t − C√t

.

(3.21)

(cid:17)

τk − E[τk] > C√t − c2

Taking now C suﬃciently large, using that Var (τk) (cid:16) k (cid:16) t and Chebyshev’s inequality we get

P(cid:16)

22

≤

Similarly, we obtain that

Var (τk)

1 − c
2

.

(C√t − c2)2 ≤
P(cid:16)

(cid:17)
τk < t − C√t

1 − c
2

,

≤

and plugging these two bounds in (3.21) completes the proof.

Proof of Theorem 1.5. (lower bound)

Clearly it suﬃces to prove the lower bound on tmix.
Let Y be the walk on G as in Deﬁnition 3.8. Let E = {Xt = Yt or Xt = π(Yt), ∀t ≤ n2}. Then
Lemma 3.9 gives us that P(Ec) ≤ ce−cn for some positive constant c.
later. Then by this time, the total number of completed rounds k(t) satisﬁes k(t) ≤ ε√n. For
each k ≤ ε√n among the k rounds, we let (cid:96)(k) and r(k) be the number of left and right rounds

So we are now going to work with the walk Y . Let t = εn3/2 for ε small enough to be determined

completed respectively. Let (ξi) be i.i.d. Bernoulli random variables taking values 1 and 0 depending
on whether the i-th round completed was on the left or on the right. Then

k(cid:88)

k(cid:88)

i=1

(cid:96)(k) =

ξi

and r(k) =

(1 − ξi).

Therefore we get

i=1

(cid:96)(k) − r(k) =

k(cid:88)

i=1

(2ξi − 1),

which shows that the diﬀerence (cid:96)(k) − r(k) is a simple random walk on Z. Let F be the event

(cid:26)

F =

√
max
k≤ε

(cid:27)

.

n|(cid:96)(k) − r(k)| < √Cεn1/4
E(cid:2)((cid:96)(ε√n) − r(ε√n))2(cid:3)

=

Applying Doob’s maximal inequality we now get

Cε√n

i )i and (T 2

P(F c) ≤
(cid:3) = γn, where β and γ are given by Lemma 3.7 and satisfy β > γ.
Recall as before (T 1
i )i are independent i.i.d. collections of random variables distributed
according to the commute time between 0 and a on the left and 0 and b on the right cycle respec-
k(cid:88)

tively. Let E(cid:2)T 1
The exact values of β and γ will not be relevant here. For all k ≤ ε√n we deﬁne

(cid:3) = βn and E(cid:2)T 2
k(cid:88)

k(cid:88)

k(cid:88)

(3.22)

1

1

1
C

.

Mk =

ξiT 1

i +

i=1

i=1

(1 − ξi)T 2

i − βn

ξi − γn

(1 − ξi).

i=1

i=1

Then clearly this is a martingale and applying Doob’s maximal inequality again we obtain for a
large enough constant C(cid:48)

(cid:18)

(cid:105)

E(cid:104)

√
M 2
ε
(C(cid:48))2n

n
3
2

(cid:19)

23

P

√
max
k≤ε

n|Mk| ≥ C(cid:48)n

3
4

≤

=

1
C

.

(3.23)

i=1

B =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)k(t)(cid:88)

P(Bc) ≤ P(cid:16)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)k(t)(cid:88)

k(t)(cid:88)

ξiT 1

i +

i=1

i=1

 .
(cid:17)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ Cn

≤

The last equality follows from the fact that Var(cid:0)T 1

(cid:1)

(cid:16) kn for
all k. Let A be the complement of the event appearing in (3.23). Recall k(t) denotes the number
of rounds completed up to time t. Let B be the event

k

i

(cid:3)

(cid:16) n for i = 1, 2, and hence E(cid:2)M 2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) < 2Cβn

i − t

On Bc, the k(t) + 1 round lasts longer than 2Cβn. Markov’s inequality then gives

k(t)(cid:88)

i=1

ξiT 1

i +

(1 − ξi)T 2
(cid:17)

+ P(cid:16)

T 1
k(t)+1 ≥ 2Cβn

T 2
k(t)+1 ≥ 2Cβn

1
C

.

(3.24)

Let kL and kR denote the left and right rounds respectively completed up to time t. On A we have

(1 − ξi)T 2

i − (kR · βn + kL · γn)

3
4 ,

and hence on A ∩ B we have

|kR · βn + kL · γn − t| ≤ c1n,

where c1 is a positive constant. If we now ﬁx |kR − kL| = j, then from the above inequality, we see
that the number of choices for kR and kL is at most 2c1.

Suppose that we know that by time t the random walk on the two cycles performed kL left rounds
and kR right rounds and that |kR − kL| = j. Fix an ordering of the rounds and consider a
biased (2/3, 1/3) random walk Z on Z for which the laziness parameter is either α or 1/2 in
[(i − 1)n + n
4 , (i − 1)n + 3n
4 ] depending on whether the corresponding round was a left or a right
one and equal to 1/2 everywhere else. Let τ(cid:96) be the ﬁrst time that Z hits (cid:96). Then for every (cid:96) we
can express E0[τ(cid:96)] as follows

m(cid:88)

i=1

E0[τ(cid:96)] =

E
(i−1)n[τin] + Em[τ(cid:96)] ,

where m is the largest integer satisfying |m − in| ≤ n. Then for all i we have

(cid:2)τin1(τ(i−1)n−n/4 < τin)(cid:3) + E

(cid:2)τin1(τ(i−1)n−n/4 > τin)(cid:3) .

(i−1)n

(i−1)n

(i−1)n

E
(i−1)n[τin] = E

(cid:2)τin1(τ(i−1)n−n/4 < τin)(cid:3) (cid:46) e−cn, and hence the diﬀerence that we get from reordering
But E
the rounds is negligible, once we ﬁx the laziness of the last interval. For each diﬀerent choice of
j ≤ √εn1/4 and a choice of kR and kL satisfying |kR − kL| = j let (cid:96) be such that t − E0[τ(cid:96)] > 0
is minimised. Since the ordering of the rounds does not aﬀect E0[τ(cid:96)] up to terms exponentially
small in n, for all δ > 0 from Lemma 3.14 we get a positive constant c2 and an interval It =
((cid:96) − c2√t, (cid:96) + c2√t) so that

As we proved above on the event A ∩ B ∩ G there are M = c2√εn1/4 diﬀerent choices for the

number of left and right rounds and each of them gives rise to an interval It. Let I be the union of

P(Zt ∈ It) ≥ 1 − δ.

(3.25)

24

one on each cycle, then from (3.22), (3.23) and (3.24) we get for n suﬃciently large

all these M intervals. Then |I| (cid:46) √εn1/4√t (cid:16) εn. If we now set (cid:101)I = I mod n and take two copies,
P(cid:16)
(cid:17)
(cid:17)
A, B, F, Yt /∈(cid:101)I
Xt /∈(cid:101)I
P(cid:16)
(cid:17)
A, B, F, Yt /∈(cid:101)I

≤ P(Ac) + P(Bc) + P(F c) + P(Ec) + P(cid:16)
(cid:17)
A, B, F, Yt /∈(cid:101)I
t /∈ I(cid:1) P(cid:16)
P(cid:0)Zi

+ P(cid:16)

ξ1 = i1, . . . , ξε

We now have

+ e−cn,

(cid:88)

(cid:88)

n = iε

(cid:17)

4
C

≤

√

n

√

.

≤

j≤√

εn1/4

√

n∈{0,1}

i1,...,iε

|#{i=1}−#{i=0}|=j

where Zi stands for a walk on Z with the laziness parameter arranged according to the vector
i = (i1, . . . , iε
n). Note that to get the inequality above, once we ﬁx the left and right rounds, we
coupled in the obvious way the walk Y to the walk Z mod n and the probability that the coupling
fails is e−cn. Using (3.25) now gives that

√

≤ δ + e−cn.

large constant C we obtain

Putting all estimates together shows that when t = εn3/2 and I and(cid:101)I are as above, for a suﬃciently
Since π((cid:101)I) ≤ c3ε, for a positive constant c3, it immediately follows that

≥ 1 − 2δ.

P(cid:16)

(cid:17)
A, B, G, Yt /∈(cid:101)I
P(cid:16)
(cid:17)
Xt ∈(cid:101)I

(cid:107)P t(0,·) − π(cid:107)TV ≥ 1 − 2δ − c3ε.

Taking ε and δ suﬃciently small ﬁnishes the proof of the lower bound.

4 Exploration time

In this section we prove Theorems 1.7 and 1.8. We start by introducing notation that will be used
throughout the section. For any two vertices u and v in G we write H(u, v) = Eu[τv] and if A is
a set we write H(u, A) = Eu[τA]. We write C(u, v) for the commute time between u and v, i.e.
C(u, v) = H(u, v) + H(v, u). Finally, if e is an edge, then we write C(e) for the commute time
between the endpoints of e.

We next note the following two inequalities for commute and cover times in Eulerian digraphs.

Lemma 4.1. Consider two vertices v and w such that d(v, w) = (cid:96) (in the undirected version of G),
then the commute time between v and w is bounded by m(cid:96).

Proof. Let us ﬁrst assume (cid:96) = 1, and write d for dout(v). On each excursion from the vertex v
to itself, w is hit with probability at least 1
d , since w can be hit on the ﬁrst step of the excursion.
So the expected number of excursions needed before visiting w is at most d. Each excursion has
an expected length of m
d , so by Wald’s identity, the expected number of steps before hitting w and
going back to v is at most dm
In general, let v = v0 → v1 → . . . → v(cid:96) = w be a path from v to w in the undirected version of G.
Then we may bound

d = m.

(cid:96)−1(cid:88)

i=0

C(u, v) ≤

C(vi, vi+1).

(4.1)

We just saw each summand on the right hand side is less than m, so this ﬁnishes the proof.

25

Claim 4.2. There exists a positive constant c so that the following is true. Let G be an Eulerian
digraph on n vertices, m directed edges and minimal degree dmin. Then for all v ∈ V we have

mn
dmin

.

Ev[Tn] ≤ 16 ·
(cid:88)

Ev[Tn] ≤

C(e) ≤ 16 ·

e∈T

mn
dmin

.

Proof. In [7, Theorem 2] it was shown that in every undirected graph G there is a spanning tree T
so that for all v

an Eulerian digraph is bounded above by the commute time (cid:101)C(e) in the corresponding undirected

It follows immediately from the min-max characterisation of commute times in [4] that C(e) in
graph and this completes the proof.

4.1 Proof of Theorem 1.7

We start by generalizing some well-known facts from the reversible case to our more general setting.
Note that we are not optimising the constants in the results below.

The ﬁrst lemma is a well known graph-theoretic fact, but we include the proof for the reader’s
convenience.

Lemma 4.3. Consider an undirected graph G on n vertices and assume each node has at least d
distinct neighbours. Let A ⊆ V be a set of cardinality |A| < n. Then,

d(v, Ac) ≤

3|A|
d

+ 1.

(4.2)

Proof. Let w ∈ Ac be a vertex closest to v among all vertices in Ac. Let v = v0, v1, . . . , v(cid:96) = w
be the shortest path leading from v to w with (cid:96) = d(v, Ac). If (cid:96) = 1 the statement holds trivially,
so we may assume (cid:96) ≥ 2. Note that v0, . . . , v(cid:96)−2 have all their neighbours in A, since otherwise
this would contradict the minimality of (cid:96). Moreover, note that (v3i)0≤3i≤(cid:96)−2 must have disjoint
neighbours, as otherwise this would also contradict the minimality of (cid:96). There are 1 + (cid:98)((cid:96) − 2)/3(cid:99)
indices i ≥ 0, such that 3i ≤ (cid:96) − 2. Hence we ﬁnd that

≤ |A|,

(4.3)

|A|
d .

which in turn implies ((cid:96) − 1)/3 ≤
Throughout this section we make the assumption that G is d-regular.
Deﬁne Nv(t) := #{s < t : Xs = v} the number of visits to v up to time t and for a subset A ⊆ V ,
deﬁne Nv(A) := Nv(τA) the number of visits to v before reaching the set A. A version of the
following two lemmas is given in [1, Chapter 6], where everything is done in the undirected setting.
We adapt the proofs to our broader context.

Lemma 4.4. Let A ⊆ V and v ∈ A. Then,
(1) Ev[τAc] ≤ 10|A|2
(2) Ev[Nv(Ac)] ≤ 10|A|

26

(cid:18)

d

1 +

(cid:23)(cid:19)

(cid:22) (cid:96) − 2

3

(3) Ev[Nv(t)] ≤ 8√t for all t ≤ 10n2.
Proof. (1) We can assume that d ≤ 2|A|, otherwise the result is trivial. Contract Ac to a single
vertex. The resulting graph has at most 2d|A| directed edges (excluding self-loops) and is still
Eulerian. Lemma 4.3 tells us there is a path of length (cid:96) ≤ 3|A|
(cid:17)
d + 1 from v to Ac in the undirected
version of G. So Lemma 4.1 now gives that the expected time to hit Ac is smaller than the commute
time which is in turn upper bounded by 2d|A| ·
(2) Let τ Ac
the proof of part (1) gives that Ev

v be the ﬁrst time that the random walk returns to v after having ﬁrst hit Ac. Note that

≤ 10|A|2. Then using [10, Lemma 10.5] we get that

(cid:16) 3|A|

(cid:2)τ Ac

≤ 10|A|2.

d + 1

(cid:3)

v

(cid:2)τ Ac

v

(cid:3) π(v) ≤ 10|A|2 ·

1

|V | ≤ 10|A|,

Ev[Nv(Ac)] = Ev

since A ⊆ V .
(3) For s > 0 to be chosen later let

Since G is assumed to be Eulerian, P is bistochastic and, for all (cid:96) ∈ N,

A = {w : Ew[Nv(t)] > s}.

(cid:88)
(cid:88)

w

p((cid:96))
wv = 1,

Ew[Nv(t)] = t,

which in turn implies, summing over (cid:96) ≤ t − 1, that

w

so |A| ≤ t/s. As long as t

s < n, we can now use the result above to get

Ev[Nv(t)] ≤ Ev[Nv(Ac)] + s ≤ 10

t
s

+ s.

Choosing s = √10t restricts t to be smaller than 10n2, and gives the desired bound.

Proof of Theorem 1.7. Let α and C be constants to be chosen later to satisfy α2C ≤ 10. We
can always assume k ≤ αn, otherwise we can bound Tk by the cover time, which is smaller than
16n2 ≤ 16k2/α2, using Claim 4.2. Let t = Ck2. Observe that if we knew that for all v

Pv(Tk ≥ t) ≤

1
2

,

(4.4)

2 ), which is enough to conclude.

then the strong Markov property would imply that Tk is stochastically dominated by t × ξ where
ξ ∼ Geom( 1
Let v1, v2, . . . be the distinct random vertices visited by the walk, in the chronological order they
appeared. Write ti for the hitting time of the vertex vi. Conditioning on vi, ti and using the strong
Markov property we get

Ev[Nvi(t)] =

(cid:88)

u,s

Pv(vi = u, ti = s)Eu[Nu(t − s)] ≤ 8√t,

(4.5)

27

where in the last step we used Lemma 4.4, since t ≤ 10n2 by the choice of α and C. We use
the convention that Nu(·) is 0 when evaluated at negative times. Markov’s inequality together
with (4.5) now gives

Pv(Tk ≥ t) = Pv

Nvi(t) = t

8k√t

t

≤

=

8
√C

,

(cid:33)

(cid:32)k−1(cid:88)

i=1

where in the last equality follows from the choice of t. Taking √C = 16 and α such that α2C = 10
proves (4.4) and this ﬁnally shows that

Ev[Tk] ≤ max

which concludes the proof.

(cid:18)

(cid:19)

2C,

16
α2

k2 = 512k2,

4.2 Proof of Theorem 1.8.

For an undirected graph G = (V, E) we write Gk for the graph (V,(cid:101)E) where (u, v) ∈ (cid:101)E if and

only if there is a path from u to v of length at most k in G. Recall a Hamiltonian cycle is a cycle
in the graph that visits all vertices exactly once. A graph is called Hamiltonian if it contains a
Hamiltonian cycle.

As in [3], the following fact will be crucial. We include the proof for completeness.

Lemma 4.5. For any undirected graph G, its cube G3 is Hamiltonian.

Proof. By taking a spanning tree of G we can reduce to the case where G is a tree. We will prove
the following stronger statement by induction : for any v ∈ G, there exists a labelling of the vertices
v = v1, . . . , vn of G such that

• d(vi, vi+1) ≤ 3 for all i ≤ n − 1 and
• d(v1, vn) = 1.

We will call a labelling proper if it fulﬁls the previous two conditions.
The result obviously holds when the tree has size 1. Assume it holds for trees of size n − 1 and
let the size of G be n. Let v ∈ G and w a neighbour of v. Since G is a tree, G − (v, w) has two
connected components Gv and Gw, containing v and w respectively.

By the induction hypothesis we can properly label the vertices of Gv as v = v1, . . . , v(cid:96) and properly
label the vertices of Gw as w = w1, . . . , wr, where r and l are related by n = (cid:96) + r.

We build the labelling on G as follows (running forward on Gv and backwards on Gw).

(cid:40)

(cid:101)vi =

vi
wn+1−i

if 1 ≤ i ≤ (cid:96)
if (cid:96) + 1 ≤ i ≤ n

Now observe that

d((cid:101)v(cid:96),(cid:101)v(cid:96)+1) = d(v(cid:96), wr) ≤ d(v1, v(cid:96)) + d(v1, w1) + d(w1, wr) = 3,

since the labellings on Gv and Gw are proper and w1 = w and v1 = v are neighbours. Finally, note

that d((cid:101)vn, v1) = d(w, v) = 1.

28

Figure 3: The diﬀerent subsets of the vertices.

Before proving Theorem 1.8 we deﬁne phases for the walk and the notions of good and bad vertices
following closely [3].

Call v the starting vertex. Let v1, . . . , vn be the arrangement (i.e. a cycle in the graph G3) we get
using Lemma 4.5 on the undirected version of G.

A naive attempt would be to start a new phase each time a new vertex is discovered, but this seems
hard to analyse. Instead, we will start a new phase each time the walk moves either to a set of good
vertices (to be deﬁned) or when it reaches the next vertex speciﬁed by the ordering of the cycle.
In the latter case, we say the walker moves along the cycle. We emphasise that the orientation on
the cycle has nothing to do with the orientation of G, i.e. we know nothing about d(vi, vi+1), since
the cycle is built ignoring the directions on the edges of G.

We will show O(k) phases are needed and they all have O(k2) expected length. This will be enough
to show Theorem 1.8.

The deﬁnitions we give below are the same as in [3]. At the beginning of phase i, let Yi be the
visited vertices so far. The last vertex visited in phase i − 1 will be denoted si and ri is its right
successor on the cycle. A vertex vj /∈ Yi is called good in phase i if for all (cid:96) ≤ n,

|{vj, vj+1, . . . , vj+(cid:96)−1} ∩ Yi| ≤

(cid:96)
2

.

Note that if k > n, then we take vk to be vk−n, i.e. we always move around the cycle.
We deﬁne Ui to be the set of all good vertices in phase i. The set of vertices Bi = V − (Ui ∪ Yi)
are called bad (see Figure 4.2).

Lemma 4.6 ([3]). At most 2k phases are needed before visiting k distinct vertices.

Proof. The proof can be found in [3], but we brieﬂy sketch it here. Recall phases end either when
the walk moves to the right on the cycle or when it moves to a good vertex. By deﬁnition, for any
length (cid:96) ≤ n, at least half of the vertices following a good vertex on the cycle are unvisited, so as
long as the walk keeps moving on the cycle, half of the starting vertices of the phases are new.

At this point, we are still working on the undirected version of G, and the following combinatorial
fact still holds.

29

2 , then |Bi| ≤ |Yi|.

Lemma 4.7 ([3]). If |Yi| ≤ n
Proof. We again proceed as in [3]. The proof is by induction. If |Yi| = 1, then Bi is empty and the
result holds. Suppose it holds for |Yi| ≤ (cid:96), with any n such that (cid:96) < n
2 and let Yi be of size (cid:96) + 1.
As (cid:96) + 1 ≤ n
2 , we can ﬁnd v ∈ Yi that is the right neighbour (on the cycle) of some vertex in w ∈ Y c
i .
Remove both v and w, and apend w’s left neighbour to v’s right neighbour. In this new graph of
2 , and so

size n − 2, (or we should say cycle, since all that matters is the cycle) |(cid:101)Yi| = (cid:96) − 1 ≤ n−2
by induction hypothesis, |(cid:101)Bi| ≤ |(cid:101)Yi|. But now notice Yi = (cid:101)Yi ∪ {v}, and Bi ⊆ (cid:101)Bi ∪ {w}, so we can

conclude.

So far we have been following [3]. The proof there for the undirected case is completed by noting
that if s ∈ W ∩ ∂W c, then H(s, W c) ≤ 2m(W ), where we recall that H(a, B) stands for the
expected hitting time of B starting from a and m(W ) is the number of edges of W . Using this, the
authors in [3] bound the expected length of a phase and together with Lemma 4.6 this concludes
the proof.

However, for the Eulerian directed case obtaining an upper bound on the expected length of a phase
requires extra care. This is because of the fact that an arbitrary subset of the vertices doesn’t span
an Eulerian digraph, and all we know is that either (si, ri) ∈ E or (ri, si) ∈ E, since the cyclic
arrangement was chosen for the undirected version of G. The following lemma solves this issue.
Note that we write A (cid:116) B to mean the disjoint union of A and B. Also we denote by ∂A the set of
undirected neighbours of A and we write E(A, B) for the set of directed edges from A to B.
Lemma 4.8. Let W (cid:116) Z = V and s ∈ W ∩ ∂Z. Then H(s, Z) ≤ 12|W|2.
Proof. We ﬁrst claim that if A (cid:116) B = V and a ∈ A ∩ ∂B, then
H(a, B) ≤ |A|2 + 2|E(A, B)|.

(4.6)
Indeed, the graph obtained by contracting B to a single vertex is still Eulerian and has at most |A|2+
2|E(A, B)| edges. Some edges to or from B might be multiple edges. Nevertheless, Lemma 4.1
covers this case and gives H(a, B) ≤ C(a, B) ≤ |A|2 + 2|E(A, B)|.
We now deﬁne W1 = {v ∈ W : deg(v) ≤ 2|W|} and W2 = W \ W1. It then follows that

W1 ∩ ∂W2 ⊆ W1 ∩ ∂(W2 (cid:116) Z).

(4.7)

Next we let

t1 := max{H(s, Z) : s ∈ W1 ∩ ∂(W2 (cid:116) Z)},
t2 := max{H(s, Z) : s ∈ W2}.

Let s ∈ W1 ∩ ∂(W2 (cid:116) Z). Then

H(s, Z) ≤ H(s, W2 (cid:116) Z) + t2.

Since s ∈ ∂(Z (cid:116) W2), using (4.6) we get

H(s, W2 (cid:116) Z) ≤ |W1|2 + 2 · (2|W| · |W1|) ≤ 5|W|2,

where the bound on |E(W1, W2 (cid:116) Z)| follows from the deﬁnition of W1. Putting the last two
inequalities together gives

t1 ≤ 5|W|2 + t2.

30

(4.8)

Let s∗

∈ W2 be such that, H(s∗, Z) = t2. Conditioning on the ﬁrst step of the walk yields

t2 ≤

1
2

+

1
2

(1 + αt2 + (1 − α)t1).

(4.9)

Indeed, when the walk is in W2 it exits to Z with probability at least half by construction. Con-
ditioning on the walk not exiting to Z, the parameter α ∈ [0, 1] is the probability that the walk
started at s∗ stays in W2 after one step. Under the same conditioning, with probability 1 − α, the
walk will move to some vertex in W1 ∩ ∂W2 which together with (4.7) explains the last term on the
right hand side.
Plugging (4.8) in (4.9), we get t2 ≤ 2 + 5|W|2, and hence t1 ≤ 2 + 10|W|2 ≤ 12|W|2. Therefore, we
showed that if s ∈ W ∩ ∂Z, then H(s, Z) ≤ 12|W|2 in all cases and this ﬁnishes the proof of the
lemma.

Proof of Theorem 1.8. If 2k > n, we bound E[Tk] by the expected cover time which is smaller
than n3 ≤ 8k3 by Claim 4.2. So we can assume that 2k ≤ n.
Let Φi be the length of phase i before having visited k distinct vertices. Then using Lemma 4.8
with W = Yi ∪ Bi \ {ri}, Z = {ri} ∪ Ui and s = si the starting vertex of phase i, we obtain

E[Φi] = E[E[Φi | Yi, si]] ≤ E(cid:2)3 × 12(2|Yi|)2(cid:3) = 144E(cid:2)

|Yi|2(cid:3) ,

where the factor 3 comes from the fact that ri is not necessarily a neighbour of si, but d(si, ri) ≤ 3.
We also have

Since the number of phases before discovering k vertices is not greater than 2k, we can now write

(4.10)

E[Φi1(Yi ≤ k)] ≤ 144k2.
∞(cid:88)

2k(cid:88)

i=0

2k(cid:88)

Tk =

Φi1(Yi ≤ k) =

Φi1(Yi ≤ k).

i=0

Taking expectations, and plugging (4.10) yields

E[Tk] =

E[Φi1(Yi ≤ k)] ≤ 2k × 144k2 = 288k3

and this concludes the proof.

i=0

Acknowledgements

The authors would like to thank Omer Angel, Robin Pemantle and Mohit Singh for useful dis-
cussions. The ﬁrst author is also grateful to Laurent Massouli´e for giving him the opportunity to
spend time at Microsoft Research in Redmond.

References

[1] David Aldous and James Fill. Reversible Markov Chains and Random Walks on Graphs. In

preparation, http://www.stat.berkeley.edu/∼aldous/RWG/book.html.

31

[2] Omer Angel, Yuval Peres, and David B. Wilson. Card shuﬄing and Diophantine approxima-

tion. Ann. Appl. Probab., 18(3):1215–1231, 2008.

[3] Greg Barnes and Uriel Feige. Short random walks on graphs. In in Proceedings of the Twenty-

Fifth Annual ACM Symposium on Theory of Computing, pages 728–737, 1993.

[4] Peter G. Doyle and Jean Steiner. Commute time geometry of ergodic Markov chains, 2011.

arXiv:1107.2612.

[5] E. Gin´e, G. R. Grimmett, and L. Saloﬀ-Coste. Lectures on probability theory and statistics,
volume 1665 of Lecture Notes in Mathematics. Springer-Verlag, Berlin, 1997. Lectures from
the 26th Summer School on Probability Theory held in Saint-Flour, August 19–September 4,
1996, Edited by P. Bernard.

[6] Sharad Goel, Ravi Montenegro, and Prasad Tetali. Mixing time bounds via the spectral proﬁle.

Electron. J. Probab., 11:no. 1, 1–26, 2006.

[7] Jeﬀ D. Kahn, Nathan Linial, Noam Nisan, and Michael E. Saks. On the cover time of random

walks on graphs. Journal of Theoretical Probability, 2(1):121–128, 1989.

[8] L. Kuipers and H. Niederreiter. Uniform distribution of sequences. Wiley-Interscience [John

Wiley & Sons], New York-London-Sydney, 1974. Pure and Applied Mathematics.

[9] Gregory F. Lawler and Vlada Limic. Random walk: a modern introduction, volume 123 of
Cambridge Studies in Advanced Mathematics. Cambridge University Press, Cambridge, 2010.

[10] David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov chains and mixing times.
American Mathematical Society, Providence, RI, 2009. With a chapter by James G. Propp
and David B. Wilson.

[11] R. Lyons and S. Oveis Gharan. Sharp Bounds on Random Walk Eigenvalues via Spectral

Embedding. ArXiv e-prints, November 2012.

[12] Ravi Montenegro and Prasad Tetali. Mathematical aspects of mixing times in Markov chains.

Found. Trends Theor. Comput. Sci., 1(3):x+121, 2006.

[13] Yuval Peres and Perla Sousi. Mixing times are hitting times of large sets. J. Theoret. Probab.,

28(2):488–519, 2015.

[14] V. V. Petrov. Sums of independent random variables. Springer-Verlag, New York-Heidelberg,
1975. Translated from the Russian by A. A. Brown, Ergebnisse der Mathematik und ihrer
Grenzgebiete, Band 82.

32

