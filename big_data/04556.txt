6
1
0
2

 
r
p
A
4

 

 
 
]
T
S
h
t
a
m

.

[
 
 

3
v
6
5
5
4
0

.

3
0
6
1
:
v
i
X
r
a

On Estimation in Tournaments and Graphs under

Monotonicity Constraints
Sabyasachi Chatterjee and Sumit Mukherjee

5734 S. University Avenue

Chicago, IL 60637

e-mail: sabyasachi@galton.uchicago.edu

1255 Amsterdam Avenue

New York, NY 10027

e-mail: sm3949@columbia.edu

Abstract: We consider the problem of estimating the probability matrix governing a tourna-
ment or linkage in graphs. We assume that the probability matrix satisﬁes natural monotonicity
constraints after being permuted in both rows and columns by the same latent permutation. The
minimax rates of estimation for this problem under a mean squared error loss turns out to be
O(1/n) upto logarithmic factors. This minimax rate is achieved by the overall least squares
estimate which is perhaps impractical to compute because of the need to optimize over the set
of all permutations. In this paper, we investigate in detail a simple two stage estimator which
is computationally tractable. We prove that the maximum squared error risk of our estimator
√
scales like O(1/
n) up to log factors. In addition, we prove an automatic adaptation property
of our estimator, meaning that the risk of our estimator scales like O(1/n) upto log factors
for several sub classes of our parameter space which are of natural interest. These sub classes
include probability matrices satisfying appropriate notions of smoothness, and subsume the
popular Bradley Terry Model in the tournament case and the β model and Stochastic Block
Models with monotonicity, in the graph case.

1. Introduction

In this paper we consider two statistical estimation problems. We begin by describing the two set
ups.

• Consider the situation of n teams playing in a league tournament where each team plays
every other team once. The results of the tournament can be written as a data matrix y of
zeroes and ones by setting yij = 1 for i < j if team i wins against team j, and 0 otherwise.
Let θij be the probability that team i wins against team j with θji = 1 − θij whenever i (cid:54)= j.
During ﬁnal stages of preparation of this manuscript, we came to know of an independent work in preparation on

a related topic by Sivaraman Balakrishnan, Nihar Shah and Martin Wainwright.

1

Set θii = 0 for all 1 ≤ i ≤ n as a matter of convention. The upper triangular part of the data
matrix y is modeled as

yij ∼ Bern(θij), ∀1 ≤ i ≤ j ≤ n

(1.1)

where yij in the upper triangular part is jointly independent and Bern(p) refers to the stan-
dard Bernoulli distibution with success probability p. The lower triangular part of the data
matrix is ﬁlled in an antisymmetric manner; that is

yij = 1 − yji, ∀1 ≤ j < i ≤ n.

(1.2)

We are interested in the problem of estimating the parameter matrix of probabilities θij un-
der an assumption commonly made in the ranking literature known as Strong Stochastic
Transitivity(SST) (see Shah et al. (2015) and reference therein). This assumption posits the
existence of an ordering among the teams which is unknown to the statistician. This order-
ing is then reﬂected on the probabilities θij as follows. Let team j have a higher rank (better)
than team k. Then for any team i, the probability of team i defeating team k would be no
less than the probability of team i defeating team j, which gives θij ≤ θik.
The estimation problem described above was formally introduced in Chatterjee (2015) and
the model described above was termed as the Nonparametric Bradley Terry Model. The
terminology is apt because it generalizes the very commonly used Bradley Terry model
ubiquitous in the ranking literature (see Bradley and Terry (1952)). In this paper, we also
refer to this model as the anti-symmetric model, following the terminology set in Chatterjee
(2015). More background and connections to previous works in the ranking literature are
described in subsection 1.2.

• Consider now the situation of observing a random graph on n nodes with no self loops. Let
θij now be the probability of node i and node j being linked. Again we set θii = 0 for all
1 ≤ i ≤ n as a matter of convention. The random graph can be now encoded as an adjacency
matrix y of zeroes and ones. Again, the upper triangular part of the adjacency matrix is
modelled as

yij ∼ Bern(θij) ∀ 1 ≤ i ≤ j ≤ n

(1.3)

where yij in the upper triangular part is jointly independent. The lower triangular part of the
data matrix is now ﬁlled in a symmetric manner; that is

yij = yji ∀ 1 ≤ j < i ≤ n.

(1.4)

Inspired by the SST assumption in the ranking literature, here we assume that that the vertices
can be arranged in an order (unknown to the statistician) of increasing tendency of getting
linked to other vertices. This assumption will again impose monotonicity constraints on the
edge probabilities θij. For example if node j is more "active" or "popular" than node k then
for any node i we must have θik ≤ θij. For an example where such an assumption seems

2

natural, consider a social network with n people labeled {1, 2,··· , n} where the ith person
has a popularity parameter pi ∈ [0, 1]. The chance that person i and person j are friends
is f (pi, pj), where f is increasing in both co-ordinates to signify that increasing popularity
leads to more friendship ties. The function f also needs to be symmetric, as the chance that
i and j are friends is symmetric in (i, j). Indeed, in this case there is (at least) one ordering
which sorts the nodes of the network in increasing order of popularity.
We pose and study the problem of estimating the edge probability matrix θ in the above set
up, and we refer to this model of random graphs as the symmetric model, differentiating it
from the antisymmetric (tournament) case. Under our model assumptions, the problem of es-
timating the edge probabilities is very closely related to the problem of estimating graphons
in the spirit of Gao et al. (2015) where we assume monotonicity (without smoothness) of
the graphon in both variables, instead of smoothness assumptions made in Gao et al. (2015).
Detailed descriptions of connections to previous works in graphon estimation are given in
subsection 1.3.

In this paper we look at the two estimation problems in the antisymmetric and the symmetric
model in a uniﬁed way. In particular, the purpose of this paper is to introduce and study the risk
properties of a very natural two step estimator which is described in subsection 1.4. The two step
estimator has the same form in both the models and the technique of analyzing the risk properties
of the estimator in both the models is the same.

1.1. Formal Setup of our problem

In this subsection we deﬁne two parameter spaces; one for the antisymmetric model and one for the
symmetric model. Let us introduce some notation ﬁrst. Denote Sn to be the set of all permutations
on n symbols. For any n × n matrix θ and any permutation π ∈ Sn we deﬁne θ ◦ π to be the n × n
matrix such that (θ ◦ π)ij = θπ(i),π(j). Let Π be the n × n permutation matrix corresponding to the
permutation π ∈ Sn. Then we can also write θ ◦ π = ΠT θ Π.

Deﬁne the space of matrices

θji = 1 − θij ∀ i (cid:54)= j;

θii = 0 ∀i}.

T = {θ ∈ Rn×n : θij ≤ θik ∀i < k < j;

(1.5)
Any matrix in T when only looked at the upper triangular part above the diagonal is non increasing
in any row (as j grows) and non decreasing in any column (as i grows). The lower triangular part
is just 1 minus the upper triangular part and the diagonals are zero. Basically T is the space of
matrices which satisfy the SST assumption with known ranking where the ranking is such that
player n is best, followed by player n−1 and so on. Then our parameter space for the antisymmetric
model can be written as

ΘT = {θ ◦ π : θ ∈ T , π ∈ Sn}.

(1.6)

3

Similarly, deﬁne the space of matrices

θji = θij ∀i (cid:54)= j;

G = {θ ∈ Rn×n : θij ≤ θik ∀i < j < k;

(1.7)
Any matrix in G when only looked at the upper triangular part above the diagonal is non decreasing
in both rows and columns. The lower triangular part is symmetrically ﬁlled, and the diagonals are
zero. Again, G is the space of adjacency matrices which are consistent with the monotonicity
restrictions imposed by the ordering where node n is most popular followed by node n − 1 and so
on. Then our parameter space for the symmetric model can be written as

θii = 0 ∀i}.

ΘG = {θ ◦ π : θ ∈ G, π ∈ Sn}.

(1.8)

For Θ = ΘT or ΘG we study the problem of estimating the underlying matrix of probabilities θ.
The loss function we consider is the mean Frobenius squared metric deﬁned for any two matrices

θ and(cid:101)θ as follows:

R(θ,(cid:101)θ) =

n(cid:88)

n(cid:88)

i=1

j=1

1
n2

(θij −(cid:101)θij)2.

(1.9)

1.2. Literature survey on estimation in Tournaments

The classical Bradley-Terry model, introduced in Bradley and Terry (1952) is subsumed in our
tournament model. In this set up there are n players, with the ith player having skill level repre-
sented by a real number wi. If player i plays against player j, the chance that player i wins is

ewi

ewi + ewj

=

1

1 + ewj−wi

.

ewi +ewj for i (cid:54)= j and θii = 0 by convention, it follows that θ ∈ T . A similar
Thus setting θij = ewi
model was proposed by Thurstone (1927) which takes the form θij = Φ(wi − wj) for i (cid:54)= j, where
Φ is the standard normal distribution function. Again in this case we have θij + θji = 1, where we
use the identity Φ(x) + Φ(−x) = 1. More generally, for any symmetric distribution function F
on R (i.e. F (x) + F (−x) = 1), setting θij = F (wi − wj) for i (cid:54)= j we have θ ∈ T . In particular
1+e−x we get back the Bradley-Terry model, and if F = Φ we get back
for the choice F (x) = 1
Thurstone’s model.

We focus our review on two recent papers which are directly related to our setting. The ﬁrst is
the work of Chatterjee (Chatterjee (2015)) which gives a Universal Singular Value Thresholding
algorithm (USVT) for estimating an underlying mean matrix from its noisy observation. In his
paper, Chatterjee proposes the assumption θ ∈ ΘT as a generalization of the Bradley Terry model.
As an application of his results, in (Chatterjee, 2015, Theorem 2.11) Chatterjee shows that for
θ ∈ T one has

E(cid:107)(cid:101)θU SV T − θ(cid:107)2

1
n2

(cid:16) 1

(cid:17)

.

2 = O

n1/4

4

More recently Shah et al. (2015) have studied the estimation problem in the tournament model

and among other things, have improved the risk bound of the USVT estimator to achieve

E(cid:107)(cid:101)θU SV T − θ(cid:107)2

1
n2

(cid:16) 1√

(cid:17)

.

2 = O

n

Shah et al. (2015) also showed a minimax lower bound of O(1/n) in this estimation problem
and have showed that the brute force least squares estimator (LSE), which involves optimizing over
the space of all permutations, achieves their lower bound upto log factors.

They also show that if the true parameter θ is of the generalized Bradley-Terry form for some
known distribution function F which is strongly log concave and smooth, and the skill parameters

(w1,··· , wn) are bounded away from ±∞, then using maximum likelihood estimators (cid:98)wi for wi

(which needs time polynomial in n to compute by a convex program) they are able to show

E(cid:107)(cid:101)θM LE − θ(cid:107)2

2 = O

1
n2

(cid:16) 1

(cid:17)

.

n

Infact they show that the MLE is a minimax rate optimal estimator for this model upto constant
factors. We note here that computing the maximum likelihood estimate of (w1,··· , wn) requires
the knowledge of the distribution function F .

In Shah et al. (2015) the authors also provide another subclass of ΘT where they show a mini-
max rate optimal estimator which is computable in polynomial time. They show that if the compo-
nents of θ are all bounded away from 1/2 by a constant amount, then an algorithm due to Braver-

man and Mossel (2008) (which needs time polynomial in n) gives a very good estimate(cid:98)π for the
unknown π. Using(cid:98)π to permute the rows and columns of the data matrix y as y ◦(cid:98)π−1, they project
the permuted data onto the space T . This two step estimator(cid:101)θ has the minimax rate optimal worst

case error bound

1
n2

E(cid:107)(cid:101)θ − θ(cid:107)2

2 = O

(cid:16) 1

(cid:17)

n

for the subclass when the components of θ are all bounded away from 1/2 by a constant amount.
However, we note that implementing the algorithm due to Braverman and Mossel (2008) is not
practical as the degree of the polynomial governing its runtime is prohibitively high.

To summarize, it is not known whether there exists a computationally feasible estimator in this
problem achieveing the minimax risk O(1/n). Hence it becomes important to investigate risk prop-
erties of estimators which are practically computable. The USVT algorithm is one such practical
√
n) mean squared error (MSE). In this paper, we propose a differ-
algortithm achieving a O(1/
√
ent estimator, achieving provably the same O(1/
n) MSE as the USVT while having automatic
adaptation properties. By automatic adaptation properties, we mean that the MSE of our estimator
√
n) for various subclasses of ΘT which are of contem-
provably achieves better rates than O(1/
porary interest. See Subsection 1.5 for details and precise results.

5

1.3. Literature survey on estimation in graphs

Statistical estimation of the expected adjacency matrix in a graph is a vast ﬁeld with various sub
ﬁelds. We only focus on works which are most relevant to this paper. One can think of the ex-
pected adjacency matrix converging to a symmetric measurable function W : [0, 1]2 (cid:55)→ [0, 1], as
n → ∞. This limiting bivariate function is called a graphon and the study of graphons for un-
derstanding networks has received a lot of recent attention. Given a graphon W and a uniformly
random permutaion π from Sn one can deﬁne a random adjacency matrix y with vertex set [n],
such that

(cid:17)

.

π(j)

n

(cid:16)π(i)

,

n

P(yij = 1) = W

Note that this is exactly the symmetric model deﬁned in (1.3). For more details on deﬁnitions
of graphons, their properties, and applications in Statistics refer to Airoldi et al. (2013), Chan
and Airoldi (2014), Olhede and Wolfe (2014) Wolfe and Olhede (2013), Diaconis and Janson
(2007), Lovász (2012), Bickel and Chen (2009) and references there in. We survey some of the
works which are more related to the current paper.

In Chatterjee et al. (2011) the authors consider the β model which is an exponential family on
the space of all graphs with degrees as sufﬁcient statistics. The β model and its modiﬁcations are
used for modeling networks in the Social Science literature (see Blitzstein and Diaconis (2011);
Jackson et al. (2008); Newman (2003); Robins et al. (2007) and references there in). It turns out
that under the β model the edges are all independent, with P(yij = 1) = eβi
eβi+βj where β :=
(β1,··· , βn) are n parameters. In Chatterjee et al. (2011) authors point out that the β model can
be thought of as a symmetric version of the Bradley-Terry model. Under the assumption that the

components of the vector β are uniformly bounded, they show that the vector of MLEs (cid:98)β are
consistent, and satisfy maxi∈[n] |(cid:98)βi − βi| = Op(
The more recent works in Chan and Airoldi (2014) and Gao et al. (2015) speciﬁcally focus on
tion g(x) =(cid:82) 1
estimation of graphons by assuming block structure/smoothness assumptions on the underlying
graphon W . In Chan and Airoldi (2014) the authors assume that W is Lipschitz, the integral func-
0 W (x, y)dy has a derivative which is bounded away from 0 and ∞, and averaging
the function W gives a function H W with a sparse derivative. Similar to our set up, they don’t
assume that the labels of the vertices are observed in the correct order. To account for this, they
graph has degrees in ascending order. In the second step, they compute block averages (cid:98)H of the
propose a two step estimator which starts by permuting the labels so that the permuted labeled

(cid:113) log n

n ).

permuted graph with a speciﬁed bandwidth, and then ﬁnd a L2 approximation of this block average
matrix with a sparse derivative, which they output as an estimate. This algorithm, named as Sort
and Smooth Algorithm (SAS), is shown to have an expected MSE O(n−1) upto log factors under
the aforementioned assumptions.

6

Discretizing the domain and now with the interpretation that the graphon is a n × n matrix of
evaluations of W, Gao et al. (2015) consider the minimax error rate of estimation, when the true
graphon W is either a block function with k × k blocks, or is a Holder continuous function of
smoothness coefﬁcient α > 0 (see Gao et al. (2015) for the deﬁnition of α Holder continuity). For
the k × k stochastic block model, they show that the minimax error is O
, whereas for
if α ≥ 1. In
the α Holder continuity case the minimax error is O(n
both cases, they show that a block estimator with a suitable partition achieves the minimax error
rate. However, their study is limited to sharply characterizing the minimax risks in this estimation
problem and they do not study any numerically efﬁcient estimator.

α+1 ) if α < 1, and O

(cid:16) log n

(cid:16) k2

n2 + log k

n

(cid:17)
(cid:17)

− 2α

n

The estimation problem in our symmetric model (1.3) can be thought of as a graphon estimation
problem exactly in the sense of Gao et al. (2015). While they impose smoothness conditions on the
matrix θ we are interested in the case when θ is a permuted version of a monotone matrix, not nec-
essarily smooth. In this sense, our estimation problem is analogous to a non parametric regression
problem where the mean function is non decreasing in both coordinates. What makes the problem
more delicate is that this regression function has now been permuted by a latent permutation π.

The ﬁeld of Shape Constrained Inference has studied estimation of signals under monotonicity
constraints in detail. Estimation of vectors with monotonicity constraints has been widely studied
in the literature (see Chatterjee et al. (2015b); Zhang (2002) and references there-in. In particular,
it follows from Zhang (2002) that given a set of n independent data points with increasing mean,
the expected risk of the least squared estimate (LSE) in this problem in the Gaussian setting is
O(n−2/3), which is also the minimax optimal rate.

Very closely related is the recent work Chatterjee et al. (2015a)) who consider the matrix esti-
mation under monotonicity constraints in the Gaussian setting. Letting {yij, i, j ∈ [n]} be n × n
mutually independent normal variables with mean θij, and θlse denote the constrained least square
estimate of θ, they show that for any θ which is non decreasing in both rows and columns one has

Eθ(cid:107)θlse − θ(cid:107)2

2 = O

1
n2

(cid:16)log4 n

(cid:17)

.

n

They also show that O(n−1) is the minimax error in this case, and so the LSE is minimax rate
optimal upto log factors. If the true isotonic function happens to be a constant function (or more
generally a rectangular piecewise constant function), they show that the LSE automatically adapts
to this setting by giving an improved error rate of O(n−2 log8 n). One can think of their problem
as our estimation problem in a graph with known ordering of the nodes.

To summarize, we have posed an estimation problem in graphs which can be thought of as
the problem of estimating a graphon which is non decreasing in either coordinate. This problem,

7

although being related to the above works in the literature, seem to have not been studied in the
literature so far. We unify this problem with the problem of estimation in tournaments and state
our main results in subsection 1.5.

1.4. The estimator

Our estimator consists of two steps. The idea is to ﬁrst get an estimated ranking and then use this
ranking to estimate the parameter matrix. We deﬁne our estimator below:

j=1 yij be the ith row sum of the data matrix y. In the ﬁrst step we sort the
vertices according to the row sums (r1, . . . , rn) of the data matrix y and obtain a permutation

Let ri = (cid:80)n
(cid:98)σ such that r((cid:98)σ(1)) ≤ ··· ≤ r((cid:98)σ(n)), which is essentially an estimator of σ = π−1. We call
this step the sorting step, which we use to construct a sorted data matrix y ◦(cid:98)σ deﬁned as
(y ◦(cid:98)σ)ij = (y(cid:98)σ(i), y(cid:98)σ(j)).
In the second step we project the sorted data matrix y◦(cid:98)σ onto the relevant parameter space. In
We now unsort the projected and sorted data matrix P (y ◦(cid:98)σ) by applying the permutation
(cid:98)σ−1 and obtain our estimator. Thus our ﬁnal two step estimator(cid:98)θ for the parameter matrix θ

the antisymmetric model, we project onto the set T and in the symmetric model, we project
onto the set G. Both T and G are closed convex sets of matrices and hence there exists a
unique projection onto them. Let the projection operator be denoted by P in both cases.

(a) Sorting

(b) Projection

is

(cid:98)θ =(cid:0)P (y ◦(cid:98)σ)(cid:1) ◦(cid:98)σ−1.

(1.10)

Note that in the antisymmetric (tournament) model, the row sums of the data matrix y corre-
spond to the number of wins or victories for each player, and the column sums of the data matrix
correspond to the number of defeats for each player. Hence our sorting step just sorts the teams
according to the number of victories (or equivalently the number of defeats, as sum of victory and
defeat of each player is n − 1). Similarly, in the symmetric (graph) model, the row sums of the
adjacency matrix y correspond to the empirical degrees of the nodes in the graph. Therefore, our
sorting step sorts the vertices according to the empirical degrees. Our sorting step is thus perhaps
the simplest way to get an idea about the underlying latent ranking.

1.4.1. Computation of the estimator

The ﬁrst step of our algorithm just needs computation of the row sums and then sorting, which
combined clearly will only take at most O(n2) operations. The projection step is thus going to
be dominating the computation time. Fortunately there are very efﬁcient ways of computing the
projection. First of all, the projection by deﬁnition has to be zero on the diagonals and would be

8

anti symmetric/symmetric according to our model. Hence in either of the models, it sufﬁces to
compute the projection for the upper diagonal part. It turns out that the spaces T and G, without
the constraint that all elements have to live in [0, 1], can be viewed as the space of isotonic functions
on a Directed Acyclic Graph (DAG) on the domain {(i, j) : 1 ≤ i < j ≤ n}. Isotonic functions on
a DAG mean that the function can only increase on following a directed path in the DAG. It can be
checked that the problem of computing the projection P can be done by ﬁrst performing isotonic
regression on the appropriate DAG and then hard threshholding the values to lie between 0 and 1.
Below we illustrate the DAG corresponding to T and G for n = 4.

(a) DAG of a tournament

(b) DAG of a graphon

The results in Kyng et al. (2015) deal with the problem of computing the projection onto the
space of isotonic functions on an arbitrary DAG. They basically prove that the computational cost
of the problem is O(m3/2) upto log factors where m is the number of edges in the DAG. They
also claim that in practice, one can compute an approximate projection in O(m) steps. The number
of edges in the DAG corresponding to both T and G increase quadratically with n and hence the
results in Kyng et al. (2015) imply provably O(n3) runtime and linear (O(n2)) runtime in practice
for the projection step. Thus, the computation time of our estimator compares favourably with any
other competing algorithms such as some form of Singular Value Threshholding.

1.5. Main results

Having deﬁned our estimator, we are now ready to state our main results.
Deﬁnition 1.1. Henceforth we will use the notation Θ in place of ΘT or ΘG. The implication is
that all results hold with Θ replaced by either of the two parameter sets.

9

4,14,24,33,13,22,14,14,24,33,13,22,1Theorem 1.1. For any positive integer n > 1 with Θ ⊂ Rn×n we have

(cid:114)

E(cid:107)(cid:98)θ − θ(cid:107)2 ≤ C

1
n2

sup
θ∈Θ

log n

n

,

(1.11)

where C is some universal constant.
Remark 1.1. We now remark that the brute force LSE in our problem achieves a MSE of O(1/n)
upto log factors which is minimax rate optimal. This fact has been shown by Shah et al. (2015)
when Θ = ΘT . The same fact can be shown to be true when Θ = ΘG by employing, among other
techniques, a similar technique of proof as in Lemma 2.1 alongwith an application of Lemma 3.1
in Chatterjee and Lafferty (2015). We do not carry this out in this manuscript.

√
Comparing Theorem 1.1 to the minimax rate, the 1/
n rate is clearly worse than the minimax
rate of estimation. This raises the important question of whether there exists a computationally
feasible estimator achieving the minimax rate in this problem. This question deserves further study
and is beyond the scope of the current manuscript. Another natural question that arises is whether
the upper bound in Theorem 1.1 is tight. See Subsection 1.7 for further discussions. Note that the
rates of estimation of our estimator matches the rate of estimation of the USVT estimator as shown
in Shah et al. (2015) for Θ = ΘT .

Our next theorem reveals automatic adaptation properties of our estimator. Even though our
n) rate of estimation globally, it does achieve the O(1/n) rate

√
estimator provably achieves O(1/
of estimation for θ ∈ Θ satisfying an appropriate notion of smoothness.

Deﬁnition 1.2. For any θ ∈ Θ let ri(θ) :=(cid:80)n

k=1 θi,k denote the row sums of the matrix θ, and set

(cid:80)n
k(cid:54)=i,j[θi,k − θj,k]2
(ri − rj)2

S(θ) := n max

i,j∈[n]:ri(cid:54)=rj

.

(1.12)

Deﬁne S(θ) = 0 if ri = rj for all i (cid:54)= j. S(θ) can be thought of as an average smoothness
coefﬁcient of θ. If S(θ) is small, we can expect to have an improved bound on the MSE.
Remark 1.2. Note that for any given θ ∈ Θ and any π ∈ Sn we have S(θ) = S(θ ◦ π). In other
words, S(θ) is invariant under permutations.
Theorem 1.2. There exists a universal constant C such that for any θ ∈ Θ we have

E(cid:107)(cid:98)θ − θ(cid:107)2 ≤ C

1
n2

(cid:16)S(θ)

n

(cid:17)

+

(log n)2

n

.

(1.13)

As applications of Theorem 1.2, we show that an expected MSE of O(1/n) upto log factors is
attained for our estimator in the following sub-parameter spaces of T and G respectively which are
of potential interest:

10

1.5.1. Tournaments

(a) Additive Model

Let θi,j = f (i/n) − f (j/n) + 1
In this case ri = nf (i/n) − Sn + (n−1)

2, where f : [0, 1] (cid:55)→ [0, 1/2] is an isotonic function.

j=1 f (j/n). This gives

, where Sn :=(cid:80)n
(cid:12)(cid:12)(cid:12) f (i/n) − f (j/n)

nf (i/n) − nf (j/n)

(cid:12)(cid:12)(cid:12) =

1
n

,

(cid:12)(cid:12)(cid:12)θik − θjk

ri − rj

2

(cid:12)(cid:12)(cid:12) =

n . Thus an application of Theorem 1.2 gives E(cid:107)(cid:98)θ − θ(cid:107)2 ≤ O( (log n)2

and so S(θ) = n−2
all θ which are of the additive form upto a permutation.

n

) for

(b) Strictly Increasing Lipschitz Functions Assume θi,j is any Lipschitz matrix (see deﬁni-

tion (1.3)), i.e. there exists L1 < ∞ such that

(1.14)
Also assume that the row sums {ri}i∈[n] are lower Lipschitz, i.e. there exists L2 > 0 such
that

|θi,k − θj,k| ≤ L1
n

|i − j|.

(1.15)

In this case we have

|ri − rj| ≥ L2

(cid:12)(cid:12)(cid:12) θi,k − θj,k

ri − rj

(cid:12)(cid:12)(cid:12) ≤

n − 2
n

|i − j|.

L1

(n − 2)L2

,

and so S(θ) ≤ nL2
(n−2)L2
for all θ satisfying (1.14) and (1.15) upto a permutation.

1

2

. Thus an application of Theorem 1.2 gives E(cid:107)(cid:98)θ − θ(cid:107)2 = O( (log n)2

)

n

(c) Generalized Bradley-Terry model

Suppose θi,j = F (wi − wj) where F is a symmetric distribution function (i.e. F (x) +
F (−x) = 1) with a strictly positive continuous density function f, and (w1,··· , wn) are
parameters. Note that the usual Bradley Terry and the Thurstone models do satisfy this con-
dition. Also assume that the weights are bounded, supi∈[n] |wi| ≤ M for some M < ∞. In
this case we have
L2|wi−wj| ≤ |θi,k−θj,k| ≤ L1|wi−wj|, L1 := sup
|x|≤2M

f (x) < ∞, L2 := inf
|x|≤2M

f (x) > 0.

Also we have |ri − rj| ≥ (n − 2)L2|wi − wj| and thus it follows that

(cid:12)(cid:12)(cid:12) θi,k − θj,k

ri − rj

(cid:12)(cid:12)(cid:12) ≤

L1

(n − 2)L2

.

11

Thus it follows from calculations similar to the previous example that E(cid:107)(cid:98)θ−θ(cid:107)2 = O( (log n)2
timate(cid:98)θM LE for which E(cid:107)(cid:98)θM LE − θ(cid:107)2 = O( 1

).
Recall the results of Shah et al. (2015), who show that if the complete knowledge of the
function F is available, then by computing the mle of (w1,··· , wn) they can produce an es-
n) upto log factors, under the strong assumption
that F is smooth and strongly log concave. Under a weaker assumption on F (existence of
a continuous density) our algorithm obtains the same error bound of O( 1
n) upto log factors
without the knowledge of F , and happens to be computationally tractable.

n

(d) Block Tournament Matrices

Let θ be a block function upto a permutation with L×L blocks with equal sizes with L ﬁxed.
Denoting the underlying values of each block by a k × k matrix B we have

L(cid:88)

ri =

n
L

B(cid:100)Li/n(cid:101),s,

which gives(cid:12)(cid:12)(cid:12) θi,k − θj,k

ri − rj

(cid:12)(cid:12)(cid:12) =

L
n

s=1

(cid:12)(cid:12)(cid:12) B((cid:100)Li/n(cid:101),(cid:100)Lk/n(cid:101)) − B((cid:100)Lj/n(cid:101),(cid:100)Lk/n(cid:101))
(cid:80)L
s=1 B((cid:100)Li/n(cid:101), s) − B((cid:100)Lj/n(cid:101), s)

(cid:12)(cid:12)(cid:12) ≤ L

n

.

Thus we have S(θ) ≤ L2 = O(1). An application of Theorem 1.2 then shows that the the
MSE is O( (log n)2

).

n

1.5.2. Graphs

(a) Additive model

we have ri = (n − 2)f (i/n) + Sn with Sn :=(cid:80)n

Let θi,j = f (i/n) + f (j/n), where f : [0, 1] (cid:55)→ [0, 1/2] is any isotonic function. In this case

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12) =

j=1 f (j/n). This gives

(cid:12)(cid:12)(cid:12)θik − θjk
n−2. Thus an application of Theorem 1.2 gives E(cid:107)(cid:98)θ − θ(cid:107)2 = O( (log n)2

nf (i/n) + Sn − nf (j/n) − Sn

f (i/n) − f (j/n)

(cid:12)(cid:12)(cid:12) =

1

n − 2

,

ri − rj

and so S(θ) = n
θ which are of the additive form upto a permutation.

) for all

n

In particular this bound holds when f : [0, 1] (cid:55)→ [0, 1/2] is any α Hölder continuous monotone
function for some α > 0, which for the particular choice f (x) = xα gives a power law degree
α+1 ) for the
distribution. We note that the work of Zhou et. al gives an upper bound of O(n
expected MSE of Least squares for general α Hölder functions for α ∈ (0, 1). Under the extra
assumptions that the underlying true function is the sum of α Hölder continuous monotone
functions, the computationally feasible algorithm of this paper adapts to give a better bound to
the MSE.

− 2α

12

Let θi,j = f (i/n)f (j/n), where f : [0, 1] (cid:55)→ [0, 1] is any isotonic function. In this case

(b) Product model

ri = f (i/n)Sn − f (i/n)2 with Sn :=(cid:80)n
(cid:12)(cid:12)(cid:12) =
n(cid:88)

(cid:12)(cid:12)(cid:12)θik − θjk

ri − rj

and so

n

S(θ) ≤

(Sn − 1)2 max
i,j∈[n]

and so again we have E(cid:107)(cid:98)θ − θ(cid:107)2 = O( (log n)2

k=1

n

f (k/n)2 =

j=1 f (j/n). This gives

f (k/n)

Sn − f (i/n) + f (j/n)

≤ f (k/n)
Sn − 1

,

(cid:80)n
(cid:16) Sn−1

(cid:17)2

1
n

k=1 f (k/n)2

n→∞→

(cid:82) 1
(cid:16)(cid:82) 1

(cid:17)2 ,

0 f 2(t)dt
0 f (t)dt

n

).

In this case for any function f : [0, 1] (cid:55)→ [0, 1] which is α Hölder continuous the function
f (x)f (y) is α Hölder continuous as well. Thus, similar to the additive model, adaptation holds
when the true function is product of two α Hölder continuous monotone functions.

(c) Strictly Increasing Lipschitz functions

same calculations apply, and so we have E(cid:107)(cid:98)θ − θ(cid:107)2 = O( (log n)2

With the same Lipschitz assumptions (1.14) and (1.15) on θ as in the tournament case, the
). Apart from monotonicity,
this is a subset of the conditions used in Airoldi-Chan, as we do not assume smoothness or
sparsity of the gradient of the block averages of θ. In this case, both ours and Airoldi-Chan’s
algorithm obtain the same error bound of O(1/n) upto log factors. Our algorithm is computa-
tionally more tractable though than the SAS algorithm proposed in Airoldi-Chan.

n

(d) Monotone Stochastic Block model

we have E(cid:107)(cid:98)θ − θ(cid:107)2 = O( (log n)2

Consider the class of monotone block matrices with L×L blocks with equal sizes with number
of blocks L ﬁxed. This is a subset of the popular Stochastic Block Model widely studied in the
networks literature.
Again the same calculations as for block matrices in the tournament model applies, and so
). The same MSE upto log factors is attained by the Least
squares estimator for the general stochastic block model without monotonicity restrictions or
block functions with arbitrary number of blocks for a general (i.e. not necessarily monotonic)
block function (see Gao et al. (2015) et. al). Thus in this case we cannot prove adaptation of
our algorithm, but at least it gives a computationally simpler alternative .

n

(e) Generalized β model

eβi+βj

Consider the β model with parameter vector β := (β1,··· , βn) ∈ Rn, which deﬁnes θi,j =
1+eβi+βj for i (cid:54)= j.
We propose the following generalized β model θi,j = F (βi + βj), where F is a distribution
function on R. In particular the choice F (x) = ex
1+ex (which is the logistic distribution function)

13

gives back the usual β model. We can bound the expected MSE for the generalized β model,
under the assumption that F (x) has a continuous strictly positive density f (x), as follows:
Assume that the weight vector β is bounded, i.e. supi∈[n] |βi| ≤ M for some M < ∞. In this
case we have
L2|βi − βj| ≤ |θi,k − θj,k| ≤ L1|βi − βj|, L1 := sup
|x|≤2M

f (x) < ∞, L2 := inf
|x|≤2M

f (x) > 0.

Also we have |ri − rj| ≥ nL2|βi − βj| and thus it follows that

(cid:12)(cid:12)(cid:12)θi,k − θj,k

ri − rj

(cid:12)(cid:12)(cid:12) ≤

L1

(n − 2)L2

.

Thus it follows as before that E(cid:107)(cid:98)θ − θ(cid:107)2 = O( (log n)2
et al. (2011) the authors show that the MLE(cid:98)β satisﬁes maxi∈[n] |(cid:98)βi − βi| = Op(

). As pointed out before, in Chatterjee
n ) under

(cid:113) log n

n

the assumption that β is bounded. Using this estimate gives

n(cid:88)

(cid:104)

i,j=1

1
n2

− e(cid:98)βi+(cid:98)βj
1 + e(cid:98)βi+(cid:98)βj

eβi+βj

1 + eβi+βj

(cid:17)

.

(cid:105)2
(cid:16) (log n)2

(cid:16)log n
(cid:17)

= Op

n

As shown above, Theorem 1.2 gives a similar bound Op
of log n, but works for all distributions F and does not require the knowledge of F .

which is worse by a factor

n

n

1.5.3. Lipschitz functions
In the previous subsection we demonstrated that if θ ∈ Θ, upto a permutation, is a Lipschitz
function such that the row sums {ri}n
i=1 are strictly increasing as in (1.15), then our estimator has
an MSE which is O( (log n)2
). Recall that a similar MSE of O(1/n), upto log factors, is obtained
by Airoldi-Chan for their estimator in the symmetric model under a similar assumption of strictly
increasing row sums. A natural question that now arises is what is the MSE of our estimator if
we assume the parameter matrix θ is Lipschitz but the row sums need not satisfy (1.15). Our next
result shows that if θ ∈ Θ is Lipschitz, then under no further restrictions we provably have an MSE
of O(n−3/4(log n)3/2). We ﬁrst make a deﬁnition.
Deﬁnition 1.3. Call a matrix θ ∈ Θ a Lipschitz matrix with Lipschitz constant L > 0 if the
following holds for all i (cid:54)= j, k (cid:54)= l

(1.16)
Theorem 1.3. Suppose θ ∈ Θ is a Lipschitz matrix with Lipschitz constant L. Then for a universal
constant C we have

|θij − θkl| ≤ L
n

(|i − j| + |k − l|).

n2(cid:107)(cid:98)θ − θ(cid:107)2 ≤ C max(1,

E 1

√
L)

(log n)3/2

.

n3/4

We prove the above theorem in Section 3.

14

1.5.4. Minimax Lower bound
Since our results imply a O(1/n) scaling of the MSE upto log factors for matrices θ ∈ Θ with
S(θ) = O(1), a natural question is whether the minimax lower bound for the class of matrices
with bounded S(θ) is also O(1/n). We show that this is indeed true in our next theorem.
Theorem 1.4. For all sufﬁciently large n there exists a constant c > 0 such that

sup

i=1

j=1

n(cid:88)
n(cid:88)
where the inﬁmum is taken over all estimators(cid:101)θ.

θ∈Θ:S(θ)≤2

inf(cid:101)θ

1
n2

E((cid:101)θij − θij)2 ≥ c

n

We prove this Theorem in Section 4.

Remark 1.3. Considering the symmetric model, it was shown in Chatterjee et al. (2015a) that if
the true underlying θ is piecewise constant with rectangular pieces, then the MSE of the LSE can
achieve parametric rates of convergence. In particular, consider a 2 × 2 block matrix θ. Then the
LSE at θ as shown in Chatterjee et al. (2015a) achieves a MSE scaling like O(1/n2). Note that
there is no latent permutation considered in Chatterjee et al. (2015a). Our problem, because of the
latent permutation, actually is harder. This is because for the parameter space of all 2 × 2 block
matrices upto permutations, one can actually show a minimax lower bound scaling like O(1/n).
This can be proved by an application of Fano’s lemma which we do not carry out here.

1.6. Main Contribution

In this paper we study a natural two step estimator for estimating the matrix of probabilities for
two very related models, the tournament model and the graph model under the assumption that the
underlying true ordering is unknown. Our estimator achieves the same worst case risk bound as
the USVT estimator shown in Shah et al. (2015) and is computationally feasible. We also demon-
strate adaptive properties of our estimate for interesting sub-classes of the parameter space, which
includes the stochastic block model, and the parametric Bradley Terry model and certain smooth-
ness classes as special cases. We also give an improved bound on MSE for Lipschitz functions,
without the assumption that the row sums are strictly increasing. We also give a minimax lower
bound for the sub-parameter space which demonstrate adaptation, thus proving that our estimator
is minimax rate optimal for this sub-parameter space. Hence, we make a case that this estimator,
which is computationally feasible, compares favourably with the existing USVT estimator in terms
of statistical risk.

1.7. Future Work

The major open question in this problem is whether there exists a polynomial time estimator
achieving the minimax rate of estimation of O(1/n). In light of our results presented in Section 1.5

15

there are two other natural questions. The ﬁrst question is whether the upper bound for our estima-
tor in Theorem 1.1 is tight. We do not know the answer to this question yet. A natural intuition is
that the matrix θ deﬁned as

θi,j =

1
2

I{i + j > n}

(1.17)

is a hard case for our estimator in the symmetric model. This intuition is born out of the fact that
the quantity S(θ) is not of constant order and grows with n for this particular θ. The following
simulation suggests that our exponent of n in the upper bound in Theorem 1.1 may be tight.

FIGURE 1. We have simulated our data when the true underlying n × n parameter matrix is given by (1.17). We have
simulated 100 times for each sample size n ranging from 50 to 130 in increments of 10. For each sample size we have
taken the average of the MSE obtained in the 100 runs as our estimated expected mean squared error. We have then
ﬁtted the least squares line to the log of these estimates versus the log sample size. The slope of the ﬁtted line should
give us an idea about the true exponent. In our simulations, the slope comes out to be −0.56 which is close to the
predicted exponent −0.5. We have used the package ISO in R to do our simulation.

The second question is whether the supremum risk of our estimator over the class of all Lips-
chitz matrices satisfying (1.16) without the strictly increasing row sum condition (1.15) is O(1/n)
or not. Here, we have been able to prove that an upper bound to the MSE is O(n−3/4) but we have
not been able to come up with a Lipschitz matrix with MSE growing larger than O(1/n) even
empirically.

16

2. Risk Analysis

The goal of this section is to prove risk bounds for our two step estimator. Recall the observed data
matrix y is distributed as Bern(θ∗ ◦ π) where θ∗ is the true underlying matrix, belonging to either
T or G and π is the true underlying latent permutation.

Without loss of generality we will assume that the underlying latent permutation π is the identity
permutation and therefore y = Bern(θ∗). We now introduce some nomenclature. For any matrix
θ ∈ Rn×n and y = Bern(θ) deﬁne the random variable

Rrow(θ) =

where(cid:98)σ is a permutation sorting the row sums (r1, . . . , rn) of the data matrix y, that is r((cid:98)σ1) ≤
··· ≤ r((cid:98)σn). This quantity Rrow(θ) is a measure of how jumbled θ gets in a Frobenius norm
squared sense , when applied the sorting permutation(cid:98)σ. This plays an important role in our analy-

sis.
Lemma 2.1. The following inequality holds with probability not less than 1 − exp(−n/8),

(2.1)

n2(cid:107)θ ◦(cid:98)σ − θ(cid:107)2

1

n2(cid:107)(cid:98)θ − θ∗(cid:107)2 ≤ max{C

1

(log n)2

n

, Rrow(θ∗)} + 2Rrow(θ∗)

where C is an appropriately large universal constant. Consequently we have

n2(cid:107)(cid:98)θ − θ∗(cid:107)2 ≤ 3ERrow(θ∗) + C

E 1

(log n)2

n

.

The above lemma upper bounds the risk of our estimator by a sum of two terms. The (log n)2/n
term is essentially the minimax rate of our problem (upto logarithmic factors) which is attained by
the global least squares estimate. The term Rrow(θ∗) is the excess risk of our estimator as compared
to the risk of the global least squares estimate, which of course, is computationally prohibitive.

The method of proof of Lemma 2.1 uses tools from Empirical Process Theory and the general
theory of Least Squares Estimation proposed in Chatterjee (2014). The techniques closely mirror
the proof technique used in the proof of Theorem 2.1 in Chatterjee et al. (2015a) with appropriate
modiﬁcations. The proof of Lemma 2.1 is deferred to Section 6.

Analyzing the estimated permutation(cid:98)σ is one of the key contributions of this paper. Speciﬁcally
we prove the following lemma concerning the behavior of(cid:98)σ.
Lemma 2.2. For any C > 0 setting ti := max{√
Proof. Deﬁne the vector of row sums of the data matrix d = (d1, . . . , dn) where di =(cid:80)n

4Cri log n, 4C log n/3} we have
P(|ri − r(cid:98)σ(i)| > max(ti + t(cid:98)σi, 4C log n} for some i ∈ [n]) ≤ 2n1−C.

(2.2)

j=1 yij for

all 1 ≤ i ≤ n. Setting An := ∩n

i=1{|di − ri| ≤ ti} we will ﬁrst show that

P(Ac

n) ≤ 2n1−C.

17

(2.3)

(cid:80)n
j=1 θij − θ2

ij, and using the inequality ri ≥ nσ2 we have

With σ2

i := 1
n

t2
i

2nσ2 ≥ 2Cri log n

nσ2

≥ 2C log n,

3t2
i
2ti

=

3ti
2

≥ 2C log n.

Thus an application of Bernstein’s inequality gives

P(|di − ri| > ti) ≤ 2exp

A union bound then proves (2.3).

(cid:110) −

t2
i /2
nσ2
i + ti/3

(cid:111) ≤ 2e−C log n = 2n−C.

It thus sufﬁces to show that after conditioning on the event An we have for all 1 ≤ i ≤ n,

|ri − r(cid:98)σ(i)| ≤ max(ti + t(cid:98)σ(i), 4C log n).
For this, setting(cid:98)σ(i) = j, we split the proof into the following cases:
• (cid:98)σ(i) > i,

r(cid:98)σ(i) > 4C log n

In this case we will show that

(2.4)

|ri − r(cid:98)σ(i)| = r(cid:98)σ(i) − ri ≤ ti + t(cid:98)σ(i).

and

If this does not hold, with j :=(cid:98)σ(i) we have rj − tj > ri + ti. Consequently the intervals
[ri − ti, ri + ti] and [rj − tj, rj + tj] are disjoint, and so dj > di. By construction of(cid:98)σ we
have(cid:98)σ(j) >(cid:98)σ(i) = j. Finally for any k > j we have rk − 4C log n/3 ≥ rj − 4C log n/3,
(rk −(cid:112)4Crk log n) − (rj −(cid:112)4Crj log n) =(rk − rj) −(cid:112)4C log n(
(cid:105) ≥ 0,

rk +
as rj ≥ 4C log n, and rk ≥ rj by monotonicity. Combining this gives

(cid:104)√
rk − tk = min(rk − 4C log n/3, rk −(cid:112)4Crk log n)

≥ min(rj − 4C log n/3, rj −(cid:112)4Crj log n) = rj − tj

rk − √
rj −(cid:112)4C log n

This implies that (cid:98)σ(k) > (cid:98)σ(i) for every k > j as well, and so (cid:98)σ(k) > j for all k ∈
{j, j + 1,··· , n}, which is impossible as n − j + 1 elements in the domain of(cid:98)σ cannot get
• (cid:98)σ(i) > i,

mapped to n − j elements in the range.

r(cid:98)σ(i) ≤ 4C log n

√
rk − √
=(

rj)

√

√

rj)

In this case we have

|ri − r(cid:98)σ(i)| = r(cid:98)σ(i) − ri ≤ r(cid:98)σ(i) ≤ 4C log n.

18

• (cid:98)σ(i) ≤ i

In this case we will again show that

If not, then we have ri − ti > rj + tj, and so the intervals [ri − ti, ri + ti] and [rj − tj, rj + tj]

|ri − r(cid:98)σ(i)| = ri − r(cid:98)σ(i) ≤ ti + t(cid:98)σ(i).
are disjoint, giving(cid:98)σ(j) <(cid:98)σ(i) = j. Since for any k < j we have
rk +(cid:112)Crk log n ≤ rj +(cid:112)Crj log n
by monotonicity, this implies(cid:98)σ(k) < j for all k ∈ {1, 2,··· , j}, which is again impossible.
Remark 2.1. We note here that lemma 2.2 can be used to derive concentration results for(cid:98)σ under

the assumption that the row sums ri are strictly increasing. In particular, if mini∈[n] |ri − ri−1| is
bounded away from 0, then by Lemma 2.2 with high probability we have

|i −(cid:98)σ(i)| = O((cid:112)n log n).

max
i∈[n]

Of course, if the row sums are not increasing, no concentration of(cid:98)σ towards identity is expected.
In particular if the row sums are constant, then(cid:98)σ(i) is expected to behave in a uniformly random

manner.

We also note that both the bounds above are adaptive in terms of sparsity of the underlying
graph. For e.g. if the entries of the matrix θ are mostly 0 or small, the row sums ri will be small as
well, thus giving a better bound.

We are now ready to ﬁnish the proofs of Theorems 1.1 and 1.2 by combining Lemma 2.1 and

Lemma 2.2.

Proof of Theorem 1.1. To begin, note that

n(cid:88)

[θij − θ(cid:98)σ(i),(cid:98)σ(j)]2 ≤2

i,j=1

≤2

≤2

i,j=1

n(cid:88)
n(cid:88)
n(cid:88)

i,j=1

n(cid:88)
[θij − θ(cid:98)σ(i),j]2 + 2
n(cid:88)
|θij − θ(cid:98)σ(i),j| + 2
n(cid:88)
|cj − c(cid:98)σ(j)| + 8n,
|ri − r(cid:98)σ(i)| + 2

[θ(cid:98)σ(i),j − θ(cid:98)σ(i),(cid:98)σ(j)]2
|θ(cid:98)σ(i),j − θ(cid:98)σ(i),(cid:98)σ(j)|

i,j=1

i,j=1

where we have used the fact that |θij − θi,(cid:98)σ(j)| belongs to [0, 1] in the third and fourth lines, and
θij − θi,k} have the same sign for all i ∈ [n] − {j, k} for any ﬁxed j, k in the fourth line. Here

i=1

j=1

19

(c1,··· , cn) are the column sums of θ. Now ci = ri in the symmetric case, and ci + ri = n − 1 in
the tournament case. Thus combining we have the bound

n(cid:88)

i,j=1

[θij − θ(cid:98)σ(i),(cid:98)σ(j)]2 ≤ 4

|ri − r(cid:98)σ(i)| + 8n.

n(cid:88)
i=1{|ri − r(cid:98)σ(i)| ≤ 2(cid:112)2n log n}.

i=1

Now Lemma (2.2) with C = 2 gives that for all n large we have

(2.5)

This gives

[θij − θ(cid:98)σ(i),(cid:98)σ(j)]21An

P(An) ≤ 2n−1, An := ∩n

E

n(cid:88)
n(cid:88)

i,j=1

i,j=1

i,j=1

=E

n + E

n) + E

≤n2P(Ac

[θij − θ(cid:98)σ(i),(cid:98)σ(j)]2
[θij − θ(cid:98)σ(i),(cid:98)σ(j)]21Ac

n(cid:88)
n(cid:88)
[θij − θ(cid:98)σ(i),(cid:98)σ(j)]21An
n(cid:88)
|ri − r(cid:98)σ(i)|1An
2n3/2(cid:112)log n
n2(cid:107)(cid:98)θ − θ∗(cid:107)2 ≤ ERrow(θ) + C

≤2n + 8n + 4E
≤10n + 8

≤ 10
n

(log n)2

√

i,j=1

i=1

n

E 1

where we have used the trivial bound ri ≤ n. The last display alongwith Lemma 2.1 now gives

(cid:114)

√

+ 8

2

log n

n

+ C

(log n)2

n

,

from which the desired conclusion follows.
Proof of Theorem 1.2. By deﬁnition of S(θ) for all i, j ∈ [n] we have

[θi,k − θj,k]2 ≤ 2 +

(ri − rj)2

n

S(θ).

Since θk,i = θi,k in the symmetric case and θk,i = 1 − θi,k in the anti symmetric case, we readily
have

n(cid:88)

k=1

n(cid:88)

[θk,i − θk,j]2 ≤ 2 +

(ri − rj)2

n

S(θ)

k=1

20

as well. Thus

n(cid:88)

i,j=1

n(cid:88)
[θij − θ(cid:98)σ(i),(cid:98)σ(j)]2 ≤2
(cid:104)

i,j=1

n(cid:88)
(ri − r(cid:98)σ(i))2(cid:105)
n(cid:88)
n(cid:88)
(ri − r(cid:98)σ(i))2.

i,j=1

i=1

≤2

2n +

S(θ)

n

=8n +

4S(θ)

n

[θij − θ(cid:98)σ(i),j]2 + 2

[θ(cid:98)σ(i),j − θ(cid:98)σ(i),(cid:98)σ(j)]2

(cid:104)

+ 2

2n +

S(θ)

n

n(cid:88)

[rj − r(cid:98)σ(j))2(cid:105)

j=1

As before, (2.2) with C = 2 gives that for all n large we have

Thus, proceeding similarly we have

i=1

P(An) ≤ 2n−1, An := {|ri − r(cid:98)σ(i)| ≤ 2(cid:112)2n log n}.
n(cid:88)
n(cid:88)

i,j=1

E

[θij − θ(cid:98)σ(i),(cid:98)σ(j)]2
[θij − θ(cid:98)σ(i),(cid:98)σ(j)]21Ac
n(cid:88)

4S(θ)

E

n(cid:88)
n + E
(ri − r(cid:98)σ(i))21An

i,j=1

[θij − θ(cid:98)σ(i),(cid:98)σ(j)]21An

=E

i,j=1

≤2n + 8n +
n
≤10n + 32n log n

i=1

As before, an application of Lemma 2.1 gives

n2(cid:107)(cid:98)θ − θ∗(cid:107)2 ≤ ERrow(θ) + C

E 1

(log n)2

n

≤ 10
n

+ 32

log n

n

+ C

(log n)2

n

,

from which the desired conclusion follows.

3. Lipschitz Matrices

In this section we prove Theorem 1.3. We ﬁrst prove the following lemma.
Lemma 3.1. Fix any positive integer n. Take any sequence (a1, a2, . . . , an) ∈ [0, 1]n of length n
satisfying the Lipschitz condition for some L > 0,
|ai+1 − ai| ≤ L
n

∀1 ≤ i ≤ n − 1.

(3.1)

Then we have the following inequality

.

(3.2)

L, 1}((cid:80)n

i=1 ai)3/2
√

n

n(cid:88)

√
i ≤ 4 max{
a2

i=1

21

Proof. We prove the lemma when L = 1. This implies the result for L ≤ 1 as well, and a simple
i=1 ai, inequality (3.2) holds
n ≥ n, and the
2 without loss of generality, we can ﬁnd a positive

scaling argument proves (3.2) in the case L > 1. Setting λ := (cid:80)n
Then there exists an t ∈ [0, 1) such that λ = (cid:80)l

trivially in case λ ≥ n+1
LHS of (3.2) is at most n. Thus assuming λ < n+1
integer 1 ≤ l ≤ n − 1 such that 1

√
2 because in that case the RHS of (3.2) is at least 3 (n+1)3/2

n . We claim that v = (0, . . . , 0, t, t +

n + ··· + l+1
n .

n ≤ λ < 1

23/2

i=0

t+i

1/n, . . . , t + l/n) is a solution to the maximization problem Mn:

n + ··· + l
n(cid:88)
n(cid:88)

i=1

ai = λ;

i=1

maximize

subject to

i such that 0 ≤ ai ≤ 1, i ∈ [n];
a2

|ai+1 − ai| ≤ 1
n

∀1 ≤ i ≤ n − 1.

Given the claim, we have

i=1

and so

n(cid:88)

i ≤ 1
v2
n2

i2 =

(l + 1)2(l + 2)

l+1(cid:88)
n(cid:80)n
((cid:80)n
i=1 vi)3/2 ≤ l3/2(l + 1)3/2
23/2n3/2 =

i=1 v2
i

3n2

√

i=1

,

n(cid:88)

i=1

vi ≥ 1
n

l(cid:88)

i=1

i =

l(l + 1)

2n

,

(2l + 2)3/2(l + 2)

l3/2

≤ 4,

l(cid:88)

j
n

λ =

thus proving the Lemma.

For proving the claim, for simplicity of exposition we assume t = 0, noting that a similar proof

works for any t ∈ [0, 1). Thus we assume that

(3.3)

j=1

a solution to the maximization problem Mn.

for some integer l ∈ [1, n − 1]. In this case, we will prove that(cid:101)v = (0, . . . , 0, 1/n, 2/n, . . . , l/n) is
Take any feasible vector (cid:101)x. Our proof strategy is to show that (cid:107)(cid:101)x(cid:107)2 ≤ (cid:107)(cid:101)v(cid:107)2. Without loss
of generality we can assume that (cid:101)x is monotone non decreasing. If (cid:101)x is not non-decreasing in
the ﬁrst place, we can sort(cid:101)x to make it non decreasing. The sorting does not change the objective
We now keep on applying a transformation on (cid:101)x to ﬁnally obtain(cid:101)v, such that the objective
function cannot decrease at every transformation. This will then show the optimality of(cid:101)v.
Assume(cid:101)x (cid:54)= (cid:101)v. Let i1 be the minimum integer such that(cid:101)xi1 (cid:54)= (cid:101)vi1. Note that we must have
(cid:101)xi1 ≥ (cid:101)vi1. Otherwise the Lipschitz constraint (3.1) would ensure that(cid:80)n
i=1(cid:101)xi < λ which is a

function value, does not change the sum constraint. It can also be checked that the sorting operation
preserves the Lipschitz constraint (3.1).

22

negative;

1

n − i1

(xi1+j − xi1) ≥ 0

Lipschitz constraint (3.1) with L = 1 for all i (cid:54)= i1.

contradiction. Now we describe how we transform(cid:101)x and obtain(cid:101)y. We set(cid:101)yj =(cid:101)xj for all 1 ≤ j ≤
i1 − 1. Deﬁne(cid:101)yi1 =(cid:101)xi1 −  where  ≥ 0 is so deﬁned such that we match the i1 th coordinate; that
is(cid:101)yi1 =(cid:101)vi1. Also deﬁne(cid:101)yj =(cid:101)xj + /(n − i1) for i1 + 1 ≤ j ≤ n.
It is easy to see(cid:101)y remains monotonically non decreasing. Secondly note that the sum does not
change, that is(cid:80)n
i=1(cid:101)yi = λ. Also observe that the difference of the objective value would be non
n−i1(cid:88)
(cid:107)(cid:101)y(cid:107)2 − (cid:107)(cid:101)x(cid:107)2 = (cid:107)(cid:107)(cid:101)y −(cid:101)x(cid:107)2 + 2

where the last inequality follows from the monotonicity of(cid:101)x. Also by construction(cid:101)y satisﬁes the
Now that we have matched the coordinates of(cid:101)v and(cid:101)y in the ﬁrst i1 coordinates, we can look
for the next mismatch. Now deﬁne i2 to be the minimum integer larger than i1 such that(cid:101)yi2 (cid:54)=(cid:101)vi2.
Note that again we must have(cid:101)yi2 ≥(cid:101)vi2. Otherwise because of the fact that(cid:101)y is Lipschitz in the last
n − i1 coordinates and is equal to(cid:101)x in the ﬁrst i2 − 1 coordinates, the Lipschitz constraint would
ensure that(cid:80)n
i=1(cid:101)yi < λ which is a contradiction. We can now transform(cid:101)y to obtain(cid:101)z similarly as
we transformed(cid:101)x to(cid:101)y. We would then ensure that(cid:101)zj =(cid:101)vj for all 1 ≤ j ≤ i2. We will again have
monotonicity of(cid:101)z and also(cid:80)n
Also by construction(cid:101)z would satisfy the Lipschitz constraint (3.1) with L = 1 for all i (cid:54)= i2.
It is now clear that we can now keep repeating this process of eliminating mismatches with(cid:101)v
to ﬁnally end up with the vector(cid:101)v. This proves that(cid:101)v is a maximizer of the optimization problem

i=1(cid:101)zi = λ. Importantly, we will have

(cid:107)(cid:101)z(cid:107)2 − (cid:107)(cid:101)y(cid:107)2 ≥ 0.

(3.4)

(3.5)

j=1

Mn.

We are now ready to prove Theorem 1.3.

Proof of Theorem 1.3. Following similar calculations as in the proofs of Theorem 1.1 and Theorem
1.2 we have

n(cid:88)

[θij − θ(cid:98)σ(i),(cid:98)σ(j)]2 ≤8n + 2

i,j=1

i=1

n(cid:88)

(cid:110) (cid:88)
j(cid:54)=i,(cid:98)σ(i)

[θij − θ(cid:98)σ(i),j]2(cid:111)

n(cid:88)

j=1

(cid:110) (cid:88)
i(cid:54)=j,(cid:98)σ(j)

+ 2

[θ(cid:98)σ(i),j − θ(cid:98)σ(i),(cid:98)σ(j)]2(cid:111)

(3.6)

Now consider the second term on the right side of the above inequality. We can apply Lemma 3.1
in the following way. For each ﬁxed i let θ[i,] ∈ Rn−2 be the ith row of θ without the coordinates i

and(cid:98)σ(i). Deﬁne the vector a := θ[i,] − θ[(cid:98)σ(i),] in case i >(cid:98)σ(i) and with sign reversed in the other
case. By deﬁnition a has elements in [0, 1]. Also, in case i >(cid:98)σ(i) we have
aj+1 − aj =(cid:0)θi,j+1 − θ(cid:98)σ(i),j+1

(cid:1) =(cid:0)θi,j+1 − θi,j) −(cid:0)θ(cid:98)σ(i),j+1 − θ(cid:98)σ(i),j

(cid:1) −(cid:0)θi,j − θ(cid:98)σ(i),j

(3.7)

(cid:1)

23

Since θ is Lipschitz, the two terms on the right side of the above inequality lie between 0 and L/n,
and so the vector a satisﬁes the Lipschitz condition 3.1 with Lipschitz constant L. Since

(cid:88)

j

aj ≤ |ri − r(cid:98)σ(i)|,
((cid:80)

√

an application of Lemma 3.1 would imply for each 1 ≤ i ≤ n, the crucial bound

[θij − θ(cid:98)σ(i),j]2 =

j ≤ 4 max(1,
a2

L)

j aj)3/2√
n

≤ 4 max(1,

√
L)

|ri − r(cid:98)σ(i)|3/2

√

.

n

(cid:88)
j(cid:54)=i,(cid:98)σ(i)

(cid:88)

j

The third term in the right side of inequality (3.6) can be treated similarly to ﬁnally get the upper
bound

n(cid:88)

16 max(1,
√
n

i,j=1

[θij − θ(cid:98)σ(i),(cid:98)σ(j)]2 ≤ 8n +
n2||(cid:98)θ − θ||2

2 ≤ 10
n

E 1

+ 16 max(1,

√

√
L)

n(cid:88)

i=1

|ri − r(cid:98)σ(i)|3/2

L)29/4 log n3/2

n3/4 + C

(log n)2

,

n

From this, using Lemma 2.2 and Lemma 2.1 as in the proofs of Theorem 1.1 and Theorem 1.2 we
have

from which the result follows.

4. Minimax lower bound

To prove Theorem 1.4 we use the following standard version of Fano’s lemma.
Lemma 4.1. For any δ > 0 let W be a 2δ packing set of Θ with respect to the Frobenius metric.
Then we have the following lower bound on the minimax risk:
1 − maxw (cid:54)=w

E((cid:101)θij − θij)2 ≥ δ2

(cid:48)∈W D(Pw, Pw

n(cid:88)

n(cid:88)

(cid:48) ) + log 2

(cid:16)

(cid:17)

n2

log |W|

inf(cid:101)θ

sup
θ∈ΘG

1
n2

i=1

j=1

where Pw refers to the distribution of the data matrix y when the true underlying matrix is w,
D refers to the Kulback Leibler divergence between probability measures and the inﬁmum in the

display is taken over all estimators(cid:101)θ.

Let us now deﬁne a matrix L ∈ G ⊂ Rn×n as follows:
√
i + j
n
n

Lij =

1
2

+

.

n−2. In order
Recall the deﬁnition of average smoothness S in (1.12). One can check that S(L) = n
to prove Theorem 1.4 we actually prove a O(n) minimax lower bound for the smaller parameter
space

A = {a L ◦ π : π ∈ Sn}

in our next theorem where a > 0 is a small enough positive constant.

24

√
Remark 4.1. Note that L is also a Lipschitz matrix with Lipschitz constant 1/
n and is also
additive. Hence a minimax lower bound of O(1/n) for the parameter space A also automatically
gives a O(1/n) minimax lower bound for the class of Lipschitz matrices as deﬁned in (1.16) and
also in the additive model. We think that a minimax lower bound of O(1/n) can be proved for
each of the sub classes discussed in subsection 1.5 (e.g. monotone stochastic block model, Strictly
Increasing Lipschitz matrices etc), although we do not attempt to carry this out in this manuscript.

We now prove Theorem 1.4 for the symmetric model. The following proof can easily be modi-

ﬁed to work for the anti symmetric model.
Proof of Theorem 1.4. We ﬁrst claim that there exists a set of permutations F ⊂ Sn such that
|F| ≥ exp(c1n) and for any two distinct permutations π (cid:54)= π
√
(i) + π
n
n

(cid:48) we have the separation condition
(cid:48)

(cid:16)π(i) + π(j)

(cid:17)2 ≥ 1

n(cid:88)

n(cid:88)

√
n

− π

(4.1)

1
n2

(j)

6n

n

(cid:48)

i=1

j=1

where c1 is a positive universal constant. To prove the above claim consider the largest packing set
|F| ⊂ Sn such that for any two distinct elements π, π
(cid:48) ∈ F the separation condition (4.1) holds.
Since F is the largest packing set we have the set relation

(4.2)

(cid:91)

(cid:16)

(cid:17)

1
6n

B

π,

= Sn

π∈F
(cid:48) ∈ Sn such that (4.1) holds.

(cid:16)

Here B

(cid:17)

(cid:32)

(cid:33)

π, 1
6n

denotes the set of all π

(cid:48)

(cid:48)

P

√

1
n2

− π

n(cid:88)

n(cid:88)
(cid:16)

We claim that there exists a positive constant c1 such that

(cid:17)2 ≤ 1

(cid:16) π(i) + π(j)
(cid:17) ≤ n! exp(−c1n). This fact coupled with the set relation (4.2)

√
(i) + π
n

where P refers to the uniform distribution on the random permutation π
π ∈ F the cardinality of B
then gives the required lower bound on the cardinality |F|. We use the set {L ◦ π : π ∈ F} as our
packing set in the proof of Fano’s lemma. It is a fact that the KL divergence between two Bernoulli
distributions with success probabilities p

. Given (6.2), for any

≤ e−c1n,

π, 1
6n

(4.3)

(j)

6n

j=1

i=1

n

n

n

(cid:48)

(cid:48)

(cid:90) p

< p satisﬁes the equality
p − x
x(1 − x)
< p ≤ 0.6 then we have KL(Bern(p), Bern(p

D(Bern(p), Bern(p

)) =

(cid:48)
p

In case 0.5 ≤ p
the entries of our matrix are between 0.4 and 0.6 for large enough n.

(cid:48)

(cid:48)

(cid:48)

dx.

Hence we then have for all sufﬁciently large n,

)) ≤ 5(p − p

(cid:48)

)2. In our case, all

max
(cid:48)∈F
π,π

D(PL◦π, PL◦π

(cid:48) ) ≤ 5 max
(cid:48)∈F

π,π

(cid:0)(L ◦ π)ij − (L ◦ π

(cid:1) ≤ 10

(cid:48)

)ij

a2
n

.

n(cid:88)

n(cid:88)

i=1

j=1

25

The desired minimax lower bound can now be proved by choosing a small enough and applying
Lemma 4.1. Note that we have so far ignored the fact that the diagonals are zero in our parame-
ter space. When i = j in the summands in (4.1), we have terms of the form (π(i) − π
(i))2/n3
upto a constant factor. Since there are only n such terms appearing in the sum in (4.1) the separa-
tion condition (4.1) still holds with a slightly worse constant. The only thing remaining now is to
prove (6.2), which we do below:

(cid:48)

Note that

n(cid:88)

(cid:16) π(i) + π(j)

√

i,j=1

n

n

1
n

− π(i) + π(j)

√
n

n

(π(i) + π(j) − π(cid:48)(i) − π(cid:48)(j))2

(cid:17)2

i,j=1

n(cid:88)
n(cid:88)
n(cid:88)

i=1

i=1

=

=

=

1
n4

2
n3

2
n3

(π(i) − π(cid:48)(i))2

(σ(i) − i)2,

(cid:32)

Pn

1
n

n(cid:88)

(cid:16)σ(i)

− i
n

n

(cid:33)

(cid:17)2 ≤ 1

12

where σ = π(cid:48) ◦ π−1 is distributed according to Pn as well. By (Mukherjee, Theorem 4.1) we have

≤ −

µ∈M:(cid:82)

inf

[0,1]2 (x−y)2dµ≤ 1

D(µ||u) = − : c1

i=1

with uniform marginals. Since D(µ||u) is uniquely minimized at µ = u, and(cid:82)

where u is the uniform distribution on [0, 1]2, and M is the set of all probability measures on [0, 1]2
[0,1]2(x− y)2dxdy =

12

1

6 > 1

12, it follows that c1 > 0, and so (6.2) follows.

5. Acknowledgements

We want to thank Bodhisattva Sen for introducing us to this problem, and for his helpful comments
and suggestions.

6. Appendix

6.1. Proof of Lemma 2.1
The observed data matrix y is distributed as Bern(θ∗ ◦ π) where θ∗ ∈ T or θ∗ ∈ G. We will
henceforth, without loss of generality, assume π is the identity permutation in which case y is
distributed as Bern(θ∗). By deﬁnition of our estimator and an application of the triangle inequality
we then have the following inequality;

(cid:107)(cid:98)θ − θ∗(cid:107) = (cid:107)P (y ◦(cid:98)σ) ◦(cid:98)σ−1 − θ∗(cid:107) ≤ (cid:107)P (y ◦(cid:98)σ) − θ∗(cid:107) + (cid:107)θ∗ ◦(cid:98)σ − θ∗(cid:107).

(6.1)

26

Proposition 6.1. Let S = T or S = G and θ∗ ∈ S. In either case, one has the deterministic
identity

(cid:107)P (y ◦(cid:98)σ) − θ∗(cid:107) = arg max

t≥0

fθ∗(t) − t2
2

n(cid:88)

n(cid:88)
([y ◦(cid:98)σ] − θ∗)ij(θ − θ∗)ij.

where we deﬁne

fθ∗(t) =

sup

θ∈S:(cid:107)θ−θ∗(cid:107)≤t

i=1
Moreover we have the following upper bound;

j=1

(cid:107)P (y ◦(cid:98)σ) − θ∗(cid:107)2 ≤ s2

(6.2)

for any s satisfying fθ∗(s) − s2

2 ≤ 0.

This proposition follows from the proof arguments given in the proof of Theorem 1.1 in Chat-
terjee (2014). For the sake of completeness, we give a sketch of the proof of the above proposition
below.
Proof. Let z ∈ Rn be any vector. Following along similar lines as in the proof of Theorem 1.1
in Chatterjee (2014), one can actually obtain that for any matrix v ∈ Rn×n and any θ∗ ∈ S,

(cid:107)P (z) − θ∗(cid:107) = arg max

t≥0

sup

θ∈S:(cid:107)θ−θ∗(cid:107)≤t

(z − θ∗)ij(θ − θ∗)ij

{(cid:0)

n(cid:88)

n(cid:88)

i=1

j=1

(cid:1) − t2

2

}.

(cid:1) ≤ 0

(cid:0)

n(cid:88)

n(cid:88)

where P is the projection operator to S. The right side of the above inequality equals the maximizer
of a function of t which can be shown to be strictly concave function decaying to −∞ as t → ∞.
Also when t = 0 this function is 0 as can be seen easily. Hence, if s > 0 is a number satisfying

sup

θ∈S:(cid:107)θ−θ∗(cid:107)≤s

(z − θ∗)ij(θ − θ∗)ij − s2
2

i=1

j=1

then we have (cid:107)P (z)−θ∗(cid:107) ≤ s. Now in our case, set z = y◦(cid:98)σ to ﬁnish the proof of the proposition.
Proof of Lemma 2.1. Note that y ◦(cid:98)σ = θ∗ ◦(cid:98)σ + (y − θ∗) ◦(cid:98)σ. This implies the following:
(cid:104)(y − θ∗) ◦(cid:98)σ, θ − θ∗(cid:105).

(cid:104)[θ∗ ◦(cid:98)σ] − θ∗, θ − θ∗(cid:105) +

fθ∗(t) ≤

sup

sup

θ∈S:|θ−θ∗|≤t

θ∈S:|θ−θ∗|≤t

Here the (cid:104)(cid:105) notation denotes the sum of entrywise products of the two matrices in question. An

application of the Cauchy Schwarz Inequality to the ﬁrst term and replacing(cid:98)σ with a supremum

over all permutations in the second term now gets us the following:

fθ∗(t) ≤ t(cid:107)[θ∗ ◦(cid:98)σ] − θ∗(cid:107) + sup

sup

π∈Sn

θ∈Θn:|θ−θ∗|≤t

(cid:104)y − θ∗, (θ − θ∗) ◦ π(cid:105).

27

where we have also used log n! ≤ n log n. Note that fθ∗(t) does not grow beyond t > n since
|θ − θ∗| ≤ n for all θ ∈ S. Hence if we consider t ≤ n then we have from the last display

fθ∗(t) ≤ Cn log n log(nt) + ((cid:112)n log n +
fθ∗(t) ≤ Cn(log n)2 + ((cid:112)n log n +

√

n + (cid:107)[θ∗ ◦(cid:98)σ] − θ∗(cid:107)) t

√
√

n + (cid:107)[θ∗ ◦(cid:98)σ] − θ∗(cid:107)) t.
n log n,(cid:107)θ∗◦(cid:98)σ(cid:107)} one gets fθ∗(s)− s2

Now Lemma 6.1 (stated and proved below) upper bounds the second term in the right side of the
above inequality with high probability. Infact it gives us the following inequality with probability
not less than 1 − exp(−n/8),

The last display then shows that by setting s = max{C
2 < 0
for an approriately large universal constant C. Note that the choice of s satisﬁes s ≤ n for all large
n so that we can now use the last display. Now just as in inequality (6.1) we have

(cid:107)(cid:98)θ − θ∗(cid:107)2 ≤ 2(cid:107)P (y ◦(cid:98)σ) − θ∗(cid:107)2 + 2(cid:107)[θ∗ ◦(cid:98)σ] − θ∗(cid:107)2.

(6.3)

An application of inequality (6.2) to the ﬁrst term on the right side of the last display now ﬁnishes
the proof of the lemma.
Lemma 6.1. Fix any n > 1. Let S = T ⊂ Rn×n or S = G ⊂ Rn×n. Fix θ∗ ∈ S. Let y ∼ Bern(θ∗).
For any t > 0 deﬁne

gθ∗(t) = sup
π∈Sn

sup

θ∈S:|θ−θ∗|≤t

(cid:104)y − θ∗, (θ − θ∗) ◦ π(cid:105)

For any t > 1/n, the following inequality is true for some universal constant C with probability
not less than 1 − exp(−n/8),

gθ∗(t) ≤ Cn log n log(nt) + ((cid:112)log n! +

√
n) t.

In order to prove the above lemma, we need to make use of the following results.

Theorem 6.1 (Chaining). Let A be any subset of Rn × Rn. Suppose  is a random n × n matrix
with independent entries. Moreover let

|ij| ≤ 1 and Eij = 0 ∀1 ≤ i ≤ n, 1 ≤ j ≤ n.

Then for every θ∗ ∈ A we have
(cid:21)

(cid:20)

(cid:104), θ − θ∗(cid:105)

E

sup
θ∈A

≤ 12

inf

0<δ≤diam(A)

(cid:40)(cid:90) diam(A)

δ

(cid:112)

log N (τ,A) dτ + 4nδ

(cid:41)

.

(6.4)

where diam(A) is the diameter of the set A.
Theorem 6.2 (Ledoux). If f : [−1, 1]n×[−1, 1]n → R is a convex Lipschitz function with Lipschitz
constant L and  is a mean zero random matrix with independent entries in [−1, 1], then we have

P(f () > u) ≤ exp(−u2/8L2).

(6.5)

28

Let us now introduce some notations. For any θ ∈ Rn×n denote the Euclidean ball with center
θ and radius r > 0 by B(θ, r). Also let us deﬁne the space of matrices which are non decreasing
in both rows and columns as

M = {θ ∈ Rn×n : θij ≤ θkl iff i ≤ k and j ≤ l}.

We actually need to consider the class of bounded monotone matrices:

Mr = {θ ∈ M :

|θij| ≤ r}.

sup

1≤i,j≤n

(6.6)

(6.7)

Proposition 6.2. Let S = T or S = G be a subset of Rn×n. Then we have the covering number
relation for any  > 0,

log N (, S) ≤ C

.

(6.8)

(cid:16)n

(cid:17)2 (cid:104)



(cid:16)n

(cid:17)(cid:105)2

log



Proof. Let us look at the case when S = T . Let us deﬁne a map D : S → Rn×n as follows:

D(θ)i,j = min{θi,i−1, θi+1,i} I{i = j} + θi,j I{i (cid:54)= j}

Let us deﬁne another map φ : Rn×n → Rn×n as

φ(θ)i,j = θi,n−j+1

Deﬁne the composition map f = φ◦D. Recall that M1 is the space of monotone matrices bounded
in absolute value by 1. It can now be checked that f (θ) ∈ M1 for all θ ∈ T and f is a one to one
mapping. We now make the observation that for any θ (cid:54)= θ
(cid:48)(cid:107) ≤ (cid:107)f (θ) − f (θ

(6.9)
Since f (S) is a closed subset of M there exists a covering set F of f (S) ⊂ M1 at radius  with
cardinality atmost N (/2,M1). Also it follows from (6.9) that the inverse image of F under the
mapping f is therefore a  covering set of S. Therefore we can conclude,

(cid:48) belonging to S we have
)(cid:107).
(cid:48)

(cid:107)θ − θ

log N (, S) ≤ log N (/2,M1).

The covering number for M1 can be obtained using Lemma 3.4 in Chatterjee et al. (2015a) and
is written below. This result is proved by using the covering number results in Gao and Wellner
(2007).

(cid:16)n

(cid:17)2 (cid:104)

(cid:16)n

(cid:17)(cid:105)2

(6.10)
where  > 0 and C is a universal constant. The last two displays ﬁnish the proof in the case S = T .
In the case when S = G the entire proof goes through by deﬁning φ to be the identity map.

log





log N (,M1) ≤ C

We are now ready to prove Lemma 6.1.

29

Proof of Lemma 6.1. Fix θ∗ ∈ S. Denote y − θ∗ by . It is not too hard to show that

gθ∗(t) = sup
π∈Sn

sup

θ∈S:|θ−θ∗|≤t

(cid:104), (θ − θ∗) ◦ π(cid:105)

for any ﬁxed t > 0 is a Lipschitz function of  with Lipschitz constant t. Also  is a mean zero
random matrix bounded in absolute value by 1. Therefore by the concentration inequality in The-
orem 6.2, for any u > 0, we have with probability not less than 1 − exp(−u2/8),

gθ∗(t) ≤ Egθ∗(t) + tu

Now we will upper bound Egθ∗(t).

For ease of notation, deﬁne

A = {(θ − θ∗) ◦ π : θ ∈ S, π ∈ Sn}.

(6.11)

(6.12)

An application of the chaining result in Theorem (6.1) results in the upper bound

(cid:26)(cid:90) t

δ

Egθ∗(t) ≤ 12 inf
0<δ≤t

log N (,A) ≤ log n! + C

(cid:27)

.

(6.13)

(cid:112)log N (,A) d + 4nδ
(cid:17)2 (cid:104)
(cid:17)(cid:105)2
(cid:16)n
(cid:16) n
(cid:27)
(cid:17)(cid:105)
(cid:16) n
+ 4 +(cid:112)log n! d
(cid:17) ≤ log n2 because the variable of integration
(cid:16) n

(6.14)

(6.15)

log







.

.

Setting δ = 1

n in (6.13) we then obtain

(cid:26)(cid:90) t

(cid:16)n

(cid:17) (cid:104)

1/n



log

Egθ∗(t) ≤ C

Because of Lemma (6.2) the fact that π can take n! distinct values we can now write

Now we only consider t > 1
 ranges from 1/n to t. This then implies the upper bound

n in which case, log



Egθ∗(t) ≤ Cn log n log(nt) +(cid:112)log n! t.

(6.16)

n in (6.11) and combining it with the last display now ﬁnishes the proof of the

√

Setting u =
lemma.

References

Edo M Airoldi, Thiago B Costa, and Stanley H Chan. Stochastic blockmodel approximation of
a graphon: Theory and consistent estimation. In Advances in Neural Information Processing
Systems, pages 692–700, 2013.

30

Peter J Bickel and Aiyou Chen. A nonparametric view of network models and newman–girvan and
other modularities. Proceedings of the National Academy of Sciences, 106(50):21068–21073,
2009.

Joseph Blitzstein and Persi Diaconis. A sequential importance sampling algorithm for generating

random graphs with prescribed degrees. Internet Mathematics, 6(4):489–522, 2011.

Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method

of paired comparisons. Biometrika, 39(3/4):324–345, 1952.

Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In Proceedings of the
nineteenth annual ACM-SIAM symposium on Discrete algorithms, pages 268–276. Society for
Industrial and Applied Mathematics, 2008.

Stanley H Chan and Edoardo M Airoldi. A consistent histogram estimator for exchangeable graph

models. arXiv preprint arXiv:1402.1888, 2014.

Sabyasachi Chatterjee and John Lafferty. Adaptive risk bounds in unimodal regression. arXiv

preprint arXiv:1512.02956, 2015.

Sabyasachi Chatterjee, Adityanand Guntuboyina, and Bodhisattva Sen. On matrix estimation un-

der monotonicity constraints. arXiv preprint arXiv:1506.03430, 2015a.

Sabyasachi Chatterjee, Adityanand Guntuboyina, Bodhisattva Sen, et al. On risk bounds in iso-
tonic and other shape restricted regression problems. The Annals of Statistics, 43(4):1774–1800,
2015b.

Sourav Chatterjee. A new perspective on least squares under convex constraint. The Annals of

Statistics, 42(6):2340–2381, 2014.

Sourav Chatterjee. Matrix estimation by universal singular value thresholding. Ann. Statist., 43(1):
177–214, 2015. ISSN 0090-5364. . URL http://dx.doi.org/10.1214/14-AOS1272.
Sourav Chatterjee, Persi Diaconis, and Allan Sly. Random graphs with a given degree sequence.

The Annals of Applied Probability, pages 1400–1435, 2011.

Persi Diaconis and Svante Janson. Graph limits and exchangeable random graphs. arXiv preprint

arXiv:0712.2749, 2007.

Chao Gao, Yu Lu, Harrison H Zhou, et al. Rate-optimal graphon estimation. The Annals of

Statistics, 43(6):2624–2652, 2015.

Fuchang Gao and Jon A Wellner. Entropy estimate for high-dimensional monotonic functions.

Journal of Multivariate Analysis, 98(9):1751–1764, 2007.

Matthew O Jackson et al. Social and economic networks, volume 3. Princeton university press

Princeton, 2008.

Rasmus Kyng, Anup Rao, and Sushant Sachdeva. Fast, provable algorithms for isotonic regression
In Advances in Neural Information Processing Systems, pages 2701–2709,

in all l_p-norms.
2015.

László Lovász. Large networks and graph limits, volume 60. American Mathematical Soc., 2012.
Sumit Mukherjee. Estimation in exponential family on permutations. The Annals of Statistics, 44

(2):853–875.

31

Mark EJ Newman. The structure and function of complex networks. SIAM review, 45(2):167–256,

2003.

Soﬁa C Olhede and Patrick J Wolfe. Network histograms and universality of blockmodel approxi-

mation. Proceedings of the National Academy of Sciences, 111(41):14722–14727, 2014.

Garry Robins, Tom Snijders, Peng Wang, Mark Handcock, and Philippa Pattison. Recent devel-
opments in exponential random graph (p*) models for social networks. Social networks, 29(2):
192–215, 2007.

Nihar B Shah, Sivaraman Balakrishnan, Adityanand Guntuboyina, and Martin J Wainright.
Stochastically transitive models for pairwise comparisons: Statistical and computational issues.
arXiv preprint arXiv:1510.05610, 2015.

Louis L Thurstone. A law of comparative judgment. Psychological review, 34(4):273, 1927.
Patrick J Wolfe and Soﬁa C Olhede. Nonparametric graphon estimation.

arXiv preprint

arXiv:1309.5936, 2013.

Cun-Hui Zhang. Risk bounds in isotonic regression. Ann. Statist., 30(2):528–555, 2002. ISSN

0090-5364. . URL http://dx.doi.org/10.1214/aos/1021379864.

32

