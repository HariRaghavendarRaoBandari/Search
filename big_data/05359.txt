6
1
0
2

 
r
a

 

M
7
1

 
 
]

G
L
.
s
c
[
 
 

1
v
9
5
3
5
0

.

3
0
6
1
:
v
i
X
r
a

Cascading Bandits for Large-Scale Recommendation Problems

Shi Zong

Hao Ni

Dept of Electrical and Computer Engineering

Dept of Electrical and Computer Engineering

Carnegie Mellon University

szong@andrew.cmu.edu

Carnegie Mellon University

haon@cmu.edu

Kenny Sung

Nan Rosemary Ke

Dept of Electrical and Computer Engineering

D´ept d’informatique et de recherche op´erationnelle

Carnegie Mellon University

tsung@andrew.cmu.edu

Zheng Wen

Adobe Research

San Jose, CA

zwen@adobe.com

Abstract

Most recommender systems recommend a list of
items. The user examines the list, from the ﬁrst
item to the last, and often chooses the ﬁrst attrac-
tive item and does not examine the rest. This type
of user behavior can be modeled by the cascade
model. In this work, we study cascading bandits,
an online learning variant of the cascade model
where the goal is to recommend K most attrac-
tive items from a large set of L candidate items.
We propose two algorithms for solving this prob-
lem, which are based on the idea of linear gener-
alization. The key idea in our solutions is that we
learn a predictor of the attraction probabilities of
items from their features, as opposing to learning
the attraction probability of each item indepen-
dently as in the existing work. This results in
practical learning algorithms whose regret does
not depend on the number of items L. We bound
the regret of one algorithm and comprehensively
evaluate the other on a range of recommendation
problems. The algorithm performs well and out-
performs all baselines.

1 Introduction

Most recommender systems recommended a list of K
items, such as restaurants, songs, or movies. The user
examines the recommended list from the ﬁrst item to the

Universit´e de Montr´eal

nke001@gmail.com

Branislav Kveton
Adobe Research

San Jose, CA

kveton@adobe.com

last, and typically clicks on the ﬁrst item that attracts the
user. The cascade model [9] is a popular model to formu-
late this kind of user behavior. The items before the ﬁrst
clicked item are not attractive, because the user examines
these items but does not click on them. The items after the
ﬁrst attractive item are unobserved, because the user never
examines these items. The key assumption in the cascade
model is that each item attracts the user independently of
the other items. Under this assumption, the optimal solu-
tion in the cascade model, the list of K items that maxi-
mizes the probability that the user ﬁnds an attractive item,
are K most attractive items. The cascade model is sim-
ple, intuitive, and surprisingly effective in explaining user
behavior [6].

In this paper, we study on an online learning variant of the
cascade model, which is known as cascading bandits [14].
In this model, the learning agent does not know the pref-
erences of the user over recommended items and the goal
is to learn them by interacting with the user. At time t, the
agent recommends to the user a list of K items out of L
candidate items and observes the click of the user. If the
user clicks on an item, the agent receives a reward of one.
If the user does not click on any item, the agent receives a
reward of zero. The performance of the learning agent is
evaluated by its cumulative reward in n steps, which is the
total number of clicks in n steps. The goal of the agent is
to maximize it.

Kveton et al. [14] proposed two computationally and sam-
ple efﬁcient algorithms for cascading bandits. They also
proved a Ω(L− K) lower bound on the regret in cascading

bandits, which shows that the regret grows linearly with the
number of candidate items L. Therefore, cascading bandits
are impractical for learning when L is large. Unfortunately,
this setting is common practice. For instance, consider the
problem of learning a personalized recommender system
for K = 10 movies from the ground set of L = 100k
movies. In this setting, each movie would have to be shown
to the user at least once, which means at least 10k inter-
actions with the recommender system, before the system
starts behaving intelligently. Such a system would clearly
be impractical. The main contribution of our work is that
we propose linear cascading bandits, an online learning
framework that makes learning in cascading bandits practi-
cal at scale. The key step in our approach is that we assume
that the attraction probabilities of items can be predicted
from the features of items. Features are often available in
practice or can be easily derived.

We make four contributions. First, we propose linear cas-
cading bandits, a variant of cascading bandits where we
make an additional assumption that the attraction probabil-
ities of items are a linear function of the features of items.
This assumption is the key step in designing a sample ef-
ﬁcient learning algorithm for our problem. Second, we
propose two computationally efﬁcient learning algorithms,
CascadeLinTS and CascadeLinUCB, which are motivated
by Thompson sampling (TS) [22, 3] and linear UCB [1, 23],
To the best of our knowledge, this is the ﬁrst application
of linear generalization in the cascade model under par-
tial monitoring feedback. Third, we derive an upper bound
on the regret of CascadeLinUCB and discuss why a simi-
lar upper bound should hold for CascadeLinTS. Finally,
we evaluate CascadeLinTS on a range of recommendation
problems; in the domains of restaurant, music, and movie
recommendations; and demonstrate that it performs well
even when our modeling assumptions are violated.

In Section 2, we
Our paper is organized as follows.
review the cascade model and cascading bandits.
In
Section 3, we present linear cascading bandits; propose
CascadeLinTS and CascadeLinUCB; and bound the re-
In Section 4, we evaluate
gret of CascadeLinUCB.
CascadeLinTS on several recommendation problems. We
review related work in Section 5 and conclude in Section 6.

To simplify exposition, we denote random variables by
boldface letter. We deﬁne [n] = {1, . . . , n} and denote
the cardinality of set A by |A|.

2 Background

In this section, we review the cascade model [9] and cas-
cading bandits [14].

2.1 Cascade Model

The cascade model [9] is a popular model of user behavior.
In this model, the user is recommended a list of K items
A = (a1, . . . , aK) ∈ ΠK(E), where ΠK(E) is the set of
all K-permutations of some ground set E = [L], which
is the set of all possibly recommended items. The model
is parameterized by L attraction probabilities ¯w ∈ [0, 1]E
and the user scans the list A sequentially from the ﬁrst item
a1 to the last aK. After the user examines item ak, the item
attracts the user with probability ¯w(ak), independently of
the other items. If the user is attracted by item ak, the user
clicks on it and stop examining the remaining items. If the
user is not attracted by item ak, the user examines the next
recommended item ak+1. It is easy to see that the proba-
i=1 (1 − ¯w(ai)), and
that the probability that at least one item in A is attractive
i=1(1 − ¯w(ai)). This objective is maximized by

bility that item ak is examined is Qk−1
is 1 −QK

K most attractive items.

The cascade model is surprising effective in explaining how
users scan lists of items [6]. The reason is that lower ranked
items typically do not get clicked because the user is at-
tracted by higher ranked items, and never examines the rest
of the recommended list.

2.2 Cascading Bandits

Kveton et al. [14] proposed a learning variant of the cas-
cading model, which is known as a cascading bandit. For-
mally, a cascading bandit is a tuple B = (E, P, K), where
E = [L] is a ground set of L items, P is a probability dis-
tribution over a binary hypercube {0, 1}E, and K ≤ L is
the number of recommended items.

The learning agent interacts with our problem as follows.
t=1 be an i.i.d. sequence of n weights drawn from
Let (wt)n
P , where wt ∈ {0, 1}E and wt(e) is the preference of the
user for item e at time t. More precisely, wt(e) = 1 if
and only if item e attracts the user at time t. At time t, the
agent recommends a list of K items At = (at
K) ∈
ΠK(E). The list is a function of the observations of the
agent up to time t. The user examines the list, from the
ﬁrst item at
K, and clicks on the ﬁrst attractive
item. If the user is not attracted by any item, the user does
not click on any item. Then time increases to t + 1.

1 to the last at

1, . . . , at

The reward of the agent at time t is one if and only if the
user is attracted by at least one item in At. Formally, the re-
ward at time t can be expressed as rt = f (At, wt), where
f : ΠK(E) × [0, 1]E → [0, 1] is a reward function and we
deﬁne it as:

f (A, w) = 1 −

K

Yk=1
(1 − w(ak))

for any A = (a1, . . . , aK) ∈ ΠK(E) and w ∈ [0, 1]E. The

agent at time t receives feedback:

Ct = min(cid:8)k ∈ [K] : wt(at

k) = 1(cid:9) ,

where we assume that min∅ = ∞. The feedback Ct is
the click of the user. If Ct ≤ K, the user clicks on item
Ct. If Ct = ∞, the user does not click on any item. Since
the user clicks on the ﬁrst attractive item in the list, the
observed weights of all recommended items at time t can
be expressed as a function of Ct:

wt(at

k) = 1{Ct = k}

k = 1, . . . , min{Ct, K} .

(1)

Accordingly, we say that item e is observed at time t if
e = at

k for some k ∈ [min {Ct, K}].

Let the attraction weights of items in the ground set E be
distributed independently as:

P (w) = Ye∈E

Ber(w(e); ¯w(e)) ,

where Ber(·; θ) is a Bernoulli distribution with mean θ.
Then the expected reward for list A ∈ ΠK (E), the prob-
ability that at least one item in A is satisfactory, can be
expressed as E [f (A, w)] = f (A, ¯w), and depends only on
the attraction probabilities of individual items in A. There-
fore, it is sufﬁcient to learn a good approximation to ¯w to
act optimally.

The agent’s policy is evaluated by its expected cumulative
regret:

R(n) = E" n
Xt=1

R(At, wt)# ,

(2)

where R(At, wt) = f (A∗, wt) − f (At, wt) is the instan-
taneous stochastic regret of the agent at time t and:

A∗ = arg max
A∈ΠK (E)

f (A, ¯w)

is the optimal list of items, the list that maximized the re-
ward at any time t. For simplicity of exposition, we assume
that the optimal solution, as a set, is unique.

2.3 Algorithm CascadeUCB1

Kveton et al. [14] proposed and analyzed two learn-
ing algorithm for cascading bandits, CascadeUCB1 and
CascadeKL-UCB. In this section, we review CascadeUCB1.

CascadeUCB1 belongs to the family of UCB algorithms.
The algorithm operates in three stages. First, it computes
the upper conﬁdence bounds (UCBs) Ut ∈ [0, 1]E on the
attraction probabilities of all items in E. The UCB of item
e at time t is:

Ut(e) = ˆwTt−1(e)(e) + ct−1,Tt−1(e) ,

(3)

where ˆws(e) is the average of s observed attraction weights
of item e, Tt(e) is the number of times that item e is ob-
served in t steps, and:

ct,s =p(1.5 log t)/s

is the radius of a conﬁdence interval around ˆws(e) after t
steps such that ¯w(e) ∈ [ ˆws(e) − ct,s, ˆws(e) + ct,s] holds
with high probability. Second, CascadeUCB1 recommends
a list of K items with largest UCBs:

At = arg max
A∈ΠK (E)

f (A, Ut) .

Finally, after the user provides feedback Ct, the algorithm
updates its estimates of the attraction probabilities ¯w(e)
based on the observed weights of items, which are deﬁned
in (1) for all e = at

k such that k ≤ Ct.
3 Linear Cascading Bandits

Kveton et al. [14] showed that
the n-step regret of
CascadeUCB1 is O((L − K)(1/∆) log n), where L is the
number of items in ground set E; K is the number of
recommended items; and ∆ is the gap, which measures
the sample complexity. This means that the regret in-
creases linearly with the number of items L. As a result,
CascadeUCB1 is not practical when L is large. Unfortu-
nately, this setting is common practice. For instance, con-
sider the problem of learning a personalized recommender
for 10 movies from the ground set of 100k movies. To
learn, CascadeUCB1 would need to show each movie to the
user at least once, which means that the algorithm would
require at least 10k interactions with the user to start be-
having intelligently. This is clearly impractical.

In this work, we propose practical algorithms for large-
scale cascading bandits, in the setting where L is large.
The key assumption, which allows us to learn efﬁciently, is
that we assume that the attraction probability of each item
e, ¯w(e), can be approximated by a linear combination of
some known d-dimensional feature vector xe ∈ Rd×1 and
an unknown d-dimensional parameter vector of θ∗ ∈ Rd×1,
which is shared among all items. More precisely, we as-
sume that there exists θ∗ ∈ Θ such that:

eθ∗

¯w(e) ≈ xT

(4)
for any e ∈ E. The features are problem speciﬁc and we
discuss how to construct them in Section 4.3. We pro-
pose two learning algorithms, which we call cascading
linear Thompson sampling (CascadeLinTS) and cascad-
ing linear UCB (CascadeLinUCB). We prove that when
the above linear generalization is perfect, the regret of
CascadeLinUCB is independent of L and sublinear in n.
Therefore, CascadeLinUCB is suitable for learning to rec-
ommend from large ground sets E. We also discuss why
a similar regret bound should hold for CascadeLinTS,
though we do not prove this bound formally.

3.1 Algorithms

Our learning algorithms are based on the ideas of Thomp-
son sampling [22, 3] and linear UCB [1], and motivated by
the recent work of Wen et al. [23], which proposes com-
putationally and sample efﬁcient algorithms for large-scale
stochastic combinatorial semi-bandits. The pseudocode of
both algorithms is in Algorithms 1 and 2, and we outline
them below.

Both CascadeLinTS and CascadeLinUCB represent their
past observations as a positive-deﬁnite matrix Mt ∈ Rd×d
and a vector Bt ∈ Rd×1. Speciﬁcally, let Xt be a matrix
whose rows are the feature vectors of all observed items in
t steps and Yt be a column vector of all observed attraction
weights in t steps. Then:

Mt = σ−2XT
t

Xt + Id

is the gram matrix in t steps and:

Bt = XT
t

Yt ,

where Id is a d × d identity matrix and σ > 0 is parameter
that controls the learning rate.1

Both CascadeLinTS and CascadeLinUCB operate in three
stages. First, they estimated the expected weight of each
item e based on their model of the world. CascadeLinTS
randomly samples parameter vector θt from a normal dis-
tribution, which approximates its posterior on θ∗, and then
estimates the expected weight as xT
eθt. CascadeLinUCB
computes an upper conﬁdence bound Ut(e) for each item
e. Second, both algorithms choose the optimal list At with
respect to their estimates. Finally, they receive feedback,
and update Mt and Bt using Algorithm 3.
We would like to emphasize that both CascadeLinTS and
CascadeLinUCB are computationally efﬁcient. In practice,
we would update M−1
instead of Mt. In particular, note
that:

t

Mt ← Mt + σ−2xexT

e

can be equivalently updated as:

M−1

t ← M−1

t −

M−1

t xexT
e
M−1

M−1
t xe + σ2

t

xT
e

,

t

and hence M−1
can be updated incrementally and compu-
tationally efﬁciently in O(d2) time. It is easy to to see that
the per-step time complexities of both CascadeLinTS and
CascadeLinUCB are O(L(d2 + K)).

1Ideally, σ2 should be the variance of the observation noises.
However, based on recent literature [23], we believe that both al-
gorithms will perform well for a wide range of σ2.

Algorithm 1 CascadeLinTS

Inputs: Variance σ2

// Initialization
M0 ← Id and B0 ← 0
for all t = 1, . . . , n do
¯θt−1 ← σ−2M−1
θt ∼ N (¯θt−1, M−1
t−1)
// Recommend a list of K items and get feedback
for all k = 1, . . . , K do

Bt−1

t−1

k−1} xT

eθt

1,...,at

at
k ← arg max e∈[L]−{at

end for
At ← (at
Observe click Ct ∈ {1, . . . , K,∞}
Update statistics using Algorithm 3

1, . . . , at

K)

end for

3.2 Analysis and Discussion

We ﬁrst derive a regret bound on CascadeLinUCB, under
the assumptions that (1) ¯w(e) = xT
eθ∗ for all e ∈ E and (2)
kxek2 ≤ 1 for all e ∈ E. Note that condition (2) can be
always ensured by rescaling the feature vectors. The regret
bound is detailed below.
Theorem 1. Under the above assumptions, for any σ > 0
and any

c ≥

1

σsd log(cid:18)1 +

nK

dσ2(cid:19) + 2 log (nK) + kθ∗k2,

if we run CascadeLinUCB with parameters σ and c, then

R(n) ≤ 2cKs dn log(cid:2)1 + nK
dσ2(cid:3)
log(cid:0)1 + 1
σ2(cid:1)

Note that if we choose σ = 1 and

+ 1.

c =sd log(cid:18)1 +

nK

d (cid:19) + 2 log (nK) + η,

for some constant η ≥ kθ∗k2, then R(n) ≤ ˜O (Kd√n)
where the ˜O notation hides the logarithmic factors.

The proof is in Appendix and we outline it below. First,
we deﬁne event Gt,k = {item at
k is examined in step t} for
any time t and k ∈ [K], and bound the n-step regret as
k)]# ,

1{Gt,k} [ ¯w(a∗,t

k ) − ¯w(at

R(n) ≤ E" n
Xt=1

Xk=1

K

where a∗,t
k
step t. Second, we deﬁne an event

is an optimal item in A∗ matched to item at

k in

E =n(cid:12)(cid:12)xT

e (¯θt−1 − θ∗)(cid:12)(cid:12) ≤ ckxekM−1

t−1 ∀t ≤ n, ∀e ∈ Eo ,

Algorithm 2 CascadeLinUCB

Inputs: Variance σ2, constant c (Section 3.2)

// Initialization
M0 ← Id and B0 ← 0
for all t = 1, . . . , n do
¯θt−1 ← σ−2M−1
for all e ∈ E do
Ut(e) ← min(cid:26)xT

t−1

e

end for

Bt−1

¯θt−1 + cqxT

e

M−1

t−1xe, 1(cid:27)

4 Experiments

tistical dependencies between partial monitoring and ran-
domized exploration, the analysis for CascadeLinTS is
more challenging than that of Thompson sampling in re-
lated bandit problems. Therefore, we leave the formal anal-
ysis of CascadeLinTS for future work. It is well known
that Thompson sampling tends to outperform UCB-like al-
gorithms in practice [3]. Therefore, we only empirically
evaluate CascadeLinTS.

We validate CascadeLinTS on several problems of vari-
ous sizes and from various domains. In each problem, we
conduct several experiments that demonstrate that our ap-
proach is scalable and stable with respect to its tunable pa-
rameters, the number of recommended items K and the
number of features d.

Our experimental section is organized as follows. In Sec-
tion 4.1, we outline the experiments that are conducted on
each dataset. In Section 4.2, we introduce our metrics and
baselines. In Section 4.3, we describe how we construct the
features of items E. We present our empirical results in the
rest of the section.

4.1 Experimental Setting

All of our learning problems can be viewed as follows. The
feedback of users is a matrix W ∈ {0, 1}m×L, where row
i corresponds to user i ∈ [m] and column j corresponds to
item j ∈ E. Entry (i, j) of W , Wi,j ∈ {0, 1}, indicates
that user i is attracted by item j. The user at time t, the row
of W , is chosen at random from the pool of all users. Our
goal is to learn the list of items A∗, the columns of W , that
maximizes the probability that the user at time t is attracted
by at least one recommended item.

In each of our problems, we conduct a set of experiments.
In the ﬁrst experiment, we compare CascadeLinTS to
baselines (Section 4.2) and also evaluate its scalability. We
experiment with three variants of our problems: L = 16
items, L = 256 items, and the maximum possible value
of L in a given experiment. The number of recommended
items is K = 4 and the number of features is d = 20.

In the second experiment, we show that the performance
of CascadeLinTS is robust with respect to the number of
features d, in the sense that d affects the performance but
CascadeLinTS performs reasonably well for all settings
of d. We experiment with three settings for the number of
features: d = 10, d = 20, and d = 40. The ground set
contains L = 256 items and the number of recommended
items is K = 4.

In the third experiment, we evaluate CascadeLinTS on an
interesting subset of each dataset, such as Rock Songs. The
setting of this experiment is identical to the second exper-

// Recommend a list of K items and get feedback
for all k = 1, . . . , K do

1,...,at

k−1}

at
k ← arg max e∈[L]−{at

end for
At ← (at
Observe click Ct ∈ {1, . . . , K,∞}
Update statistics using Algorithm 3

1, . . . , at

K)

Ut(e)

end for

Algorithm 3 Update of statistics in Algorithms 1 and 2

Mt ← Mt−1
Bt ← Bt−1
for all k = 1, . . . , min{Ct, K} do

k

e ← at
Mt ← Mt + σ−2xexT
Bt ← Bt + xe1{Ct = k}

e

end for

e

t−1

M−1

= qxT
where kxekM−1
t−1xe. Then we prove a high-
probability bound P (E) ≥ 1−1/nK for any c that satisﬁes
the condition of Theorem 1. Finally, we show that by con-
ditioning on E, we have

n

K

n

K

k ) − ¯w(at
k)]

1{Gt,k} [ ¯w(a∗,t

Xk=1
Xt=1
Xk=1
≤ 2c
1{Gt,k}kxat
kkM−1
≤ 2cKs dn log(cid:2)1 + nK
dσ2(cid:3)
log(cid:0)1 + 1
σ2(cid:1)

Xt=1

,

t−1

where the ﬁrst inequality follows from the deﬁnition of E
and the second inequality follows from a worst-case bound.
The bound in Theorem 1 follows from putting the above
results together.

Recent work [20, 23] demonstrated close relationships be-
tween UCB-like algorithms and Thompson sampling algo-
rithms in related bandit problems. Therefore, we believe
that a similar regret bound to that in Theorem 1 also holds
for CascadeLinTS. However, due to the complicated sta-

iment. This experiment validates that CascadeLinTS can
also learn to recommend items in the context, of a subset
of the dataset.

In the last experiment, we evaluate how the performance of
CascadeLinTS varies with the number of recommended
items K. We experiment with three settings for the number
of recommended items: K = 4, K = 8, and K = 12.
The ground set contains L = 256 items and the number of
features is d = 20.

All experiments are conducted for n = 100k steps and av-
eraged over 10 randomly initialized runs. The tunable pa-
rameter σ in CascadeLinTS is set to 1.

4.2 Metrics and Baselines

The performance of CascadeLinTS is evaluated by its ex-
pected cumulative regret, which is deﬁned in (2). In most
of our experiments, our modeling assumptions are violated.
In particular, the items are not guaranteed to attract users
independently because the attraction indicators wt(e) are
correlated across items e. The result is that:

A∗ = arg max
A∈ΠK (E)

E [f (A, w)] > arg max
A∈ΠK (E)

f (A, ¯w) .

It is NP-hard to ﬁnd A∗, because E [f (A, w)] does not de-
compose into the product of expectations as we assume in
our model (Section 2.2). However, since E [f (A, w)] is
submodular and monotone in A, a (1 − 1/e) approxima-
tion to A∗ can be computed greedily, by iteratively adding
items that attract most users that are not attracted by any
previously added item. We denote this approximation by
A∗ and use it instead of the optimal solution.

We compare CascadeLinTS to two baselines. The ﬁrst
baseline is CascadeUCB1 (Section 2.3). This baseline does
not leverage the structure of our problem and learns the at-
traction probability of each item e independently. The sec-
ond baseline is RankedLinTS (Algorithm 4). This baseline
is a variant of ranked bandits (Section 5), where the base
bandit algorithm is LinTS. This base algorithm is the same
as in CascadeLinTS. Therefore, any observed difference
in the performance of cascading and ranked bandits must
be due to the efﬁciency of using the base algorithm, and
not the algorithm itself. In this sense, our comparison of
CascadeLinTS and RankedLinTS is fair. The tunable pa-
rameter σ in RankedLinTS is also set to 1.

4.3 Features

In most recommender problems, good features of items are
rarely available. Therefore, they are typically learned from
data [13]. As an example, in movie recommendations, all
state of the art approaches are based on collaborative ﬁl-
tering rather than on the features of movies, such as movie
genres.

Algorithm 4 Ranked bandits with linear TS.

Inputs: Variance σ2

// Initialization
∀k ∈ [K] : Mk
for all t = 1, . . . , n do

0 ← Id and Bk

0 ← 0

for all k = 1, . . . , K do

¯θk
t−1 ← σ−2(Mk
t−1)−1Bk
t ∼ N (¯θk
θk
t−1, (Mk
t−1)−1)
at
k ← arg max e∈[L]−{at

t−1

end for

1,...,at

k−1} xT

eθk
t

K)

1, . . . , at

// Recommend a list of K items and get feedback
At ← (at
Observe click Ct ∈ {1, . . . , K,∞}
// Update statistics
∀k ∈ [K] : Mk
∀k ∈ [K] : Bk
for all k = 1, . . . , min{Ct, K} do

t ← Mk
t ← Bk

t−1

t−1

k

e ← at
t ← Mk
t ← Bk

Mk
Bk

end for

t + σ−2xexT
e
t + xe1{Ct = k}

end for

Motivated by the successes of collaborative ﬁltering in rec-
ommender systems, we derive the features of our items us-
ing low-rank matrix factorization. In particular, let W ∈
{0, 1}m×L be our feedback matrix for m users and L
items. We randomly divide the rows of W into two matri-
ces, training matrix Wtrain ∈ {0, 1}(m/2)×L and test matrix
Wtest ∈ {0, 1}(m/2)×L. We use Wtrain to learn the features
of items and Wtest in place of W to evaluate our learning al-
gorithms. Most existing real-world recommender systems
already have some data about their users. Such data can be
used to construct Wtrain.
Let Wtrain ≈ U ΣV T be rank-d truncated SVD of Wtrain,
where U ∈ R(m/2)×d, Σ ∈ Rd×d, and V ∈ RL×d. Then
the features of items are the rows of V Σ. Speciﬁcally, for
each item e ∈ E and feature i ∈ [d], xe(i) = Ve,iΣi,i.
4.4 Restaurant Recommendations

Our dataset is from Yelp Dataset Challenge2. This dataset
has ﬁve parts, including business information, checkin in-
formation, review information, tip information, and user
information. We only consider the business and review
information. The dataset contains 78k businesses, out of
which 11k are restaurants; and 2.2M reviews written by
550k users. We extract L = 3k most reviewed restaurants

2https://www.yelp.com/dataset_challenge

t
e
r
g
e
R

4000
3500
3000
2500
2000
1500
1000
500
0
0

t
e
r
g
e
R

1400
1200
1000
800
600
400
200
0
0

t
e
r
g
e
R

1400
1200
1000
800
600
400
200
0
0

L =16, K =4

CascadeUCB1
CascadeLinTS
RankedLinTS

20k

40k

60k

Step n

L =16, K =4

CascadeUCB1
CascadeLinTS
RankedLinTS

t
e
r
g
e
R

80k

100k

t
e
r
g
e
R

20k

40k

60k

80k

100k

Step n

L =16, K =4

CascadeUCB1
CascadeLinTS
RankedLinTS

t
e
r
g
e
R

20k

40k

60k

80k

100k

Step n

4000
3500
3000
2500
2000
1500
1000
500
0
0

1400
1200
1000
800
600
400
200
0
0

1400
1200
1000
800
600
400
200
0
0

Yelp Restaurant Dataset

L =256, K =4

CascadeUCB1
CascadeLinTS
RankedLinTS

20k

40k

60k

Step n

t
e
r
g
e
R

80k

100k

Million Song Dataset

L =256, K =4

CascadeUCB1
CascadeLinTS
RankedLinTS

t
e
r
g
e
R

20k

40k

60k

80k

100k

Step n

MovieLens 1M Dataset

L =256, K =4

CascadeUCB1
CascadeLinTS
RankedLinTS

t
e
r
g
e
R

20k

40k

60k

80k

100k

Step n

4000
3500
3000
2500
2000
1500
1000
500
0
0

1400
1200
1000
800
600
400
200
0
0

1400
1200
1000
800
600
400
200
0
0

L =3000, K =4

CascadeUCB1
CascadeLinTS
RankedLinTS

20k

40k

60k

80k

100k

Step n

L =10000, K =4

CascadeUCB1
CascadeLinTS
RankedLinTS

20k

40k

60k

80k

100k

Step n

L =3952, K =4

CascadeUCB1
CascadeLinTS
RankedLinTS

20k

40k

60k

80k

100k

Step n

Figure 1: The n-step regret of CascadeUCB1, CascadeLinTS and RankedLinTS on three problems. We vary the number
of items in the ground set E, from L = 16 to the maximum value in each problem.

and m = 20k most reviewing users.

to d = 10, the regret improves and is roughly halved.

Our objective is to maximize the probability that the user
is attracted by at least one recommended restaurant. We
build the model of users from past review data and assume
that the user is attracted by the restaurant if the user re-
viewed this restaurant before. This indicates that the user
visited the restaurant at some point in time, likely because
the restaurant attracted the user at that time.

4.4.1 Results

The results of our ﬁrst experiment are reported in Fig. 1.
When the ground set is small, L = 16, all compared
methods perform similarly.
In particular, the regret of
CascadeLinTS is similar to that of RankedLinTS. The re-
gret of CascadeUCB1 is about two times larger than that of
CascadeLinTS. As the size of the ground set increases,
the gap between CascadeLinTS and the other methods
increases.
In particular, when L = 3k, the regret of
CascadeUCB1 is orders of magnitude larger than that of
CascadeLinTS, and the regret of RankedLinTS is almost
three times larger.

In the second experiment (Fig. 2a), we observe that
CascadeLinTS performs well for all settings of d. When
the number of features doubles to d = 40, the regret
roughly doubles. When the number of features is halved

In the third experiment (Fig. 2b), CascadeLinTS is eval-
uated on the subset of American Restaurants. This is the
largest restaurant category in our dataset. We observe that
CascadeLinTS can learn for any number of features d,
similarly to Fig. 2a.

In the last experiment (Fig. 2c), we observe that the re-
gret of CascadeLinTS increases with the number of rec-
ommended items, from K = 4 to K = 8. This result is
surprising and seems to contradict to Kveton et al. [14],
who ﬁnd both theoretically and empirically that the regret
in cascading bandits decreases with the number of recom-
mended items K. We investigate this further and plot the
cumulative reward of CascadeLinTS in Fig. 2d. The re-
ward increases with K, which is expected and validates that
CascadeLinTS learns better policies for larger K. There-
fore, the increase in the regret in Fig. 2c must be due to
the fact that the expected reward of the optimal solution,
f (A∗, ¯w), increases faster with K than that of the learned
policies. We believe that the optimal solutions for larger
K are harder to learn because our modeling assumptions
are violated. In particular, the linear generalization in (4) is
imperfect and the items in E are not guaranteed to attract
users independently.

d =10
d =20
d =40

20k

d =10
d =20
d =40

40k

60k

Step n

4000
3500
3000
2500
2000
1500
1000
500
0
0

t
e
r
g
e
R

80k

100k

2000

1500

1000

500

t
e
r
g
e
R

d =10
d =20
d =40

20k

d =10
d =20
d =40

t
e
r
g
e
R

2500
2000
1500
1000
500
0
0

2500

2000

t
e
r
g
e
R

1500

1000

500

0
0

2500

2000

t
e
r
g
e
R

1500

1000

500

0
0

Yelp Restaurant Dataset

1600
1400
1200
1000
800
600
400
200
0
0

t
e
r
g
e
R

80k

100k

K =4
K =8
K =12

20k

Million Song Dataset

100k
80k
60k
40k
20k
0
0

d
r
a
w
e
R

80k

100k

40k

60k

Step n

40k

60k

Step n

t
e
r
g
e
R

3000

2500

2000

1500

1000

500

0
0

K =4
K =8
K =12

d
r
a
w
e
R

20k

40k

60k

80k

100k

Step n

40k
35k
30k
25k
20k
15k
10k
5k
0
0

K =4
K =8
K =12

20k

K =4
K =8
K =12

40k

60k

80k

100k

Step n

20k

40k

60k

80k

100k

Step n

20k

40k

60k

80k

100k

0
0

20k

40k

60k

80k

100k

Step n

Step n

MovieLens 1M Dataset

d =10
d =20
d =40

d =10
d =20
d =40

1000

800

600

400

200

t
e
r
g
e
R

t
e
r
g
e
R

20k

40k

60k

80k

100k

0
0

20k

40k

60k

80k

100k

Step n

(a)

Step n

(b)

3000

2500

2000

1500

1000

500

0
0

K =4
K =8
K =12

K =4
K =8
K =12

100k

80k

60k

40k

20k

d
r
a
w
e
R

20k

40k

60k

80k

100k

0
0

20k

40k

60k

80k

100k

Step n

(c)

Step n

(d)

Figure 2: a. The n-step regret of CascadeLinTS for varying number of features d. b. The n-step regret of CascadeLinTS
in a subset of each dataset for varying number of features d. c. The n-step regret of CascadeLinTS for varying number of
recommended items K. d. The n-step reward of CascadeLinTS for varying number of recommended items K.

4.5 Million Song Recommendation

Million Song Dataset3 is a collection of audio features and
metadata for a million contemporary pop songs. Instead
of storing any audio, the dataset consists of features de-
rived from the audio, user-song proﬁle data, and genres of
songs. The purpose of this dataset is to encourage research
on algorithms for learning from commercial-size datasets.
We extract L = 10k most popular songs from this dataset,
as measured by the number of song-listening events; and
m = 400k most active users, as measured by the number
of song-listening events.

Our objective is to maximize the probability that the user
is attracted with at least one recommended song and plays
it. We build the model of users from their past listening
patterns and assume that the user is attracted by the song if
the user listened to this song before. This indicates that the
user was attracted by the song at some point in time.

4.5.1 Results

The results of our ﬁrst experiment are reported in Fig. 1.
Similarly to Section 4.4, we observe that when the ground
set is small, L = 16, the regret of all compared meth-
ods is similar. As the size of the ground set increases, the
gap between CascadeUCB1 and the rest of the methods in-
creases, and the regret of CascadeUCB1 is orders of mag-

nitude larger than that of CascadeLinTS. The regret of
CascadeLinTS is similar to that of RankedLinTS for all
settings of L.

We report the regret of CascadeLinTS for various num-
bers of features d, on the whole dataset and its subset of
Rock Songs, in Fig. 2a and 2b, respectively. Similarly to
Section 4.4, we observe that CascadeLinTS performs well
for all settings of d. The lowest regret in both experiments
is achieved at d = 10.

In the last experiment (Fig. 2c), we observe that the re-
gret of CascadeLinTS increases with the number of rec-
ommended items K. As in Section 4.4, we observe that the
cumulative reward of our learned policies increases with
K. Therefore, the increase in the regret must be due to
the fact that the expected reward of the optimal solution,
f (A∗, ¯w), increases faster with K than that of the learned
policies. This is due to the mismatch between our model
and real-world data.

4.6 Movie Recommendation

MovieLens datasets4 contain the ratings of users for movies
from the MovieLens website. The datasets come in dif-
ferent sizes and we choose MovieLens 1M for our experi-
ments. This dataset contains 1M anonymous ratings of 4k
movies by 6k users who joined MovieLens in 2000.

3http://labrosa.ee.columbia.edu/millionsong/

4http://grouplens.org/datasets/movielens/

We build the model of users from their historical ratings.
The ratings are on a 5-star scale and we assume that the user
is attracted by a movie if the user rates it with more than 3
stars. Therefore, the feedback matrix is deﬁned as Wi,j =
1{user i rates movie j with more than 3 stars}. Our goal is
to maximize the probability of recommending at least one
attractive movie.

4.6.1 Results

The results of our ﬁrst experiment are reported in Fig. 1.
Similarly to Section 4.4, we observe that the regret of
all compared methods is similar when the ground set is
small, L = 16. The gap between CascadeUCB1 and the
rest of the methods increases when the size of the ground
set increases.
In particular, the regret of CascadeUCB1
is orders of magnitude larger than that of CascadeLinTS.
The regret of CascadeLinTS is always lower than that of
RankedLinTS for all settings of L.

We report the regret of CascadeLinTS for various num-
bers of features d, on the whole dataset and its subset of
Adventures, in Fig. 2a and 2b, respectively. Similarly to
Sections 4.4 and 4.5, we observe that CascadeLinTS per-
forms well for all settings of d. The lowest regret in both
experiments is achieved at d = 20.

In the last experiment (Fig. 2c), we observe that the regret
of CascadeLinTS increases with the number of recom-
mended items K. As in Sections 4.4 and 4.5, we observe
that the cumulative reward of our learned policies increases
with K. Therefore, the increase in the regret must be due
to the fact that the expected reward of the optimal solution,
f (A∗, ¯w), increases faster with K than that of the learned
policies.

5 Related Work

Our work is closely related to cascading bandits [14, 7],
which are learning variants of the cascade model of user
behavior [9]. The key difference is that we assume that the
attraction weights of items are a linear function of known
feature vectors, which are associated with each item; and an
unknown parameter vector, which is learned. This leads to
very efﬁcient learning algorithms whose regret is sublinear
in the number of items L. We compare CascadeLinTS to
CascadeUCB1, one of the proposed algorithms by Kveton
et al. [14], in Section 4.

Ranked bandits [19] are a popular approach in learning to
rank. The key idea in ranked bandits is to model each po-
sition in the recommended list as an independent bandit
problem, which is then solved by a base bandit algorithm.
The solutions in ranked bandits are (1 − 1/e) approximate
and their regret grows linearly with the number of recom-
mended items K. On the other hand, ranked bandits do not
assume that items attract the user independently. Slivkins

et al. [21] proposed contextual ranked bandits. We com-
pare CascadeLinTS to contextual ranked bandits with lin-
ear generalization in Section 4.

Our learning problem is a partial monitoring problem
where we do not observe the attraction weights of all rec-
ommended items. Bartok et al. [4] studied general partial
monitoring problems and proposed learning algorithms for
solving them. The algorithm of Bartok et al. [4] scales

at least linearly with the number of actions, which is (cid:0) L
K(cid:1)

in our setting. Therefore, the algorithm is impractical for
large L and moderate K. Agrawal et al. [2] studied a vari-
ant of partial monitoring where the reward is observed. The
algorithm of Agrawal et al. [2] cannot be applied to our
problem because the algorithm assumes a ﬁnite parameter
set. Lin et al. [18] and Kveton et al. [16] studied combi-
natorial partial monitoring. Our feedback model is similar
to that of Kveton et al. [16]. Therefore, we believe that our
algorithm and analysis can be relatively easily generalized
to combinatorial action sets.

Our learning problem is combinatorial as we learn K most
attractive items out of L candidate items. In this sense, our
work is related to stochastic combinatorial bandits, which
are frequently studied with a linear reward function and
semi-bandit feedback [10, 5, 15, 17, 23, 8]. Our work
differs from these approaches in both the reward function
and feedback. Our reward function is a non-linear function
of unknown parameters. Our feedback model is less than
semi-bandit, because the learning agent does not observe
the attraction weights of all recommended items.

6 Conclusions

In this work, we propose linear cascading bandits, a frame-
work for learning to recommend in the cascade model at
scale. The key assumption in linear cascading bandits is
that the attraction probabilities of items are a linear func-
tion of the features of items, which are known; and an un-
known parameter vector, which is unknown and we learn
it. We design two algorithms for solving our problem,
CascadeLinTS and CascadeLinUCB. We bound the regret
of CascadeLinUCB and suggest that a similar regret bound
can be proved for CascadeLinTS. We comprehensively
evaluate CascadeLinTS on a range of recommendation
problems and compare it to several baselines. We report or-
ders of magnitude improvements over learning algorithms
that do not leverage the structure of our problem, the fea-
tures of items. We observe empirically that CascadeLinTS
performs very well.

We leave open several questions of interest. For instance,
we only bound the regret of CascadeLinUCB. Based on the
existing work [23], we believe that a similar regret bound
can be proved for CascadeLinTS. Moreover, note that our
analysis of CascadeLinUCB is under the assumption that
items attract the user independently and that the linear gen-

eralization is perfect. Both of these assumptions tend to be
violated in practice. Our current analysis cannot explain
this behavior and we leave it for future work.

The main limitation of the cascade model [9] is that the
user clicks on at most one item. This assumption is often
violated in practice. Recently, Katariya et al. [12] proposed
a generalization of cascading bandits to multiple clicks, by
proposing a learning variant of the dependent click model
[11]. We strongly believe that our results can be general-
ized to this setting and leave this for future work.

References

[1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepes-
vari. Improved algorithms for linear stochastic ban-
dits. In Advances in Neural Information Processing
Systems 24, pages 2312–2320, 2011.

[2] Rajeev Agrawal, Demosthenis Teneketzis,

and
Venkatachalam Anantharam. Asymptotically efﬁ-
cient adaptive allocation schemes for controlled i.i.d.
processes: Finite parameter space. IEEE Transactions
on Automatic Control, 34(3):258–267, 1989.

[3] Shipra Agrawal and Navin Goyal. Analysis of
Thompson sampling for the multi-armed bandit prob-
lem. In Proceeding of the 25th Annual Conference on
Learning Theory, pages 39.1–39.26, 2012.

[4] Gabor Bartok, Navid Zolghadr, and Csaba Szepes-
vari. An adaptive algorithm for ﬁnite stochastic par-
tial monitoring. In Proceedings of the 29th Interna-
tional Conference on Machine Learning, 2012.

[5] Wei Chen, Yajun Wang, and Yang Yuan. Combinato-
rial multi-armed bandit: General framework, results
and applications. In Proceedings of the 30th Interna-
tional Conference on Machine Learning, pages 151–
159, 2013.

[6] Aleksandr Chuklin, Ilya Markov, and Maarten de Ri-
jke. Click Models for Web Search. Morgan & Clay-
pool Publishers, 2015.

[7] Richard Combes, Stefan Magureanu, Alexandre
Proutiere, and Cyrille Laroche. Learning to rank: Re-
gret lower bounds and efﬁcient algorithms. In Pro-
ceedings of the 2015 ACM SIGMETRICS Interna-
tional Conference on Measurement and Modeling of
Computer Systems, 2015.

[9] Nick Craswell, Onno Zoeter, Michael Taylor, and
Bill Ramsey. An experimental comparison of click
position-bias models. In Proceedings of the 1st ACM
International Conference on Web Search and Data
Mining, pages 87–94, 2008.

[10] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain.
Combinatorial network optimization with unknown
variables: Multi-armed bandits with linear rewards
and individual observations. IEEE/ACM Transactions
on Networking, 20(5):1466–1478, 2012.

[11] Fan Guo, Chao Liu, and Yi Min Wang. Efﬁcient
In Proceed-
multiple-click models in web search.
ings of the 2nd ACM International Conference on Web
Search and Data Mining, pages 124–131, 2009.

[12] Sumeet Katariya, Branislav Kveton, Csaba Szepes-
vari, and Zheng Wen. DCM bandits: Learning to rank
with multiple clicks. CoRR, abs/1602.03146, 2016.

[13] Yehuda Koren, Robert Bell, and Chris Volinsky. Ma-
trix factorization techniques for recommender sys-
tems. IEEE Computer, 42(8):30–37, 2009.

[14] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and
Azin Ashkan. Cascading bandits: Learning to rank in
the cascade model. In Proceedings of the 32nd Inter-
national Conference on Machine Learning, 2015.

[15] Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda
Eydgahi, and Brian Eriksson. Matroid bandits: Fast
combinatorial optimization with learning.
In Pro-
ceedings of the 30th Conference on Uncertainty in
Artiﬁcial Intelligence, pages 420–429, 2014.

[16] Branislav Kveton, Zheng Wen, Azin Ashkan, and
Csaba Szepesvari. Combinatorial cascading bandits.
In Advances in Neural Information Processing Sys-
tems 28, pages 1450–1458, 2015.

[17] Branislav Kveton, Zheng Wen, Azin Ashkan, and
Csaba Szepesvari. Tight regret bounds for stochas-
tic combinatorial semi-bandits.
In Proceedings of
the 18th International Conference on Artiﬁcial Intel-
ligence and Statistics, 2015.

[18] Tian Lin, Bruno Abrahao, Robert Kleinberg, John
Lui, and Wei Chen. Combinatorial partial monitor-
ing game with linear feedback and its applications. In
Proceedings of the 31st International Conference on
Machine Learning, pages 901–909, 2014.

[8] Richard Combes, Mohammad Sadegh Talebi,
Alexandre Proutiere, and Marc Lelarge. Combi-
natorial bandits revisited.
In Advances in Neural
Information Processing Systems 28, pages 2107–
2115, 2015.

[19] Filip Radlinski, Robert Kleinberg, and Thorsten
Joachims. Learning diverse rankings with multi-
armed bandits.
In Proceedings of the 25th Interna-
tional Conference on Machine Learning, pages 784–
791, 2008.

[20] Daniel Russo and Benjamin Van Roy. Learning to
optimize via posterior sampling. Mathematics of Op-
erations Research, 39(4):1221–1243, 2014.

[21] Aleksandrs Slivkins, Filip Radlinski, and Sreenivas
Gollapudi. Ranked bandits in metric spaces: Learn-
ing diverse rankings over large document collections.
Journal of Machine Learning Research, 14(1):399–
436, 2013.

[22] William. R. Thompson. On the likelihood that one
unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3-4):285–
294, 1933.

[23] Zheng Wen, Branislav Kveton, and Azin Ashkan.
Efﬁcient learning in large-scale combinatorial semi-
bandits.
In Proceedings of the 32nd International
Conference on Machine Learning, 2015.

Appendix

A Proof for Theorem 1

A.1 Notations

We start by deﬁning some notations. For each time t, we deﬁne a random permutation (a∗,t
At as follows: for any k = 1, . . . , K, if at
arbitrarily. Notice that under this random permutation, we have:

k ∈ A∗, then we set a∗,t

K ) of A∗ based on
k. The remaining optimal items are positioned

1 , . . . , a∗,t

k = at

¯w(a∗,t

k ) ≥ ¯w(at
k)

and Ut(at

k) ≥ Ut(a∗,t

k ) ∀k = 1, 2, . . . , K

1, . . . , at

1 , . . . , a∗,t

K) and the permutation (a∗,t

Moreover, we use Ht to denote the “history” (rigorously speaking, σ-algebra) by the end of time t. Then both
K ) of A∗ are Ht−1-adaptive. In other words, they are condition-
At = (at
ally deterministic at the beginning of time t. To simplify the notation, in this paper, we use Et[·] to denote E[·|Ht−1] when
appropriate.
When appropriate, we also use h·,·i to denote the inner product of two vectors. Speciﬁcally, for two vectors u and v with
the same dimension, we use hu, vi to denote uTv.
A.2 Regret Decomposition

We ﬁrst prove the following technical lemma:
Lemma 1. For any B = (b1, . . . , bK) ∈ ℜK and C = (c1, . . . , cK) ∈ ℜK, we have
i=1 bii × [bk − ck] ×hQK

QK
k=1 bk −QK

k=1 ck =PK

k=1hQk−1

j=k+1 cji .

Proof. Notice that

Thus we have

i=1 bii × [bk − ck] ×hQK
i=1 bii ×hQK

j=k+1 cji
j=k+1 cji −hQk−1

k=1 ck.

k=1hQk−1
PK
k=1nhQk
=PK
k=1 bk −QK
=QK

i=1 bii ×hQK

j=k cjio

(a)

R(At, wt) =f (A∗, wt) − f (At, wt)
k)) −QK
i=1 (1 − wt(at
i=1 (1 − wt(at

k=1 (1 − wt(at
k=1hQk−1
k=1hQk−1

=QK
= PK
≤PK

(b)

k=1(cid:0)1 − wt(a∗,t
k )(cid:1)
i))i(cid:2)wt(a∗,t
k)(cid:3)hQK
k ) − wt(at
i))i(cid:2)wt(a∗,t
k ) − wt(at
k)(cid:3) ,

j )(cid:1)i
j=k+1(cid:0)1 − wt(a∗,t

(5)

where equality (a) is based on Lemma 1 and inequality (b) is based on the fact thatQK

that At and the permutation (a∗,t
thus we have

1 , . . . , a∗,t

j=k+1(cid:0)1 − wt(a∗,t
j )(cid:1) ≤ 1. Recall
6= at

i for all i < k,

k

K ) of A∗ are deterministic conditioning on Ht−1, and a∗,t
k)(cid:3)i
k ) − wt(at
k ) − wt(at
k)(cid:3)
k ) − ¯w(at
k)(cid:3) .

i))i(cid:2)wt(a∗,t
i))i Et(cid:2)wt(a∗,t
i))i(cid:2) ¯w(a∗,t

i=1 (1 − wt(at
i=1 (1 − wt(at
i=1 (1 − wt(at

k=1hQk−1
EthQk−1
EthQk−1

Et[R(At, wt)] ≤ EthPK
= PK
= PK

k=1

k=1

For any t ≤ n and any e ∈ E, we deﬁne event

k=1

k=1

k ) − ¯w(at

i)). Thus, we have

t=1PK

i=1 (1 − wt(at

Hence, from the tower property, we have

We further deﬁne event E as

notice that 1{Gt,k} =Qk−1

k ) − ¯w(at
k)(cid:3) .
k)(cid:3)i .
t−1xe, ∀e ∈ E, ∀t ≤ n(cid:27) ,

k is examined in episode t(cid:9) ,
Et[1{Gt,k} ](cid:2) ¯w(a∗,t
1{Gt,k} (cid:2) ¯w(a∗,t

Gt,k =(cid:8)item at
Et[Rt] ≤PK
R(n) ≤ EhPn
E =(cid:26)(cid:12)(cid:12)hxe, ¯θt−1 − θ∗i(cid:12)(cid:12) ≤ cqxT
and ¯E as the complement of E. Then we have
≤ P (E)EhPn
+P ( ¯E)EhPn
k)(cid:3)(cid:12)(cid:12)(cid:12)Ei + nKP ( ¯E),
≤ EhPn
t=1PK
k)(cid:3) ≤ 1. Notice that from the deﬁnition of event E, we have
t−1xe ∀e ∈ E, ∀t ≤ n

1{Gt,k} (cid:2) ¯w(a∗,t
1{Gt,k} (cid:2) ¯w(a∗,t
k ) − ¯w(at

1{Gt,k} (cid:2) ¯w(a∗,t

k ) − ¯w(at
k ) − ¯w(at

t=1PK
t=1PK

k)(cid:3)(cid:12)(cid:12)(cid:12)Ei
k)(cid:3)(cid:12)(cid:12)(cid:12)
¯Ei

¯w(e) = hxe, θ∗i ≤ hxe, ¯θt−1i + cqxT

e M −1

e M −1

R(n)

k=1

k=1

k=1

(a)

(b)

where inequality (a) is based on the law of total probability, and the inequality (b) is based on the naive bounds (1) P (E) ≤ 1
and (2) 1{Gt,k} (cid:2) ¯w(a∗,t

k ) − ¯w(at

under event E. Moreover, since ¯w(e) ≤ 1 by deﬁnition, we have ¯w(e) ≤ Ut(e) for all e ∈ E and all t ≤ n under event E.
Hence under event E, we have

¯w(at

k) ≤ ¯w(a∗,t

k ) ≤ Ut(a∗,t

k ) ≤ Ut(at

k) ≤ hxat

k

, ¯θt−1i + cqxT

at
k

M −1

t−1xat

k

∀t ≤ n.

(6)

(7)

(8)

Thus we have

¯w(a∗,t

k ) − ¯w(at
k)

where inequality (a) follows from the fact that ¯w(a∗,t
the fact that hxat

, ¯θt−1 − θ∗i ≤ cqxT

t−1xat

M −1

at
k

k

k

(a)

k

(b)

at
k

at
k

M −1

t−1xat

, ¯θt−1 − θ∗i + cqxT

≤hxat
≤ 2cqxT
, ¯θt−1i + cqxT
k ) ≤ hxat
under event E. Thus, we have

at
k

,

k

k

M −1

t−1xat

k

M −1

t−1xat

k

and inequality (b) follows from

R(n) ≤ 2cEhPn

t=1PK

k=1

1{Gt,k}qxT

at
k

M −1

t−1xat

k(cid:12)(cid:12)(cid:12)Ei + nKP ( ¯E).

Deﬁne Kt = min{Ct, K}, notice that
1{Gt,k}qxT
PK
R(n) ≤ 2cEhPn

Thus, we have

k=1

M −1

t−1xat

k

at
k

t=1PKt

k=1qxT

at
k

In the next two subsections, we will provide a worst-case bound onPn

=PKt

M −1

t−1xat

k

.

at
k

M −1

t−1xat

k=1qxT
k(cid:12)(cid:12)(cid:12)Ei + nKP ( ¯E).
k=1qxT
t=1PKt

at
k

(9)

and a bound on P ( ¯E).

M −1

t−1xat

k

at
k

M −1

t−1xat

t=1PKt

k=1qxT
A.3 Worst-Case Bound onPn
k ≤ Kr dn log[1+ nK
k=1qxT
Lemma 2. Pn
Proof. To simplify the exposition, we deﬁne zt,k =qxT

t=1PKt

t−1xat

log(1+ 1

M −1

at
k

at
k

k

dσ2 ]
σ2 )

.

M −1

t−1xat

k

for all (t, k) s.t. k ≤ Kt. Recall that

Mt = Mt−1 +

1
σ2

Kt

Xk=1

xat

k

xT
at
k

1
σ2 M

− 1
t−1xat

2

k

xT

at
k

M

2

− 1

t−1(cid:19) M

1
2

t−1(cid:21)

Thus, for all (t, k) s.t. k ≤ Kt, we have that
det [Mt] ≥ det(cid:20)Mt−1 +
1
σ2 xat
= det [Mt−1] det(cid:20)I +
= det [Mt−1](cid:18)1 +
1
σ2 xT

k

at

xT

k(cid:21) = det(cid:20)M
1
σ2 M

− 1
t−1xat

xT
at
k

k

2

1
2

t−1(cid:18)I +
t−1(cid:21)

M

− 1

2

M −1

t−1xat

at
k

Thus, we have

(det [Mt])Kt ≥ (det [Mt−1])Kt

Since det [Mt] ≥ det [Mt−1] and Kt ≤ K, we have

(det [Mt])K ≥ (det [Mt−1])K

So we have

(det [Mn])K ≥ (det [M0])K

since M0 = I. On the other hand, we have that

n

Yt=1

Kt

Yk=1 1 +

z2
t,k

σ2 ! .

k(cid:19) = det [Mt−1] 1 +
σ2 ! .

z2
t,k

Kt

Kt

Yk=1 1 +
Yk=1 1 +
σ2 ! =
Yt=1

z2
t,k

n

z2
t,k

σ2 ! .
Yk=1 1 +

Kt

z2
t,k

σ2 ! ,

xat

k

xT
at

k! = d +

1
σ2

n

Kt

Xt=1

Xk=1

kxat

kk2

2 ≤ d +

nK
σ2 ,

kk2 ≤ 1 and Kt ≤ K. From the trace-determinant inequality, we

≥(cid:20) 1

d

trace (Mn)(cid:21)dK

≥ [det(Mn)]K ≥

dK log(cid:20)1 +

M −1

0 xat

k

1

log(cid:0)1 + 1
σ2(cid:1)

n

Kt

nK

Xk=1

log 1 +

dσ2(cid:21) ≥
Xt=1
2 ≤ 1, thus we have z2
kk2
= kxat
log 1 +
σ2 ! ≤
Xt=1
Xk=1

z2
t,k

Kt

n

y

log(cid:16)1+
log(cid:16)1+ 1

σ2 (cid:17)
σ2 (cid:17)

z2
t,k

σ2 ! .

n

Kt

Yk=1 1 +
Yt=1
σ2 ! .

z2
t,k

2
t,k

z

log(cid:18)1+
log(1+ 1

σ2 (cid:19)
σ2 )

t,k ≤
dK log(cid:2)1 + nK
dσ2(cid:3)
log(cid:0)1 + 1
σ2(cid:1)

(10)

. 5 Hence we have

.

5Notice that for any y ∈ [0, 1], we have y ≤

= h(y). To see it, notice that h(y) is a strictly concave function, and

trace (Mn) = trace I +

Xk=1
where the last inequality follows from the fact that kxat
have 1

d , thus we have

Xt=1

1
σ2

n

Kt

1

d trace (Mn) ≥ [det(Mn)]
dσ2(cid:21)dK
nK

(cid:20)1 +

Taking the logarithm, we have

Notice that z2

t,k = xT

at
k

M −1

t−1xat

k ≤ xT

at
k

n

Xt=1

Kt

Xk=1

z2
t,k ≤

h(0) = 0 and h(1) = 1.

Finally, from Cauchy-Schwarz inequality, we have that

n

Xt=1

Kt

Xk=1

zt,k ≤

√nKvuut

n

Xt=1

Kt

Xk=1

z2

t,k ≤ Ks dn log(cid:2)1 + nK
dσ2(cid:3)
log(cid:0)1 + 1
σ2(cid:1)

.

A.4 Bound on P ( ¯E)
Lemma 3. For any σ > 0, any δ ∈ (0, 1), and any

c ≥

1

σsd log(cid:18)1 +

nK

dσ2(cid:19) + 2 log(cid:18) 1

δ(cid:19) + kθ∗k2,

we have P ( ¯E) ≤ δ.
Proof. We start by deﬁning some useful notations. For any t = 1, 2, . . . , any k = 1, 2, . . . , Kt, we deﬁne

One key observation is that ηt,k’s form a Martingale difference sequence (MDS).6 Moreover, since ηt,k’s are bounded in
[−1, 1] and hence they are conditionally sub-Gaussian with constant R = 1. We further deﬁne that

t

Kτ

ηt,k = wt(at

k) − ¯w(at
k).

Vt =σ2Mt = σ2I +

St =

t

Kτ

Xτ =1

Xk=1

xaτ

k

Xτ =1

Xk=1
ηt,k = Bt −

xaτ

k

xT

aτ
k

t

Kτ

Xτ =1

Xk=1

xaτ

k

¯w(at

k) = Bt −" t
Xτ =1

Kτ

Xk=1

xaτ

k

xT
aτ

k# θ∗

As we will see later, we deﬁne Vt and St to use the “self normalized bound” developed in [1] (see Algorithm 1 of [1]).
Notice that

Mt ¯θt =

1
σ2 Bt =

1
σ2

St +

1

σ2 " t
Xτ =1

Kτ

Xk=1

xaτ

k

xT

aτ

k# θ∗ =

1
σ2

St + [Mt − I] θ∗,

where the last equality is based on the deﬁnition of Mt. Hence we have

Thus, for any e ∈ E, we have

¯θt − θ∗ = M −1

St − θ∗(cid:21) .

σ2

t (cid:20) 1
St − θ∗(cid:21)(cid:12)(cid:12)(cid:12)(cid:12)

St − θ∗kM −1

t

(cid:12)(cid:12)hxe, ¯θt − θ∗i(cid:12)(cid:12) =(cid:12)(cid:12)(cid:12)(cid:12)

t

1
σ2

e M −1
xT

StkM −1

σ2
1
σ2

t (cid:20) 1
t (cid:20)k
≤kxekM −1
= kθ∗k2, and k 1
σ2 StkM −1
t (cid:20) 1
(cid:12)(cid:12)hxe, ¯θt − θ∗i(cid:12)(cid:12) ≤ kxekM −1

≤ kxekM −1
t k
t (cid:21) ,
+ kθ∗kM −1
σkStkV−1
+ kθ∗k2(cid:21) .

σkStkV−1

t

0

where the ﬁrst inequality follows from the Cauchy-Schwarz inequality and the second inequality follows from the triangle
), so we
inequality. Notice that kθ∗kM −1
have

t ≤ kθ∗kM −1

t = σ2V−1

(since M −1

= 1

t

t

t

(11)

Notice that the above inequality always holds. We now provide a high-probability bound on kStkV−1
based on “self
normalized bound” proposed in [1]. From Theorem 1 of [1], we know that for any δ ∈ (0, 1), with probability at least
1 − δ, we have

t

kStkV−1

t ≤s2 log(cid:18) det(Vt)1/2 det(V0)−1/2

δ

(cid:19) ∀t = 0, 1, . . .

6Notice that the notion of “time” is indexed by the pair (t, k), and follows the lexicographical order.

Notice that det(V0) = det(σ2I) = σ2d. Moreover, from the trace-determinant inequality, we have

trace (Vt)

= σ2 +

[det(Vt)]1/d ≤

Xk=1
where the second inequality follows from the assumption that kxat
kk2 ≤ 1 and Kτ ≤ K, and the last inequality follows
from t ≤ n. Thus, with probability at least 1 − δ, we have
nK

2 ≤ σ2 +

Xτ =1

kxat

kk2

d

,

tK
d ≤ σ2 +

nK
d

t

Kτ

1
d

kStkV−1

t ≤sd log(cid:18)1 +

dσ2(cid:19) + 2 log(cid:18) 1

δ(cid:19) ∀t = 0, 1, . . . , n − 1.

That is, with probability at least 1 − δ, we have

t " 1
(cid:12)(cid:12)hxe, ¯θt − θ∗i(cid:12)(cid:12) ≤ kxekM −1

σsd log(cid:18)1 +

nK

dσ2(cid:19) + 2 log(cid:18) 1

δ(cid:19) + kθ∗k2#

for all t = 0, 1, . . . , n − 1 and ∀e ∈ E. Recall that by deﬁnition of event E, the above inequality implies that, if

c ≥

1

σsd log(cid:18)1 +

then P (E) ≥ 1 − δ. That is, P ( ¯E) ≤ δ.
A.5 Conclude the Proof

nK

dσ2(cid:19) + 2 log(cid:18) 1

δ(cid:19) + kθ∗k2,

Putting it together, for any σ > 0, any δ ∈ (0, 1), and any
σsd log(cid:18)1 +

c ≥

1

we have that

(12)

δ(cid:19) + kθ∗k2,

E# + nKP ( ¯E)

nK

dσ2(cid:19) + 2 log(cid:18) 1
k(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

t−1xat

M −1

+ nKδ.

Kt

R(n) ≤2cE" n
Xt=1

at
k

Xk=1qxT
≤2cKs dn log(cid:2)1 + nK
dσ2(cid:3)
log(cid:0)1 + 1
σ2(cid:1)

Choose δ = 1

nK , we have the following result: for any σ > 0 and any

c ≥

we have

nK

dσ2(cid:19) + 2 log (nK) + kθ∗k2,

1

σsd log(cid:18)1 +
R(n) ≤ 2cKs dn log(cid:2)1 + nK
dσ2(cid:3)
log(cid:0)1 + 1
σ2(cid:1)

+ 1.

