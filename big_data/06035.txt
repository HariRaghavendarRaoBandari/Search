L0-norm Sparse Graph-regularized SVD for Biclustering

Wenwen Min1, Juan Liu1∗, and Shihua Zhang2∗

1School of Computer, Wuhan University, Wuhan 430072, China

2National Center for Mathematics and Interdisciplinary Sciences, Academy of Mathematics

and Systems Science, Chinese Academy of Sciences, Beijing 100190, China

mww@whu.edu.cn, liujuan@whu.edu.cn and zsh@amss.ac.cn

6
1
0
2

 
r
a

 

M
9
1

 
 
]

G
L
.
s
c
[
 
 

1
v
5
3
0
6
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Learning the “blocking” structure is a central chal-
lenge for high dimensional data (e.g., gene expres-
In [Lee et al., 2010], a sparse singu-
sion data).
lar value decomposition (SVD) has been used as
a biclustering tool to achieve this goal. However,
this model ignores the structural information be-
tween variables (e.g., gene interaction graph). Al-
though typical graph-regularized norm can incor-
porate such prior graph information to get accu-
rate discovery and better interpretability, it fails to
consider the opposite effect of variables with differ-
ent signs. Motivated by the development of sparse
coding and graph-regularized norm, we propose a
novel sparse graph-regularized SVD as a powerful
biclustering tool for analyzing high-dimensional
data. The key of this method is to impose two
penalties including a novel graph-regularized norm
(|u|L|u|) and L0-norm ((cid:107)u(cid:107)0) on singular vec-
tors to induce structural sparsity and enhance in-
terpretability. We design an efﬁcient Alternating It-
erative Sparse Projection (AISP) algorithm to solve
it. Finally, we apply our method and related ones
to simulated and real data to show its efﬁciency in
capturing natural blocking structures.

1 Introduction
Singular Value Decomposition (SVD) is a powerful matrix
decomposition model which has been widely used in different
ﬁelds [Alter et al., 2000; Aharon et al., 2006; Zhou et al.,
2015]. Suppose X is a matrix of size n × p (e.g., a gene
expression matrix), then the classical SVD of X is:

X = U DV T =

dkukvk,

(1)

k=1

where r is the rank of X, uk and vk(k = 1,··· , r) are
left and right singular vectors respectively, and both U =
(u1,··· , ur) and V = (v1,··· , vr) are orthogonal matri-
ces. The D = diag(d1, d2,· · ·, dr) is a diagonal matrix.
However, the non-sparse singular vectors (uk, vk) can some-
times be difﬁcult to interpret. Many studies impose spar-
sity on singular vectors to improve this [Shi et al., 2013;

r(cid:88)

Jing et al., 2015]. The advantage of sparsity is that the sparse
singular vectors are better able to capture inherent structures
and patterns of the input data.

We here introduce a formal framework of the rank one

sparse SVD [Witten et al., 2009],

u,v,d

(cid:107)X − duvT(cid:107)2

minimize
subject to (cid:107)u(cid:107)2 ≤ 1, P1(u) ≤ c1,
(cid:107)v(cid:107)2 ≤ 1, P2(v) ≤ c2,

F

(2)

where P1(u) and P2(v) are two penalty functions. For dif-
ferent purpose, different sparsity penalties can be induced
in this framework [Witten et al., 2009; Lee et al., 2010;
Sill et al., 2011]. For example, Witten et al.
[2009] has
proposed a penalized matrix decomposition (PMD) by using
L1-penalty to induce sparsity [Witten et al., 2009]. In addi-
tion, Lee et al. [2010] has developed a sparse SVD (ALSVD)
by imposing Adaptive Lasso penalty to the left/right singular
vectors and employed it as a biclustering tool to capture bi-
cluster structure of a gene expression data. However, these
sparse SVD models have several limitations. First, they im-
pose sparseness via L1-norm penalty, while little work is
concerned with L0-norm one. Compared to L1-norm, L0-
norm can enforce to get a desirable level of sparsity. Sec-
ond, they ignore the prior graph knowledge for variables.
Recent studies have shown that graph-regularized penalty
can help to get better interpretability and higher generaliza-
tion performance for regression tasks and others [Ando and
Zhang, 2007; Li and Yeung, 2009; Li and Li, 2008; 2010;
Jiang et al., 2013]. To our knowledge, there is yet no study to
incorporate graph structure into the sparse SVD framework.
In this paper, we propose a novel sparse graph-regularized
SVD to model the high-dimensional data with prior graph
knowledge. This method is inspired by recent develop-
ments of sparse coding and graph-regularized norm [Lee
et al., 2006; Ando and Zhang, 2007; Zheng et al., 2011;
He et al., 2011]. It imposes a novel graph-regularized norm
(|u|L|u|) instead of the general one (uLu) and the L0-
norm ((cid:107)u(cid:107)0) penalty on singular vectors to induce struc-
tural sparsity. In summary, our key contributions are three-
fold: (1) impose L0-norm penalty to graph-regularized SVD
(sgSVD); (2) consider smoothing of absolute signal in graph-
regularized penalty (sgSVD*); (3) design an efﬁcient Alter-
nating Iterative Sparse Projection (AISP) algorithm to solve

it. Finally, we apply sgSVD* and related methods to simu-
lated and real data to demonstrate its efﬁciency in capturing
blocking data structure.

2 Related Works
Recent studies have a growing interest in regularized SVD
including the following main three aspects.

(1) Sparse SVD. Witten et al.

[2009] proposed the ﬁrst
sparse SVD via L1-norm and fused lasso as sparsity-inducing
norm. Lee et al. [2010] recommended to use different penal-
ties, such as Lasso, adaptive Lasso and Smoothly Clipped
Absolute Deviation (SCAD), as penalty functions. A natural
choice of the penalty function is L0 norm. Min et al. [2015]
proposed a sparse SVD via L0-norm penalty (L0SVD).

(2) Graph-regularized SVD. Graph-regularized norm has
been used in many different techniques such as nonnega-
tive matrix factorization [Cai et al., 2011]. However, to
our knowledge, there is yet no study to incorporate graph-
regularized norm into the sparse SVD framework. We be-
lieve that sparse graph-regularized SVD is a promising tool
as explored in other problems [Li and Yeung, 2009; Jenatton
et al., 2010; Zheng et al., 2011] to enforce the smoothness
of variable coefﬁcients. In addition, L2,1 is widely used to
enforce the structural sparsity at the inter-group level [Jacob
et al., 2009]. We can also consider it as a penalty function of
SVD in future studies.

(3) The relationship between SVD and PCA. As we all
know, principal component analysis (PCA) can be efﬁciently
solved by using SVD. However, the identiﬁed non-sparse
principal components can sometimes be difﬁcult to inter-
pret. To solve it, recent studies have developed several dif-
ferent sparse PCA models [Deshpande and Montanari, 2014;
Gu et al., 2014; Shen and Huang, 2008; Witten et al., 2009].
However, it is difﬁcult to develop effective algorithms for
solving these sparse PCA models. There are a kind of com-
monly used methods based on regularized SVD for solv-
ing them [Shen and Huang, 2008]. Moreover, Witten et al.
[2009] proposed another model based on regularized SVD to
ensure the orthogonally of their left singular vectors.

3 The Proposed Approach
3.1 Sparse Graph-regularized Penalty
Given a simple graph G, its Laplacian matrix L is deﬁned as
L = D − A, where A is the adjacency matrix of graph G,
Aij = 1 if vertex i and j is connected in the graph, Aij = 0
otherwise, and D is a diagonal matrix whose diagonal ele-
j=1 Aij). We
ﬁrst consider the ordinary graph-regularized norm and im-
pose sparsity on singular vectors in SVD framework with the
following penalty,

ments di is the degree of vertex i (i.e., di =(cid:80)p

P (v) = λ1(cid:107)v(cid:107)1 + λ2vT Lv,

The ﬁrst term (cid:107)v(cid:107)1 =(cid:80)

(3)
where λ1 ≥ 0 and λ2 ≥ 0 are two regularization parameters.
i |vi| is the L1-norm penalty to in-
i∼j Aij(vi −

duce sparsity. The second term vT Lv = 1
2

(cid:80)

vj)2 is a quadratic Laplacian penalty which forces the coefﬁ-
cients of v to be smooth. Thus, we can create a sparse graph-
regularized SVD using the penalty Eq.
(3) in the frame-
work Eq.
(2). The L1-norm has some good nature and
has been applied to induce sparsity in SVD [Witten et al.,
2009]. Whereas little work has been done using a more nat-
ural sparseness regularization, i.e., L0-norm penalty. In this
paper, we adopt L0-norm penalty rather than L1-norm one to
induce sparsity on singular vectors. Thus, we replace Eq. (3)
with the following one,

where (cid:107)v(cid:107)0 =(cid:80)

P (v) = λ0(cid:107)v(cid:107)0 + λ2vT Lv,
i I(vi) is the L0-norm penalty.

2

The classical graph-regularized penalty in Eq. (4) ensures
that two positively correlated variables would tend to inﬂu-
ence their coefﬁcients in the same direction. However, it
makes two negatively correlated variables have opposite ef-
fect. Thus, the classical graph-regularized penalty may not
perform well when the coefﬁcients of a number of variables
linked on the prior graph have different signs. This would
lead to the absolute values of their coefﬁcients are not ex-
pected to be locally smooth. To solve this drawback, we re-
deﬁne a sparse graph-regularized penalty by only considering
the magnitude of their coefﬁcients,

P (v) = λ1(cid:107)v(cid:107)0 + λ2|v|T L|v|,

(cid:80)
(5)
where |v|T L|v| = 1
i∼j Aij(|vi| − |vj|)2 and |v| =
(|v1|,|v2|,··· ,|vp|)T . This is also reﬂected by the fact that
linked genes in a gene interaction graph are usually signiﬁ-
cantly positively or negatively correlated. This new penalty
Eq.
(5) encourages highly inter-correlated variables corre-
sponding to a dense subnetwork in G to be jointly selected.

Mathematically, the normalized Laplacian matrix (cid:101)L de-
ﬁned by (cid:101)L = D−1/2LD−1/2 can be used to replace L in
ized (cid:101)L depends on the speciﬁc problems. In addition, there

Eq. (3), Eq. (4), and Eq. (5). The choice of L and normal-

are a wide variety of graph matrix representations [Wilson
and Zhu, 2008], which can also be considered to replace L.
In this paper, we apply the sparse graph-regularized penalty
with L for biclustering gene expression data.
3.2 L1-norm Sparse Graph-regularized SVD
The sparse graph-regularized penalty via L0-norm is non-
convex, which leads to a challenge issue for efﬁcient imple-
mentation. Here, we ﬁrst discuss a sparse graph-regularized
SVD by replacing L0-norm in Eq.(5) with L1-norm, denoted
as L1-sgSVD*. Formally, we should deal with the following
minimization problem,

(4)

(cid:107)X − duvT(cid:107)2

u,v,d

minimize
subject to (cid:107)u(cid:107)2 ≤ 1,(cid:107)u(cid:107)1 ≤ s1,|u|T L1|u| ≤ s2,
(cid:107)v(cid:107)2 ≤ 1,(cid:107)v(cid:107)1 ≤ c1,|v|T L2|v| ≤ c2,

F

(6)

where d is a positive singular value, u is a n × 1 column
vector, v is a p×1 column vector, both L1 and L2 are Laplace
matrices, and s1, s2, c1 and c2 are four super-parameters.

The objective function (cid:107)X − duvT(cid:107)2

(6) is bi-
convex to u and v. Thus, we propose an alternating iterative

F in Eq.

learning strategy to solve the optimization problem Eq. (6).
Next we consider optimization over v for a ﬁxed u, and get
the following constrained optimization problem,

F

v,d

(cid:107)X − duvT(cid:107)2

minimize
subject to (cid:107)v(cid:107)2 ≤ 1,
(cid:107)v(cid:107)1 ≤ c1,
|v|T L|v| ≤ c2.

(7)

We note that this objective function ||X − duvt||2
(cid:107)X(cid:107)2
tion ||X − duvT||2
z = X T u. The problem Eq. (7) can thus be written as

F =
F − 2duT Xv + d2. Obviously, minimizing this func-
F is equivalent to minimizing −uXv. Let

− vT z

v

minimize
subject to (cid:107)v(cid:107)2 ≤ 1,
(cid:107)v(cid:107)1 ≤ c1,
|v|T L|v| ≤ c2.

(8)

The |v|T L|v| is general non-convex, which leads to a chal-
lenge issue for solving the problem Eq. (8). Next we propose
Theorem 1 to cleverly remove the absolute operator in the
constrained condition.
Theorem 1. Suppose v∗ is an optimal solution of Eq. (8),
i zi ≥ 0 for all i.
then v∗
Proof. Suppose the Theorem is false. Then there is at least
exists i to meet v∗

i zi < 0. We ﬁrst construct a vector(cid:101)v which
satisﬁes(cid:101)vj = v∗
j for all j (cid:54)= i and(cid:101)vi = −v∗
the vector (cid:101)v meets (cid:107)(cid:101)v(cid:107)0 = (cid:107)v∗(cid:107)0 and (cid:107)(cid:101)v(cid:107)2 = (cid:107)v∗(cid:107)2. Let
f1 = −v∗z and f2 = −(cid:101)vz, then f1− f2 = −2v∗
f2 < f1). Thus, we get a vector (cid:101)v which corresponds to a

more smaller objective value than that of the optimal solution
v∗. It leads to a desired contradiction. Thus, the Theorem is
true.

i . Obviously,

i zi > 0 (i.e.,

Based on Theorem 1, we can replace Eq. (8) with the fol-

lowing optimization problem,

v

− vT|z|
minimize
subject to (cid:107)v(cid:107)2 ≤ 1,
(cid:107)v(cid:107)1 ≤ c1,
vT Lv ≤ c2,
vk ≥ 0,∀k,

(9)

where |z| = (|z1|, ...,|zp|)T . A standard way to solve the
optimization problem Eq. (9) is to formulate the Lagrangian
form as follows:
L(v) = −vT|z| + λ

σvT Lv− p(cid:88)

p(cid:88)

ηvT v +

τivi,

(10)
where λ ≥ 0, η ≥ 0, σ ≥ 0, τi ≥ 0 are Lagrangian multi-
pliers. Actually, we can easily get Theorem 2 to conﬁrm the
function Eq.
(10) is a convex function (we omit the proof
here).

i=1

vi +

1
2

i=1

1
2

Theorem 2. The function Eq. (10) is a convex function.

Thus, the optimal solution of Eq. (10) is characterized by

its subgradient equation as follows:

∇vL = −|z| + λe + ηv + σLv − τ = 0.

(11)
Ideally, we can get the optimal solution of Eq. (10) by solv-
ing the corresponding linear system in Eq. (11). However,
this is inefﬁcient due to the numerical difﬁculty of inverting
a Hessian matrix. As an alternative way, we adopt a simple
coordinate descent method [Friedman et al., 2007] to learn
vector v. The subgradient of vk in Eq. (10) is

∂L
∂vk

= −|zk| + λ + ηvk + σdkvk − σAkv − τk,

(12)

where dk is the degree of node k and Ak is the kth column of
the adjacency matrix A. The complementary slackness KKT
condition implies that if vk > 0, then τk = 0 and if vk = 0,
then τk > 0. Thus, let Eq. (12) be zero, then

max(|zk| + σAkv − λ, 0)

, k = 1, 2,···

vk =

η + σdk

(13)
Let ˇvk = max(|zk| + σAkv − λ, 0), and ˇv = (ˇv1, ..., ˇvp)T .
. Finally,
To meet the normalizing condition, we let ˜v = ˇv(cid:107)ˇv(cid:107)2
based on Theorem 1, we get the optimal solution of Eq. (7)
as follows:
(14)
where “ • ” denotes element-wise product. Likewise, ﬁxed u
in Eq. (6), we can obtain the coordinate update for u.

v = ˜v • sign(z),

Algorithm 1 L1-sgSVD*.
Require: Data matrix X ∈ Rn×p; Prior networks A1 ∈

Rn×n and A2 ∈ Rp×p; Parameters λu, λv, σu, σv.

Let z = Xv, A = A1 and u = |u|
for i = 1 to n do
ui = max(|zi| + σuAiu − λu, 0)

end for
u = u(cid:107)u(cid:107)2
u = u • sign(z)
Let z = X T u, A = A2 and v = |v|
for k = 1 to p do
vk = max(|zk| + σvAkv − λv, 0)

Ensure: u, v, d.
1: Initialize v with (cid:107)v(cid:107)2 = 1
2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: until d convergence
17: return u, v, d.

end for
v = v(cid:107)v(cid:107)2
v = v • sign(z)
d = zT v

However, there are still two problems remain unresolved:
(1) How to solve the singular value d? (2) What is the termi-
nation condition to stop the iterations? Note that the objective
function of Eq. (6), (cid:107)X−duvT(cid:107)2
F−2duT Xv+d2.
Given u and v, this is a quadratic function about d. Hence the

F = (cid:107)X(cid:107)2

optimal solution d∗ = uT Xv, and the corresponding optimal
value is (cid:107)X(cid:107)2
F − d∗2. Note that the minimum is only related
with d. Therefore, we can control iteration by monitoring the
change of d. Once the change of d is smaller than a small pos-
itive constant , where  represents a tolerance level, we stop
the iterations. Finally, we solve the optimization problem Eq.
(6) (Lagrangian form) by using an efﬁcient Alternating Itera-
tive Projection Algorithm (Algorithm 1). Note that the time
complexity of Algorithm 1 is O(T np + T p2 + T n2), where
T is the number of iterations.
3.3 L0-norm Sparse Graph-regularized SVD
In this section, we consider a novel sparse graph-regularized
SVD via L0-norm (denoted as L0-sgSVD* or sgSVD*) as
follows:

(cid:107)X − duvT(cid:107)2

u,v,d

minimize
subject to (cid:107)u(cid:107)2 ≤ 1,(cid:107)u(cid:107)0 ≤ ku,|u|T L1|u| ≤ c1,
(cid:107)v(cid:107)2 ≤ 1,(cid:107)v(cid:107)0 ≤ kv,|v|T L2|v| ≤ c2.

F

(15)

To solve problem Eq. (15), we ﬁrst ﬁx u and let z = X T u.

We then get the following optimization problem:

− vT z

v

minimize
subject to (cid:107)v(cid:107)2 ≤ 1,
(cid:107)v(cid:107)0 ≤ kv,
|v|T L|v| ≤ c2.

(16)

Similar with Theorem 1, we also have Theorem 3 to cleverly
remove the absolute operator in the last constrained condition
of Eq. (16).
Theorem 3. Suppose v∗ is an optimal solution of Eq. (16),
then v∗

i zi ≥ 0 for all i.

Therefore, to solve Eq. (16), we ﬁrst solve the following

optimization problem,

v

− vT|z|
minimize
subject to (cid:107)v(cid:107)2 ≤ 1,
(cid:107)v(cid:107)0 ≤ kv,
vT Lv ≤ c2,
vk ≥ 0,∀k.

We can further consider the following problem,

(17)

(18)

f (v)

minimize
subject to (cid:107)v(cid:107)0 ≤ kv,

v

2 σvT Lv −(cid:80)p

2 ηvT v + 1

where f (v) = −vT|z| + 1
i=1 τivi,
and η ≥ 0, σ ≥ 0, τi ≥ 0 are Lagrangian multipliers. Similar
with Theorem 2, we can also prove f (v) is a convex function.
Note that when σ = 0, Eq. (18) corresponds to L0SVD. If we
remove the constrained condition (cid:107)v(cid:107)0 ≤ kv in problem Eq.
(18), then we can obtain an update rule for v by using coor-
dinate descent method [Friedman et al., 2007] (the derivation
is similar to Eq. (13)). The complementary slackness KKT

condition implies that if vk > 0, then τk = 0 and if vk = 0,
then τk > 0. Thus, we obtain the following update rule,

max(|zk| + σAkv, 0)

, k = 1, 2,···

vk =

η + σdk

(19)
Let ˇvk = max(|zk| + σAkv, 0) and v = (ˇv1, ..., ˇvp)T . To
satisfy the condition (cid:107)v(cid:107)0 ≤ kv, we force the p − kv ele-
ments of v with the smallest absolute values to be zeros, i.e.,
project the vector v onto the closets absolute vector in Eu-
clidean space,

(cid:98)v = v • I(|v| ≥ |v|(kv)),

(20)
where I(·) is the indicator function, “ • ” denotes element-
|v|. To meet the normalizing condition, let (cid:98)v = (cid:98)v/(cid:107)(cid:98)v(cid:107)2.
wise product and |v|(kv) denotes the kv-th order statistic of
Based on Theorem 3, we can get a feasible solution of Eq.
(16) as follows,

(21)
Hence, we obtain an approximate solution of Eq.
(16) by
using a sparse projection strategy. Note that the iterative way
of Eq. (20) is consistent with the iterative threshold algorithm
[She, 2009]. In the same manner, for a ﬁxed v, we can also
obtain the coordinate update of u.

v =(cid:98)v • sign(z).

Finally, we propose an Alternating Iterative Sparse Projec-
tion (AISP) algorithm (Algorithm 2) to solve the optimiza-
tion problem (15) (Lagrangian form). When σ1 = σ2 = 0,
L0-sgSVD* algorithm reduces to L0SVD [Min et al., 2015].
We can prove the Eq. (21) is optimal solution for rank one
L0SVD model by ﬁxing u. In addition, prior graph informa-
tion is considered to be more important with large σu (σv).
Moreover, Algorithm 2 has the same time complexity with
Algorithm 1.

Algorithm 2 L0-sgSVD*.
Require: Data matrix X ∈ Rn×p; Prior networks A1 ∈

Rn×n and A2 ∈ Rp×p; Parameters ku, kv, σu, σv.

Let z = Xv, A = A1 and u = |u|
for i = 1 to n do

ˇui = |zi| + σuAiu

end for
(cid:98)u = u(cid:107)u(cid:107)2
u = ˇu • I(|ˇu| ≥ |ˇu|(ku))
u =(cid:98)u • sign(z)

Ensure: u, v, d.
1: Initialize v with (cid:107)v(cid:107)2 = 1
2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: until d convergence
19: return u, v, d.

ˇvk = |zk| + σvAkv
end for
(cid:98)v = v(cid:107)v(cid:107)2
v = ˇv • I(|ˇv| ≥ |ˇv|(kv))
v =(cid:98)v • sign(z)

d = zT v

Let z = X T u and A = A2
for k = 1 to p do

Figure 1: (A) Illustration of sgSVD*, sgSVD and L0SVD with a toy example (γ = 0.06) to explain the difference among them.
(B, C) Evaluation of different methods using simulation data with two cases. (B) The signs of coefﬁcients in left (or right)
singular vector are forced to be the same. (C) The signs of coefﬁcients in left (or right) singular vector are chosen at random.
Note that we only show the results for v due to the symmetry of u and v in form.

4 Experiments
4.1 Synthetic Data Experiments
Here, we evaluate the performance of sgSVD* with simu-
lated data and compare its performance with sgSVD, L0SVD
[Min et al., 2015], ALSVD [Lee et al., 2010] and SCADSVD
(sparse SVD via SCAD penalty) [Fan and Li, 2001].
Without loss of generality, we let a rank-one true signal
matrix X∗ = duvT and d = 1, where u and v are vectors of
size n × 1 and p × 1, respectively. The observed matrix X is
generated as the sum of X∗ and a noise matrix , i.e., X =
uvT + γ, where the elements of  are randomly sampled
from a standard normal distribution (i.e., ij ∼ N (0, 1)), and
γ is a nonnegative parameter to control the signal-to-noise
ratio. Accordingly, we generate two sparse singular vectors
u and v with n = p = 100 as follows:

(cid:101)u = [sample({−1, 1}, 50), rep(0, 50)], u =(cid:101)u/(cid:107)(cid:101)u(cid:107)2,
(cid:101)v = [sample({−1, 1}, 50), rep(0, 50)], v =(cid:101)v/(cid:107)(cid:101)v(cid:107)2,

where sample({−1, 1}, a) denotes a vector of size a, whose
elements are sampled from {−1, 1}, and rep(0, b) denotes a
vector of size b, whose elements are zeros. Here we create a
data matrix X for each γ ranging from 0.02 to 0.06 in steps
of 0.005 and two prior graphs for row and column variables
of X, whose ﬁrst 50 vertexes are connected with probability
p11 = 0.3, and remaining ones are connected with probability
p12 = 0.1. Moreover, to test sgSVD whether depends on the
signs of singular vectors, we also simulate the data generated
from the variables with the same signs by setting u := |u|
and v := −|v| with the same parameters (e.g., n, p, γ).

We apply the ﬁve methods to the simulation data with
σu = 0.1 and σv = 0.1 which are selected by using cross
validation strategy. We enforce each singular vector (u or
v) to contain 50 non-zeros elements (the same sparsity level)

for each method by tuning the parameters to get comparable
results for different methods. We ﬁrst demonstrate the mag-
nitude of singular vectors with absolute operator is how to
overcome the opposite effect of classical graph-regularized
penalty with a simple example which corresponds to Figure
1C with γ = 0.06 (Figure 1A). Moreover, we can clearly
ﬁnd that the performance on both sensitivity and speciﬁcity of
sgSVD* and sgSVD are superior to that of L0SVD, ALSVD
and SCADSVD (Figure 1B). As to the signals with diverse
signs, sgSVD* demonstrates distinct better performance than
sgSVD and others (Figure 1C). These results suggest that (1)
sgSVD* indeed show effectiveness compared to other meth-
ods (e.g., sgSVD and L0SVD); (2) sgSVD does suffer distinct
limitations when some connected variables in the prior graph
have different signal signs.
4.2 Applications to Gene Expression Data
We apply sgSVD* to two cancer gene expression datasets to
demonstrate its effectiveness in deciphering sample-speciﬁc
gene modules. We download a gene expression dataset from
the Cancer Genome Project (CGP) [Garnett et al., 2012]
and obtain a data matrix with 13321 genes across 641 cell
lines (samples). We further download a breast cancer gene
expression dataset (BRCA) from TCGA database [Koboldt
et al., 2012], and obtain a data matrix consisting of 11221
genes and 518 samples. For both applications, we construct
a prior gene graph consisting of 13,321 genes and 262,462
interactions (a gene interaction network) from the Pathway-
Commons database.

We apply sgSVD* to CGP and BRCA data with ku = 200
(genes), σu = 0.4 and a prior gene graph denoted as A1
for gene variables, kv = 50 (samples), σv = 0, i.e., there
is no prior graph for sample variables here. For each data,
we ﬁrst obtain the ﬁrst pair singular vectors u and v. Next,

ABTrue vsgSVDsgSVD*L0SVDSensitivitySensitivitySpecificitySpecificityγγγγCFigure 2: Comparison of three methods using CGP and BRCA data. (A) The distributions of log(FC) scores of identiﬁed
modules by each method. sgSVD* shows distinct high enriched scores compared to those of sgSVD and L0SVD (Wilcoxon
signed rank test p-value < 0.01) on both data. (B) The number of signiﬁcantly enriched functional terms of modules by each
method with respect to different signiﬁcant levels with log10(p-value) from -4 to -1. sgSVD* shows distinct high enriched
number of terms compared to those of sgSVD and L0SVD (p-value < 0.01) on both data.

we subtract the signal of current pair singular vectors from
the input data (i.e., X := X − duT v), and apply sgSVD*
again to identify the next pair singular vectors. The ﬁrst 40
pair singular vectors are considered to identify 40 gene mod-
ules. Similarly, we also apply sgSVD and L0SVD to the
CGP and BRCA data for comparison. We ﬁnd that the gene
pairs within each identiﬁed module do show distinct expres-
sion correlations by comparing the sum of absolute Pearson
correlation coefﬁcient between each pair within a module to
those randomly generated ones. We also test σv with different
values and ﬁnd the corresponding results are basically con-
sistent. Herein below, we thus only show the results obtained
with σv = 0.4.

For a given module i of ni genes and mi interactions
(edge) in the prior graph, we assess the edge-enrichment of
the module using a Fold Change (FC) score computed as FC
= mi/(ni
2 )
, where M is the total number of interactions and
M/(N
2 )
N is the total number of genes in the prior gene graph. We
can clearly see that that the FC scores of the modules identi-
ﬁed from CGP and BRCA data by sgSVD* are signiﬁcantly
higher than those of sgSVD and L0SVD (Figure 2A). In addi-
tion, the statistical signiﬁcance of the FC is calculated by us-
ing the right tailed hypergeometric test. We ﬁnd the percent-
ages of edge-enriched gene modules of sgSVD* are much
higher than that of sgSVD and L0SVD for different signiﬁ-
cance levels on both BRCA and CGP data (Table 1). These
results suggest the modules identiﬁed using sgSVD* show
distinct biological relevance than those of other methods.

To further valid our ﬁndings with biological functions, we
also evaluate the functional enrichment of each module. We
ﬁnd that the gene modules identiﬁed by sgSVD* are signif-
icantly enriched in known Gene Ontology (GO) biological
process (BP) terms at different signiﬁcance levels than those
of sgSVD and L0SVD in both data (Figure 2B). In addition,
we ﬁnd that the number of enriched modules for most sig-
niﬁcance levels of sgSVD are lower than those of L0SVD in

Table 1: The percentage (%) of signiﬁcantly edge-enriched
modules at different signiﬁcance levels.

L0SVD sgSVD sgSVD*

L0SVD sgSVD sgSVD* Level
0.10
0.05
0.01
0.005
0.001

67.5
65.0
62.5
62.5
60.0

57.5
57.5
57.5
57.5
55.0

CGP

62.5
60.0
57.5
57.5
57.5

70.0
70.0
65.0
65.0
65.0

BRCA

60.0
57.5
57.5
57.5
57.5

57.5
57.5
55.0
55.0
55.0

BRCA data. It is probably because the coefﬁcients of a num-
ber of variables linked on the prior graph have different signs
in this data. Furthermore, to assess whether the number of
signiﬁcant GO biological processes are in favor of chance, we
also perform the same test on forty random modules whose
gene labels are randomly permuted. However, no signiﬁcant
GO BP term is discovered. All these results demonstrate that
sgSVD* can identify more biologically relevant gene mod-
ules than other methods.

5 Conclusion
In this paper, we propose a L0-norm sparse graph-regularized
SVD for biclustering high-dimensional data. We adopt the
graph-regularized penalty and L0-norm to induce structure
sparsity of singular vectors and enhance intelligibility of data
structure. Importantly, we consider the magnitude of singular
vectors with absolute operator to overcome the opposite ef-
fect of classical graph-regularized penalty. To solve this novel
model, we ﬁrst ﬁnd a clever trick to remove the absolute oper-
ator in the new graph-regularized penalty, and then design an
efﬁcient Alternating Iterative Sparse Projection (AISP) algo-
rithm to solve it. We expect the proposed method can be ap-
plied to high-dimensional data from diverse domains. More-
over, it will be valuable to extend current concept into other
statistical learning frameworks [e.g., Canonical Correlation
Analysis (CCA), partial least square (PLS)].

ABlog(FC)Number of enriched GO termslog10(p-value)BRCACGPBRCACGPAcknowledgment
This work was supported by the National Science Foundation
of China [61379092, 61422309], Natural Science Foundation
of Hubei Province of China 2014CFB194, and the Outstand-
ing Young Scientist Program of CAS, and the Key Laboratory
of Random Complex Structures and Data Science, CAS.
References
[Aharon et al., 2006] Michal Aharon, Michael Elad, and Al-
fred Bruckstein. K-svd: An algorithm for designing over-
IEEE
complete dictionaries for sparse representation.
Trans. on Signal Processing, 54(11):4311–4322, 2006.

[Alter et al., 2000] Orly Alter, Patrick O Brown, and David
Botstein. Singular value decomposition for genome-wide
expression data processing and modeling. Proceedings of
the National Academy of Sciences, 97(18):10101–10106,
2000.

[Ando and Zhang, 2007] Rie Kubota Ando and Tong Zhang.
Learning on graph with laplacian regularization. NIPS,
19:25, 2007.

[Cai et al., 2011] Deng Cai, Xiaofei He, Jiawei Han, and
Thomas S Huang. Graph regularized nonnegative matrix
factorization for data representation. IEEE Trans. on Pat-
tern Analysis and Machine Intelligence, 33(8):1548–1560,
2011.

[Deshpande and Montanari, 2014] Yash Deshpande and An-
drea Montanari. Sparse pca via covariance thresholding.
In NIPS, pages 334–342, 2014.

[Fan and Li, 2001] Jianqing Fan and Runze Li. Variable se-
lection via nonconcave penalized likelihood and its oracle
properties. Journal of the American statistical Association,
96(456):1348–1360, 2001.

[Friedman et al., 2007] Jerome Friedman, Trevor Hastie,
Holger H¨oﬂing, and Robert Tibshirani. Pathwise coor-
dinate optimization. The Annals of Applied Statistics,
1(2):302–332, 2007.

[Garnett et al., 2012] Mathew J Garnett, Elena J Edelman,
Sonja J Heidorn, Chris D Greenman, et al. Systematic
identiﬁcation of genomic markers of drug sensitivity in
cancer cells. Nature, 483(7391):570–575, 2012.

[Gu et al., 2014] Quanquan Gu, Zhaoran Wang, and Han
In NIPS, pages

Liu. Sparse pca with oracle property.
1529–1537, 2014.

[He et al., 2011] Ran He, Wei-Shi Zheng, Bao-Gang Hu, and
Xiang-Wei Kong. Nonnegative sparse coding for discrim-
inative semi-supervised learning. In CVPR, pages 2849–
2856. IEEE, 2011.

[Jacob et al., 2009] Laurent Jacob, Guillaume Obozinski,
and Jean-Philippe Vert. Group lasso with overlap and
graph lasso. In ICML, pages 433–440, 2009.

Jenatton,

[Jenatton et al., 2010] Rodolphe

Guillaume
Obozinski, and Francis Bach. Structured sparse principal
component analysis. In AISTATS, pages 366–373, 2010.

[Jiang et al., 2013] Bo Jiang, Chibiao Ding, Bio Luo, and Jin
Tang. Graph-laplacian pca: Closed-form solution and ro-
bustness. In CVPR, pages 3492–3498, 2013.

[Jing et al., 2015] Liping Jing, Peng Wang, and Liu Yang.
Sparse probabilistic matrix factorization by laplace distri-
bution for collaborative ﬁltering. In IJCAI, pages 1771–
1777, 2015.

[Koboldt et al., 2012] Daniel C Koboldt, Robert S Fulton,
Michael D McLellan, Schmidt Heather, et al. Comprehen-
sive molecular portraits of human breast tumours. Nature,
490(7418):61–70, 2012.

[Lee et al., 2006] Honglak Lee, Alexis Battle, Rajat Raina,
and Andrew Y Ng. Efﬁcient sparse coding algorithms. In
NIPS, pages 801–808, 2006.

[Lee et al., 2010] Mihee Lee, Haipeng Shen, Jianhua Z
Huang, and JS Marron. Biclustering via sparse singular
value decomposition. Biometrics, 66(4):1087–1095, 2010.
[Li and Li, 2008] Caiyan Li and Hongzhe Li. Network-
constrained regularization and variable selection for anal-
ysis of genomic data. Bioinformatics, 24(9):1175–1182,
2008.

[Li and Li, 2010] Caiyan Li and Hongzhe Li. Variable selec-
tion and regression analysis for graph-structured covari-
ates with an application to genomics. The Annals of Ap-
plied Statistics, 4(3):1498, 2010.

[Li and Yeung, 2009] Wu-Jun Li and Dit-Yan Yeung. Re-
In IJCAI, pages

lation regularized matrix factorization.
1126–1131, 2009.

[Min et al., 2015] Wenwen Min,

Juan Liu, and Shihua
Zhang. A novel two-stage method for identifying mirna-
gene regulatory modules in breast cancer. In Bioinformat-
ics and Biomedicine (BIBM), pages 151–156, 2015.

[She, 2009] Yiyuan She. Thresholding-based iterative selec-
tion procedures for model selection and shrinkage. Elec-
tronic Journal of Statistics, 3:384–415, 2009.

[Shen and Huang, 2008] Haipeng Shen and Jianhua Z
Sparse principal component analysis via reg-
Journal of

Huang.
ularized low rank matrix approximation.
multivariate analysis, 99(6):1015–1034, 2008.

[Shi et al., 2013] Jianping Shi, Naiyan Wang, Yang Xia, Dit-
Yan Yeung, Irwin King, and Jiaya Jia. Scmf: sparse co-
variance matrix factorization for collaborative ﬁltering. In
IJCAI, pages 2705–2711, 2013.

[Sill et al., 2011] Martin Sill, Sebastian Kaiser, Axel Ben-
ner, and Annette Kopp-Schneider. Robust biclustering by
sparse singular value decomposition incorporating stabil-
ity selection. Bioinformatics, 27(15):2089–2097, 2011.

[Wilson and Zhu, 2008] Richard C Wilson and Ping Zhu. A
study of graph spectra for comparing graphs and trees. Pat-
tern Recognition, 41(9):2833–2841, 2008.

[Witten et al., 2009] Daniela M Witten, Robert Tibshirani,
and Trevor Hastie. A penalized matrix decomposition,
with applications to sparse principal components and
canonical correlation analysis. Biostatistics, 10(3):515–
534, 2009.

[Zheng et al., 2011] Miao Zheng, Jiajun Bu, Chun Chen,
Can Wang, Lijun Zhang, Guang Qiu, and Deng Cai. Graph
regularized sparse coding for image representation. IEEE
Trans. on Image Processing, 20(5):1327–1336, 2011.

[Zhou et al., 2015] Xun Zhou, Jing He, Guangyan Huang,
and Yanchun Zhang. Svd-based incremental approaches
for recommender systems. Journal of Computer and Sys-
tem Sciences, 81(4):717–733, 2015.

