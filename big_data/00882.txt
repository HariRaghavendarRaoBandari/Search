6
1
0
2

 
r
a

M
 
4

 
 
]

M

I
.

h
p
-
o
r
t
s
a
[
 
 

2
v
2
8
8
0
0

.

3
0
6
1
:
v
i
X
r
a

Draft version March 7, 2016
Preprint typeset using LATEX style emulateapj v. 5/2/11

PHOTOMETRIC SUPERNOVA CLASSIFICATION WITH MACHINE LEARNING

Michelle Lochner1, Jason D. McEwen2, Hiranya V. Peiris1, Ofer Lahav1, Max K. Winter1
1Department of Physics and Astronomy, University College London, Gower Street, London WC1E 6BT, UK and

2Mullard Space Science Laboratory, University College London, Surrey RH5 6NT, UK

Draft version March 7, 2016

ABSTRACT

Automated photometric supernova classiﬁcation has become an active area of research in recent
years in light of current and upcoming imaging surveys such as the Dark Energy Survey (DES)
and the Large Synoptic Survey Telescope (LSST), given that spectroscopic conﬁrmation of type for
all supernovae discovered with these surveys will be impossible. Here, we develop a multi-faceted
classiﬁcation pipeline, combining existing and new approaches. Our pipeline consists of two stages:
extracting descriptive features from the light curves and classiﬁcation using a machine learning algo-
rithm. Our feature extraction methods vary from model-dependent techniques, namely SALT2 ﬁts, to
more independent techniques ﬁtting parametric models to curves, to a completely model-independent
wavelet approach. We cover a range of representative machine learning algorithms, including naive
Bayes, k -nearest neighbors, support vector machines, artiﬁcial neural networks and boosted decision
trees. We test the pipeline on simulated multi-band DES light curves from the Supernova Photometric
Classiﬁcation Challenge. Using the commonly-used area under the Receiver Operating Characteristic
curve (AUC) as a metric, we ﬁnd that the SALT2 ﬁts and the wavelet approach, with the boosted
decision trees algorithm, each achieves an AUC of 0.98, where 1 represents perfect classiﬁcation. We
ﬁnd that a representative training set is essential for good classiﬁcation, whatever the feature set or
algorithm, suggesting that spectroscopic follow-up is best focused on fewer objects at a broad range of
redshifts than on many bright objects. Importantly, we ﬁnd that by using either one of the two best
feature extraction methods (SALT2 model ﬁts and wavelet decomposition) and a boosted decision
tree algorithm, accurate classiﬁcation is possible purely from light curve data, without the need for
any redshift information.
Subject headings: methods:data analysis — cosmology:observations — supernovae:general

1.

INTRODUCTION

Astronomy is entering an era of deep, wide-ﬁeld sur-
veys and massive datasets, which requires the adoption
of new, automated techniques for data reduction and
analysis.
In the past, supernova datasets were small
enough to allow spectroscopic follow-up for the major-
ity of objects, conﬁrming the type of each. Only type
Ia’s are currently used for cosmology, and the type is
of course required for astrophysical modeling and stud-
ies. With the onset of surveys such as the Dark En-
ergy Survey (DES) (Dark Energy Survey Collaboration
2005; Dark Energy Survey Collaboration et al. 2016) and
the upcoming Large Synoptic Survey Telescope (LSST)
(LSST Science Collaboration 2009), only a small fraction
of the dataset can be spectroscopically followed up. The
current commonly used dataset, the Joint Light curve
Analysis (JLA) (Betoule et al. 2014), contains only 740
supernovae, while DES is expected to detect thousands
(Bernstein et al. 2012) and LSST hundreds of thousands
(LSST Science Collaboration 2009) of supernovae. Thus
alternative approaches to supernova science must be de-
veloped to leverage these largely photometric datasets.

Supernova cosmology is possible without strictly know-
ing the supernova type using,
for example, Bayesian
methods (M. Kunz, B. Bassett and R. Hlozek 2007; R.
Hlozek et al. 2012; J. Newling et al. 2012; Knights et al.
2013; Rubin et al. 2015). However, these techniques ben-
eﬁt from having a reasonable probability for the type of

Contact email: dr.michelle.lochner@gmail.com

each object in the dataset, so some form of probabilis-
tic classiﬁcation is useful. Additionally, studies of core-
collapse supernovae and other transients rely on good
photometric classiﬁcation. Further, the observing strat-
egy for LSST has not yet been ﬁnalized and the eﬀect
of observing strategy on supernova classiﬁcation has not
yet been established. Here we outline a multi-faceted
pipeline for photometric supernova classiﬁcation. In fu-
ture work, we will apply it to LSST simulations to under-
stand the eﬀect of observing strategy on classiﬁcation.

Current photometric supernova classiﬁcation tech-
niques focus on empirically-based template ﬁtting (Sako
et al. 2008; Sako et al. 2014). However, in the past few
years there have been several innovative techniques pro-
posed to address this problem (see Kessler et al. (2010b)
and references therein).

Here, we apply machine learning to this problem, as a
well-established method of automated classiﬁcation used
in many disciplines. As astronomical datasets become
larger and more diﬃcult to process, machine learning
has become increasingly popular (Ball & Brunner 2010;
Bloom & Richards 2012). Machine learning techniques
have been proposed as a solution to an earlier step in
the supernova pipeline, that of classifying transients from
images (du Buisson et al. 2015; Wright et al. 2015). Ma-
chine learning is also already being employed at some
level for photometric supernova classiﬁcation in the Sloan
Digital Sky Survey (SDSS) (Frieman et al. 2008; Sako
et al. 2008), using the parameters from template-ﬁtting
as features (Sako et al. 2014).

2

Lochner et al.

We investigate the eﬀect of including host galaxy pho-
tometric redshift information in automated classiﬁcation.
Reliable redshifts are important for current classiﬁcation
techniques in order to reliably ﬁt the templates. How-
ever, for future surveys such as LSST, well-constrained,
unbiased redshifts may be diﬃcult to obtain and could
negatively aﬀect classiﬁcation. A classiﬁcation technique
which is independent of redshift information could there-
fore be invaluable.

Additionally, we investigate the eﬀect of providing the
machine learning algorithms with a non-representative
training set. In general, one would expect the spectro-
scopically conﬁrmed training set to be biased in terms of
brightness (and hence redshift) because it is far easier to
obtain spectra for brighter objects. However, any classi-
ﬁcation technique is likely to underperform with a biased
training set. In our analysis, we explore both represen-
tative and non-representative training sets.

In this paper, we investigate four diﬀerent methods of
extracting useful features from light curves, as well as ﬁve
representative machine learning algorithms. In Sec. 2 we
summarize the simulated dataset used. Sec. 3 explains
our feature extraction methods, varying from highly
model dependent approaches to a completely model in-
dependent method. Section 4 introduces the machine
learning algorithms used in this work. We present our
results in Sec. 6 and conclude in Sec. 7.

2. SUPERNOVA SIMULATIONS

To test and compare methods, we use existing super-
nova simulations from the Supernova Photometric Classi-
ﬁcation Challenge (SPCC) (Kessler et al. 2010a,b), which
are simulated DES light curves built from an existing li-
brary of non-Ia templates and using both SALT2 (Guy
et al. 2007) and MLCS2k2 (Jha et al. 2007) type Ia
models. Simulated photometric host galaxy redshifts are
available for each object and we investigate the eﬀect of
withholding this information from the machine learning
algorithms, in order to see if redshift is an essential in-
gredient for classiﬁcation. We have not tested whether
or not the results would improve if spectroscopic red-
shifts were available, since for DES and LSST this is an
unlikely scenario.

In the original challenge, a training set of 1103
spectroscopically-conﬁrmed objects was provided while
a test set of 20216 objects was retained to compare the
techniques under study. It is important to note that this
training set is non-representative in brightness and hence
also in redshift. This was done to emulate the way spec-
troscopic follow-up is currently performed, which prior-
itizes bright objects. We return to this point in Sec. 6.
Figure 1 shows an example type Ia light curve from the
challenge data.

3. EXTRACTING USEFUL LIGHT CURVE FEATURES
Feature extraction is a crucial step in the machine
learning process. Raw data almost inevitably requires
dimensionality reduction or some form of summarizing
of key features in the data, and very few data sets can
be successfully classiﬁed in a raw format. Good fea-
ture extraction techniques will produce a feature set of
much lower dimensionality than the original data with
features that adequately describe the original data and
are well-separated between classes. Examples of com-

Fig. 1.— An example simulated DES type Ia supernova light
curve (object ID 009571), at redshift 0.42, from the Supernova
Photometric Classiﬁcation Challenge (Kessler et al. 2010a,b). The
photometric light curve is measured in the four DES griz ﬁlter
bands, showing the rise in brightness after the explosion of the
star and subsequent decay. The points and error bars are the data
points, while the curves are from the best ﬁtting SALT2 model (see
Sec. 3).
monly used features include principal component analy-
sis (PCA) (Pearson 1901; Hotelling 1933) coeﬃcients or
derived quantities such as amplitude, period, phase etc.
For a general overview of feature extraction see Li et al.
(2016).

We test a variety of feature sets and evaluate their per-
formance based on the eﬀectiveness of subsequent clas-
siﬁcation for a variety of machine learning algorithms.
Alternative feature selection approaches agnostic of ma-
chine learning algorithms could be considered, although
as these approaches evaluate the feature sets in isolation,
they could lead to biases after a machine learning algo-
rithm is applied. A detailed investigation of alternative
approaches to choosing and evaluating features is beyond
the scope of this article.

We explore three diﬀerent, broad approaches to fea-
ture extraction, ranging from a highly model-dependent
method based on supernova templates to a completely
model-independent wavelet based approach, with a para-
metric approach somewhere in between. These bracket
a wide range of approaches one could consider using to
extract features from light curves. We run a pipeline
whereby we extract the four feature sets from the same
data, and then run ﬁve diﬀerent machine learning algo-
rithms (described in Sec. 4) on each feature set to com-
pare performance.

Before discussing the feature extraction methods we
have studied, we introduce a useful visualization tech-
nique for understanding feature sets.

3.1. Visualizing feature sets with t-distributed stochastic

neighbor embedding

Although feature extraction generally reduces dimen-
sionality from raw data, the number of features often re-
mains large, making it diﬃcult to determine whether or
not they have good separability. t-Distributed stochastic
neighbor embedding (t-SNE) (Van der Maaten & Hinton
2008) is a sophisticated technique designed to produce
a low-dimensional representation of a high-dimensional
space, whilst clustering similar features together. t-SNE
works by ﬁrst computing, for each pair of points, the
probability that the two points are similar based on their
Euclidean distance. The aim then is to ﬁnd the cor-

020406080100120140Time (days)50050100150200FluxgrizPhotometric Supernova Classiﬁcation With Machine Learning

3

Priors on the SALT2 model parameters.

TABLE 1

Parameter name

z
t0
x0
x1
c

Prior

U (0.01, 1.5)
U (−100, 100)
U (−10−3, 10−3)

U (−3, 3)
U (−0.5, 0.5)

responding low-dimensional values for these points that
preserves the same probability. Stochastic neighbor em-
bedding (SNE) determines these values by minimizing
the sum of Kullback-Leibler divergences (a simple mea-
sure of divergence between probability distributions) over
all data points, using a gradient descent algorithm. t-
SNE diﬀers from SNE in the choice of exact probability
distribution deﬁning the similarity between points (using
a Student-t distribution instead of a Gaussian) and in the
exact choice of cost function to minimize (see Van der
Maaten & Hinton (2008) for details). If the embedded
features are well-separated on the t-SNE plot, one can
generally expect accurate classiﬁcation. However, the
converse is not strictly true and poorly-separated fea-
tures may still be well-classiﬁed with a sophisticated ma-
chine learning algorithm. Each of the feature extraction
methods discussed below has an accompanying t-SNE
plot for visualization purposes (Fig. 3.2).

3.2. Template ﬁtting approach to classiﬁcation

Historically, classiﬁcation of photometric supernovae
(generally used to identify candidates for spectroscopic
follow-up) has focused on the use of supernova templates
constructed from existing data sets of supernovae (Sako
et al. 2008). Light curve features such as stretch and
color parameters have been used to classify photometric
supernovae from SDSS (Sako et al. 2011). We choose
the SALT2 (Spectral Adaptive Light curve Template 2)
model (Guy et al. 2007) of type Ia supernovae as it is the
most commonly used.

In the SALT2 model, the ﬂux in a given band of a type

Ia light curve is given by

F (t, λ) = x0 × [M0(t, λ) + x1M1(t, λ) + ...]

× exp[cCL(λ)],

(1)

where t is the phase (time since maximum brightness in
the B -band), λ is the rest frame wavelength, M0(t, λ)
is average spectral sequence (using past supernova data
as described in Guy et al. 2007), and Mk(t, λ) for k > 0
are higher order components to capture variability in the
supernovae. In practice, only M0(t, λ) and M1(t, λ) are
used. CL(λ) is the average color correction law. For each
object, the redshift z is also used as either a given or ﬁt-
ted parameter. Thus this method has 5 features: z, t0
(the time of peak brightness in the B -band), x0, x1 and c.
We use the implementation of SALT2 in sncosmo1 (Bar-
bary 2014) and obtain the best ﬁtting parameters using
MultiNest2 (Feroz & Hobson 2008; Feroz et al. 2009,
2013) with the pyMultiNest3 (Buchner, J. et al. 2014)
software. Table 1 lists the priors used on all parameters.

1 http://sncosmo.readthedocs.org/en/v1.2.x/
2 https://ccpforge.cse.rl.ac.uk/gf/project/multinest/
3 https://johannesbuchner.github.io/PyMultiNest/

Figures 2(a) and 2(b) show the t-SNE plots for the
SALT2 features both without and with redshift informa-
tion. The t-SNE plots show a lot of structure, partly due
to the relative low-dimensionality of the feature space,
which corresponds to a highly constrained 2D plot. The
features are fairly well-separated, especially when red-
shift is included, but it is clear that it is diﬃcult to dis-
tinguish between type Ia and Ibc supernovae.

3.3. General parametric approach to classiﬁcation
The template-ﬁtting approach relies on a signiﬁcant
amount of prior knowledge about supernovae. An alter-
native is to use a more general parametric description
and use the ﬁtted parameters as features in the machine
learning pipeline. We use two diﬀerent parametric mod-
els that have been proposed in the literature as good pa-
rameterizations of a supernova light curve and were both
used in solutions to the SPCC. Model 1 is the parame-
terization used in Newling et al. (2011), while Model 2 is
proposed in Karpenka et al. (2013).

We use these models because they are both proposed
solutions to precisely the problem of supernova classiﬁ-
cation. While similar, the models have some diﬀerences,
especially in the treatment of the tail of the light curves,
the ability to describe double peaks and the small diﬀer-
ence in number of parameters, which makes it interesting
to compare them.

We also considered the linear piecewise model proposed
in Sanders et al. (2015) but found that the relatively large
number of parameters (11 per ﬁlter) made it less robust
when ﬁtting to data, and consequently classiﬁcation with
this feature set did not typically perform well.

3.3.1. Model 1

The model used in Newling et al. (2011) describes the

ﬂux of a supernova (in any band) by

(cid:18)
− t − φ

(cid:19)

exp

σ

k−kek + Ψ(t),

(2)

Ψ(t) =

cubic spline
ψ

−∞ < t < ∞
φ < t < τ
τ < t < ∞

,

(3)

and A + ψ is the peak ﬂux, φ is the starting time of the
explosion, k governs the relative rise and decay times
of the light curve and σ is the width or ‘stretch’ of the
light curve. The time of the peak ﬂux, τ , is given by
τ = kσ + φ. The function Ψ(t) is a ‘tail’ function that
ensures that the ﬂux tends to a ﬁnite value as t → ∞ and
the cubic spline is determined to have zero derivative at
t = φ and t = τ .

Each ﬁlter band is ﬁtted individually with these ﬁve
parameters (A, φ, σ, k, ψ), giving a total of 20 features
for each object (or 21 if redshift is included). The pa-
rameters are all ﬁtted with MultiNest, as in Sec. 3.2,
and the best ﬁt is taken to be the maximum posterior
value for each parameter. The priors on the parameters
(which are the same for all four ﬁlter bands) are given in
Table 2.

Figures 2(c) and 2(d) show the t-SNE plots for the
Model 1 feature set both without and with redshift in-

F (t) = A

where

σ

(cid:18) t − φ
(cid:19)k
0

4

Lochner et al.

(a) SALT2 without redshift (5 features)

(b) SALT2 with redshift (5 features)

(c) Parametric Model 1 without redshift (20 features)

(d) Parametric Model 1 with redshift (21 features)

(e) Parametric Model 2 without redshift (24 features)

(f) Parametric Model 2 with redshift (25 features)

(g) Wavelets without redshift (20 features)

(h) Wavelets with redshift (21 features)

Fig. 2.— t-SNE (t-distributed stochastic neighbor embedding) visualizations for feature sets with and without redshift information. This
is a two-dimensional visualization of the high-dimensional feature space. Each of the three classes of supernovae is represented by a diﬀerent
color; if the classes are well-separated in the embedding space, we would expect these features to provide good classiﬁcation. Note that all
units are arbitrary and the original dimensionality is indicated in brackets.

2015105051015Embedded feature 11510505101520Embedded feature 2IaIIIbc1510505101520Embedded feature 12015105051015Embedded feature 2IaIIIbc2015105051015Embedded feature 12015105051015Embedded feature 2IaIIIbc2015105051015Embedded feature 12015105051015Embedded feature 2IaIIIbc15105051015Embedded feature 115105051015Embedded feature 2IaIIIbc15105051015Embedded feature 115105051015Embedded feature 2IaIIIbc201510505101520Embedded feature 1201510505101520Embedded feature 2IaIIIbc1510505101520Embedded feature 120151050510152025Embedded feature 2IaIIIbcPhotometric Supernova Classiﬁcation With Machine Learning

5

Priors on the Model 1 parameters.

TABLE 2

Parameter name

log(A)

φ

log(σ)
log(k)
log(ψ)

Prior
U (0, 10)
U (−60, 100)
U (−3, 4)
U (−4, 4)
U (−6, 10)

formation respectively. In both cases, the features do not
appear to be easily separable.

3.3.2. Model 2

The proposed parameterization from Karpenka et al.
(2013) is similar to Model 1 but allows for the possibility
of double peaks in light curves. The ﬂux is given by

(cid:20)

(cid:21)

F (t) = A

1 + B(t − t1)2

exp[−(t − t0)/Tfall]

1 + exp[−(t − t0)/Trise]

, (4)

where t is assumed to be the earliest measurement in
the r-band light curve. We again ﬁt all parameters using
MultiNest and take the maximum posterior parameters.
There are 6 parameters per ﬁlter band, thus forming a
feature set of 24 features (or 25 including redshift) for
each object. We use the same priors as in Karpenka
et al. (2013), given in Table 3.

Priors on the Model 2 parameters.

TABLE 3

Parameter name

log(A)
log(B)

t0
t1

Trise
Tfall

Prior

U (log(10−5), log(1000))
U (log(10−5), log(100))

U (0, 100)
U (0, 100)
U (0, 100)
U (0, 100)

The t-SNE plots for the Model 2 features can be seen
in Fig. 2(e) and Fig. 2(f) without and with redshift re-
spectively. As in Model 1, the features do not appear to
be well-separated.

3.4. Wavelet decomposition approach to classiﬁcation
While the model-dependent approaches to feature ex-
traction considered so far are useful for incorporating
prior information about SN light curves, they are sensi-
tive to mis-speciﬁcation of light curve models. It is quite
possible important light curve characteristics cannot be
captured eﬃciently by simple models, such as Models
1 and 2 described above. A model-independent, non-
parameteric approach to feature extraction is therefore
required as a complementary approach that is not depen-
dent on prior information.

A wavelet decomposition is a good choice for feature
extraction since certain wavelet transforms can be con-
structed that are approximately invariant under trans-
lation and stretch. One could then expect the wavelet
coeﬃcients of the light curves of two similar supernovae
measured at diﬀerent times, or two similar objects at
diﬀerent redshifts, to themselves be similar. Recently,
Varughese et al. (2015) have shown that a wavelet-based
approach to photometric supernova classiﬁcation can be

highly eﬀective. Here we consider a substantially diﬀer-
ent method in the actual wavelet decomposition of the
light curves and subsequent classiﬁcation, but also ﬁnd
wavelets to be an excellent feature extraction method.

The procedure we follow to extract features using
wavelets is to ﬁrst interpolate the light curves onto the
same grid of points (using Gaussian processes), then to
perform a redundant wavelet decomposition, and lastly
to use PCA to reduce the dimensionality of the feature
set.

3.4.1. Interpolation using a Gaussian process regression

In order to perform a wavelet decomposition, all light
curves are interpolated onto the same uniform grid of
points. We use Gaussian process regression since it has
the advantage over (for example) spline interpolation in
that it allows the straightforward inclusion of uncertainty
information and produces a less biased estimate of inter-
polated values.

A Gaussian process is the generalization of a Gaus-
sian distribution, forming a distribution for which each
point of the input variable is associated with a normally-
distributed random variable (MacKay 2003). A Gaus-
sian process is fully speciﬁed by a mean function of the
input variable (time, in the case of a light curve) and
a covariance function, which speciﬁes the covariance be-
tween values of the function at pairs of points in time.
Gaussian process regression then proceeds by determin-
ing the mean function and hyperparameters of the covari-
ance matrix. We use the package GaPP4 used in Seikel
et al. (2012) to perform the Gaussian process regression
of each of the light curves.

3.4.2. Wavelet decomposition

Wavelet transforms are an invaluable tool in signal
analysis due to the ability of wavelets to to localize signal
content in scale and time simultaneously, whereas real
space or Fourier representations provide signal localiza-
tion, respectively, in time or frequency only (see Valens
(1999) for a gentle introduction to wavelets). For the
continuous wavelet transform (CWT), the set of wavelet
atoms forming the transform dictionary are scale- and
translation-invariant, i.e., a scaled or translated atom
also belongs to the dictionary, hence the CWT achieves
scale- and translation-invariance (Mallat 2009). How-
ever, the discrete wavelet transform (DWT), which per-
mits fast transforms and exact synthesis from a ﬁnite,
discrete set of wavelet coeﬃcients, suﬀers from a lack
of translation-invariance due to the critical sub-sampling
performed. Approximate translation-invariance can be
achieved by eliminating sub-sampling from the DWT,
leading to the redundant `a trous wavelet transform
(Holschneider et al. 1989; Mallat 2009), which is also
called the stationary wavelet transform.

We adopt the `a trous wavelet transform, which also
achieves dyadic scale-invariance, and use the symlet fam-
ily of wavelets, which are a more symmetric version of the
widely-used Daubechies family of wavelets (Daubechies
1988). We use the implementation provided in the
PyWavelets5 software package. Our results were found

4 http://www.acgc.uct.ac.za/~seikel/GAPP/main.html
5 http://www.pybytes.com/pywavelets/contents.html

6

Lochner et al.

not to be highly dependent on the family of wavelet cho-
sen. Alternative wavelet constructions that achieve bet-
ter translation- and scale-invariance could be considered
(e.g. Kingsbury 2001; Lo et al. 2009; Xiong et al. 2000)
but a detailed optimization of the wavelet transform used
was not performed, highlighting the robustness of our ap-
proach.

For this work, we used 100 points on the Gaussian pro-
cess curve and a two-level wavelet transform, returning
400 (highly redundant) coeﬃcients per ﬁlter, or 1600 co-
eﬃcients per object.

3.4.3. Dimensionality reduction with PCA

The output of the wavelet decomposition is highly
redundant (which preserves translation invariance) and
thus extremely high-dimensional. The ﬁnal step in the
process is to run a PCA to reduce dimensionality. PCA
is a linear transform that transforms a set of correlated
variables into linearly uncorrelated variables using singu-
lar value decomposition of the matrix of variables. From
this, a set of eigenvectors (components) and eigenvalues
are obtained. The components with large eigenvalues
describe the majority of the variability of the dataset
and dimensionality reduction is achieved by discarding
components with negligible eigenvalues. After PCA, we
reduce the dimensionality of the feature set from 1600 to
20 dimensions whilst retaining 98% of the information in
the dataset (as measured by determining the fraction of
the sum of eigenvalues of components retained to that of
all components).

We show the t-SNE plots for the wavelet features with-
out and with redshift in Fig. 2(g) and Fig. 2(h). Here,
the features seem encouragingly well-separated between
classes, even between type Ia and Ibc in some parts of
feature space. We would thus expect the wavelet features
to perform well in classiﬁcation.

4. MACHINE LEARNING FOR CLASSIFICATION

Machine learning is a powerful tool for modern astron-
omy (Bloom & Richards 2012; Ball & Brunner 2010),
becoming increasingly popular in the era of massive data
sets. It allows the automation of tasks previously per-
formed by humans and also, as in this case, allows the
classiﬁcation of objects that are seemingly inseparable
to the human eye. Supervised machine learning algo-
rithms learn a mapping function from training data to
allow them to classify new test objects. A large variety
of machine learning algorithms exist, and it is beyond
the scope of this paper to consider each one. Instead, we
consider ﬁve popular machine learning algorithms that
are fairly representative of the main approaches used.
All these algorithms and several others are comprehen-
sively compared in Caruana & Niculescu-Mizil (2006),
who ﬁnd (as we do) that boosted decision trees and ran-
dom forests often outperform other classiﬁers for many
problems. For all algorithms, we use the Python pack-
age scikit-learn6 (Pedregosa et al. 2011). The selected
algorithms used are able to compute not just the classi-
ﬁcation of a particular object, but also a classiﬁcation
probability, which allows one to construct a ﬁnal dataset
of the desired purity by changing the threshold probabil-
ity for belonging to the class of interest.

6 http://scikit-learn.org/

4.1. Description of machine learning algorithms

The algorithms used in this work are: naive Bayes (a
basic probabilistic classiﬁer), k -nearest neighbors (a clas-
siﬁer based on clustering of features), a multilayer per-
ceptron (the simplest form of artiﬁcial neural network), a
support vector machine (which works by ﬁnding a sepa-
rating hyperplane between classes) and boosted decision
trees (an ensemble method that combines a multitude
of decision trees, a weaker classiﬁer). What follows is a
brief overview of the ﬁve algorithms considered, in each
case referring the reader to references for more details.

4.1.1. Naive Bayes (NB)

Given a set of features for an object, x1, x2, ...xn, the
Naive Bayes (NB) algorithm (Zhang 2004) states that
the probability of that object belonging to class y is

(cid:89)

P (y|x1, ..., xn) ∝ P (y)

P (xi|y),

(5)

i

assuming independence between features. P (y) can be
estimated by determining the frequency of y in the train-
ing set. In the implementation of NB that we use, the
likelihood, P (xi|y), is assumed to be Gaussian, the pa-
rameters of which are determined from the training data
by maximizing the likelihood. The predicted class is sim-
ply determined to be the class that maximizes Eq. (5).

The advantage of NB is that it is fast and scales very
well to high dimensions. The disadvantage is that it as-
sumes independence between features and that the fea-
tures are Gaussian-distributed. One or both of these
assumptions is frequently broken.

4.1.2. K-Nearest neighbors (KNN)

K -Nearest neighbors (KNN) (Altman 1992) is a sim-
ple algorithm which classiﬁes an object by performing a
majority vote of the classes of the k nearest neighbors.
We use a Euclidean distance measure to determine the
nearest neighbors, applying a weight to each neighbor
inversely proportional to the distance.

The probability of an object belonging to class y is
found by summing the weights of all neighbors also be-
longing to class y and dividing by the sum of the weights
of all neighbors. The advantage of KNN is its relative
simplicity and its use as a clustering, unsupervised learn-
ing algorithm. The disadvantages are that it is com-
putationally intensive for large datasets, given that all
pairwise distances have to be calculated; further, the re-
sults are highly dependent on the exact training set used,
making the results fairly inconsistent especially for small
training sets.

4.1.3. Artiﬁcial neural networks (ANN)

Artiﬁcial neural networks (ANN) (Jeﬀrey & Rosner
1986) are a family of machine learning algorithms in-
spired by biological systems of interconnected neurons.
The aim of an ANN is to learn a non-linear function
to map input features to output classes. For this work
we use the commonly-used multi-layer perceptron (MLP)
implemented in scikit-learn.

Each neuron transforms the inputs of the previous
layer by constructing a weighted sum of the features,
followed by some non-linear activation function, in this

Photometric Supernova Classiﬁcation With Machine Learning

7

case a hyperbolic tan function. While in theory diﬀerent
topologies could be used, the MLP is constructed as a
series of layers of neurons. In each layer, each neuron is
connected to all the neurons of the previous layer, but
not any others.
The standard topology is: n → h1 → h2 . . . hm →
c, where n is the number of input features, hi is the
number of neurons in layer i and c is the ﬁnal class. For
this problem, we ﬁnd it does not improve performance
to add more than one hidden layer. The scikit-learn
implementation of a MLP uses backpropagation (Werbos
1974) to determine the weights.

The probability of an object belonging to a particu-
lar class is easily estimated from a MLP by normalizing
the ﬁnal activation function values so that they sum to
one, thus treating them as probabilities. Neural networks
have the advantage of being capable of learning highly
non-linear mappings, often allowing highly accurate clas-
siﬁcations. One disadvantage of neural networks is their
sensitivity to tuning of hyperparameters such as number
of hidden layers and the number of nodes in each layer.

4.1.4. Support vector machines (SVM)

Support vector machines (SVM) (Cortes & Vapnik
1995) are a type of classiﬁer that works by ﬁnding the
hyperplane in feature space that best separates classes
(Ball & Brunner 2010). This hyperplane is deﬁned by the
vectors to the data points nearest the hyperplane, called
the support vectors. Linear SVMs can be extended to
the non-linear case by using the kernel trick (Aızerman
1964) to transform the features to a high-dimensional
space so that non-linear relationships become linear. For
this work, we use a radial basis function as a kernel,
which is a Gaussian function of the Euclidean distance
between features of diﬀerent objects.

Probabilities from an SVM can be obtained using Platt
scaling (Platt et al. 1999), which performs a logistic re-
gression of the SVM’s decision function scores. Platt
scaling is computationally intensive, requiring an extra
cross-validation step to determine the parameters of the
transformation function. SVMs perform extremely well
in general, even in high dimensions, and are highly versa-
tile as diﬀerent kernel functions can be considered to im-
prove feature separation in any particular problem. The
disadvantage of SVMs lies largely in their computational
complexity when computing probabilities (if required).

4.1.5. Boosted decision trees (BDT)

Random forests (Breiman 2001) and boosted decision
trees (Friedman 2002) are ensemble methods built up of
a multitude of decision trees (Morgan & Sonquist 1963;
Breiman et al. 1984).

Decision trees construct a model to map input features
to output classes using a series of decision rules (Ball &
Brunner 2010). In our case, the decisions are based on
whether or not a given feature is in a particular range.
The tree is trained by recursively selecting which feature
and boundary gives the highest information gain in terms
of classiﬁcation. The problem with decision trees is that
the trees created are often complex and do not generalize
well beyond the training set, leading to over-ﬁtting. They
are also sensitive to small changes in the data, leading to
large variance in predicted classes.

These problems can be overcome by combining decision
trees in an ensemble method. Ensemble learning creates
a multitude of decision trees on diﬀerent subsets of data
and averages the resulting classiﬁcations. There are two
main approaches of combining classiﬁers: boosting and
bagging. Random forests are created by bagging, which
selects subsets of data with replacement (randomly in the
case of random forests) on which to perform the classi-
ﬁcation. Boosting repeatedly ﬁts the same dataset but
with each iteration, it increases the weights of incorrectly
classiﬁed objects, meaning subsequent classiﬁers focus on
diﬃcult cases. We found boosted decision trees (using
the AdaBoost algorithm (Freund & Schapire 1997)) gave
the same performance as bagging and were marginally
faster in this case.

Probabilities are straightforward to estimate from de-
cision trees: the probability of belonging to a given class
is simply proportional to the fraction of trained objects
in that class on the corresponding leaf of the tree. With
an ensemble method, the probabilities from each of the
decision trees in the ensemble is averaged to produce a
ﬁnal probability. Boosted decision trees are in general
excellent classiﬁers (Dietterich 2000). Averaging over a
large number of classiﬁcations makes them robust and
unbiased estimators. The disadvantage is that they can
be computationally intensive, although their computa-
tion time is still comparable to SVMs and ANNs.

4.2. Interpreting machine learning results

There are a variety of metrics for measuring the perfor-
mance of a machine learning algorithm. Every machine
learning algorithm we consider returns a probability for
each classiﬁcation. It is common to produce a set of pre-
dicted classiﬁcations by simply selecting, for each object,
the class associated with the highest probability. How-
ever, this is not the full picture, as one can always obtain
a more pure (but less complete) sample by varying the
threshold probability at which an object is considered to
be of a given class. Many metrics common in machine
learning literature (such as purity or completeness) will
depend on this subjective threshold and can be optimized
for a speciﬁc class or goal (for example, obtaining a pure
sample of type Ia supernovae for cosmology). Here we
use a more general metric, receiver operating character-
istic (ROC) curves, to directly compare algorithms.

4.3. Receiver operating characteristic (ROC) curves
A confusion matrix is useful for understanding most
commonly used machine learning metrics. For a binary
classiﬁcation of two classes, positive and negative, the
confusion matrix is shown in Table 4. An ideal classiﬁer
would produce only true positives without any contami-
nation from false positives or losses from false negatives.
In reality, any classiﬁcation problem has a trade-oﬀ be-
tween purity and completeness.

An excellent method of comparing machine learning
algorithms and visualizing this trade-oﬀ is to use ROC
curves (Green & Swets 1966; Hanley & McNeil 1982;
Spackman 1989) (see Fawcett (2004) for an introductory
tutorial). These allow one to get an overview of the algo-
rithms without having to select an arbitrary probability
threshold at which to consider an object as belonging to
a particular class. ROC curves can only compare one

8

TABLE 4

Lochner et al.

as

Confusion matrix for a classification problem with two

classes: positive (P) and negative (N).

completeness =

TP

TP + FN

.

(9)

True class

P

N

Predicted class

P True positive (TP)

False positive (FP)

N False negative (FN) True negative (TN)

class (assumed to be the desired class) against all oth-
ers, but can easily be constructed for each class in turn.
In this paper, all ROC curves are plotted assuming the
binary classiﬁcation of type Ia vs. non-Ia.

ROC curves are constructed by comparing the true
positive rate against the false positive rate, as the proba-
bility threshold is varied. The true positive rate (TPR),
also called recall, sensitivity or completeness, is the ra-
tio between the number of correctly classiﬁed positive
objects and the total number of positive objects in the
data set,

TPR =

TP

TP + FN

.

(6)

The false positive rate (FPR), also referred to as the
false alarm rate or 1− speciﬁcity, is the ratio between
the number of objects incorrectly classiﬁed as positive
and the total number of negative objects in the data set,

FPR =

FP

FP + TN

.

(7)

It is straightforward to see that a good classiﬁcation
algorithm will maximize the true positive rate (thus re-
sulting in a more complete data set) while simultaneously
minimizing the false positive rate (thus reducing con-
tamination). The area-under-the-curve (AUC) of a ROC
curve provides a straightforward way to directly compare
classiﬁers. An AUC of 1 represents perfect classiﬁcation,
visually represented by a curve which reaches the top
left corner. By contrast, a diagonal line would corre-
spond to an AUC of 0.5, meaning the classiﬁer does no
better than random. In the machine learning literature,
an AUC higher than 0.9 typically indicates an excellent
classiﬁer.

In Sec. 6 we use ROC curves to examine the diﬀerences
between machine learning algorithms and our three dif-
ferent approaches to feature extraction.

4.4. Purity and completeness

Commonly-used metrics for classiﬁcation include pu-
rity and completeness. These are only deﬁned for classi-
ﬁed objects, meaning some choice of probability thresh-
old must be made. Usually, an object’s class is selected
as that with the highest probability. A more pure subset
can be obtained by applying a high threshold probabil-
ity. For any subset chosen, it is useful to determine the
purity and completeness to evaluate it.

We can use the confusion matrix in Table 4 to deﬁne

purity (also known as precision) as

purity =

TP

TP + FP

(8)

4.5. Algorithm parameter choices

In supervised learning, the data are split into a training
set, where each object has a known class label, and a test
set, with unknown objects on which the classiﬁer is run.
Choice of training set is crucial, as a training set which is
not representative of the entire dataset can dramatically
decrease a classiﬁer’s performance (see Sec. 6). Addi-
tionally, almost all machine learning algorithms have hy-
perparameters that need to be optimized. In this work,
we use ﬁve-fold cross-validation (Kohavi et al. 1995) to
determine the values of the hyperparameters (see Table
5 for some example parameter values).

Cross-validation involves

repeatedly dividing the
dataset into randomly chosen training and validation
sets. Over a grid of hyperparameter values, cross-
validation uses each validation set to evaluate the per-
formance of the classiﬁer, ﬁnding the hyperparameters
that maximize the AUC.

5. PHOTOMETRIC SUPERNOVA CLASSIFICATION

PIPELINE

For each of the four feature sets (Sec. 3) and ﬁve ma-
chine learning algorithms (Sec. 4) considered, we perform
the following procedure.

We split the data into a training and a test set, con-
sidering both a representative and a non-representative
training set separately. The non-representative training
set is the same as the one used in the SPCC. For a repre-
sentative training set, we randomly select a set of objects
the same size as the one in the challenge (1103 objects).
We were able to repeat the representative case by ran-
domly drawing several new training sets and running the
full pipeline to see by how much results vary. We did not
attempt to create a new non-representative training set
with the same properties as the original.

In the SPCC, photometric host galaxy redshift infor-
mation was provided, so we also investigate the impor-
tance of redshift information by running the full pipeline
with all four feature extraction methods both with and
without redshift. This is important because accurate
photometric redshifts are diﬃcult to obtain and the sen-
sitivity of current and proposed classiﬁcation techniques
to the presence of redshift information has not yet been
established. In the case of parametric models 1 and 2 and
the wavelet decomposition, the redshift is simply added
as an additional feature. For the SALT2 case, we ﬁx the
redshift in the model to the photometric redshift instead
of allowing it to vary, thus better constraining the other
parameters for type Ia’s.

Before running any machine learning algorithm, we
rescale the features with a standard scaling. We scale the
features by removing the mean (centering on zero) and
scaling to unit variance (dividing by the standard devia-
tion). After rescaling, we run each of the machine learn-
ing algorithms outlined in Sec. 4 using cross-validation to
determine the optimal hyperparameters. We then com-
pare the results using ROC curves.

and the completeness, which is equivalent to the TPR,

6. RESULTS

Photometric Supernova Classiﬁcation With Machine Learning

9

(a) SALT2 model, no redshift

(b) SALT2 model, with redshift

(c) Parametric model 1, no redshift

(d) Parametric model 1, with redshift

(e) Parametric model 2, no redshift

(f) Parametric model 2, with redshift

(g) Wavelets, no redshift

(h) Wavelets, with redshift

Fig. 3.— Receiver operating characteristic (ROC) curves for the various feature extraction methods, using a representative training set
without (ﬁrst column) and with (second column) redshift data included. Each curve represents a diﬀerent machine learning algorithm (as
outlined in Sec. 4) with the area-under-curve (AUC) score in brackets. An AUC of 1 indicates perfect classiﬁcation and corresponds to
a curve close to the top left corner. It is clear that boosted decision trees (BDT), support vector machines (SVM) and artiﬁcial neural
networks (ANN) are the best-performing algorithms, superior to k -nearest neighbors (KNN) and naive Bayes (NB). The SALT2 and wavelet
feature sets provide the best classiﬁcation.

0.00.20.40.60.81.0False positive rate0.00.20.40.60.81.0True positive rateNB (0.843)KNN (0.779)ANN (0.930)SVM (0.949)BDT (0.976)0.00.20.40.60.81.0False positive rate0.00.20.40.60.81.0True positive rateNB (0.920)KNN (0.912)ANN (0.963)SVM (0.967)BDT (0.984)0.00.20.40.60.81.0False positive rate0.00.20.40.60.81.0True positive rateNB (0.848)KNN (0.849)ANN (0.906)SVM (0.893)BDT (0.934)0.00.20.40.60.81.0False positive rate0.00.20.40.60.81.0True positive rateNB (0.867)KNN (0.873)ANN (0.935)SVM (0.923)BDT (0.952)0.00.20.40.60.81.0False positive rate0.00.20.40.60.81.0True positive rateNB (0.684)KNN (0.768)ANN (0.852)SVM (0.845)BDT (0.904)0.00.20.40.60.81.0False positive rate0.00.20.40.60.81.0True positive rateNB (0.725)KNN (0.769)ANN (0.860)SVM (0.863)BDT (0.914)0.00.20.40.60.81.0False positive rate0.00.20.40.60.81.0True positive rateNB (0.509)KNN (0.912)ANN (0.929)SVM (0.954)BDT (0.969)0.00.20.40.60.81.0False positive rate0.00.20.40.60.81.0True positive rateNB (0.531)KNN (0.886)ANN (0.945)SVM (0.970)BDT (0.979)10

Lochner et al.

Figure 3 shows the ROC curves for a representative
training set, both without and with redshift information
included. From the ROC curves, it is clear that not all
machine learning algorithms perform equally and indeed,
in every case the boosted decision trees outperform the
other algorithms. These ROC curves also illustrate that
if the fundamental assumptions of an algorithm are bro-
ken, it performs poorly. Naive Bayes performs reason-
ably well in most cases, but is barely better than ran-
dom in the case of the wavelets, which are highly non-
Gaussian in their distribution. We can also see that the
SALT2 features and the wavelet features outperform the
parametric models, implying that it is best to use either
a highly model-dependent approach, making use of prior
knowledge, or with a highly model-independent approach
for supernova classiﬁcation.

Figure 4 shows the Venn diagram for object classiﬁ-
cation for each of the four feature extraction methods,
using boosted decision trees. Objects were classiﬁed ac-
cording to the class with highest probability. From this,
one can see that type II supernovae are straightforward
to classify, with all feature extraction methods achieving
excellent accuracy. Type Ia’s are somewhat more diﬃ-
cult, although the SALT2, Model 1 and wavelets methods
agree well. There is little agreement between classiﬁers
when it comes to the type Ibc’s. This is unsurprising as
Ibc light curves are often similar to Ia’s and there are
relatively few Ibc’s in the dataset. The SALT2 method
does signiﬁcantly better than the others at identifying
Ibc’s.

As stated in Sec. 4.2, the probability threshold at which
you select an object’s class is arbitrary, meaning that
the given class for each supernova will change depending
on the threshold used. It is well known that commonly
used metrics such as accuracy, purity and completeness
are dependent on the threshold used and do not give a
complete picture (hence why we advocate the use of the
AUC as a comparison metric). However as an example of
the values of these metrics for the SPCC dataset, we can
vary the threshold probability and insist on a 90% pure
type Ia dataset. Then the corresponding completeness
is 85%, 41%, 7.8%, 83% for SALT 2, Model 1, Model 2
and wavelets respectively. However, if a more complete
dataset is more important, we ﬁnd that demanding a
90% completeness results in a purity of 87%, 77%, 75%,
87% respectively.

of a representative training set). This is important since
obtaining reliable redshifts for every object in future sur-
veys will be challenging. We have shown that it will be
possible to obtain a relatively pure subsample of super-
novae without redshift information, on which to focus
follow-up observations.

It is also interesting to note that the wavelet method,
which uses no prior knowledge of supernovae, is able to
perform as well as the current state-of-the-art classiﬁ-
cation technique, based on supernova templates. This
is promising since the simulated dataset used here was
constructed using largely the same known templates, so
it is expected that the SALT2 model should perform
well. However, the wavelet approach requires no previous
knowledge and so should perform well with new datasets.
This also suggests wavelet approaches to classiﬁcation are
likely to be useful for broader transient classiﬁcation, as
also noted in Varughese et al. (2015).

6.2. The eﬀect of representativeness

We also investigate the eﬀect of using a representative
or non-representative training set. It is well known that
most machine learning algorithms underperform when
the training set is in any way not representative of the
distribution of features. The training set from the SPCC
was known to be biased in the way spectroscopic follow-
up datasets usually are, in that only the brightest objects
tend to be followed up due to telescope time constraints
and the diﬃculty in obtaining good spectra.
It is not
surprising then that, as shown in Fig. 6, all feature ex-
traction methods and all algorithms perform signiﬁcantly
worse with the non-representative training set. It should
be noted that redshift information was not provided here,
which only mitigates the eﬀect of non-representativeness
in the case of the SALT2 model. This is because the
SALT2 model is already based on a large training set of
supernovae and thus, if redshift information is given, will
produce similar sets of features.

The clear conclusion is that the more representative
the training set, the better any classiﬁcation method will
perform. As the training set was only around 5% of the
size of the full dataset, we would argue that if spectro-
scopic follow-up time is limited, it is rather better to
obtain fewer, fainter objects of all types, than to get a
larger sample of brighter objects.

6.1. The eﬀect of redshift

6.3. Feature importances

The eﬀect of redshift is summarized succinctly in
Fig. 5, comparing all feature extraction methods and ma-
chine learning algorithms. Figure 5 shows that providing
redshift information plays an important role for SALT2
features, mildly improves the results of the parametric
features, and is fairly unimportant to the wavelet fea-
tures. The notable exception to this is when considering
the best machine learning algorithm, boosted decision
trees. With this algorithm, including redshift informa-
tion is not essential as the algorithm is capable of diﬀer-
entiating between classes without redshift information.

This raises the interesting point that with an algorithm
with poor performance, such as KNN, redshift informa-
tion is crucial to classify well with the SALT2 model.
However, a better machine learning algorithm eliminates
the need for redshift information (under the assumption

It is possible to investigate the relative importance of
each feature in a feature set using boosted decision trees.
This is illustrated for each feature set in Fig. 7, both with
and without redshift.

For the SALT2 model, the most important features
are the shape and color parameters. Notice that, coun-
terintuitively, the importance of redshift goes down when
external redshift information is added. By adding red-
shift information, the Ia’s in the dataset are better ﬁt by
the SALT2 model as expected, resulting in accurate esti-
mates of the stretch and color parameters. On the other
hand, with the redshift ﬁxed the best ﬁts for the non-Ia’s
result in unphysical stretch and color parameters. Be-
cause the distributions of these important features now
diﬀer much more between the non-Ia’s and Ia’s, they
increase in importance when classifying. Because all the

Photometric Supernova Classiﬁcation With Machine Learning

11

(a) All types

(b) Type Ia

(c) Type II

(d) Type Ibc

Fig. 4.— Venn diagrams showing the number of correctly classiﬁed objects for each feature extraction method using boosted decision
trees, with each area indicating the percentage of objects correctly classiﬁed. This is repeated for all objects and for each type of supernova
individually. It is clear that, for example, type II supernovae are easy to classify, with all feature extraction methods agreeing on the types
and achieving excellent accuracy, while type Ibc’s are much more diﬃcult to classify.

Fig. 5.— Area-under-curve (AUC) as a function of feature set and machine learning algorithm. Higher AUC scores correspond to better
classiﬁcation. Solid lines indicate redshift was included as a feature, while dashed lines represent feature sets without redshift information.
The ﬁve machine learning algorithms considered are on the x-axis, while each color corresponds to a diﬀerent feature extraction method.
Error bars are from the standard deviation of the AUC from ten runs with randomized training sets. When using BDT, the SALT2 and
wavelet features are able to classify equally well with or without redshift. By comparison, the SALT2 features are highly sensitive to
redshift when using, for example, the NB or KNN algorithm.

4%0%1%0%1%5%0%0%0%0%5%1%2%2%70%SALT2WaveletsModel 1Model 2Total 202162%1%1%0%2%9%0%1%0%0%15%2%3%2%51%SALT2WaveletsModel 1Model 2Total 48230%0%0%0%0%0%0%0%0%0%1%0%1%3%91%SALT2WaveletsModel 1Model 2Total 1273127%0%5%0%2%23%1%0%0%0%5%0%3%0%2%SALT2WaveletsModel 1Model 2Total 2662NBKNNANNSVMBDTAlgorithm0.40.50.60.70.80.91.0AUCSALT2Model 1Model 2WaveletsRedshiftNo redshift12

Lochner et al.

Fig. 6.— Area-under-curve (AUC) as a function of feature set and machine learning algorithm. Higher AUC scores correspond to better
classiﬁcation. Solid lines indicate a representative training set was used, while dashed lines indicate the use of the non-representative
training set of Kessler et al. (2010a,b). The ﬁve machine learning algorithms considered are on the x-axis, while each color corresponds
to a diﬀerent feature extraction method. Error bars for the representative training set case are the standard deviation from ten runs
with randomized training sets. It is clear that any feature extraction method or machine learning algorithm used is highly sensitive to
non-representativeness in the training set.

NBKNNANNSVMBDTAlgorithm0.40.50.60.70.80.91.0AUCSALT2Model 1Model 2WaveletsRepresentativeNon-representativePhotometric Supernova Classiﬁcation With Machine Learning

13

We thank Michael Betancourt, Rahul Biswas, Saurabh
Jha, Bob Nichol, Stephen Smartt and Mark Sulli-
van for useful discussions. We acknowledge support
from STFC for UK participation in LSST through
grant ST/N00258X/1 and travel support provided by
STFC for UK participation in LSST through grants
ST/L00660X/1 and ST/M00015X/1. HVP and ML were
supported by the European Research Council under the
European Community’s Seventh Framework Programme
(FP7/2007- 2013) / ERC grant agreement no 306478-
CosmicDawn. OL and ML acknowledge a European Re-
search Council Advanced Grant FP7/291329 (TESTDE)
and support from STFC.

importances must add up to one, this necessarily dictates
a decrease in the importance of other features, including
the redshift.

There are a few features which are more important for
the parametric models, but none notable. Interestingly,
it is the third and fourth principal components for the
wavelet features which are most important, while all oth-
ers have fairly equivalent importance.

7. CONCLUSIONS

Classiﬁcation of photometric supernovae is an impor-
tant problem in light of current and upcoming surveys.
We have compared several diﬀerent approaches to this
problem, some based on current techniques and some
new. We have shown that in terms of feature extrac-
tion, both SALT2 model ﬁts and wavelet decomposition
outperform parametric ﬁts. Out of the ﬁve representa-
tive machine learning algorithms we used, boosted deci-
sion trees performed the best, followed by support vec-
tor machines and artiﬁcial neural networks. The SALT2
ﬁts and the wavelet approach both achieved an AUC of
0.98, representing excellent classiﬁcation. The SALT2
and wavelet methods can both produce a dataset that is
about 90% pure and 84% complete, although the purity
can be increased at the cost of completeness.

Importantly, we found that with a powerful ensemble
algorithm like boosted decision trees and a representa-
tive training set, redshift information does not improve
classiﬁcation performance, meaning a dataset can be well
classiﬁed without any redshift information at all. This is
extremely useful for current and upcoming surveys where
reliable host galaxy photometric redshifts may be diﬃ-
cult to obtain.

Although redshift information is needed for cosmology,
this means a well-classiﬁed subset of supernovae will be
available without any redshift information, with which
follow-up studies can be done if necessary. This also
implies uncertainties in classiﬁcation will be completely
uncorrelated with uncertainties in redshift estimates. It
also means there will be a large sample of robustly clas-
siﬁed supernovae available for core collapse research.

On the other hand, with all algorithms and feature
extraction methods considered, a representative training
set is crucial for good performance. The training set
used was very small (around 5% of the dataset) suggest-
ing that a spectroscopic follow-up program should rather
focus on observing fainter objects of all types rather than
obtaining a larger training set which is not representa-
tive.

There are many further improvements one can consider
in both the feature extraction methods and the machine
learning algorithms. We have provided a framework here
for directly and easily comparing these and any future
approaches to the photometric supernova classiﬁcation
problem.

In future work we will apply this pipeline to SDSS data
and compare against current photometric classiﬁcations.
Additionally, the pipeline developed here will be crucial
in investigating how changes in observing strategy aﬀect
classiﬁcation for LSST.

ACKNOWLEDGMENTS

14

Lochner et al.

(a) SALT2

(b) Parametric model 1

(c) Parametric model 2

Fig. 7.— Relative importances of individual features with and without redshift information, as obtained from the boosted decision trees
algorithm. The SALT2 model is able to estimate the redshift, which is thus still used as a feature even when no redshift information is
provided (and a broad, ﬂat prior is used for the ﬁtting). For parametric models 1 and 2, the ﬁlter is given in square brackets before the
parameter. For the wavelets, the 20 principal components used are labeled C1 to C20.

(d) Wavelets

zt0x0x1cFeature0.000.050.100.150.200.250.300.350.40Relative importanceWithout redshiftWith redshift[g]log(A)[g][g]log()[g]log(k)[g]log()[r]log(A)[r][r]log()[r]log(k)[r]log()[i]log(A)[i][i]log()[i]log(k)[i]log()[z]log(A)[z][z]log()[z]log(k)[z]log()zFeature0.000.020.040.060.080.100.12Relative importanceWithout redshiftWith redshift[g]log(A)[g]log(B)[g]t0[g]t1[g]Trise[g]Tfall[r]log(A)[r]log(B)[r]t0[r]t1[r]Trise[r]Tfall[i]log(A)[i]log(B)[i]t0[i]t1[i]Trise[i]Tfall[z]log(A)[z]log(B)[z]t0[z]t1[z]Trise[z]TfallzFeature0.000.010.020.030.040.050.060.070.080.09Relative importanceWithout redshiftWith redshiftC0C1C2C3C4C5C6C7C8C9C10C11C12C13C14C15C16C17C18C19C20zFeature0.000.020.040.060.080.100.120.140.16Relative importanceWithout redshiftWith redshiftPhotometric Supernova Classiﬁcation With Machine Learning

15

REFERENCES

Aızerman, M. 1964, Automation and Remote Control, 25, 821
Altman, N. S. 1992, The American Statistician, 46, 175
Ball, N. M., & Brunner, R. J. 2010, International Journal of

Kessler, R., Bassett, B., Belov, P., et al. 2010b, Publications of

the Astronomical Society of the Paciﬁc, 122, 1415

Kingsbury, N. 2001, Applied and Computational Harmonic

Modern Physics D, 19, 1049

Barbary, K. 2014, sncosmo v0.4.2, doi:10.5281/zenodo.11938
Bernstein, J. P., Kessler, R., Kuhlmann, S., et al. 2012, The

Astrophysical Journal, 753, 152

Betoule, M., Kessler, R., Guy, J., et al. 2014, Astronomy &

Astrophysics, 568, A22

Analysis, 10, 234

Knights, M., Bassett, B. A., Varughese, M., et al. 2013, Journal

of Cosmology and Astroparticle Physics, 1, 039

Kohavi, R., et al. 1995in , 1137–1145
Li, J., Cheng, K., Wang, S., et al. 2016, ArXiv e-prints,

arXiv:1601.07996

Bloom, J. S., & Richards, J. W. 2012, Data Mining and Machine
Learning in Time-Domain Discovery and Classiﬁcation, 89–112

Breiman, L. 2001, Machine Learning, 45, 5
Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. 1984,

Classiﬁcation and regression trees (CRC press)

Lo, E. H. S., Pickering, M. R., Frater, M. R., & Arnold, J. F.

2009, in Proceedings of the ACM International Conference on
Image and Video Retrieval, CIVR ’09 (New York, NY, USA:
ACM), 5:1–5:8

LSST Science Collaboration. 2009, ArXiv e-prints,

Buchner, J., Georgakakis, A., Nandra, K., et al. 2014, Astronomy

arXiv:0912.0201

& Astrophysics, 564, A125

Caruana, R., & Niculescu-Mizil, A. 2006, in Proceedings of the

23rd International Conference on Machine Learning, ICML ’06
(New York, NY, USA: ACM), 161–168

Cortes, C., & Vapnik, V. 1995, Machine learning, 20, 273
Dark Energy Survey Collaboration. 2005, ArXiv Astrophysics

e-prints, astro-ph/0510346

Dark Energy Survey Collaboration, Abbott, T., Abdalla, F. B.,

et al. 2016, ArXiv e-prints, arXiv:1601.00329

Daubechies, I. 1988, Communications on pure and applied

mathematics, 41, 909

Dietterich, T. G. 2000, in Multiple classiﬁer systems (Springer),

1–15

du Buisson, L., Sivanandam, N., Bassett, B. A., & Smith, M.

2015, Monthly Notices of the Royal Astronomical Society, 454,
2026

Fawcett, T. 2004, Machine learning, 31, 1
Feroz, F., & Hobson, M. P. 2008, Monthly Notices of the Royal

Astronomical Society, 384, 449

Feroz, F., Hobson, M. P., & Bridges, M. 2009, Monthly Notices of

the Royal Astronomical Society, 398, 1601

Feroz, F., Hobson, M. P., Cameron, E., & Pettitt, A. N. 2013,

ArXiv e-prints, arXiv:1306.2144

Freund, Y., & Schapire, R. E. 1997, Journal of computer and

system sciences, 55, 119

Friedman, J. H. 2002, Computational Statistics & Data Analysis,

38, 367

Frieman et al. 2008, The Astronomical Journal, 135, 338
Green, D., & Swets, J. 1966, Society, 1, 521
Guy, J., Astier, P., Baumont, S., et al. 2007, Astronomy &

Astrophysics, 466, 11

Hanley, J. A., & McNeil, B. J. 1982, Radiology, 143, 29
Holschneider, M., Kronland-Martinet, R., Morlet, J., &

Tchamitchian, P. 1989, in Wavelets (Springer), 286–297

Hotelling, H. 1933, Journal of educational psychology, 24, 417
J. Newling et al. 2012, Monthly Notices of the Royal

Astronomical Society, 421, 913

Jeﬀrey, W., & Rosner, R. 1986, The Astrophysical Journal, 310,

473

Jha, S., Riess, A. G., & Kirshner, R. P. 2007, The Astrophysical

Journal, 659, 122

Karpenka, N. V., Feroz, F., & Hobson, M. P. 2013, Monthly

Notices of the Royal Astronomical Society, 429, 1278

Kessler, R., Conley, A., Jha, S., & Kuhlmann, S. 2010a, ArXiv

e-prints, arXiv:1001.5210

M. Kunz, B. Bassett and R. Hlozek. 2007, Phys.Rev.D, 75, 103508
MacKay, D. J. 2003, Information theory, inference and learning

algorithms (Cambridge university press)

Mallat, S. G. 2009, A wavelet tour of signal processing, 3rd edn.

(Burlington: Academic Press)

Morgan, J. N., & Sonquist, J. A. 1963, Journal of the American

statistical association, 58, 415

Newling, J., Varughese, M., Bassett, B., et al. 2011, Monthly

Notices of the Royal Astronomical Society, 414, 1987
Pearson, K. 1901, The London, Edinburgh, and Dublin

Philosophical Magazine and Journal of Science, 2, 559

Pedregosa, F., Varoquaux, G., Gramfort, A., et al. 2011, Journal

of Machine Learning Research, 12, 2825

Platt, J., et al. 1999, Advances in large margin classiﬁers, 10, 61
R. Hlozek et al. 2012, The Astrophysical Journal, 752, 79
Rubin, D., Aldering, G., Barbary, K., et al. 2015, The

Astrophysical Journal, 813, 137

Sako, M., Bassett, B., Becker, A., et al. 2008, The Astronomical

Journal, 135, 348

Sako, M., Bassett, B., Connolly, B., et al. 2011, The

Astrophysical Journal, 738, 162

Sako, M., Bassett, B., Becker, A. C., et al. 2014, ArXiv e-prints,

arXiv:1401.3317

Sanders, N. E., Soderberg, A. M., Gezari, S., et al. 2015, The

Astrophysical Journal, 799, 208

Seikel, M., Clarkson, C., & Smith, M. 2012, Journal of Cosmology

and Astroparticle Physics, 6, 036

Spackman, K. A. 1989, in Proceedings of the sixth international
workshop on Machine learning, Morgan Kaufmann Publishers
Inc., 160–163

Valens, C. 1999, ed. Clemens Valens
Van der Maaten, L., & Hinton, G. 2008, Journal of Machine

Learning Research, 9, 85

Varughese, M. M., von Sachs, R., Stephanou, M., & Bassett,

B. A. 2015, Monthly Notices of the Royal Astronomical Society,
453, 2848

Werbos, P. 1974
Wright, D. E., Smartt, S. J., Smith, K. W., et al. 2015, Monthly

Notices of the Royal Astronomical Society, 449, 451

Xiong, H., Zhang, T., & Moon, Y. S. 2000, IEEE Transactions on

Image Processing, 9, 2100

Zhang, H. 2004, AA, 1, 3

APPENDIX

HYPERPARAMETER SELECTION

Table 5 shows the values of the hyperparameters for each machine learning algorithm for an example case. These
hyperparameters are selected using cross-validation (as described in Sec. 4) to maximize the AUC. These values vary
slightly depending on training set used.

16

Lochner et al.

TABLE 5

Hyperparameters for each machine learning algorithm and each feature set for the case where redshift is included and a

representative dataset is used.

SALT2

Model 1

Model 2

-

-

-

NB
KNN n neighbors=141
n neighbors=46
SVM C=0.56, γ=1.78 C=1780,γ=0.0032 C=31.6,γ=0.0032
ANN
BDT n estimators=40

n neighbors=101

neurons=110

n estimators=75

n estimators=65

neurons=110

neurons=115

Wavelets

-

n neighbors=121
C=0.56,γ=1.78

neurons=80

n estimators=70

