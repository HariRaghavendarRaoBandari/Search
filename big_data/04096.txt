6
1
0
2

 
r
a

 

M
3
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
6
9
0
4
0

.

3
0
6
1
:
v
i
X
r
a

Multi-Target Tracking Using A Randomized

Hypothesis Generation Technique

W. Faber and S. Chakravorty Department of Aerospace Engineering

Texas A&M University

College Station, TX Islam I. HusseinApplied Defense Solutions

Columbia, MD

Abstract

In this paper, we present a randomized version of the ﬁnite set statistics (FISST) Bayesian recursions for multi-
object tracking problems. We propose a hypothesis level derivation of the FISST equations that shows that the
multi-object tracking problem may be considered as a ﬁnite state space Bayesian ﬁltering problem, albeit with a
growing state space. We further show that the FISST and Multi-Hypothesis Tracking (MHT) methods for multi-
target tracking are essentially the same. We propose a randomized scheme, termed randomized FISST (R-FISST),
where we sample the highly likely hypotheses using Markov Chain Monte Carlo (MCMC) methods which allows
us to keep the problem computationally tractable. We apply the R-FISST technique to a ﬁfty-object birth and death
Space Situational Awareness (SSA) tracking and detection problem. We also compare the R-FISST technique to the
Hypothesis Oriented Multiple Hypothesis Tracking (HOMHT) method using an SSA example.

I. INTRODUCTION

In this paper, we present a randomized approach to approximate the full Bayesian recursions involved in solving
the Finite Set Statistics (FISST) based approach to the problem of multi-object tracking and detection. We show
that the FISST recursions can essentially be considered as a discrete state space Bayesian ﬁltering problem on
“Hypothesis Space” with the only input from the continuous problem coming in terms of the likelihood values of
the different hypotheses. The number of objects is implicit in this technique and can be a random variable. The
”Hypothesis Space” perspective allows us to develop a randomized version of the FISST recursions where we sample
the highly likely children hypotheses using a Markov Chain Monte Carlo (MCMC) technique thereby allowing us
to keep the total number of possible hypotheses under control, and thus, allows for a computationally tractable
implementation of the FISST equations, which otherwise grows at an exponential rate, and thus, can quickly lead to
the problem becoming intractable. The method is applied to a ﬁfty-object SSA tracking and detection problem that
has an unﬁxed number of objects throughout the simulation. The method is then compared to that of a well-known
tracking method, HOMHT, using a ﬁfteen-object SSA example and a variety of data association scenarios.

In the last 20 years, the theory of FISST-based multi-object detection and tracking has been developed based
on the mathematical theory of ﬁnite set statistics [1], [2]. The greatest challenge in implementing FISST in real-
time, which is critical to any viable SSA solution, is computational burden. The ﬁrst-moment approximation of

1

FISST is known as the Probability Hypothesis Density (PHD) approach [2], [3]. The PHD has been proposed as a
computationally tractable approach to applying FISST. The PHD ﬁlter essentially ﬁnds the density of the probability
of an object being at a given location, and thus, can provide information about the number of objects (integral of
the PHD over the region of interest) and likely location of the objects (the peaks of the PHD). The PHD can
further employ a Gaussian Mixture (GM) or a particle ﬁlter approximation to reduce the computational burden (by
removing the need to discretize the state space). This comes at the expense of approximating the general FISST
pdf with its ﬁrst-moments [3]–[6]. The PHD ﬁlter does not attempt to solve the full FISST recursions, in particular,
by considering the PHD, the ﬁlter gets rid of the data association problem inherent in these problems. In other
previous work, a GM approximation was applied, not to the ﬁrst-moment of the FISST pdfs, but to the original
full propagation and update equations derived from FISST [7], [8]. This eliminates any information loss associated
with using the ﬁrst-moment PHD approximation, while at the same time increasing the computational tractability
of the multi-object FISST pdfs. This approach is similar in spirit to the concept of the “para-Gaussian” pdf that
was described in [9].

In this paper, in contrast, we introduce a hypothesis level derivation of the FISST equations that makes it clear
as to how the full FISST recursions can be implemented without any approximation other than the approximations
inherent in the underlying tracking ﬁlter, such as an extended Kalman ﬁlter. We introduce a simpliﬁed model for
the birth and death process that allows for only one birth or one death in any time interval, thereby controlling the
number of birth and death hypotheses while still being able to converge to the correct hypothesis regarding the total
number of objects given the birth objects remain in the ﬁeld of view for long enough. Further, in order to ensure the
computational tractability of the resulting equations, we introduce an MCMC based hypothesis selection scheme
resulting in the Randomized FISST (R-FISST) approach that is able to scale the FISST recursions to large scale
problems. We call our method R-FISST, since as in FISST, the hypotheses in our method have varying number
of objects, and in essence, give a probabilistic description of the random ﬁnite set representing the multi-target
probability distribution. We also formally show the equivalence of the two methods in the appendix.

There are also non-FISST based approaches to multi-hypothesis tracking (MHT) such as the Hypothesis Oriented
MHT (HOMHT) [10]–[13], and the track oriented MHT (TOMHT) techniques [14]. The MHT techniques can be
divided into single-scan and multi-scan methods depending on whether the method uses data from previous times to
distinguish the tracks [11], [13], [15]. The single-scan (recursive) methods such as joint probabilistic data association
(JPDA) [13], [15] typically make the assumption that the tracks are independent which is not necessarily true. The
multi-scan methods such as TOMHT [13], [14] are not recursive. The primary challenge in these methods is
the management of the various different hypotheses related to the tracks which TOMHT does using an efﬁcient
tree structure, and the MCMCDA, and other related tracking techniques [16]–[18], do through the use of MCMC
methods in sampling the data associations. We also use MCMC to sample children hypotheses given the parent
hypothesis, however, our approach is a truly recursive technique which does not assume track independence as
the above mentioned single scan methods. We essentially do an efﬁcient management of the growing number of
hypotheses at every generation through the judicious use of MCMC. A primary contribution of this paper is to show

2

that the MHT technique and the FISST technique are essentially the same, modulo the set-theoretic representation
of multi-target pdfs in FISST (which, however, does not provide any extra information). This is made possible
through the hypothesis level derivation of the tracking equations that considers the full hybrid state of the problem.
This allows us to identify the critical structure inherent in the FISST recursions that enables us to unify the two
approaches. The contributions of this paper are as follows:

1) We introduce a hypothesis level derivation of the FISST recursions and show its equivalence to the standard
FISST recursions. This allows us to implement the full FISST recursions as opposed to ﬁrst moment
approximations of the recursions such as PHD.

2) We unify the FISST and MHT methodologies based on the hypothesis level derivation.
3) We propose a randomized MCMC based hypothesis generation technique, called RFISST, that allows us to

keep the FISST recursions computationally tractable.

4) We apply our methods to the SSA problem and perform a detailed comparison of our technique with the

HOMHT technique.

The rest of the paper is organized as follows. In Section II, we introduce the hypothesis level derivation of
the FISST equations. In Section III, we introduce the MCMC based randomized hypothesis selection technique
that results in the RFISST algorithm. In Section IV, we show an application of the RFISST technique to a ﬁfty-
object birth death SSA scenario. Section V discusses the comparison with HOMHT. In the appendix, we show the
equivalence of the FISST and the hypothesis level FISST equations. This paper is an expanded and uniﬁed version
of the references [19], [20].

II. A HYPOTHESIS BASED DERIVATION OF THE FISST EQUATIONS

In this section, we shall frame the multi-object tracking equations at the discrete hypothesis level ( which we
believe are equivalent to the FISST equations) which then shows clearly as to how the full FISST recursions may
be implemented. The derivation below assumes that the number of measurements is always less than the number
of objects, which is typically the case in the SSA problem. We never explicitly account for the number of objects,
since given a hypothesis, the number of objects and their probability density functions (pdf) are ﬁxed, which allows
us to derive the results without having to consider the random ﬁnite set (RFS) theory underlying FISST. Albeit the
equations derived are not as general as the FISST equations, in particular, the birth and death models employed here
are quite simple, we believe that the level of generality is sufﬁcient for the SSA problem that is our application.

A. Framing FISST at the Hypothesis Level

We consider ﬁrst the case when the number of objects is ﬁxed, which we shall then generalize to the case when
the number of objects is variable, i.e, there is birth and death in the object population. Assume that the number of
objects is M, and each object state resides in (cid:60)N . Consider some time instant t − 1, and the data available for the
multi-object tracking problem till the current time F t−1. Let Hi denote the ith hypothesis at time t − 1, and let
{X} denote the underlying continuous state. For instance, given the N− object hypothesis, the underlying state

M(cid:89)

space would be {X} = {X1, X2,··· XN} where Xj denotes the state of the jth object under hypothesis Hi and
resides in (cid:60)N . Let p({X}, i/F t−1) denote the joint distribution of the state-hypothesis pair after time t− 1. Using
the rule of conditional probability:

3

p({X}, i/F t−1) = p({X}/i,F t−1)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

p(i/F t−1)
wi=prob. ofHi

(cid:125)

,

(1)

MT-pdf underlyingHi

where MT-pdf is the multi-object pdf underlying a hypothesis. Given the hypothesis, the MT-pdf is a product of
independent individual pdfs underlying the objects, i.e.,

p({X}/i,F t−1) =

pk(xk),

(2)

where pk(.) is the pdf of the kth object.

Remark 1: In random ﬁnite set theory, the arguments of the MT-pdf {x1, x2 ··· xM} above are interchangeable

k=1

and thus, the MT-pdf is represented as:

p({X}/i,F t−1) =

(cid:88)

M(cid:89)

¯σ

k=1

pσk (xk),

(3)

where ¯σ = {σ1, σ2 . . . σM} represents all possible permutations of the indices {1, 2··· M}. Hence, in any integration
M ! is used. In our case, we explicitly assign the index
involving such a set of indices, a normalizing factor of
xk to the target k or more precisely, the kth component of the MT-pdf, pk(.). Note that such an assignment is
always possible and there is no information loss in such a representation. Moreover, at the expense of a bit more
bookkeeping, this allows us to keep track of the labels of the different components of our multi-target pdfs. Please
also see the appendix where we show the equivalence of the hypothesis level equations derived here and the FISST
recursions.

1

Next, we consider the prediction step between measurements. Each hypothesis Hi splits into AM children
hypotheses, and let us denote the jth child hypothesis as Hij. The children hypotheses correspond to the different
data associations possible given a measurement of size m, i.e., m returns, and

min(m,M )(cid:88)

(cid:18)M

(cid:19)(cid:18)m
(cid:19)

n=0

n

n

AM =

n!.

(4)

We want to note here that this is a pseudo-prediction step since we assume that we know the size of the return m.
However, it allows us to ﬁt the MT-tracking method nicely into a typical ﬁltering framework. Using the rules of
total and conditional probability, it follows that the predicted multi-object pdf in terms of the children hypotheses

is:

p−({X}, (i, j)/F t−1) =

(cid:90)

p({X}, (i, j)/{X(cid:48)}, i)p({X(cid:48)}, i/F t−1)d{X(cid:48)} =

(cid:90)
(cid:124)

p({X}/(i, j),{X(cid:48)})p({X(cid:48)}/i,F t−1)d{X(cid:48)}

p(i/F t−1)
,

(cid:124) (cid:123)(cid:122) (cid:125)

p(j/i)

pij

(cid:124)

(cid:125)

(cid:123)(cid:122)

wi

(cid:125)

(cid:123)(cid:122)

p−({X}/(i,j),F t−1)

4

(5)

where p−(., (i, j)/F t−1) is the joint distribution of the state and hypothesis pairs before the measurement at time t.
We have used the fact that p((i, j)/{X(cid:48)}, i) = p(j/i) = pij, and pij is the transition probability of going from the
parent i to the child j and wi is the probability of the parent hypothesis Hi. Let pk(xk/x(cid:48)
k) denote the transition
density function of the kth object. Expanding the predicted MT-pdf, we obtain:

p−({X}/(i, j),F t−1) ≡

p({X}/(i, j),{X(cid:48)})p({X(cid:48)}/(i),F t−1)d{X(cid:48)},

(6)

(cid:90)

(cid:90)

pk(xk/x(cid:48)
k)

where

≡

p({X}/(i, j),{X(cid:48)}) ≡ M(cid:89)
(cid:90) (cid:89)

(cid:89)

k=1

pk(xk/x(cid:48)
k)

k

k(cid:48)

p({X}/(i, j),{X(cid:48)})p({X(cid:48)}/(i),F t−1)d{X(cid:48)}

(cid:90)

(cid:89)

(cid:89)

pk(cid:48)(x(cid:48)

k)dx(cid:48)

1 ··· dx(cid:48)

M =

pk(xk/x(cid:48)

k)pk(x(cid:48)

k)dx(cid:48)

k =

p−
k (xk),

(7)

k

k

where p−

k (xk) is the prediction of the kth object pdf underlying the hypothesis Hij.

Remark 2: Eq. 5 has a particularly nice hybrid structure: note that the ﬁrst factor is the multi-object continuous
pdf underlying the child hypothesis Hij, while the second factor pijwi is the predicted weight of the hypothesis
Hij. For the no birth and death case, in the absence of any a priori information regarding the sensor, all pij are
equal to 1
, where recall that AM is the total number of data associations possible (Eq. 4). However, if a priori
AM
information, for instance, in terms of a probability of detection pD is available, then:

(cid:0)m
(cid:1)k!
D(1 − pD)M−k
pk
if ij is a data association in which k of the M targets are associated to measurements. The(cid:0)m
j pij = 1. To see this, note that there are exactly(cid:0)m
so that pij is a valid probability distribution , i.e.,(cid:80)
that m measurements can be assigned to k targets, and there are(cid:0)M
(cid:19) pk
(cid:18)M
(cid:0)m
(cid:1)k!
D(1 − pD)M−k

(cid:1)k! factor is required
(cid:1)k! ways
(cid:1) ways of choosing the k targets to associate
(cid:18)m
(cid:19)

to the k chosen measurements. We are assuming here that M < m which is almost always the case. Thus:

(cid:88)

M(cid:88)

k! = 1.

pij =

pij =

(9)

(8)

k

k

k

k

k

k

,

j

k=0

k

Note that the MT-pdf underlying Hij is simply the product of the predicted individual object pdf, and in the case
of no birth and death, it is the same for all children hypothesis Hij.

Given the prediction step above, let us consider the update step given the measurements {Zt} = {z1,t,··· zm,t},
where there are m measurement returns. We would like to update the weights of each of the multi-object hypotheses

to obtain p({X}, (i, j)/{Zt},F t−1) by incorporating the measurement {Zt}. Using Bayes rule:

p({X}, (i, j)/{Zt},F t−1) = ηp({Zt}/{X}, (i, j))p−({X}, (i, j)/F t−1),

5

where

(cid:90)

(cid:88)

η =

p({Zt}/{X(cid:48)}, (i(cid:48), j(cid:48)))p−({X(cid:48)}, (i(cid:48), j(cid:48))/F t−1)d{X(cid:48)},

where

(cid:82) p({Zt}/{X(cid:48)}, (i(cid:48), j(cid:48)))p−({X(cid:48)}, (i(cid:48), j(cid:48))/F t−1)d{X(cid:48)} are deﬁned in Eqs. 13 and 15 below. Using the prediction

p({Zt}/{X}, (i, j))

the MT-likelihood

normalizing

function

Bayes

and

the

factor

i(cid:48),j(cid:48)

(cid:123)(cid:122)

equation 5, it follows that:

(cid:124)

p({X},(i,j)/F t)

(cid:125)

p({X}, (i, j)/{Zt},F t−1)

= ηp({Zt}/X, (i, j))p−({X}/(i, j),F t−1)pijwi.

(10)

We may then factor the above equation as follows:

p({X}, (i, j)/F t) =

p({Zt}/{X}, (i, j))p−({X}/(i, j),F t−1)

lij

(cid:80)

lij

wij
pijwi

(cid:122)(cid:125)(cid:124)(cid:123)
(cid:124) (cid:123)(cid:122) (cid:125)

wi(cid:48)j(cid:48)

i(cid:48),j(cid:48) li(cid:48),j(cid:48) pi(cid:48)j(cid:48)wi(cid:48)

,

(11)

where

(cid:90)

lij =

p({Zt}/{X(cid:48)}, (i, j))p−({X(cid:48)}/(i, j),F t−1)d{X(cid:48)}.

(12)

Note that lij is likelihood of the data {Zt} given the multi-object pdf underlying hypothesis Hij, and the particular
data association that is encoded in the hypothesis.

Remark 3: It behooves us to understand the updated pdf underlying the child hypothesis Hij, the ﬁrst factor on
the right hand side of Eq. 11. Let pD denote the probability of detection of a object given that it is in the ﬁeld
of view (FOV) of the monitoring sensor(s). Let g(z) denote the probability that the observation z arises from a
clutter source. Let Hi denote an M−object hypothesis with object states {X} = {X1,··· XM} governed by the
pdfs p1(x1),··· pM (xM ). Let the child hypothesis Hij correspond to the following data association hypothesis:
z1 → Xj1 ,··· zm → Xjm. Then, we deﬁne the MT-likelihood function:

p({Zt}/{X}, (i, j)) ≡ p({z1 ··· zm}/{X1 = x1,··· XM = xM}, (i, j)) = [

p(zk/Xjk = xjk )],

(13)

where p(zk/Xjk = xjk ) is simply the single object observation likelihood function for the sensor. Thus,

p({Zt}/{X}, (i, j))p−({X}/(i, j),F t−1) = [

p(zk/Xjk = xjk )p−

jk

(xjk )][

p−
l (xl)],

(14)

where l (cid:54)= jk denotes all objects Xl that are not associated with a measurement under hypothesis Hij. Further,

m(cid:89)

k=1

m(cid:89)

k=1

(cid:89)

l(cid:54)=jk

deﬁning the MT-Bayes factor as:

(cid:90)
m(cid:89)

[

(cid:90)
(cid:90)

≡

m(cid:89)

(
k=1

= [

lij =

p({Zt}/{X(cid:48)}, (i, j))p−({X(cid:48)}/(i, j),F t−1)d{X(cid:48)}

p(zk/Xjk = x(cid:48)

jk

)p−

jk

(x(cid:48)

jk

)] × [

l)]dx(cid:48)

1..dx(cid:48)

M

k=1

p(zk/Xjj = x(cid:48)

jk

)p−

jk

(x(cid:48)

jk

)dx(cid:48)

jk

l(cid:54)=jk
)] × [

(cid:89)

p−
l (x(cid:48)
(cid:90)
(cid:89)
m(cid:89)

l(cid:54)=jk

=

k=1

p−
l (x(cid:48)

l)dx(cid:48)
l]

p(zk/Xjk ),

where p(zk/Xjk ) ≡(cid:82) p(zk/Xjk = x(cid:48)
(cid:82) p({Zt}/{X(cid:48)}, (i, j))p−({X(cid:48)}/(i, j),F t−1)d{X(cid:48)} =

)dx(cid:48)
p({Zt}/{X}, (i, j))p−({X}/(i, j),F t−1)

)p−

(x(cid:48)

jk

jk

jk

jk. Hence,

(cid:81)m
(cid:82) p(zk/Xjk = x(cid:48)
(cid:81)m
k=1 p(zk/Xjk = xjk )p−
(xjk )
)p−
(x(cid:48)
)dx(cid:48)
m(cid:89)

× (cid:89)
pjk (xjk /zk) × (cid:89)

k=1

l(cid:54)=jk

=

jk

jk

jk

jk

jk

k=1

l(cid:54)=jk

p−
l (xl),

(16)

6

(15)

p−
l (xl)

(17)

(18)

(xjk ), and p−

where pjk (xjk /zk) denotes the updated object pdf of Xjk using the observation zk and the predicted prior pdf
l (xl) is the predicted prior pdf of Xl whenever l (cid:54)= jk, i.e., the pdf of object Xl is not updated with
p−
jk
any measurement. In the above, we have assumed that all the measurements are assigned to objects, however, some
of the measurements can also be assigned to clutter, in which case, the object pdfs are updated exactly as above,
i.e., all objects’ predicted prior pdfs associated with data are updated while the unassociated objects’ predicted
priors are not updated, except now the likelihoods lijof the children hypothesis Hij are given by:

where

p(zi/Xji) =

i=1

lij =

p(zi/Xji),

m(cid:89)

(cid:82) p(zi/x)pji(x)dx if Xji ∈ T

if Xji ∈ C

g(zi)

where T is the set of all objects and C is clutter, m(cid:48) is the number of objects associated to measurements, and the
above equation implies that the measurement zi was associated to clutter if Xji ∈ C.

Remark 4: The recursive equation 11 above has a particularly nice factored hybrid form. The ﬁrst factor is just a
continuous multi-object pdf that is obtained by updating the predicted multi-object pdf obtained by associating the
measurements in {Zt} to objects according to the data association underlying Hij. The second factor corresponds
to the update of the discrete hypothesis weights.

Remark 5: Given that there is an efﬁcient way to predict/ update the multi-object pdfs underlying the different
hypotheses, Eq. 11 actually shows that the FISST recursions may essentially be treated as a purely discrete problem

7

living in the “Hypothesis level” space. The ”hypothesis level” weights are updated based on the likelihoods lij which
is determined by the continuous pdf underlying Hij. Also, the continuous pdf prediction and updates are independent
of the hypothesis level prediction and updates, i.e, the hypothesis probabilities do no affect the multi-object pdfs
underlying the hypotheses.
Thus, given that the likelihoods of different hypothesis lij arise from the underlying multi-object pdf and the
encoded data association in the hypotheses Hij, the FISST updates can be written purely at the hypothesis level
as follows:

(cid:80)

wij :=

lijwij

i(cid:48),j(cid:48) li(cid:48)j(cid:48)wi(cid:48)j(cid:48)

,

(19)

where wij = pijwi. Thus, we can see that the FISST update has a particularly simple Bayesian recursive form when
viewed at the discrete hypothesis level, given that the multi-object pdfs underlying the hypotheses Hij are tracked
using some suitable method. We can summarize the above development of the Bayesian recursion for multi-object
tracking as follows:

Proposition 1: Given an M−object hypothesis Hi, and its children hypotheses Hij, that correspond to the data

associations {zi → Xji}, the joint MT-density, hypothesis weight update equation is:

p({X}, (i, j)/F t) = p({X}/(i, j),F t)

wijlij

i(cid:48),j(cid:48) wi(cid:48)j(cid:48)li(cid:48)j(cid:48)

,

where wij = pijwi, lij is given by Eq. 17, and the MT-pdf underlying Hij:

m(cid:89)

p({X}/(i, j),F t) =

pjk (xjk /zk)

p−
l (xl),

(cid:80)
(cid:89)

l(cid:54)=jk

where pjk (Xjk /zjk ) denotes the predicted prior of object Xjk, p−
is the predicted prior for all objects Xl that are not associated.

jk

k=1

(xk), updated by the observation zjk, and p−

l (xl)

We may renumber our hypothesis Hij into a parent of the next generation of hypothesis through a suitable
map F ((i, j)) that maps every pair (i, j) into a unique positive integer i(cid:48), and start the recursive procedure again.
However, the trouble is that the number of hypotheses grows combinatorially at every time step since at every step
the number of hypotheses grow by the factor AM (Eq. 4), and thus, the above recursions can quickly get intractable.

B. Relationship to MHT

for the (cid:0)m

k

The equations derived above are essentially the same equations as those derived in the MHT framework except

(cid:1)k! factor in each hypothesis weight that is needed to normalize the MT-likelihood function similar to
(cid:90)

the case of a standard likelihood function. More speciﬁcally, consider Eqs. 8 and 13. These two equations assure
us that:

(cid:90) (cid:88)

m(cid:89)

p({z1, .., zm}/{x1, ..xM}, i)dz1..dzm =

pij

p(zl/Xjl = xjl )dz1..dzm = 1,

(20)

j

l=1

for any MT-state {x1, ..xM}, given that the parent hypothesis i is an M-target hypothesis, i.e., the total likelihood of
all possible children hypothesis resulting from a parent hypothesis normalizes to unity, something that is typically
required of the likelihood function p(z/x) in a standard tracking/ ﬁltering problem. The MHT likelihood for a child
hypothesis j of parent i is of the form:

ηM HT
ij

(z1, ..zm) = pk

D(1 − pD)M−k

p(zl/ˆxjl ),

(21)

m(cid:89)

l=1

k(1 − pD)M−k

(cid:0)m

(cid:1)k!

k

m(cid:89)

l=1

8

(23)

(24)

where ˆxjl is the mean of the pdf of the target Xjl or p(zk/ˆxjl ) = g(zl) if measurement zl is associated to clutter.
The equivalent hypothesis likelihood in the hypothesis level FISST (H-FISST) derivation is:

ηHF ISST
ij

(z1, ..zm) = pijlij =

pD

p(zl/Xjl ),

(22)

where the terms under the product in the above equation have been deﬁned in Eq. 17. Thus, it may be seen that the

main difference in the two likelihoods is the factor(cid:0)m

(cid:1)k! and the fact that p(zl/ˆxjl ) is an approximation of p(zl/Xjl )

k

for observations that have been associated to a target. Thus, in general, the weights of the different hypotheses will be
different due to the MT-likelihood normalization required in H-FISST and the approximation of the true likelihood
of target-observation associations in MHT, however, the MT-pdfs underlying the different hypotheses in the two
methods are exactly the same. The normalization of the likelihood is necessary from a probabilistic perspective
since otherwise the distribution on the ﬁltered state pdf, resulting from all possible observations, does not constitute
a probability distribution, i.e., it does not add up to unity. This can easily be seen for the case of a standard ﬁltering
problem which carries over to the mutli-target tracking problem. Let the ﬁltered state pdf, the belief state be denoted

by b(x). Suppose that the likelihood function(cid:82) p(z/x)dz (cid:54)= 1. Consider the distribution on the future belief state

b(cid:48)(x). This is given by:

(cid:90)

Note that if(cid:82) p(z/x)dz (cid:54)= 1 then(cid:82) p(z/b)dz (cid:54)= 1. Hence,

z

p(b(cid:48)/b) =

p(b(cid:48)/z, b)p(z/b)dz, wherep(z/b) =

(cid:90)

(cid:90) (cid:90)

p(b(cid:48)/b)db(cid:48) =

p(b(cid:48)/z, b)p(z/b)dzdb(cid:48) =

(cid:90)

p(z/x)b(x)dx.

(cid:90)

p(z/b)dz (cid:54)= 1.

We know that the ﬁltered pdf (the belief process) has to evolve according to a Markov chain [21], [22] but the
above development shows that the evolution equation violates the requirement that the transition probability of a
Markov chain needs to be a probability distribution over all future states, if the likelihood does not normalize to unity.

Furthermore, the proposed hybrid derivation (in that it includes both the continuous and discrete parts of the
problem) as opposed to MHT which is a purely discrete derivation at the hypothesis level [10], reveals the critical
hybrid structure (Eq. 11) inherent to multi-target tracking problems, and which, in turn allows us to unify the
HFISST development with the FISST based formulation of the multi-target tracking problem, and thus, allows
for the uniﬁcation of FISST and MHT: methods that have thus far been thought to be different from each other

9

(please see the appendix for more details) modulo the difference in the hypothesis weights due to the MT-likelihood
normalization required in the FISST formulation.

C. Incorporating Birth and Death in Hypothesis level FISST

The development thus far in this section has assumed (implicitly) that there are a ﬁxed and known number of
objects. However, this is not necessarily true since new objects can arrive while old objects can die. Thus, we have
to incorporate the possibility of the birth and death of objects. In the following, we show that this can be done in
quite a straightforward fashion using Eqs. 5, 11 and 19.

Let α denote the birth probability of a new object being spawned and β denote the probability that an object
dies in between two measurements. We will assume that α2, β2 ≈ 0. This assumption implies that exactly one birth
or one death is possible in between measurement updates. Consider the time instant t, and consider an M-object
hypothesis at time t, Hi. Depending on the time t, let us assume that there can be M b
t birth hypotheses and M d
t
t objects dying. In particular, for
death hypothesis corresponding to one of M b
t parts and the births correspond to a new object
the SSA problem, we can divide the FOV of the sensor into M b
being spawned in one of these FOV parts. The death hypotheses correspond to one of the M d
t objects expected
to be in the FOV dying. Hence, a child hypothesis Hij of the parent Hi can be an M + 1 object hypothesis with
t different ways. The child Hij could have M − 1 objects with probability β each in M d
probability α in exactly M b
t
t different objects dying. Thus, the child Hij could have M objects with
different ways corresponding to the M d
probability (1 − M b
t β) in exactly one way (the no birth/ death case). Please see Fig. 1 for an illustration
of the process.

t objects being spawned or one of M d

t α − M d

Remark 6: The above development amounts to modeling the birth and death processes as independent Bernoulli
processes with parameters α and β respectively. Since α2, β2, αβ ≈ 0, the birth process can be modeled as just two
outcomes: exactly 0 births with probability (1 − α)M b
t α.
Similarly, the death process amounts to exactly 0 deaths with probability (1 − β)M d
t β, and exactly one
t β. Since the two processes are independent, the joint distribution function is just a product
death with probability M d
t α− M d
of the two distributions and can be shown to amount to exactly 0 birth and death with probability 1− M b
t β,
exactly one birth with probability M b
t α. The individual birth and death
hypotheses then follow by noting that the births can happen in one of M b
t different ways and the deaths can happen
in one of M d

t α, and exactly one birth with probability M b

t α and exactly one death with probability M d

t different ways.

t ≈ 1 − M b

t ≈ 1 − M d

Further, the child hypothesis Hij can then split into further children Hijk where the total number of children is
AM , AM +1 or AM−1 depending on the number of objects underlying the hypothesis Hij, and corresponding to the
various different data associations possible given the measurement {Zt}. Note that the above process degenerates
into the no birth and death case when α = β = 0. Thus, we can see that the primary consequence of the birth and
death process is the increase in the total number of children hypotheses. However, the equations for the multi-object
ﬁltering (with a little effort, due to the fact that the child hypotheses may have different number of objects than
the parent hypothesis thereby complicating the integration underlying the prediction step) can be shown to remain

unchanged. Recall Eq. 11, which is reproduced below for clarity:

p({X}, (i, j)/F t) =

p({Zt}/{X}, (i, j))p−({X}/(i, j),F t−1)

(cid:82) p({Zt}/{X(cid:48)}, (i, j))p−({X(cid:48)}/(i, j),F t−1)d{X(cid:48)}
(cid:124)
(cid:125)

(cid:123)(cid:122)

updated pdf underlyingHij

×

(cid:80)

lij

wij
pijwi

(cid:122)(cid:125)(cid:124)(cid:123)
(cid:124) (cid:123)(cid:122) (cid:125)

wi(cid:48) j(cid:48)

i(cid:48),j(cid:48) li(cid:48),j(cid:48) pi(cid:48)j(cid:48)wi(cid:48)

10

.

(25)

The only difference from the no birth and death case is, given Hi is an M− object hypotheses, the children
hypotheses Hij can have M, M − 1 or M + 1 objects underlying them, and the corresponding pij value is
1 − M b
t β, β or α respectively. It behooves us to look closer at the prediction equations in the birth and
death case as that is the source of difference from the no birth and death case.

t α − M d

First, consider the case of a death hypothesis. Consider an M-object hypothesis, Hi, with underlying MT-pdf
k pk(xk). Suppose without loss of generality that the M th object dies. Then, the transition density for the multi-

object system is deﬁned as:

p({X}/{X(cid:48)}, (i, j)) = [

M−1(cid:89)

k=1

pk(xk/x(cid:48)

k)]δ(φ/xM ),

(26)

(cid:81)

where δ(φ/xM ) denotes the fact that the M th object becomes the null object φ with probability one. Thus, the
predicted MT-transition density underlying Hij is:

(cid:90)

M−1(cid:89)

(

M−1(cid:89)

p−({X}/(i, j),F t) ==

p(xk/x(cid:48)

k)p(x(cid:48)

k/i,F t))δ(φ/x(cid:48)

M )dx(cid:48)

1..dx(cid:48)

M =

p−(xk/i,F t),

(27)

i.e., the predicted MT-pdf is simply the predicted pdfs of all the objects that do not die.

k=1

k=1

Next, consider the case of a birth hypothesis Hij where the birthed pdf has a distribution pl

b(xM +1). The transition

pdf is now

p({X}/{X(cid:48)}, (i, j)) = [

M(cid:89)

k=1

pk(xk/x(cid:48)

k)]pM +1(xM +1/φ),

(28)

where pM +1(xM +1/φ) = pl
b(xM ). It can be shown similar to above that the predicted distribution in this case is:
pl

b(xM +1) denotes that the null object φ spawns an M + 1th object with underlying pdf

p−({X}/(i, j),F t) = [

k (xk/i,F t)]pl
p−

b(xM +1),

(29)

k=1

i.e., the predicted distribution of all the objects with the addition of the birth pdf pl

b(xM +1).

Further, each of these hypothesis split into children Hijk based on the possible data associations: if Hij is a
birth hypothesis the the number of children is AM +1, if its a death hypothesis the number of children is AM−1
and if it is no birth or death, the number of children is AM . In particular, using the development outlined above (
where we have replaced the child notation Hijk by Hij for simplicity), we can see that the transition probability

M(cid:89)

pij of a child hypothesis Hij is:



pij =

αpk
M +1,
(1 − M b

t α − M d

t β)pk

M ,

βpk

M−1,

11

(30)

if j ∈ BM +1,k
if j ∈ BM,k
if j ∈ BM−1,k

where BN,l represents all N object hypotheses in which exactly l of the objects have been associated to
measurements and

where m is the size of the measurement return.

pl
N =

(cid:0)m
(cid:1)l!
D(1 − pD)N−l
pl

l

,

(31)

Fig. 1. A schematic of the splitting of the hypothesis due to birth/ death of objects and data associations. Underlying each blob is a continuous
MT-pdf.

The above development can be summarized as the following result:
Proposition 2: Given an M-object hypothesis Hi and its children Hij, the update equation for joint MT-pdf-
hypothesis density function is given by Eq. 25, where the only differences from the no birth or death case is that
pij in the equations is different according as the hypothesis Hij birth, death or a no birth or death hypothesis and is
given by Eq. 30, and the predicted priors required in Eq. 25 is calculated from Eq. 27 if Hij is a death hypothesis,
Eq. 29 if it is a birth hypothesis and Eq 7 if it is a no birth or death hypothesis.

Remark 7: The above result remains valid for more complex models of birth and death as long as the transition

M-­‐target	  hypothesis	  (M-­‐1)-­‐target	  hypothesis	  (M+1)-­‐target	  hypothesis	  !!!!1!Mtd!!Mtb"pM+1kpMkpM!1kBirth/	  Death	  Data	  	  Associa8ons	  12

probabilities pij can be speciﬁed. More complex models of birth and death would entail that there are signiﬁcantly
(combinatorially) more children hypothesis due to the births and deaths than the number here, followed by the
requisite number of data association hypotheses, and would result in more complex expressions for the transition
probabilities than the one in Eq. 30. As regards the relationship with MHT in the case of birth and death, the same
observations as made in the no birth or death case holds: given the birth and death model, the methods result in
the same exact hypotheses, except the weights of the different hypothesis are different due to the MT-likelihood
normalization done in the HFISST method.

III. A RANDOMIZED FISST (R-FISST) TECHNIQUE

In the previous section, we have introduced the hypothesis level FISST equations and shown that they are
particularly easy to comprehend and implement. However, the number of children hypothesis increase exponentially
at every iteration and thus, can get computationally intractable very quickly. However, it can also be seen that most
children hypotheses are very unlikely and thus, there is a need for intelligently sampling the children hypotheses
such that only the highly likely hypotheses remain. In the following, we propose an MCMC based sampling scheme
that allows us to choose the highly likely hypotheses.

A. MCMC based Intelligent Sampling of Children Hypothesis

Recall Eq. 19. It is practically plausible that most children j of hypothesis Hi are highly unlikely, i.e., lij ≈ 0
and thus, wij ≈ 0. Hence, there is a need to sample the children Hij of hypothesis Hi such that only the highly
likely hypotheses are sampled, i.e., lij >> 0.

Remark 8: Searching through the space of all possibly hypotheses quickly becomes intractable as the number of

objects and measurements increase, and as time increases.

Remark 9: We cannot sample the hypothesis naively either, for instance, according to a uniform distribution since
the highly likely hypothesis are very rare under the uniform distribution, and thus, our probability of sampling a
likely hypothesis is vanishingly small under a uniform sampling distribution.
Thus, we have to resort to an intelligent sampling technique, in particular, an MCMC based approach.

Given a hypothesis Hi, we want to sample its children according to the probabilities ¯pij = wijlij. This can be
done by generating an MCMC simulation where the sampling Markov chain, after enough time has passed (the
burn in period), will sample the children hypotheses according to the probabilities ¯pij. A pseudo-code for setting up
such an MCMC simulation is shown in Algorithm 1. In the limit, as k → ∞, the sequence {jk} generated by the

Algorithm 1 MCMC Hypothesis Sampling
Generate child hypothesis j0, set k = 0.
Generate jk+1 = π(jk) where π(.) is a symmetric proposal distribution
If ¯pijk+1 > ¯pijk then jk := jk+1; k := k + 1;
else jk := jk+1 with probability proportional to ¯pijk+1
¯pijk

; k = k + 1.

MCMC procedure above would sample the children hypotheses according to the probabilities ¯pij. Suppose that we

generate C highest likely distinct children hypothesis Hij using the MCMC procedure, then the FISST recursion
Eq. 19 reduces to:

13

(cid:80)

wij :=

lijwij

i(cid:48),j(cid:48) li(cid:48)j(cid:48)wi(cid:48)j(cid:48)

,

(32)

where i(cid:48) and j(cid:48) now vary from 1 to C for every hypothesis Hi, instead of the combinatorial number AM .

Given these M ∗C hypotheses, i.e. C children of M parents, we can keep a ﬁxed number H∞ at every generation
by either sampling the H∞ highest weighted hypotheses among the children, or randomly sampling H∞ hypotheses
from all the children hypotheses according to the probabilities wij.

Remark 10: The search for the highly likely hypotheses among a very (combinatorially) large number of options
is a combinatorial search problem for which MCMC methods are particularly well suited. Thus, it is only natural
that we use MCMC to search through the children hypotheses.

Remark 11: The choice of the proposal distribution π(.) is key to the practical success of the randomized sampling
scheme. Thus, an intelligent proposal choice is required for reducing the search space of the MCMC algorithm.
We show such an intelligent choice for the proposal in the next section.

Remark 12: The discrete hypothesis level update Eq. 19 is key to formulating the MCMC based sampling scheme,

and, hence, the computational efﬁciency of the R-FISST algorithm.

B. Smart Sampling Markov Chain Monte Carlo

In this section, we reveal the process used to perform the MCMC sampling discussed in the previous section. This
process is performed at every scan to generate the highly likely children hypotheses. Consider the following SSA
scenario depicted in ﬁgure 2. In this scenario the belief is that there are ten objects in the ﬁeld of view. The sensor
then detects ﬁve measurement returns. Typically when generating the hypotheses exhaustively one would create a

Fig. 2. A possible SSA event where there exists ten objects in the ﬁeld of view and ﬁve measurement returns.

matrix where each row represents a particular hypothesis. The columns of the matrix represent the measurement

returns provided by the sensor. Each column entry represents the object that measurement is being associated to in
the particular hypothesis. The hypothesis matrix for our example scenario would look like ﬁgure 3. However, if all

14

Fig. 3. An example of a typical Hypotheses Matrix used when exhaustively generating hypotheses. This particular matrix represents a portion
of the hypothesis matrix that would be generated for the scenario in ﬁgure 2.

objects and measurements within the ﬁeld of view can be associated then according to Eq. (4), with m = 5 and
M = 10, the total number of possible hypotheses would be AM = 63, 591. Thus, the hypothesis matrix actually has
63, 591 rows. This illustrates the importance of a randomized approach. One can see that even with relatively low
numbers of objects and measurement returns exhaustively generating and maintaining the hypotheses will cause
a large computational burden. In our randomized approach we sample the rows of the hypothesis matrix based
on hypothesis probability. We do this by creating a matrix we call the data association matrix, ﬁgure 4. The data

Fig. 4.
elements represent the likelihood of the corresponding association. Green boxes here show a visual representation of an example hypothesis.

The Data Association Matrix. Each row represents a particular measurement return. Each column is a particular association. The

15

association matrix lists the objects as the columns and the measurement return as the rows. The entries of the
matrix contain the likelihood value of that particular measurement to object assignment. The last column of the
matrix is dedicated to clutter and contains the likelihood that a particular measurement is associated to clutter. The
dimensions of this matrix are m × (M + 1) which is much smaller than the dimensions of the hypothesis matrix.
This makes it much more practical to explore using the MCMC technique.

Remark 13: The numbering of the objects and measurement returns in the data association matrix is done strictly
for organization and is redone at random each time step with no record of previous numbering or labeling kept
throughout scans.

Remark 14: Computing the data association matrix does not add any computational burden because the object

to measurement likelihood is necessary in every tracking method.

We start the randomized technique by creating a row vector of length m containing a permutation of column
numbers. The green boxes in ﬁgure 4 are a visual representation of such a row vector [ 5 4 2 1 7 ]. This row vector
is used as our ﬁrst hypothesis. We then take a step in the MCMC by generating a proposed hypothesis. This is
done by randomly choosing a row (Measurement) of the data association matrix to change. We then randomly
sample a column (object) to associate the measurement to. If there is a conﬂicting assignment (i.e. a measurement
is already assigned to that object) then we automatically assign the conﬂicting measurement to clutter. We then

Fig. 5. Visualization of a single MCMC step using the Data Association Matrix. Green boxes represent the current hypothesis while blue
boxes represent the changes made for the proposed hypothesis. This particular example contains a conﬂicting assignment with measurement
return two and shows how the association is then changed to clutter.

compare the proposed hypothesis to the current hypothesis in an MCMC fashion using a criteria which stems from
) where P(i,j)k is the probability of the hypothesis at step k.
the Metropolis condition U [0, 1] < min(1,
In words, if the proposed hypothesis has a higher probability then we keep it, if not, we keep it with probability
proportional to the ratio of the hypothesis probabilities. These steps are then repeated until assumed stationary
distribution. We then continue walking for a user deﬁned amount of steps and record all hypotheses sampled during

P(i,j)k+1
P(i,j)k

16

these steps. The recorded hypotheses represent the highly likely hypotheses.

IV. APPLICATIONS

This section illustrates the application of the results from the previous sections. We illustrate the R-FISST based
approach to the multi-object tracking and detection problem inherent in SSA applications. In particular, we will
discuss the results from a ﬁfty-space object birth and death scenario. Our goal is to show that the aforementioned
methodology allows for accurate estimation while determining the correct number of objects in an environment
where the number of objects is not ﬁxed. This will allow for the methodology to be used in both catalog update
and catalog maintenance.

A. R-FISST Application to a Fifty-Object Birth and Death Scenario

In order to test the methods discussed in this paper a ﬁfty-space object tracking and detection problem was
simulated using a planar orbit model. These ﬁfty-objects were in orbits ranging from LEO to MEO and had varying
orbital properties as well as zero-mean Gaussian process noise appropriate for SSA models. The objects were
simulated for at least one orbital period. That being said each object was allowed to pass completely through the
ﬁeld of view at least one time. The objective was to accurately track all objects given only an imperfect initial
hypothesis containing some of their means and covariances. Also, to simulate a birth and death environment, the
correct number of objects is initially unknown to the algorithm. In this particular example, the initial hypothesis
only contains information on forty ﬁve of the ﬁfty-objects. The ﬁve left over will be seen as objects that are
randomly introduced to the environment or simply ”births”. This is often described as the launch of a new satellite
into orbit and is not to be confused with ”spawns” in which an object currently in orbit divides into two or more
pieces. Spawns can be accounted for by our methodology but are not explicitly programed in this example. The
R-FISST methodology must recognize the ﬁve births and provide accurate estimations of all of the objects’ states
and covariances. State vectors for this particular problem contain the objects’ position along the x and y axes as
well as the magnitude of their velocity in the x and y directions. A single noisy sensor was positioned at a ﬁxed
look direction of 15 degrees above the positive x-axis with a ﬁeld of view of 30 degrees. The sensor was used
to measure objects’ position in the x - y plane with zero-mean Gaussian measurement noise appropriate for the
application.

Figure 6 shows snapshots of the hypotheses’ weights throughout the simulation. The snapshots were taken at ten,
ﬁfty, seventy-ﬁve, and one hundred percent of the total simulation time. From this ﬁgure, one can see that, in the
beginning, the initial hypothesis caries all the weight. However, throughout the simulation the number of maintained
hypotheses (shown on the x-axis of the graphs in Figure 6) varies as does the weights of those hypotheses. The
number of hypotheses maintained has a direct correlation to the number of recent ambiguities in the ﬁeld of view.
Ambiguities occur when one or more measurement returns can be associated to multiple objects in the ﬁeld of
view.

17

(a) Hypotheses’ weights near the be-
gining of the simulation

(b) Hypotheses’ weights at 50 per-
cent completion

(c) Hypotheses’ weights at 75 per-
cent completion

Fig. 6. Snapshots of the hypotheses’ weights throughout the simulation

(d) Hypotheses’ weights at 100 per-
cent completion

Before discussing the state estimations, ﬁgure 7 shows an example of how the estimation data is to be presented.
The ﬁgure shows the actual positions of the objects labeled ”Current Position” and the estimated positions from
two seperate hypotheses. If the estimated position for a particular object is within an error bound then a green circle
will represent the objects position otherwise a red star will represent the objects position.

18

(a) Actual Positions

(b) Estimations from a correct hypoth-
esis

(c) Estimations
containing incorrect estimations

from a hypothesis

Fig. 7. An example of how estimation data is to be displayed throughout the paper. Figure 7(a) shows the actual object positions while ﬁgure
7(b) shows an example of a correct hypothesis and ﬁgure 7(c) shows a hypothesis containing incorrect position estimations.

Figure 8-10, the snapshots show the actual positions (blue) against the estimated position from the top hypotheses
(green). The black lines bound the ﬁeld of view. These snapshots were taken at the same time intervals as in ﬁgure
6 and thus the estimates throughout ﬁgures 8-10 are taken from the hypotheses with the highest weights in ﬁgure
6. Notice in ﬁgures 8-10 there are no instances of red stars. This is particularly important because it shows that the
hypotheses accurately estimated object positions throughout the simulation. Hence, the R-FISST approach accurately
tracked and detected the ﬁfty-objects.

(a) Actual Object Positions

(b) Estimation from the top hypothesis
at 10 percent completion

Fig. 8. Snapshots of the actual states (black) and the estimated states from the top hypotheses (green) at 10 percent completion. Axes in tens
of thousands of kilometers

19

(a) Actual Object Positions

(b) Estimation from the top hypothesis
at 50 percent completion

Fig. 9. Snapshots of the actual states (black) and the estimated states from the top hypotheses (green) at 50 percent completion. Axes in tens
of thousands of kilometers

(a) Actual Object Positions

(b) Estimation from the top hypothesis
at 100 percent completion

Fig. 10.
tens of thousands of kilometers

Snapshots of the actual states (black) and the estimated states from the top hypotheses (green) at 100 percent completion. Axes in

Lastly, ﬁgure 11 shows a visual representation of how weight shifts from the forty-ﬁve object assumption to the
ﬁfty-object assumption. The x-axis represents the simulation time in percent completed. The y-axis represents the
expected number of objects. The expected number of objects is found by summing the weights of all hypotheses
containing the same number of objects. The magnitudes of these summations are then compared to determine the
expected number of objects. It is important to note that weight seems to be handed off in a single ﬁle fashion until
the ﬁfty-object assumption accumulates all the weight toward the end of the simulation.

V. COMPARISONS

In this section, we show a comparison between our randomized method RFISST and a well-known tracking
method called HOMHT. This comparison helps us illustrate two main points. The ﬁrst being that the accuracy of
the estimation provided by the RFISST method is either equal to or better than that of HOMHT but never worse.
We will achieve this by showing side-by-side estimations from both methods. The second point is seen when the
number of hypotheses rapidly escalates due to a large number of objects and/or a large number of measurement
returns. Such occurrences happen often in SSA and for many reasons, for example, when a debris ﬁeld crosses the
sensor’s ﬁeld of view. In these situations HOMHT fails because it is computationally impossible to generate such
a large number of hypotheses. Due to the randomized scheme, the RFISST methodology continues to perform in
such scenarios.

20

Fig. 11. The expected number of objects throughout the simulation. The graph shows intermediate values during the transition periods. Over
time it can be seen that weight shifts from the forty-ﬁve object hypotheses to the ﬁfty-Object hypotheses

A. Comparison between RFISST and HOMHT: SSA Tracking

In order to compare both methods we simulated a ﬁfteen-space object tracking and detection problem. Each object
was given a random planar orbit ranging between LEO and MEO with unique orbital properties and zero-mean
Gaussian process noise appropriate for SSA models. The objects were simulated for long enough to where the object
with the largest period would be able to complete at least one orbit. Thus each object was allowed to pass completely
through the ﬁeld of view at least one time. In this particular simulation we initialized all orbits to begin within the
ﬁeld of view. In order to achieve an apples to apples comparison we used this simulation to test both methods. The
goal of each method would be to accurately track each object given only an imperfect initial hypothesis containing
the objects’ mean and covariance as well as measurement returns from a single noisy sensor. State vectors for this
problem consisted of the objects’ position along the x and y axes as well as the magnitude of their velocity in
the x and y directions. The single noisy sensor was positioned at a ﬁxed look direction of 15 degrees above the
positive x-axis with a ﬁeld of view of 30 degrees. The sensor was used to measure objects’ position in the x -
y plane with zero-mean Gaussian measurement noise appropriate for the application. An Extended Kalman Filter
(EKF) was used in conjunction with each method to compute the underlying state and covariance updates. That
being said both methods will produce the same estimation given the correct hypotheses were generated throughout
the simulation.

Figures 12-14 are snapshots of the simulation at the beginning middle and end of the simulation time. Each
snapshot shows the true positions of the objects and the position estimations provided by both the HOMHT method
and the RFISST method. These ﬁgures are provided to illustrate that the methods accurately track the objects.
Furthermore, it shows that each method maintained the correct hypothesis throughout the simulation. If either
method was unable to generate the correct hypothesis then the position estimations would be incorrect. These
incorrect position estimates would be seen as red stars in the snapshots.

21

(a)

Fig. 12. Estimation at the beginning of the simulation

(b)

(c)

(a)

Fig. 13. Estimation at 50 percent completion of the simulation

(b)

(c)

22

(a)

Fig. 14. Estimation at the end of the simulation

(b)

(c)

Fig. 15. The hypotheses generation time for both methods as shown on a log y-axis scale.

B. Comparison between RFISST and HOMHT: Computation Time

When tracking large numbers of objects the majority of the computational burden lies in the hypothesis generation.
HOMHT uses an exhaustive approach to generate the hypotheses. This exhaustive approach has its pros and cons.
For example, generating all the hypotheses guarantees that the hypothesis containing the correct data associations is
sampled. Also this exhaustive approach is very easy to implement. On the other hand, as the number of hypotheses
grows so does the burden placed on generating them. This can be seen as an increase of computation time. Even
with proper gating and pruning methods the number of hypotheses can be so large that generating them would

23

exceed the computers memory heap space making it computationally intractable. It is at this point that we say
the HOMHT method breaks. Using our randomized approach we never generate all hypotheses, which allows
us to handle scenarios with very high number of possible hypotheses. However this method is more difﬁcult to
implement and must be tuned to guarantee that the hypothesis containing the correct data association is sampled.
Figure 15 shows a comparison of the computation times for hypothesis generation of HOMHT and RFISST. The
y-axis is the log of the computation time in seconds while the x-axis is the log of the total number of possible
hypotheses being generated. HOMHT is represented by the blue line and resembles an exponential curve. RFISST
is represented by the green line and resembles a linear growth. It can be seen that at ﬁrst for low numbers of
possible hypotheses HOMHT performs faster. However, as the number of possible hypotheses grows into the tens
of thousands the RFISST method becomes more efﬁcient. Furthermore, as the number of possible hypotheses grows
into the hundred millions HOMHT struggles to generate the hypotheses and eventually breaks. RFISST can generate
the correct hypotheses even as the number of possible hypotheses grows to the order of 1033. In each of these
simulations the RFISST MCMC methodology was able to sample the correct hypotheses using only 100, 000 steps
in the MCMC.

VI. CONCLUSION

In this paper, we have presented an alternate hypothesis based derivation of the FISST recursions for multi-object
tracking. We have also introduced a randomized version of the FISST recursion, called R-FISST, which provides a
computationally efﬁcient randomized solution to the full FISST recursions via an MCMC sampling of likely children
hypotheses. We have also proposed a uniﬁcation of the hitherto deemed different FISST and MHT methodologies
for multi-target tracking. We have shown the capability of the R-FISST method using a ﬁfty-object birth and death
SSA scenario. We showed that given the same underlying tracking ﬁlter, the HOMHT and RFISST produce the
similar tracking performance. We further showed that in situations involving high numbers of possible hypotheses,
the exhaustive generation of hypotheses typically used in HOMHT becomes computationally intractable while the
RFISST method continues to perform well. Currently we are looking to apply our method to real data and develop
large scale GPU based implementation that can scale to realistic scenarios. We also intend to look at the integration
of sensor tasking into the tracking methodology such that the ambiguities inherent in the problem can be minimized.

VII. ACKNOWLEDGMENTS

This work is funded by AFOSR grant number: FA9550-13-1-0074 under the Dynamic Data Driven Application

Systems (DDDAS) program.

APPENDIX

24

In this appendix, we shall show the equivalence of the FISST equations and the hypothesis level derivation of

this paper. Let the FISST pdf initially of the form:

p({X}) =

N(cid:88)

i=1

ωipi({X}),

where for each i, the pdf pi({X}) is a Multi-Target FISST pdf of the form:

(cid:88)

¯σ

pi({X}) =

pi
σ1

(x1)pi
σ2

(x2)··· pi

σn

(xn),

where ¯σ = {σ1, σ2 ··· σn} denotes all possible permutations of the indices {1, 2··· n}, and ωi is a non negative

number such that(cid:80)

i ωi = 1.

(33)

(34)

In the following, for notational simplicity and clarity, we assume that the MT-pdf corresponding to the ith
component is a 2-target pdf. The work essentially carries over directly to the n-target case at the expense of more
notation. Thus, the MT-pdf is given by:

pi({x1, x2}) = pi

1(x1)pi

2(x2) + pi

1(x2)pi

2(x1).

(35)

In the case of the hypothesis level derivation, call it H-FISST, ωi corresponds to the wt of the ith hypothesis and
the underlying MT-pdf is given by pi
2(x2), i.e, the H-FISST MT-pdf is the same as the FISST MT-pdf sans
1(x1)pi
the permutation of the arguments (x1, x2).

Next, we look at the prediction step of the FISST equations. We will assume no target birth or death but the

situation is very similar even in the case of birth and death, and can be derived analogous to the following.
The MT-transition FISST pdf is given by:

p({x1, x2}/{x(cid:48)

1, x(cid:48)

2}) = p(x1/x(cid:48)

1)p(x2/x(cid:48)

2) + p(x1/x(cid:48)

2)p(x2/x(cid:48)
1),

(36)

where p(x/x(cid:48)) is the transition pdf of a single target which assume here to be the same for all targets. Again, it
may be extended to different classes of transition pdfs at the expense of more notation. The FISST predicted pdf
for the ith component is then given by:

(cid:90)

1
2!

1 (x1) =(cid:82) p(x1/x(cid:48)

where pi−

1(x(cid:48)

1)dx(cid:48)

1)pi

(p(x1/x(cid:48)

1)p(x2/x(cid:48)

2) + p(x1/x(cid:48)

2)p(x2/x(cid:48)

1)) × (pi

i ({x1, x2}) =
p−

2(x1))dx1dx2
1 (x2)pi−

2 (x1),

(37)

2(x2) + pi

1(x1)pi
= pi−

1 (x1)pi−

1(x2)pi
2 (x2) + pi−
1(.), and pi−

1, i.e., the predicted pdf of prior pdf pi

2 (x2) is the predicted pdf

for the prior pdf pi

2(.). In H-FISST, the predicted MT-pdf for the ith component would simply be:

i ({x1, x2}) =
p−

p(x1/x(cid:48)

1)p(x2/x(cid:48)

1(x(cid:48)

2)pi

2(x(cid:48)

2)dx(cid:48)

1dx(cid:48)
2,

1)pi

(cid:90)

25

(38)

i.e., it is the same as the FISST pdf without the permutations of the arguments {x1, x2}.

Next, we turn to the update step. Suppose that we get the observation {z1, z2}. Again, we assume this purely for
notational convenience and transparency of the treatment, it can easily be extended to more general observations
at the expense of more notation. The FISST MT-likelihood function is then:

p({z1, z2}/{x1, x2}) = p2

D/2{p(z1/x1)p(z2/x2) + p(z1/x2)p(z2/x1}
+pD(1 − pD)/2{p(z1/x1)g(z2) + p(z2/x1)g(z1)}
+pD(1 − pD)/2{p(z1/x2)g(z2) + p(z2/x2)g(z1)} + (1 − pD)2{g(z1)g(z2)},

(39)

where p(z/x) is the single target likelihood, g(z) is the probability of getting observation z from clutter and pD
is the probability of detection. The different terms in the likelihood function above correspond to the 7 different
data associations possible given the two observations {z1, z2} such as (z1 → T1, z2 → T2), (z1 → T2, z2 → T1),
(z1 → T1, z2 → C), and so on. The factors of 1/2 in the ﬁrst three terms and 1 in the fourth term of the likelihood

equation above correspond to the(cid:0)m
(cid:1)k! normalization factor required in the MT-likelihood function: the 1/2 factor
(cid:1)2!, the 1/2 actor corresponding to the pD(1 − pD) term is (cid:0)2
(cid:1)1! whereas the
D term is (cid:0)2
(cid:1)0!. It may also be seen that due to the above normalization,
D) is due to (cid:0)2
(cid:82) p({z1, z2}/{x1, x2})dz1dz2 = 1 for all {x1, x2} as is generally true for a standard likelihood function in tracking/

corresponding to p2
factor of 1 corresponding to (1 − p2

k

2

0

1

ﬁltering. The updated FISST MT-pdf for the ith component is then given by the equation:

pi({x1, x2}) =

1
η

ωip({z1, z2}/{x1, x2})p−

i ({x1, x2}),

(40)

where η is a suitable normalization factor that will be evaluated below and is critical to understanding the structure
of the FISST pdf. Consider the data association (z1 → T1, z2 → T2), and call it the i1th association. Using Eqs.
38, 39 and 40, it may be seen that in the product in Eq. 40, this data association corresponds to the term:

pi1({x1, x2}) = ωip2

D/2{p(z1/x2)p(z2/x2)pi−

1 (x1)pi−

2 (x2) + p(z1/x2)p(z2/x1)pi−

1 (x2)pi−

2 (x1)}.

It can be seen that the braced term in the expression above is nothing but the FISST MT-pdf:

(cid:90)

ηi1 =

1
2!

{p(z1/x1)p(z2/x2)pi−

1 (x1)pi−

ηi1{pi1

1 (x1)pi1

2 (x2) + pi1
2 (x2) + p(z1/x2)p(z2/x1)pi−
p(z1/x1)p(z2/x2)pi−

1 (x2)pi1
1 (x2)pi−
1 (x1)pi−

(cid:90)

=

2 (x1)}, where
2 (x1)}dx1dx2

2 (x2)dx1dx2,

(41)

where

pi1
1 (x1) =

pi1
2 (x2) =

(cid:82) p(z1/x(cid:48)
(cid:82) p(z2/x(cid:48)

p(z1/x1)pi−
1 (x1)
1)pi−
1 (x(cid:48)
p(z2/x2)pi−
2 (x2)
2)pi−
2 (x(cid:48)

1)dx(cid:48)

1

2)dx(cid:48)

2

26

(42)

,

,

1 (x1) is simply the updated target 1 prior pdf pi−

1 (.) with observation z1 and pi1

2 (.) is the updated target 2
i.e., pi1
prior pdf pi−
2 (.) with observation z2. Similarly, the sub-components pij(.) corresponding to the other possible data
associations ij may be found. Recall that pij represent the transition probability from the ith parent to its jth child
hypothesis. Please note the distinction from the function pij({X}) above which represents the MT-pdf underlying
the jth hypothesis (we apologize for the notational ambiguity here but we wanted to be consistent with our HFISST
derivation). Most importantly, after noting that p2
D/2 = pi1 in the H-FISST formulation, it may be seen that the
normalization factor η in Eq. 40 can be written as:

(cid:88)

i,j

η =

ηijpijωi.

(43)

(44)

Thus, the i1th component of the FISST MT-pdf can be written as:

(cid:80)

ωipi1ηi1

i(cid:48),j(cid:48) ηi(cid:48),j(cid:48)pi(cid:48)j(cid:48)ωi(cid:48)

{pi1

1 (x1)pi1

2 (x2) + pi1

1 (x2)pi1

2 (x1)}.

However, note that in the H-FISST framework ηi1 = li1. Thus, in general, the ijth component of the FISST pdf
may be written as:

pij({x1, x2,··· xn}) =

pij
1 (xσ1 )pij

2 (xσ2)··· pij

n (xσn )},

(45)

(cid:80)

ωipijlij

i(cid:48),j(cid:48) li(cid:48)j(cid:48)pi(cid:48)j(cid:48)ωi(cid:48)

{(cid:88)

¯σ

2 (x2)··· pij

where as before ¯σ = {σ1, σ2 ··· σn} represents all possible permutations of the numbers {1, 2··· n}, and where
1 (x1)pij
pij
n (xn) is the updated MT-pdf that results from using the jth possible data association for
component i. Note that this is precisely the update that is done in the H-FISST scheme modulo the set-theoretic
representation in the FISST framework due to the interchangeability of the arguments {x1, x2 ··· xn}. Hence, the
above development shows that FISST and H-FISST recursions result in precisely the same MT-pdfs, and associated
weights, modulo the representation of the underlying MT-pdfs in set theoretic terms in the FISST framework.
Further, note that as has been mentioned previously in the paper, the case of target birth and death results in there
being more candidate children hypotheses ij, parameterized through the transition probabilities pij. Thus, even in
the case of target birth and death, the individual components/ hypotheses would have the same form as above in
FISST, and thus, it follows that the H-FISST recursions and FISST recursions are the same.

[1] I. R. Goodman, R. P. S. Mahler, and H. T. Nguyen, Mathematics of Data Fusion. Kluwer Academic Publishers, 1997.
[2] R. P. S. Mahler, Statistical Multisource-Multitarget Information Fusion. Artec House, 2007.

REFERENCES

27

[3] B. Vo and W. Ma, “The gaussian mixture probability hypothesis density ﬁlter,” IEEE TRANSACTIONS ON SIGNAL PROCESSING, vol. 54,

no. 11, November 2006.

[4] B. Vo et al., “Sequential monte carlo methods for multi target ﬁltering with random ﬁnite sets,” IEEE Tr. Aerosp. Electronic Systems,

vol. 4, pp. 1224–1245, 2005.

[5] B. T. Vo et al., “An analytic implementation of the cardinalized probability hypothesis density ﬁlter,” IEEE Tr. Signal Processing, vol. 55,

pp. 3553–3567, 2007.

[6] B. Ristic et al., “Improved smc implementation of the phd ﬁlter,” in PRoc. Conf. Info. fusion (FUSION), 2010.
[7] I. Hussein, K. DeMars, C. Frueh, R. S. Erwin, and M. Jah, “An aegis-ﬁsst algorithm for joint detection and tracking in space situational

awareness,” AAS/AIAA Astrodynamics Specialist Conference, August 2012.

[8] ——, “An aegis-ﬁsst integrated detection and tracking approach to space situational awareness,” International Conference of Information

Fusion, July 2012.

[9] R. P. S. Mahler, “Multitarget markov motion models,” SPIE Conference on Signal Processing, Sensor Fusion, and Target Recognition, vol.

3720, pp. 47–56, 1999.

[10] D. B. Reid, “An algorithm for tracking multiple targets,” IEEE Transactions on Automatic Control, vol. 24, pp. 843–854, 1979.
[11] Y. B. Shalom and X. Li, Multitarget-Multisensor Tracking: Principles and Techniques, New York, 1995.
[12] Y. Blair and E. W. D. Blair, Multitarget Multisensor tracking: Applications and advances, vol. III. Norwood, MA: Artech House, 2000.
[13] S. Oh et al., “Markov chain monte carlo data association for multi-target tracking,” IEEE Tr. Automatic Control, vol. 54, pp. 481–497,

2009.

[14] S. S. Blackman and R. Popoli, Design and Analysis of Modern Tracking Systems. Norwood, MA: Artech House, 1999.
[15] Y. B. Shalom and T. Fortmann, Tracking and Data Association. San Diego, CA: Academic Press, 1988.
[16] N. Bergman and A. Doucet, “Markov chain monte carlo data association for target tracking,” in Proc. ICASSP, 2000.
[17] H. Pasula et al., “Tracking many objects with many sensors,” in Proc. IJCAI, 1999.
[18] S. Cong et al., “Markov chain monte carlo approach fro association probability evaluation,” Proc. IEE, vol. 151, pp. 185–193, 2004.
[19] W. Faber, S. Chakravorty, and I. Hussein, “A randomized sampling based approach to multi-object tracking,” International Conference of

Information Fusion,, July 2015.

[20] ——, “A randomized sampling based approach to multi-object tracking with comparison to homht,” IAAS/AIAA Astrodynamics Specialists

Conference,, August 2015.

[21] D. P. Bertsekas, Dynamic Programming and Optimal Control, vols I and II. Cambridge: Athena Scientiﬁc, 2000.
[22] P. R. Kumar and P. P. Varaiya, Stochastic Systems: Estimation, Identiﬁcation and Adaptive Control. Prentic Hall, NJ: Prentice Hall, 1986.

