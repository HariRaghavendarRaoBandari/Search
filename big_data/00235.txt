6
1
0
2

 
r
a

M
1

 

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
5
3
2
0
0

.

3
0
6
1
:
v
i
X
r
a

Oracle Estimation of a Change Point in High

Dimensional Quantile Regression∗

Sokbae Lee†, Yuan Liao‡, Myung Hwan Seo§, and Youngki Shin¶

26 February 2016

Abstract

In this paper, we consider a high dimensional quantile regression model where
the sparsity structure may diﬀer between the two sub-populations. We develop (cid:96)1-
penalized estimators of both regression coeﬃcients and the threshold parameter. Our
penalized estimators not only select covariates but also discriminate between a model
with homogeneous sparsity and a model with a change point. As a result, it is not
necessary to know or pretest whether the change point is present, or where it occurs.
Our estimator of the change point achieves an oracle property in the sense that its
asymptotic distribution is the same as if the unknown active sets of regression coef-
ﬁcients were known. Importantly, we establish this oracle property without a perfect
covariate selection, thereby avoiding the need for the minimum level condition on the
signals of active covariates. Dealing with high dimensional quantile regression with an
unknown change point calls for a new proof technique since the quantile loss function is
non-smooth and furthermore the corresponding objective function is non-convex with
respect to the change point. The technique developed in this paper is applicable to a
general M-estimation framework with a change point, which may be of independent
interest. The proposed methods are then illustrated via Monte Carlo experiments and
an application to tipping in the dynamics of racial segregation.

Keywords: change-point, variable selection, quantile regression, high-dimensional M-
estimation, sparsity, LASSO, SCAD
AMS 2000 subject classiﬁcations: Primary 62H12, 62J05; secondary 62J07

Institute

for Fiscal

Studies,

Email:sokbae@gmail.com.

∗This work was supported in part by Promising-Pioneering Researcher Program through Seoul National
University, by the European Research Council (ERC-2014-CoG-646917-ROMIA), the Social Sciences and
Humanities Research Council of Canada (SSHRCC), and by the Research and Scholarship Award grant of
University of Maryland.

†The
London, WC1E 7AE, UK.
‡Department of Mathematics, University of Maryland, College Park, MD 20742, USA. Email:
§Department of Economics, London School of Economics, Houghton Street, London, WC2A 2AE, UK;
Department of Economics, Seoul National University, 1 Gwanak-ro, Gwanak-gu, Seoul, 151-742, Republic of
Korea. Email: m.seo@lse.ac.uk.
¶Economics Discipline Group, University of Technology Sydney, PO Box 123, Broadway NSW 2007,

yuanliao@umd.edu.

7 Ridgmount

Street,

Australia. Email: yshin12@gmail.com

1

1

Introduction

In this paper, we consider a high dimensional quantile regression model where the sparsity

structure (e.g., identities and eﬀects of important or contributing regressors) may diﬀer

between the two sub-populations, thereby allowing for a possible change point in the model.
Let Y ∈ R be a response variable, Q ∈ R be a scalar random variable that determines a
possible change point, and X ∈ Rp be a p-dimensional vector of covariates. Here, Q can be a

component of X, and p is potentially much larger than the sample size n. Speciﬁcally, high

dimensional quantile regression with a change point is modelled as follows:

Y = X T β0 + X T δ01{Q > τ0} + U,

(1.1)

where (β0, δ0, τ0) is a vector of unknown parameters and the regression error U satisﬁes P(U ≤
0|X, Q) = γ for some known γ ∈ (0, 1). Unlike the mean regression, quantile regression

analyzes the eﬀects of active regressors on diﬀerent parts of the conditional distribution of

a response variable. Therefore, it allows the sparsity patterns to diﬀer at diﬀerent quantiles

and also handles heterogeneity due to either heteroscedastic variance or other forms of non-

location-scale covariate eﬀects. By taking into account a possible change point in the model,

we provide a more realistic picture of the sparsity patterns. For instance, when analyzing

high-dimensional gene expression data, the identities of contributing genes may depend on

the environmental or demographical variables (e.g., exposed temperature, age or weights).

Our paper is closely related to the literature on models with unknown change points

(e.g., Tong (1990), Chan (1993), Hansen (2000), Pons (2003), Kosorok and Song (2007),

Seijo and Sen (2011a,b) and Li and Ling (2012) among many others). Recent papers on

change points under high-dimensional setups include Enikeeva and Harchaoui (2013); Chan

et al. (2014), Frick et al. (2014), Cho and Fryzlewicz (2015), Chan et al. (2016), Callot

et al. (2016), and Lee et al. (2016) among others; however, none of these papers consider

a change point in high dimensional quantile regression. The literature on high dimensional

2

quantile regression includes Belloni and Chernozhukov (2011), Bradic et al. (2011), Wang

et al. (2012), Wang (2013), and Fan et al. (2014) among others. All the aforementioned

papers on quantile regression are under the homogeneous sparsity framework (equivalently,

assuming that δ0 = 0 in the quantile regression model). Ciuperca (2013) considers penalized

estimation of a quantile regression model with breaks, but the corresponding analysis is

restricted to the case when p is small.

In this paper, we consider estimating regression coeﬃcients α0 ≡ (βT

0 , δT

0 )T as well as

the threshold parameter τ0 and selecting the contributing regressors based on (cid:96)1-penalized

estimators. One of the strengths of our proposed procedure is that it does not require to

know or pretest whether δ0 = 0 or not, that is, whether the population’s sparsity structure

and covariate eﬀects are invariant or not. In other words, we do not need to know whether

the threshold τ0 is present in the model.

For a sparse vector v ∈ Rp, we denote the active set of v as J(v) ≡ {j : vj (cid:54)= 0}. One of

the main contributions of this paper is that our proposed estimator of τ0 achieves an oracle

property in the sense that its asymptotic distribution is the same as if the unknown active

sets J(β0) and J(δ0) were known. Importantly, we establish this oracle property without

assuming a perfect covariate selection, thereby avoiding the need for the minimum level

condition on the signals of active covariates.

Dealing with high dimensional quantile regression with an unknown change point calls

for a new proof technique since the quantile loss function is non-smooth and furthermore the

corresponding objective function is non-convex with respect to the threshold parameter τ0.

The technique developed in this paper is applicable to a general M-estimation framework

with a change point, which may be of independent interest.

The proposed estimation method in this paper consists of three main steps: in the ﬁrst

step, we obtain initial estimators of α0 and τ0, whose rates of convergence may be suboptimal;

in the second step, we re-estimate τ0 to obtain an improved estimator of τ0 that converges

at the rate of n and achieves the oracle property mentioned above; in the third step, using

3

the second step estimator of τ0, we update the estimator of α0. In particular, we propose

alternative estimators of α0, depending on the purpose of estimation (prediction vs. variable

selection).

One particular application of (1.1) comes from tipping in the racial segregation in social

sciences (see, e.g. Card et al., 2008). The empirical question addressed in Card et al. (2008) is

whether and the extent to which the neighborhood’s white population decreases substantially

when the minority share in the area exceeds a tipping point (or change point). In Section

8, we use the US Census tract dataset constructed by Card et al. (2008) and ﬁnd that the

tipping exists in the neighborhoods of Chicago and Pittsburgh.

The remainder of the paper is organized as follows. Section 2 provides an informal

description of our estimation methodology. In Section 3, we derive the consistency of the

estimators in terms of the excess risk. Section 4 presents regularity assumptions we need

to establish further asymptotic properties of the proposed estimators, which are given in

Sections 5 and 6. In Section 7, we provide discussions how to choose tuning parameters and

present the results of some Monte Carlo experiments. Section 8 illustrates the usefulness

of our method by applying it to tipping in the racial segregation. Section 9 concludes and

Appendix A describes in detail regarding how to construct the conﬁdence interval for τ0.

Apendices B and C contain high-level regularity conditions on the loss function and the

proofs of all the theoretical results, respectively.

Notation. Throughout the paper, we use |v|q for the (cid:96)q norm for a vector v with
q = 0, 1, 2. We use |v|∞ to denote the sup norm. For two sequences of positive real numbers
an and bn, we write an (cid:28) bn and equivalently bn (cid:29) an if an = o(bn).
If there exists a
positive ﬁnite constant c such that an = c· bn, then we write an ∝ bn. Let λmin(A) denote the

minimum eigenvalue of a matrix A. We use w.p.a.1 to mean “with probability approaching
one.” We write θ0 ≡ β0 + δ0. For a 2p dimensional vector α, let αJ and αJ c denote
its subvectors formed by indices in J(α0) and {1, ..., 2p}/J(α0), respectively. Likewise, let
XJ (τ ) denote the subvector of X(τ ) ≡ (X T , X T 1{Q > τ})T whose indices are in J(α0). The

4

true parameter vectors β0, δ0 and θ0 (except τ0) are implicitly indexed by the sample size
n, and we allow that the dimensions of J(β0), J(δ0) and J(θ0) can go to inﬁnity as n → ∞.

For simplicity, we omit their dependence on n in our notation.

2 Estimators

In this section, we describe our estimation method. We take the check function approach
of Koenker and Bassett (1978). Let ρ(t1, t2) ≡ (t1 − t2)(γ − 1{t1 − t2 ≤ 0}) denote the loss
function for quantile regression. Let A and T denote the parameter spaces for α0 ≡ (β0, δ0)
and τ0, respectively. For each α ≡ (β, δ) ∈ A and τ ∈ T , we write X T β + X T δ1{Q > τ} =
X(τ )T α with the shorthand notation that X(τ ) ≡ (X T , X T 1{Q > τ})T . We suppose that

the vector of true parameters is deﬁned as the minimizer of the expected loss:

E(cid:2)ρ(Y, X(τ )T α)(cid:3) .

(α0, τ0) = argmin
α∈A,τ∈T

(2.1)

By construction, τ0 is not unique when δ0 = 0.

Suppose we observe independent and identically distributed samples {Yi, Xi, Qi}i≤n. Let

Xi(τ ) and Xij (τ ) denote the i-th realization of X(τ ) and j-th element of Xi (τ ) , respectively,
i = 1, . . . , n and j = 1, . . . , 2p, so that Xij(τ ) ≡ Xij if j ≤ p and Xij(τ ) ≡ Xi,j−p1{Qi > τ}

otherwise. Deﬁne

Rn(α, τ ) ≡ 1
n

In addition, let Dj(τ ) ≡ {n−1(cid:80)n

i=1

n(cid:88)

ρ(Yi, Xi(τ )T α) =

n(cid:88)

i=1

1
n

ρ(Yi, X T

i β + X T

i δ1{Qi > τ}).

i=1 Xij(τ )2}1/2, j = 1, . . . , 2p.

We describe the main steps of our (cid:96)1-penalized estimation method. For some tuning

parameter κn, deﬁne:

Step 1: (˘α, ˘τ ) = argminα∈A,τ∈T Rn(α, τ ) + κn

5

2p(cid:88)

j=1

Dj(τ )|αj|.

(2.2)

This step produces an initial estimator (˘α, ˘τ ). The tuning parameter κn is required to satisfy

(cid:114)

κn ∝ (log p)(log n)

log p

n

.

(2.3)

(cid:80)n
j=1 dj|βj| +(cid:80)p

i=1 X 2

n

(cid:80)n

Note that we take κn that converges to zero at a rate slower than the standard (log p/n)1/2

rate in the literature. This modiﬁed rate of κn is useful in our context to deal with an

unknown τ0. A data-dependent method of choosing κn is discussed in Section 7.1.

that (cid:80)2p

j=1 Dj(τ )|αj| = (cid:80)p

Remark 2.1. Deﬁne dj ≡ ( 1

ij)1/2 and dj(τ ) ≡ ( 1
ij1{Qi > τ})1/2. Note
j=1 dj(τ )|δj|, so that the weight Dj(τ ) adequately
balances the regressors; the weight dj regarding |βj| does not depend on τ , while the weight
dj(τ ) with respect to |δj| does, which takes into account the eﬀect of the threshold τ on the

i=1 X 2

n

parameter change δ.

The main purpose of the ﬁrst step is to obtain an initial estimator of α0. The achieved

convergence rates of this step might be suboptimal due to the uniform control of the score
functions over the space T of the unknown τ0.

In the second step, we introduce our improved estimator of the change point τ0. It does

not use a penalty term, while using the ﬁrst step estimator of α0. Deﬁne:

Step 2: (cid:98)τ = argmin

τ∈T

Rn(˘α, τ ),

(2.4)

where ˘α is the ﬁrst step estimator of α0 in (2.2). In Section 5, we show that when τ0 is

identiﬁable, (cid:98)τ is consistent for τ0 at a rate of n−1. Furthermore, we obtain the limiting
distribution of n((cid:98)τ − τ0), and establish conditions under which its asymptotic distribution is

the same as if the true α0 were known, without a perfect model selection on α0, nor assuming

the minimum signal condition on the nonzero elements of α0.

In the third step, we update the Lasso estimator of α0 using a diﬀerent value of the

penalization tuning parameter and the second step estimator of τ0. In particular, we recom-

6

mend two diﬀerent estimators of α0 : one for the prediction and the other for the variable

selection, serving for diﬀerent purposes of practitioners.

For two diﬀerent tuning parameters ωn and µn whose rates will be speciﬁed later by (3.2)

and (5.1), deﬁne:

Step 3a (for prediction):

(cid:98)α = argminα∈ARn(α,(cid:98)τ ) + ωn

Step 3b (for variable selection):

(cid:101)α = argminα∈ARn(α,(cid:98)τ ) + µn

2p(cid:88)

j=1

2p(cid:88)

Dj((cid:98)τ )|αj|,

wjDj((cid:98)τ )|αj|,

(2.5)

(2.6)

where (cid:98)τ is the second step estimator of τ0 in (2.4), and the “signal-adaptive” weight wj in
Zou and Li, 2008), is calculated based on the Step 3a estimator (cid:98)α from (2.5):

(2.6), motivated by the local linear approximation of the SCAD penalties (Fan and Li, 2001;

j=1

|(cid:98)αj| < µn
|(cid:98)αj| > aµn
µn ≤ |(cid:98)αj| ≤ aµn.



1,

0,

aµn−|(cid:98)αj|

µn(a−1)

wj ≡

Here a > 1 is some prescribed constant, and a = 3.7 is often used in the literature. We take

this as our choice of a.

Remark 2.2. For (cid:98)α in (2.5), we set ωn to converge to zero at a rate of (log(p ∨ n)/n)1/2
(a more standard rate compared to κn in (2.3)). Therefore, the estimator (cid:98)α converges in

probability to α0 faster than ˘α. In addition, µn in (2.6) is chosen to be slightly larger than

ωn for the purpose of the variable selection. A data-dependent method of choosing ωn as

well as µn is discussed in Section 7.1. In Sections 5 and 6, we establish conditions under

which (cid:98)α achieves the (minimax) optimal rate of convergence in probability for α0 regardless

7

of the identiﬁability of τ0.

Remark 2.3. It is well known in linear models without the presence of an unknown τ0

(see, e.g. B¨uhlmann and van de Geer (2011)) that the Lasso estimator may not perform

well for the purpose of the variable selection. The estimator (cid:101)α deﬁned in Step 3b uses an
the asymptotic unbiasedness of the SCAD penalty. Therefore, we recommend (cid:98)α for the
prediction; while suggesting (cid:101)α for the variable selection.

entry-adaptive weight wj that corrects the shrinkage bias, and possesses similar merits of

Remark 2.4. Note that the objective function is non-convex with respect to τ in the ﬁrst and

second steps. However, the proposed estimators can be calculated eﬃciently using existing

algorithms, and we describe the computation algorithms in Section 7.1.

Remark 2.5. Note that Step 2 can be repeated using the updated estimator of α0 in Step

3. Analogously, Step 3 can be iterated after that. This would give asymptotically equivalent

estimators but might improve the ﬁnite-sample performance especially when p is very large.

Repeating Step 2 might be useful especially when ˘δ = 0 in the ﬁrst step. In this case, there

is no unique(cid:98)τ in Step 2. So, we skip the second step by setting(cid:98)τ = ˘τ and move to the third
step directly. If a preferred estimator of δ0 in the third step (either (cid:98)δ or (cid:101)δ), depending on

the estimation purpose, is diﬀerent from zero, we could go back to Step 2 and re-estimate

τ0. If the third step estimator of δ0 is also zero, then we conclude that there is no change

point and disregard the ﬁrst-step estimator ˘τ since τ0 is not identiﬁable in this case.

3 Risk Consistency

Given the loss function ρ(t1, t2) ≡ (t1 − t2)(γ − 1{t1 − t2 ≤ 0}) for the quantile regression

model, deﬁne the excess risk to be

R(α, τ ) ≡ Eρ(Y, X(τ )T α) − Eρ(Y, X(τ0)T α0).

(3.1)

8

By the deﬁnition of (α0, τ0) in (2.1), we have that R(α, τ ) ≥ 0 for any α ∈ A and τ ∈ T .

What we mean by the “risk consistency” here is that the excess risk converges in probabil-

ity to zero for the proposed estimators. The other asymptotic properties of the proposed

estimators will be presented in Sections 5 and 6.

In this section, we begin by stating regularity conditions that are needed to develop our

ﬁrst theoretical result. Recall that Xij denotes the jth element of Xi.

Assumption 3.1 (Setting).

(i) The data {(Yi, Xi, Qi)}n

i=1 are independent and identically

distributed with E|Xij|m ≤ m!

2 K m−2

1

for all j and some constant K1 < ∞.

(ii) P(τ1 < Q ≤ τ2) ≤ K2(τ2 − τ1) for any τ1 < τ2 and some constant K2 < ∞.

(iii) α0 ∈ A ≡ {α : |α|∞ ≤ M1} for some constant M1 < ∞, and τ0 ∈ T ≡ [τ , τ ]. Further-

more, the probability of {Q < τ} and that of {Q > τ} are strictly positive, and

E[X 2

ij|Q = τ ] < ∞.

sup
j≤p

sup
τ∈T

(iv) There exist universal constants D > 0 and D > 0 such that w.p.a.1,

(v) E(cid:104)(cid:0)X T δ0

(cid:1)2 |Q = τ

0 < D ≤ min
j≤2p

(cid:105) ≤ M2|δ0|2

τ∈T Dj(τ ) ≤ max
j≤2p

inf

Dj(τ ) ≤ D < ∞.

sup
τ∈T

2 for all τ ∈ T and for some constant M2 satisfying

0 < M2 < ∞.

In addition to the random sampling assumption, condition (i) imposes mild moment
restrictions on X. Condition (ii) imposes a weak restriction that the probability that Q ∈
(τ1, τ2] is bounded by a constant times (τ2 − τ1). Condition (iii) assumes that the parameter
space is compact and that the support of Q is strictly larger than T . These conditions

are standard in the literature on change-point and threshold models (e.g., Seijo and Sen
ij|Q = ·]
(2011a,b)). Condition (iii) also assumes that the conditional expectation of E[X 2

9

is bounded on T uniformly in j. Condition (iv) requires that each regressor be of the

same magnitude uniformly over the threshold τ . As the data-dependent weights Dj(τ ) are

the sample second moments of the regressors, it is not stringent to assume them to be

bounded away from both zero and inﬁnity. Condition (v) puts some weak upper bound on

(cid:1)2 |Q = τ ] for all τ ∈ T when δ0 (cid:54)= 0. A simple suﬃcient condition for condition (v)

E[(cid:0)X T δ0

is that the eigenvalues of E[XJ(δ0)X T

J(δ0)|Q = τ ] are bounded uniformly in τ , where XJ(δ0)

denotes the subvector of X corresponding to the nonzero components of δ0.

Throughout the paper, we let s ≡ |J(α0)|0, namely the cardinality of J(α0). We allow
that s → ∞ as n → ∞ and will give precise regularity conditions regarding its growth rates.

The following theorem is concerned about the convergence of R(˘α, ˘τ ) with the ﬁrst step

estimator.

Theorem 3.1 (Risk Consistency). Let Assumption 3.1 hold. Suppose that the tuning pa-

rameter κn satisﬁes (2.3). Then, R(˘α, ˘τ ) = OP (κns) .

Note that Theorem 3.1 holds regardless of the identiﬁability of τ0 (that is, whether δ0 = 0
or not). Theorem 3.1 implies the risk consistency immediately if κns → 0 as n → ∞. The

restriction on s is slightly stronger than that of the standard result s = o((cid:112)n/ log p) in the

literature for the M-estimation (see, e.g. van de Geer (2008) and Chapter 6.6 of B¨uhlmann

and van de Geer (2011)) since the objective function ρ(Y, X(τ )T α) is non-convex in τ , due

to the unknown change-point.

Remark 3.1. The extra logarithmic factor (log p)(log n) in the deﬁnition of κn (see (2.3))

is due to the existence of the unknown and possibly non-identiﬁable threshold parameter

τ0. In fact, an inspection of the proof of Theorem 3.1 reveals that it suﬃces to assume that
κn satisﬁes κn (cid:29) log2(p/s)[log(np)/n]1/2. The term log2(p/s) and the additional (log n)1/2
term inside the brackets are needed to establish the stochastic equicontinuity of the empirical

process

νn (α, τ ) ≡ 1
n

(cid:17) − Eρ

(cid:16)

Y, X (τ )T α

(cid:17)(cid:105)

(cid:16)

ρ

n(cid:88)

(cid:104)

i=1

Yi, Xi (τ )T α

10

uniformly over (α, τ ) ∈ A × T .

The following theorem shows that an improved rate of convergence is possible for the

excess risk by taking the second and third steps of estimation.

Theorem 3.2 (Improved Risk Consistency). Let Assumption 3.1 hold. In addition, assume

that |(cid:98)τ − τ0| = OP (n−1) when δ0 (cid:54)= 0. Let

(cid:114)

log(p ∨ n)

n

.

(3.2)

ωn ∝

Then, whether δ0 = 0 or not,

R ((cid:98)α,(cid:98)τ ) = Op (ωns) .

For the sake of not introducing additional assumptions at this stage, we have assumed

in Theorem 3.2 that |(cid:98)τ − τ0| = OP (n−1) when τ0 is identiﬁable.

Its formal statement is

delegated to Theorem 5.3 in Section 5.

Remark 3.2. As in Theorem 3.1, the risk consistency part of Theorem 3.2 holds whether

or not δ0 = 0. We obtain the improved rate of convergence in probability for the excess risk

by combining the fact that our objective function is convex with respect to α given each τ

with the second-step estimation results that (i) if δ (cid:54)= 0, then (cid:98)τ is within a shrinking local
neighborhood of τ0, and (ii) when δ0 = 0,(cid:98)τ does not aﬀect the excess risk in the sense that

R (α0, τ ) = 0 for all τ ∈ T .

4 Assumptions for Oracle Properties

In this section, we list a set of assumptions that will be useful to derive asymptotic

properties of the proposed estimators in Sections 5 and 6. In the following, we divide our
discussions into two important cases: (i) δ0 (cid:54)= 0 and τ0 is identiﬁed, and (ii) δ0 = 0 and thus

τ0 is not identiﬁed. The asymptotic properties are derived under both cases. Note that such

11

a distinction is only needed for presenting our theoretical results. In practice, we do not

need to know whether δ0 = 0 or not.

Assumption 4.1 (Underlying Distribution).

(i) The conditional distribution Y |X, Q has
a continuously diﬀerentiable density function fY |X,Q(y|x, q) with respect to y, whose
derivative is denoted by ˜fY |X,Q(y|x, q).

(ii) There are constants C1, C2 > 0 such that for all (y, x, q) in the support of (Y, X, Q),

| ˜fY |X,Q(y|x, q)| ≤ C1,

fY |X,Q(x(τ0)T α0|x, q) ≥ C2.

(iii) When δ0 (cid:54)= 0, Γ(τ, α0) is positive deﬁnite uniformly in a neighborhood of τ0, where

Γ(τ, α0) ≡ ∂2E[ρ(Y, XJ (τ )T α0J )]

∂αJ ∂αT
J

= E[XJ (τ )XJ (τ )T fY |X,Q(X(τ )T α0|X, Q)].

When δ0 = 0, the matrix E[XJ(β0)X T

J(β0)fY |X,Q(X T

J(β0)β0J(β0)|X, Q)] is positive deﬁnite.

Conditions (i) and (ii) are standard assumptions for quantile regression models. To follow

the notation in condition (iii), recall that αJ denotes the subvector of α whose indices are in

J(α0). Expressions XJ (τ ), XJ(β0), α0J and β0J(β0) can be understood similarly. Condition

(iii) is a weak condition that imposes non-singularity of the Hessian matrix of the population
objective function uniformly in a neighborhood of τ0 in case of δ0 (cid:54)= 0. This condition reduces

to the usual non-singularity condition when δ0 = 0.

4.1 Compatibility Conditions

We now make an assumption that is an extension of the well-known compatibility con-

dition (see B¨uhlmann and van de Geer (2011), Chapter 6).

In particular, the following

condition is a uniform-in-τ version of the compatibility condition. Recall that for a 2p di-

mensional vector α, we use αJ and αJ c to denote its subvectors formed by indices in J(α0)
and {1, ..., 2p}/J(α0), respectively.

12

Assumption 4.2 (Compatibility Condition).

(i) When δ0 (cid:54)= 0, there is a neighborhood
T0 ⊂ T of τ0, and a constant φ > 0 such that for all τ ∈ T0 and all α ∈ R2p satisfying
|αJ c|1 ≤ 5|αJ|1,

φ|αJ|2

1 ≤ sαT E[X(τ )X(τ )T ]α.

(4.1)

(ii) When δ0 = 0, there is a constant φ > 0 such that for all τ ∈ T and all α ∈ R2p

satisfying |αJ c|1 ≤ 4|αJ|1,

φ|αJ|2

1 ≤ sαT E[X(τ )X(τ )T ]α.

(4.2)

Assumption 4.2 requires that the compatibility condition hold uniformly in τ over a
neighbourhood of τ0 when δ0 (cid:54)= 0 and over the entire parameter space T when δ0 = 0. Note
that this assumption is imposed on the population covariance matrix E[X(τ )X(τ )T ]; thus, a
simple suﬃcient condition of Assumption 4.2 is that the smallest eigenvalue of E[X(τ )X(τ )T ]

is bounded away from zero uniformly in τ . Even if p > n, the population covariance can still

be strictly positive deﬁnite while the sample covariance is not.

4.2 Restricted Nonlinearity Conditions

In this subsection, we make an assumption called a restricted nonlinear condition to deal

with the quantile loss function. We extend condition D.4 in Belloni and Chernozhukov (2011)

to accommodate the possible existence of the unknown threshold in our model (speciﬁcally, a

uniform-in-τ version of the restricted nonlinear condition as in the compatibility condition).
Note that when Q ≤ τ0, X(τ0)T α0 = X T β0, while when Q > τ0, X(τ0)T α0 = X T θ0,
where θ0 ≡ β0 + δ0. Hence we deﬁne the “prediction balls” with radius r and corresponding

centers as follows:

B(β0, r) = {β ∈ B ⊂ Rp : E[(X T (β − β0))21{Q ≤ τ0}] ≤ r2},
G(θ0, r) = {θ ∈ G ⊂ Rp : E[(X T (θ − θ0))21{Q > τ0}] ≤ r2},

(4.3)

13

where B and G are parameter spaces for β0 and θ0, respectively. To deal with the case that

δ0 = 0, we also deﬁne

˜B(β0, r, τ ) = {β ∈ B ⊂ Rp : E[(X T (β − β0))21{Q ≤ τ}] ≤ r2},
˜G(β0, r, τ ) = {θ ∈ G ⊂ Rp : E[(X T (θ − β0))21{Q > τ}] ≤ r2}.

(4.4)

Assumption 4.3 (Restricted Nonlinearity). The following holds for the constants C1 and

C2 deﬁned in Assumption 4.1 (ii).

(i) When δ0 (cid:54)= 0, there exists a constant r∗

QR > 0 such that

inf
β∈B(β0,r∗
QR),β(cid:54)=β0

E[|X T (β − β0)|21{Q ≤ τ0}]3/2
E[|X T (β − β0)|31{Q ≤ τ0}]

≥ r∗

QR

2C1
3C2

> 0

(4.5)

and that

inf
θ∈G(θ0,r∗
QR),θ(cid:54)=θ0

E[|X T (θ − θ0)|21{Q > τ0}]3/2
E[|X T (θ − θ0)|31{Q > τ0}]

≥ r∗

QR

2C1
3C2

> 0.

(4.6)

(ii) When δ0 = 0, there exists a constant r∗

QR > 0 such that

inf
τ∈T

β∈ ˜B(β0,r∗

inf
QR,τ ),β(cid:54)=β0

E[|X T (β − β0)|21{Q ≤ τ}]3/2
E[|X T (β − β0)|31{Q ≤ τ}]

≥ r∗

QR

2C1
3C2

> 0

(4.7)

and that

inf
τ∈T

θ∈ ˜G(β0,r∗

inf
QR,τ ),β(cid:54)=β0

E[|X T (θ − θ0)|21{Q > τ}]3/2
E[|X T (θ − θ0)|31{Q > τ}]

≥ r∗

QR

2C1
3C2

> 0.

(4.8)

Remark 4.1. As pointed out by Belloni and Chernozhukov (2011), If X T c follows a log-

concave distribution conditional on Q for any nonzero c (e.g.

if the distribution of X is

multivariate normal), then Theorem 5.22 of Lov´asz and Vempala (2007) and the H¨older

14

inequality imply that for all α ∈ A,

E[|X(τ0)T (α − α0)|3|Q] ≤ 6(cid:8)E[{X(τ0)T (α − α0)}2|Q](cid:9)3/2

,

which provides a suﬃcient condition for Assumption 4.3. On the other hand, this assumption

can hold more generally since equations (4.5)-(4.8) in Assumption 4.3 need to hold only

locally around true parameters α0.

4.3 Additional Assumptions When δ0 (cid:54)= 0

We ﬁrst describe the additional conditions on the distribution of (X, Q).

Assumption 4.4 (Additional Conditions on the Distribution of (X, Q)). Assume δ0 (cid:54)= 0.
In addition, there exists a neighborhood T0 ⊂ T of τ0 that satisﬁes the following.

(i) Q has a density function fQ(·) that is continuous and bounded away from zero on T0.

(ii) Let ˜X denote all the components of X excluding Q in case that Q is an element of
X. The conditional distribution of Q given ˜X has a density function fQ| ˜X(q|˜x) that is
bounded uniformly in both q ∈ T0 and ˜x.

(iii) There exists M3 > 0 such that M−1

3 ≤ E[(X T δ0)2|Q = τ ] ≤ M3 for all τ ∈ T0.

Condition (i) implies that P{|Q − τ0| < ε} > 0 for any ε > 0, and condition (ii) requires
that the conditional density of Q given ˜X be uniformly bounded. When τ0 is identiﬁed,

we require δ0 to be considerably diﬀerent from zero. This requirement is given in condition

(iii). Note that this condition is concerned with E[(cid:0)X T δ0

(cid:1)2 |Q = τ ], which is an important

quantity to develop asymptotic results when δ0 (cid:54)= 0. Note that condition (iii) is a local

condition with respect to τ in the sense that it has to hold only locally in a neighborhood

of τ0.

The following additional moment conditions are useful to derive our theoretical results.

15

Assumption 4.5 (Moment Bounds).

(i) There exist constants 0 < (cid:101)C1 ≤ (cid:101)C2 < 1 such

that for all β ∈ Rp satisfying E|X T β| (cid:54)= 0,

(cid:101)C1 ≤ E[|X T β|1{Q > τ0}]

E|X T β|

≤ (cid:101)C2.

(ii) There exist positive constants M, r and the neighborhood T0 of τ0 such that

E(cid:2)(X T [(θ − β) − (θ0 − β0)])2(cid:12)(cid:12)Q = τ(cid:3) ≤ M,
E[|X T (β − β0)|(cid:12)(cid:12)Q = τ ] ≤ M,
E[|X T (θ − θ0)|(cid:12)(cid:12)Q = τ ] ≤ M,

(cid:20)
|X T (β − β0)|1{τ0 < Q ≤ τ}
(cid:20)
|X T (θ − θ0)|1{τ < Q ≤ τ0}

(τ − τ0)

(cid:21)
(cid:21)

(τ0 − τ )

E

sup

τ∈T0:τ >τ0

E

sup

τ∈T0:τ <τ0

≤ ME[|X T (β − β0)|1{Q ≤ τ0}],

≤ ME[|X T (θ − θ0)|1{Q > τ0}],

uniformly in β ∈ B(β0, r), θ ∈ G(θ0, r) and τ ∈ T0.

Remark 4.2. Condition (i) requires that Q have non-negligible support on both sides of τ0.

Note that it is equivalent to

(cid:19)

− 1

(cid:18) 1(cid:101)C2

E[|X T β|1{Q > τ0}] ≤ E(cid:2)|X T β|1{Q ≤ τ0}(cid:3)

(cid:19)

(cid:18) 1(cid:101)C1

≤

− 1

E|X T β|1{Q > τ0} .

(4.9)

Hence this assumption prevents the conditional expectation of X T β given Q from chang-

ing too dramatically across regimes. Condition (ii) requires the boundedness and certain

smoothness of the conditional expectation functions E[(X T [(θ − β) − (θ0 − β0)])2(cid:12)(cid:12)Q = τ ],
E[|X T (β − β0)|(cid:12)(cid:12)Q = τ ], and E[|X T (θ − θ0)|(cid:12)(cid:12)Q = τ ], and prohibits degeneracy in one regime.

The last two inequalities in condition (ii) are satisﬁed if

E(cid:2)(cid:12)(cid:12)X T β(cid:12)(cid:12)|Q = τ(cid:3)

E [|X T β|]

≤ M

16

for all τ ∈ T0 and for all β satisfying 0 < E(cid:12)(cid:12)X T β(cid:12)(cid:12) ≤ c for some small c > 0. In this view,

we may regard condition (ii) as a local version of condition (i).

5 Asymptotic Properties: Case I. δ0 (cid:54)= 0

We ﬁrst establish the consistency of ˘τ for τ0.

Theorem 5.1 (Consistency of ˘τ ). Let Assumptions 3.1, 4.1, 4.4, and 4.5 hold. Furthermore,

assume that κns = o(1). Then, ˘τ

p−→ τ0.

The following theorem presents the rates of convergence for the ﬁrst step estimators of

α0 and τ0. Recall that κn is the ﬁrst-step penalization tuning parameter that satisﬁes (2.3).
Theorem 5.2 (Rates of Convergence When δ0 (cid:54)= 0). Suppose that κns2 log p = o(1). Then

under Assumptions 3.1-4.5, we have:

|˘α − α0|1 = OP (κns), R(˘α, ˘τ ) = OP (κ2

ns),

and

|˘τ − τ0| = OP (κ2

ns).

It is worth noting that ˘τ converges to τ0 faster than the standard parametric rate of
n−1/2, as long as s2(log p)6(log n)4 = o(n). The main reason for such super-consistency is

that the objective function behaves locally linearly around τ0 with a kink at τ0, unlike in

the regular estimation problem where the objective function behaves locally quadratically

around the true parameter value. Moreover, the achieved convergence rate for ˘α is nearly

minimax optimal, with an additional factor (log p)(log n) compared to the rate of regular

Lasso estimation (e.g., Bickel et al. (2009); Raskutti et al. (2011)). This factor arises due to

the unknown change-point τ0. We will improve the rates of convergence for both τ0 and α0

further by taking the second and third steps of estimation.

Recall that the second-step estimator of τ0 is deﬁned as

(cid:98)τ = argmin

τ∈T

Rn(˘α, τ ),

17

where ˘α is the ﬁrst step estimator of α0 in (2.2). Consider an oracle case for which α in
Rn(α, τ ) is ﬁxed at α0. Let R∗

n (τ ) = Rn (α0, τ ) and

(cid:101)τ = argmin
i δ01{Qi > τ0}, ˙ρ1i ≡ ˙ρ(cid:0)Ui − X T

τ∈T

i δ0

R∗
n (τ ) .

Deﬁne ˙ρ (t) ≡ t (γ − 1{t ≤ 0}), so that ρ (t, s) = ˙ρ (t − s). For each i = 1, . . . , n, let Ui ≡
Yi− X T

(cid:1)− ˙ρ (Ui) and ˙ρ2i ≡ ˙ρ(cid:0)Ui + X T

(cid:1)− ˙ρ (Ui).

i β0− X T

i δ0

We now give one of the main results of this paper.

Theorem 5.3 (Oracle Estimation of τ0). Let Assumptions 3.1-4.5 hold. Furthermore, sup-

pose that κns2 log p = o(1). Then, we have that

(cid:98)τ −(cid:101)τ = op

(cid:0)n−1(cid:1)

and n ((cid:98)τ − τ0) converges in distribution to the smallest minimizer of a compound Poisson

process, which is given by

M (h) ≡ N1(−h)(cid:88)

i=1

ρ1i1{h < 0} +

N2(h)(cid:88)

i=1

ρ2i1{h ≥ 0} ,

where N1 and N2 are Poisson processes with the same jump rate fQ (τ0) and {ρ1i} and
{ρ2i} are two sequences of independent and identically distributed random variables. The

two common distributions are identical to the conditional distributions of ˙ρ1i and ˙ρ2i given
Qi = τ0, respectively. Here, N1, N2, {ρ1i} and {ρ2i} are mutually independent.

The ﬁrst conclusion of Theorem 5.3 establishes that the second step estimator of τ0 is

an oracle estimator in the sense that it is asymptotically equivalent to the infeasible, oracle

estimator (cid:101)τ . As emphasized in the introduction, we obtain the oracle property without

relying on the perfect model selection in the ﬁrst step nor on the existence of the minimum

signal condition on active covariates. The second conclusion of Theorem 5.3 follows from

combining well-known weak convergence results in the literature (see e.g. Pons (2003);

18

Kosorok and Song (2007); Lee and Seo (2008)) with the argmax continuous mapping theorem

by Seijo and Sen (2011b).

Remark 5.1. Li and Ling (2012) propose a numerical approach for constructing a conﬁdence

interval by simulating a compound Poisson process in the context of least squares estimation.

We adopt their approach to simulate the compound Poisson process for quantile regression.

See Section 8 for a detailed description of how to construct a conﬁdence interval for τ0.

We now consider the Step 3a estimator of α0 deﬁned in (2.5). Recall that ωn is the Step

3a penalization tuning parameter that satisﬁes (3.2).

Theorem 5.4 (Improved Rates of Convergence When δ0 (cid:54)= 0). Suppose that κns2 log p =

o(1). Then under Assumptions 3.1-4.5,

|(cid:98)α − α0|1 = OP (ωns)

and R((cid:98)α,(cid:98)τ ) = OP (ω2

ns).

Theorem 5.4 shows that the estimator (cid:98)α deﬁned in Step 3a achieves the optimal rate of

convergence in terms of prediction and estimation. In other words, when ωn is proportional
to {log(p ∨ n)/n}1/2 in equation (3.2) and p is larger than n, it obtains the minimax rates

as in e.g., Raskutti et al. (2011).

As we mentioned in Section 2, the Step 3b estimator of α0 has the purpose of the variable

selection. The nonzero components of (cid:101)α are expected to identify the important regressors.
Partition (cid:101)α = ((cid:101)αJ ,(cid:101)αJ c) such that (cid:101)αJ = ((cid:101)αj : j ∈ J(α0)) and (cid:101)αJ c = ((cid:101)αj : j /∈ J(α0)). Note
that (cid:101)αJ consists of the estimators of β0J and δ0J , whereas (cid:101)αJ c consists of the estimators of
We now establish conditions under which the estimator (cid:101)α deﬁned in Step 3b has the

all the zero components of β0 and δ0. Let α(j)

0J denote the j-th element of α0J .

change-point-oracle properties, meaning that it achieves the variable selection consistency

and has the limiting distributions as though the identities of the important regressors and

the location of the change point were known.

19

Theorem 5.5 (Variable Selection When δ0 (cid:54)= 0). Suppose that κns2 log p = o(1), s4 log s =

o(n), and

ωn + s

(cid:114)

log s

n

(cid:28) µn (cid:28) min
j∈J(α0)

|α(j)
0J |.

(5.1)

Then under Assumptions 3.1-4.5, we have:

|(cid:101)αJ − α0J|2 = OP

(cid:32)(cid:114)

s log s

n

(cid:33)

|(cid:101)αJ − α0J|1 = OP

,

(cid:33)

(cid:32)

(cid:114)

s

log s

n

and

P ((cid:101)αJ c = 0) → 1.

We see that (5.1) provides a condition on the strength of the signal via minj∈J(α0) |α(j)
0J |,
n. Hence

and the tuning parameter in Step 3b should satisfy ωn (cid:28) µn and s2 log s/n (cid:28) µ2

the variable selection consistency demands a larger tuning parameter than in Step 3a.

To conduct statistical inference, we now discuss the asymptotic distribution of(cid:101)αJ . Deﬁne
(cid:98)α∗
J ≡ argminαJ
distribution of (cid:101)αJ is the same as that of (cid:98)α∗
oracle case that we know τ0 as well as the true active set J(α0) a priori. The limiting

n (αJ , τ0). Note that the asymptotic distribution for (cid:98)α∗

J . Hence, we call this result the change-point-

J corresponds to an

R∗

oracle property of the Step 3b estimator and the following theorem establishes this property.

∂α E(cid:2)ρ(cid:0)Y, X T α(cid:1)|Q = t(cid:3) exists for

Theorem 5.6 (Change-Point-Oracle Properties). Suppose that all the conditions imposed

in Theorem 5.5 are satisﬁed. Furthermore, assume that ∂

all t in a neighborhood of τ0 and all its elements are continuous and bounded, and that

s3(log s)(log n) = o (n). Then, we have that (cid:101)αJ =(cid:98)α∗
forward to establish the the asymptotic normality of a linear transformation of(cid:101)αJ , i.e., L(cid:101)αJ ,

Since the sparsity index (s) grows at a rate slower than the sample size (n), it is straight-

J + op(n−1/2).

where L : Rs → R with |L|2 = 1, by combing the existing results on quantile regression with

parameters of increasing dimension (see, e.g. He and Shao (2000)) with Theorem 5.6.

Remark 5.2. Without the condition on the strength of minimal signals, it may not be possi-

20

ble to achieve the variable selection consistency or establish change-point-oracle properties.

However, the following theorem shows that the SCAD-weighted penalized estimation still

can achieve a satisfactory rate of convergence in estimation of α0 without the condition that
µn (cid:28) minj∈J(α0) |α(j)
0J |.

Theorem 5.7 (Satisfactory Rates Without Minimum Signal Condition). Assume that As-
sumptions 3.1-4.5 hold. Suppose that κns2 log p = o(1) and ωn (cid:28) µn. Then, without the
lower bound requirement on minj∈J(α0) |α(j)

0J |, we have that |(cid:101)α − α0|1 = OP (µns) .

6 Asymptotic Properties: Case II. δ0 = 0

In this section, we show that our estimators have desirable results even if there is no

change point in the true model. The case of δ0 = 0 corresponds to the high dimensional linear
quantile regression model. Since X T β0 + X T δ01{Q > τ0} = X T β0, τ0 is non-identiﬁable, and

there is no structural change on the coeﬃcient. But a new analysis diﬀerent from that of

the standard high-dimensional model is still required because in practice we do not know

whether δ0 = 0 or not. Thus, the proposed estimation method still estimates τ0 to account

for possible structural changes. The following results show that in this case, the ﬁrst step

estimator of α0 will asymptotically behave as if δ0 = 0 were a priori known.

Theorem 6.1 (Rates of Convergence When δ0 = 0). Suppose that κns = o(1). Then under

Assumptions 3.1-4.3, we have that

|˘α − α0|1 = OP (κns)

and R(˘α, ˘τ ) = OP (κ2

ns).

The results obtained in Theorem 6.1 combined with those obtained in Theorem 5.2 imply

that the ﬁrst step estimatior performs equally well in terms of rates of convergence for both

the (cid:96)1 loss for ˘α and the excess risk regardless of the existence of the threshold eﬀect. It is

straightforward to obtain an improved rate result for the Step 3a estimator, equivalent to

21

Theorem 5.4 under Assumptions 3.1-4.3. We omit the details for brevity.

We now give a result that is equivalent to Theorem 5.5.

Theorem 6.2 (Variable Selection When δ0 = 0). Suppose that κns = o(1), s4 log s = o(n),

(cid:114)

and

ωn + s

log s

n

(cid:28) µn (cid:28) min
j∈J(α0)

0J |.
|α(j)

Then under Assumptions 3.1-4.3, we have:

(cid:12)(cid:12)(cid:12)(cid:101)βJ − β0J

(cid:12)(cid:12)(cid:12)2

= OP

(cid:32)

(cid:114)

s

log s

n

(cid:33)

,

= OP

(cid:33)

,

(cid:32)(cid:114)

s log s

(cid:12)(cid:12)(cid:12)(cid:101)βJ − β0J

(cid:12)(cid:12)(cid:12)1
P ((cid:101)βJ c = 0) → 1, P ((cid:101)δ = 0) → 1.

n

and

Theorem 6.2 demonstrates that when there is in fact no change point, our estimator

for δ0 is exactly zero with a high probability. Therefore, the estimator can also be used as

a diagnostic tool to check whether there exists any structural change. Results similar to

Theorems 5.6 and 5.7 can be established straightforwardly as well; however, their details are

omitted for brevity.

7 Simulation Results

7.1 Tuning parameter selection

Recall that our estimators are obtained by three steps, which involve three tuning param-

eters in the penalization: (1) κn in Step 1 ought to dominate the score function uniformly

over the range of τ , and hence should be slightly larger than the others; (2) ωn is used in

Step 3a for the prediction, and (3) µn in Step 3b for the variable selection should be larger

than ωn. Note that the tuning parameters in both Steps 3a and 3b are similar to those of

the existing literature since the change point(cid:98)τ has been estimated.

22

In the following Monte Carlo experiments, we build on the data-dependent selection

method in Belloni and Chernozhukov (2011). Deﬁne

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ,

(7.1)

n(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

n

Λ(τ ) := max
1≤j≤2p

Xij(τ ) (γ − 1{Ui ≤ γ})

i=1

Dj(τ )

where Ui is simulated from the i.i.d. uniform distribution on the interval [0, 1]; γ is the ﬁxed

quantile level (for median regression γ = 0.5). Note that Λ(τ ) is a stochastic process indexed
by τ . Let Λ1−q be the (1− q)-quantile of supτ∈T Λ(τ ). Then, we select the tuning parameter
in Step 1 by κn = c1 · Λ1−q. Similarly, let Λ1−q((cid:98)τ ) be the (1 − q)-quantile of Λ((cid:98)τ ), where(cid:98)τ is
chosen in Step 2. We select ωn and µn in Step 3 by ωn = c1 · Λ1−q((cid:98)τ ) and µn = c2 · ωn.

Based on the suggestions of Belloni and Chernozhukov (2011) and some preliminary

simulations, we decide to set c1 = 1.1, c2 = log log n, and q = 0.1. In addition, recall that

we set a = 3.7 when calculating the SCAD weights wj in Step 3b following the convention

in the literature (e.g. Fan and Li (2001) and Loh and Wainwright (2013)). In Step 1, we
ﬁrst solve the lasso problem for α given each grid point of τ ∈ T . Then, we choose ˘τ and

the corresponding ˘α(˘τ ) that minimize the objective function. Step 2 can be solved simply

by the grid search. Step 3 is a standard lasso quantile estimation given(cid:98)τ , whose numerical

implementation is well established.

7.2 Monte Carlo Experiments

In this section we provide the results of Monte Carlo simulation studies. The baseline

model is the following median regression: for i = 1, . . . , n,

Yi = X T

i β0 + X T

i δ01{Qi > τ0} + Ui,

(7.2)

where Ui follows the standard normal distribution, and Qi follows the uniform distribution

on the interval [0, 1]. The p-dimensional covariate Xi is composed of a constant and Zi,

23

i )T , where Zi follows the multivariate normal distribution N (0, Σ) with a
i.e. X := (1, Z T
covariance matrix Σij = (1/2)|i−j|. Here, the variables Ui, Qi and Zi are independent of each

other.

The p-dimensional parameters β0 and δ0 are set to β0 = (1, 1, 1/2, 0, . . . , 0) and δ0 =

(2, 2, 1, 0, . . . , 0) in Design 1 and β0 = (1, 1, 1/2, 1/3, 1/4, 1/5, 0, . . . , 0) and δ0 = (2, 2, 1, 1/2,

1/3, 1/4, 0, . . . , 0) in Design 2. Note that the parameters in Design 2 are decaying, indicating

that the minimal signal of active regressors is weaker in Design 2 than in Design 1. In both

designs, we set the change point parameter τ0 = 0.5. The sample sizes are set to n = 200

and 400. The dimension of Xi is set to p = 250. Note that we have 500 regressors in total.
The range of τ is set to T = [0.15, 0.85]. We conduct 1,000 replications of each design.

We compare estimation results of each step. To assess the performance of our estimators,

we also compare the results with two “oracle estimators”. Speciﬁcally, Oracle 1 knows the

true active set J(α0) and the change point parameter τ0, and Oracle 2 knows only J(α0).

The threshold parameter τ0 is re-estimated in Steps 3a and 3b using updated estimates of

α0.

Tables 1–2 summarize the simulation results. We abuse notation slightly and denote

all estimators by ((cid:98)α,(cid:98)τ ). They would be understood as (˘α, ˘τ ) in Step 1, (cid:98)τ in Step 2, and
so on. We report Excess Risk, the average number of parameters selected, E[J((cid:98)α)], and
the (cid:96)2-norm of the bias, |E[(cid:98)α] − α0|2. For each sample, the excess risk is calculated by the
s (τ0)α0)(cid:3), where S = 10,000 is the number
simulation, S−1(cid:80)S
s ((cid:98)τ )(cid:98)α) − ρ(Ys, X T
(cid:16)
S−1(cid:80)S

of simulations; then we report the average value of 1,000 replications. Similarly, we also

calculate prediction errors by the simulation,

(cid:0)X T
s ((cid:98)τ )(cid:98)α − X T

s=1

(cid:2)ρ(Ys, X T

s=1

(cid:1)2(cid:17)1/2

s (τ0)α0

, and

report the average value.

We also report the root mean square error (RMSE) and the coverage probability of the

95% conﬁdence interval (Cov. Prob. of CI) of(cid:98)τ . The conﬁdence intervals for τ0 are calculated

by simulating the two-sided compound Poisson process in Theorem 5.3 by adopting the

approach proposed by Li and Ling (2012). The details are provided in Section A. Note that

24

the root mean square error of(cid:98)τ and the coverage probability of the conﬁdence interval at the
rows of Step 3a and Step 3b in the tables are estimation results of updated(cid:98)τ : we re-estimate
τ as in Step 2 using ((cid:98)Ui,(cid:98)α) and ((cid:101)Ui,(cid:101)α) from Step 3a and Step 3b instead of ( ˘Ui, ˘α).

Excess Risk E[J(ˆα)]

Table 1: Simulation Results of Design 1

|E[(cid:98)α] − α0|2 Prediction Error RMSE of(cid:98)τ Cov. Prob. of CI

0.014
0.016
0.052
0.046
0.043
0.033

NA
NA

20.010

NA

21.010
6.170

0.008
0.007
0.330
NA
0.314
0.259

0.263
0.319
0.584
0.541
0.535
0.456

NA
0.009
0.051
0.045
0.042
0.020

NA
0.930
0.820
0.900
0.920
0.920

n = 200
Oracle 1
Oracle 2
Step 1
Step 2
Step 3a
Step 3b

0.002
0.006
0.018
0.017
0.017
0.008

n = 400
NA
Oracle 1
0.980
Oracle 2
0.920
Step 1
0.950
Step 2
0.950
Step 3a
Step 3b
0.960
Note: Oracle 1 knows both J(α0) and τ0 and Oracle 2 knows only J(α0). Expectations (E) is calculated
by the average of 1,000 iterations in each design. Note that J(α0) = 6. ‘NA’ denotes ‘Not Available’ as the
parameter is not estimated in the step. The estimation results for τ at the rows of Step 3a and Step 3b are

based on the re-estimation of τ given estimates from Step 3a ((cid:98)α) and Step 3b ((cid:101)α).

0.118
0.194
0.314
0.307
0.303
0.215

0.007
0.007
0.186
NA
0.183
0.042

NA
0.003
0.005
0.003
0.003
0.003

NA
NA

20.730

NA

21.670
6.090

Overall, the simulation results conﬁrm the asymptotic theory developed in the previous

sections. First, when we compare prediction errors of Oracle 1 and those of the proposed

√

estimators in Step 3, they are mostly inside the bound of

log p. Only the case of n =

400 in Design 2 shows that the empirical risk is slightly bigger than the bound. With a

weak signal in this design, this is likely the case. Second, the root mean square error of (cid:98)τ
decreases quickly and conﬁrms the super-consistency result of(cid:98)τ . As theoretically ensured, the
super-consistency holds regardless of the signal strength. As a result,(cid:98)τ performs relatively

satisfactorily even in Design 2. Third, the model selection in Step 3b is quite satisfactory in

Design 1. It slightly under-select the relevant regressors in Design 2 but it is a natural result

considering that some signals are quite weak in the decaying design. Fourth, the coverage

25

Excess Risk E[J(ˆα)]

Table 2: Simulation Results of Design 2

|E[(cid:98)α] − α0|2 Prediction Error RMSE of(cid:98)τ Cov. Prob. of CI

0.029
0.031
0.094
0.090
0.084
0.095

NA
NA

23.700

NA

24.880
9.150

0.020
0.020
0.507
NA
0.451
0.278

0.392
0.429
0.842
0.802
0.754
0.789

NA
0.008
0.101
0.085
0.076
0.034

NA
0.960
0.830
0.910
0.910
0.950

n = 200
Oracle 1
Oracle 2
Step 1
Step 2
Step 3a
Step 3b

0.005
0.014
0.027
0.029
0.027
0.036

n = 400
NA
Oracle 1
0.990
Oracle 2
0.930
Step 1
0.970
Step 2
0.970
Step 3a
Step 3b
0.990
Note: Oracle 1 knows both J(α0) and τ0 and Oracle 2 knows only J(α0). Expectations (E) is calculated by
the average of 1,000 iterations in each design. Note that J(α0) = 12.
‘NA’ denotes ‘Not Available’ as the
parameter is not estimated in the step. The estimation results for τ at the rows of Step 3a and Step 3b are

based on the re-estimation of τ given estimates from Step 3a ((cid:98)α) and Step 3b ((cid:101)α).

0.151
0.279
0.393
0.382
0.379
0.442

0.014
0.015
0.211
NA
0.206
0.136

NA
0.003
0.005
0.004
0.004
0.004

NA
NA

25.270

NA

26.300
9.990

Figure 1: Histogram of the True Covariate Selection in Design 1, J(α0) = 6

26

Design 1: n=200Number of True Covariates SelectedDensity3456780.00.20.40.60.81.0Design 1: n=400Number of True Covariates SelectedDensity3456780.00.20.40.60.81.0Figure 2: Histogram of the True Covariate Selection in Design 2, J(α0) = 12

probabilities of the conﬁdence interval are close to 95% except Step 1 of each design with

n = 200. Thus, we recommend practitioners to use(cid:98)τ in Step 2 or the re-estimated version of

it based on the estimates from Step 3a or Step 3b. Finally, Figures 1–2 show the frequency

of the true covariates selected from Step 3b in each design. In Design 1, the true covariates

are almost perfectly selected when n = 400, while we miss some small-sized coeﬃcients when

the parameters are decaying in Design 2.

In summary, the proposed estimation procedure works well in ﬁnite samples and conﬁrms

the theoretical results developed earlier. We emphasize that the recommended estimator for

α0 is either obtained in Step 3a (for the prediction) or Step 3b (for the variable selection).

We see from Tables 1–2 that the measures of both the prediction and the variable selection

are improved in Step 3a and Step 3b, respectively, compared to the preliminary estimator

in Step 1.

27

Design 2: n=200Number of True Covariates SelectedDensity024681012140.00.10.20.30.40.5Design 2: n=400Number of True Covariates SelectedDensity024681012140.00.10.20.30.40.58 Estimating a Change Point in Racial Segregation

As an empirical illustration, we investigate the existence of tipping in the dynamics of

racial segregation using the dataset constructed by Card et al. (2008). They show that the

neighborhood’s white population decreases substantially when the minority share in the area

exceeds a tipping point (or threshold point), using U.S. Census tract-level data. Lee et al.

(2011) develop a test for the existence of threshold eﬀects and apply their test to this dataset.

Diﬀerent from these existing studies, we consider a high-dimensional setup by allowing both

possibly highly nonlinear eﬀects of the main covariate (minority share in the neighborhood)

and possibly higher-order interactions between additional covariates.

We build on the speciﬁcations used in Card et al. (2008) and Lee et al. (2011) to choose

the following median regression with a constant shift due to the tipping eﬀect:

Yi = g0(Qi) + δ01{Qi > τ0} + X(cid:48)

iβ0 + Ui,

(8.1)

where for census tract i, the dependent variable Yi is the ten-year change in the neighbor-

hood’s white population, Qi is the base-year minority share in the neighborhood, and Xi is

a vector of six tract-level control variables and their various interactions depending on the

model speciﬁcation. The basic six variables in Xi include the unemployment rate, the log of

mean family income, the fractions of single-unit, vacant, and renter-occupied housing units,
and the fraction of workers who use public transport to travel to work. The function g(·) is

approximated by the cubic b-splines with 20 knots over equi-quantile locations, so the degree

of freedom is 24 including the intercept term.

In the ﬁrst set of models, we consider possible interactions among the six tract-level con-

trol variables up to six-way interactions. Speciﬁcally, the vector X in the six-way interactions

will be composed of the following 63 regressors,

{X (1), . . . , X (6), X (1)X (2), . . . , X (5)X (6), . . . , X (1)X (2)X (3)X (4)X (5)X (6)},

28

Table 3: Median Regression with a Tipping Eﬀect (Chicago)

No. of Reg.

No. of Selected Reg.

in Step 3b

CI for τ0

6 control variables
No Interaction
Two-way Interaction
Three-way Interaction
Four-way Interaction
Five-way Interaction
Six-way Interaction

31
46
66
81
87
88

18
23
25
26
25
25

48.74
48.74
48.74
48.74
48.74
48.74

[46.50, 52.20]
[43.28, 59.13]
[46.87, 51.40]
[47.06, 51.72]
[46.76, 51.11]
[46.86, 51.24]

12 control variables
No Interaction
Two-way Interaction
Three-way Interaction
Four-way Interaction
Five-way Interaction
Six-way Interaction
Note: The sample size is n = 1, 813. The parameter τ is estimated by the grid search over {Qi} ∈ [10, 60].

Both (cid:98)τ and the 95% conﬁdence interval are the results of re-estimation after Step 3b: τ is estimated again
using ((cid:101)Ui,(cid:101)α) from Step 3b.

[44.21, 55.65]
[46.28, 51.31]
[46.69, 51.05]
[47.42, 50.24]
[46.76, 51.10]
[46.19, 51.59]

48.74
48.74
48.74
48.74
48.74
48.74

37
103
323
818
1610
2534

21
23
28
28
28
27

(cid:98)τ

(cid:98)τ

Table 4: Median Regression with a Tipping Eﬀect (Pittsburgh)

No. of Reg.

No. of Selected Reg.

in Step 3b

CI for τ0

6 control variables
No Interaction
Two-way Interaction
Three-way Interaction
Four-way Interaction
Five-way Interaction
Six-way Interaction

31
46
66
81
87
88

8
6
6
6
6
6

53.45
53.45
53.45
53.45
53.45
53.45

[45.59, 60.00]
[44.81, 60.00]
[45.20, 60.00]
[45.12, 60.00]
[45.73, 60.00]
[46.02, 60.00]

12 control variables
No Interaction
Two-way Interaction
Three-way Interaction
Four-way Interaction
Five-way Interaction
Six-way Interaction
Note: The sample size is n = 663. The parameter τ is estimated by the grid search over {Qi} ∈ [10, 60].

Both (cid:98)τ and the 95% conﬁdence interval are the results of re-estimation after Step 3b: τ is estimated again
using ((cid:101)Ui,(cid:101)α) from Step 3b.

[45.89, 60.00]
[44.63, 60.00]
[46.03, 60.00]
[44.65, 60.00]
[45.74, 60.00]
[46.25, 60.00]

53.45
53.45
53.45
53.45
53.45
53.45

37
103
323
818
1610
2534

5
6
10
10
10
9

29

where X (j) is the j-th element among those tract-level control variables. Note that the

lower order interaction vector (e.g. two-way or three-way) is nested by the higher order

interaction vector (e.g. three-way or four-way). The total number of regressors varies from

31 when there is no interaction to 88 when there are full six-way interactions. In the next

set of models, we add the square of each tract-level control variable and generate similar

interactions up to six. In this case the total number of regressors varies from 37 to 2,534.

For example, the number of regressors in the largest model consists of #(b-sline basis) +

#(indicator function) + #(interactions up to six-way out of 12) = 24 + 1 +(cid:80)6

(cid:0)12
(cid:1) =

k

k=1

2, 534. We use the census-tract-level samples of Chicago and Pittsburgh whose base year

is 1980. The sample size of Chicago is 1,813 and that of Pittsburgh is 663. Note that the

number of regressors is much larger than the sample size in some model speciﬁcations.

Tables 3–4 summarize the estimation results. We report the total number of regressors

in each model and the number of selected regressors in Step 3b. The change point τ is
estimated by the grid search over {Qi} ∈ [10, 60]. In this empirical example, we report the

estimates of τ0 and the conﬁdence intervals updated after Step 3b (that is, τ is re-estimated

using the estimates of α0 in Step 3b). If this estimate is diﬀerent from the previous one

in Step 2, then we repeat Step 3 and Step 2 until it converges. The reported conﬁdence

intervals for τ0 is the intersection of the simulated conﬁdence intervals and the parameter

space [10, 60].

The estimation results suggest several interesting points. First, the proposed method

selects sparse representations in all model speciﬁcations even when the number of regressors

is relatively large. Furthermore, the sizes of selected regressors are stable and do not vary

much with the number of regressors. Second, the estimated change points are quite robust to

the model speciﬁcation. This result is reconﬁrmed in Figure 3, where we plot the predicted

values over Qi at the sample median of Xi with observations in the data. They are from

the model of six-way interactions with 12 control variables and the vertical line indicates the

location of a tipping point. In these estimation results, white population at the threshold

30

Figure 3: Estimation Results: Six-way Interactions with 12 Control Variables

Note: Each dot denotes the tract-level observation of Minority Share and Change in White Population from

1980 to 1990. The vertical line stands for the tipping point ((cid:98)τ ), which is re-estimated after Step 3b. The graph
(blue line) represents the predicted value of Yi given Qi and med(Xi). Speciﬁcally, (cid:98)Yi =(cid:80)24
1(Qi >(cid:98)τ )(cid:101)δ + med(Xi)T(cid:101)β, where B(·) is the cubic b-spline basis with 20 knots. Parameters (cid:101)βspline,(cid:101)δ and (cid:101)β
are estimation results from Step 3b. Notice that(cid:101)g(Qi) =(cid:80)24

j=1 B(Qi)(cid:101)β(j)

j=1 B(Qi)(cid:101)β(j)

spline.

spline +

point dropped 6.61 percentage points in Chicago and 7.15 percentage points in Pittsburgh,

respectively. Finally, the conﬁdence intervals are quite tight in all cases and they provide

convincing evidence of the tipping eﬀect.

In summary, this empirical example shows that the proposed method works well in the

real empirical setup. The estimation results also conﬁrm that there exists a tipping point in

the racial segregation when we consider high-dimensional median regression.

9 Conclusions

In this paper, we have developed (cid:96)1-penalized estimators of a high dimensional quantile

regression model with an unknown change point due to a covariate threshold. We have

shown among other things that our estimator of the change point achieves an oracle property

without relying on a perfect covariate selection, thereby avoiding the need for the minimum

31

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll020406080100−30−20−1001020Estimation Results: ChicagoQ: Minority RatioY: Change in White Populationllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll020406080100−30−20−1001020Estimation Results: PittsburghQ: Minority RatioY: Change in White Populationlevel condition on the signals of active covariates. We have illustrated the usefulness of our

estimation methods via Monte Carlo experiments and an application to tipping in the racial

segregation.

In a recent working paper, Leonardi and B¨uhlmann (2016) consider a high dimensional

mean regression model with multiple change points whose number may grow as the sample

size increases. They have proposed a binary search algorithm to choose the number of

change points. It is an important future research topic to develop a computationally eﬃcient

algorithm to detect multiple changes for high dimensional quantile regression models.

32

Appendices

We ﬁrst provide the algorithm of constructing the conﬁdence interval for τ0 in Appendix

A. To provide theoretical results, we consider a general M-estimation framework that includes

quantile regression as a special case. We provide high-level regularity conditions on the loss

function in Appendix B. Under these conditions, we derive asymptotic properties and then

we verify all the high level assumptions for the quantile regression model in Appendix C.

Hence, our general results are of independent interest and can be applicable to other models,

for example logistic regression models.

A The Algorithm of Constructing the Conﬁdence In-

terval for τ0

The detailed algorithm for constructing the conﬁdence interval based on the Step 2

estimator is as follows:

1. Simulate two independent Poisson processes N1(−h) for h < 0 and N2(h) for f > 0

with the same jump rate (cid:98)fQ((cid:98)τ ) over h ∈ [−Hn, Hn], where fQ(·) is the pdf of Q, n

is the sample size, and H > 0 is a large constant. For estimating fQ(·), we use the

kernel density estimator with a normal density kernel and the rule-of-thumb bandwidth,
1.06 · min{s, (Q0.75 − Q0.25)/1.34} · n−1/5, where s is the standard deviation of Q and
Q0.75 − Q0.25 is the interquartile range of Q. A Poisson process N (h) is generated by

the following algorithm:

(b) Generate  from the uniform distribution on [0, 1].

(a) Set h = 0 and k = 0.

(c) h = h + [−(1/(cid:98)fQ((cid:98)τ )) log()].

(d) If h > nH, then stop and goto Step (f). Otherwise, set k = k + 1 and hk = h.
(e) Repeat Steps (b)–(d).

33

(f) The algorithm generates {hk} for k = 1, . . . , K. Transform it into the Poisson

process N (h) ≡(cid:80)K

k=1 1{hk ≤ h} for h ∈ [0, nH].

2. Using the residuals { ˘Ui} and the estimate ˘δ from Step 1, construct the empirical
˘δ)− ˙ρ( ˘Ui) for i = 1, . . . , n,

distributions of ˘˙ρ1i ≡ ˙ρ( ˘Ui−X T
where ˙ρ (t) ≡ t (γ − 1{t ≤ 0}) is the check function as deﬁned in Section 5.

i

˘δ)− ˙ρ( ˘Ui) and ˘˙ρ2i ≡ ˙ρ( ˘Ui+X T

i

3. Simulate ˙ρ1j for j = 1, . . . , N1(−h) and ˙ρ2j for j = 1, . . . , N2(h).

4. Recall that

M (h) ≡ N1(−h)(cid:88)

i=1

ρ1i1{h < 0} +

N2(h)(cid:88)

i=1

ρ2i1{h ≥ 0}

from Section 5. Construct the function M (·) for h ∈ [−Hn, Hn] using values from
Steps 1–3 above. Find the smallest minimizer h of M (·).

5. Repeat Steps 1–4 above and generate {h1, . . . , hB}.

6. Construct the 95% conﬁdence interval of(cid:98)τ from the empirical distribution of {hb} by
[(cid:98)τ + h0.025/n,(cid:98)τ + h0.975/n], where h0.025 and h0.975 are 2.5 and 97.5 percentiles of {hb},

respectively.

It is straightforward to modify the algorithm above for the conﬁdence intervals with Step

3a and Step 3b estimators. We set H = 0.5, and B = 1, 000 in this simulation studies.

B Regularity conditions on the general loss function

Let Y be a scalar variable of outcome and X be a vector of p-dimensional observed

characteristics. Suppose there is an observable scalar variable Q such that the conditional

distribution of Y or some feature of that (given X) depends on:

X T β01{Q ≤ τ0} + X T θ01{Q > τ0} = X T β0 + X T δ01{Q > τ0},

34

where δ0 = θ0 − β0. Let ρ : R × R → R+ be a loss function under consideration, whose

analytical form is clear in speciﬁc models. Suppose the true parameters are deﬁned as the

minimizer of the expected loss:

(β0, δ0, τ0) ≡ argmin
(β,δ)∈A,τ∈T

E(cid:2)ρ(Y, X T β + X T δ1{Q > τ})(cid:3) ,

(B.1)

where A and T denote the parameter spaces for (β0, δ0) and τ0. Here β represents the

components of “baseline parameters”, while δ represents the structural changes; τ is the

change point value where the structural changes occur, if any. By construction, τ0 is not
unique when δ0 = 0. For each (β, δ) ∈ A and τ ∈ T , deﬁne 2p × 1 vectors:

α ≡ (βT , δT )T , X(τ ) ≡ (X T , X T 1{Q > τ})T .

Then X T β + X T δ1{Q > τ} = X(τ )T α, and by letting α0 ≡ (βT

0 , δT

0 )T , we can write (B.1)

more compactly as:

(α0, τ0) = argmin
α∈A,τ∈T

E(cid:2)ρ(Y, X(τ )T α)(cid:3) .

In quantile regression models, for a given quantile γ ∈ (0, 1), recall that

ρ(t1, t2) = (t1 − t2)(γ − 1{t1 − t2 ≤ 0}).

B.1 When δ0 (cid:54)= 0 and τ0 is identiﬁed

For a constant η > 0, deﬁne

(cid:110)

r : E(cid:0)(cid:2)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:3) 1{Q ≤ τ0}(cid:1)

r1(η) ≡ sup

r

≥ ηE[(X T (β − β0))21{Q ≤ τ0}] for all β ∈ B(β0, r)

35

(B.2)

(cid:111)

and

(cid:110)
r : E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0

(cid:1)(cid:3) 1{Q > τ0}(cid:1)

r2(η) ≡ sup

r

≥ ηE[(X T (θ − θ0))21{Q > τ0}] for all θ ∈ G(θ0, r)

(cid:111)

,

where B(β0, r) and G(θ0, r) are deﬁned in (4.3). Note that r1(η) and r2(η) are the maximal
radii over which the excess risk can be bounded below by the quadratic loss on {Q ≤ τ0}
and {Q > τ0}, respectively.

Assumption B.1.

(i) Let Y denote the support of Y . There is a Liptschitz constant

L > 0 such that for all y ∈ Y, ρ(y,·) is convex, and

|ρ(y, t1) − ρ(y, t2)| ≤ L|t1 − t2|,∀t1, t2 ∈ R.

(ii) For all α ∈ A, almost surely,

E(cid:2)ρ(Y, X(τ0)T α) − ρ(Y, X(τ0)T α0)|Q(cid:3) ≥ 0.

(iii) There exist constants η∗ > 0 and r∗ > 0 such that r1(η∗) ≥ r∗ and r2(η∗) ≥ r∗.

(iv) There is a constant c0 > 0 such that for all τ ∈ T0,

E(cid:2)(cid:0)ρ(cid:0)Y, X T θ0
E(cid:2)(cid:0)ρ(cid:0)Y, X T β0

(cid:1) − ρ(cid:0)Y, X T β0
(cid:1) − ρ(cid:0)Y, X T θ0

(cid:1)(cid:1) 1{τ < Q ≤ τ0}(cid:3) ≥ c0E(cid:2)(X T (β0 − θ0))2 1{τ < Q ≤ τ0}(cid:3) ,
(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3) ≥ c0E(cid:2)(X T (β0 − θ0))2 1{τ0 < Q ≤ τ}(cid:3) .

In this paper, we focus on a convex Lipchitz loss function, which is assumed in condition

(i). It might be possible to weaken the convexity to a “restricted strong convexity condition”

as in Loh and Wainwright (2013). For simplicity, we focus on the case of a convex loss,

which is satisﬁed for quantile regression. However, unlike the framework of M-estimation

36

in Negahban et al. (2012) and Loh and Wainwright (2013), we do allow ρ(t1, t2) to be non-

diﬀerentiable, which admits the quantile regression model as a special case.

Condition (iii) requires that the excess risk can be bounded below by a quadratic function

locally when τ is ﬁxed at τ0, while condition (iv) is an analogous condition when α is ﬁxed at
α0. conditions (iii) and (iv), combined with the convexity of ρ(Y,·), helps us derive the rates

of convergence (in the (cid:96)1 norm) of the Lasso estimators of (α0, τ0). Furthermore, these two

conditions separate the conditions for α and τ , making them easier to interpret and verify.

Remark B.1. Condition (iii) of Assumption B.1 is similar to the restricted nonlinear impact

(RNI) condition of Belloni and Chernozhukov (2011). One may consider an alternative

formulation as in van de Geer (2008) and B¨uhlmann and van de Geer (2011) (Chapter 6),

which is known as the margin condition. But the margin condition needs to be adjusted

to account for structural changes as in condition (iv).

It would be an interesting future

research topic to develop a general theory of high-dimensional M-estimation with an unknown

sparsity-structural-change with general margin conditions.

Remark B.2. Assumptions B.1 (iv) and 4.4 (iii) together imply that for all τ ∈ T0, there

exists a constant c0 > 0 such that

∆1(τ ) ≡ E(cid:2)(cid:0)ρ(cid:0)Y, X T θ0
∆2(τ ) ≡ E(cid:2)(cid:0)ρ(cid:0)Y, X T β0

(cid:1) − ρ(cid:0)Y, X T β0
(cid:1) − ρ(cid:0)Y, X T θ0

(cid:1)(cid:1) 1{τ < Q ≤ τ0}(cid:3) ≥ c2
(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3) ≥ c2

0

0

P [τ < Q ≤ τ0] ,
P [τ0 < Q ≤ τ ] .

(B.3)

Note that Assumption B.1 (ii) implies that ∆1(τ ) is monotonely non-increasing when τ < τ0,

and ∆2(τ ) is monotonely non-decreasing when τ > τ0, respectively. Therefore, Assumptions
B.1 (ii), B.1 (iv) and 4.4 (iii) all together imply that (B.3) holds for all τ in the T , not
just in the T0 since T is compact. Equation (B.3) plays an important role in achieving a

super-eﬃcient convergence rate for τ0, since it states the presence of a kink in the expected

loss and that of a jump in the loss function at τ0.

We now move to the set of assumptions that are useful to deal with the Step 3b estimator.

37

Deﬁne

mj(τ, α) ≡ ∂E[ρ(Y, X(τ )T α)]
Also, let mJ (τ, α) ≡ (mj(τ, α) : j ∈ J(α0)).

∂αj

, m(τ, α) ≡ (m1(τ, α), ..., m2p(τ, α))T .

Assumption B.2. E[ρ(Y, X(τ )T α)] is three times continuously diﬀerentiable with respect to
α, and there are constants c1, c2, L > 0 and a neighborhood T0 of τ0 such that the following
conditions hold: for all large n and all τ ∈ T0,

(i) there is Mn > 0, which may depend on the sample size n, such that

|mj(τ, α0) − mj(τ0, α0)| < Mn|τ − τ0|;

max
j≤2p

(ii) there is r > 0 such that for all β ∈ B(β0, r), θ ∈ G(θ0, r), α = (βT , θT − βT )T satisﬁes:

|mj(τ, α) − mj(τ, α0)| < L|α − α0|1 ;

max
j≤2p

sup
τ∈T0

(iii) α0 is in the interior of the parameter space A, and

(cid:18) ∂2E[ρ(Y, XJ (τ )T α0J )]
(cid:19)
(cid:12)(cid:12)(cid:12)(cid:12)∂3E[ρ(Y, XJ (τ )T αJ )]

∂αi∂αj∂αk

∂αJ ∂αT
J

max
i,j,k∈J

> c1,

(cid:12)(cid:12)(cid:12)(cid:12) < L.

inf
τ∈T0

λmin

sup

|αJ−α0J|1<c2,

sup
τ∈T0

The score-condition in the population level is expressed by m(τ0, α0) = 0 since α0 is in
the interior of A by condition (iii). Conditions (i) and (ii) regulate the continuity of the score

m(τ, α), and condition (iii) assumes the higher-order diﬀerentiability of the expectation of

the loss function. Condition (i) requires the Lipschitz continuity of the score function with

respect to the threshold. The Lipschitz constant may grow with n, since it is assumed
uniformly over j ≤ 2p.

not aﬀect the asymptotic behavior of (cid:101)α. For quantile regression models, we will show that

In many examples, Mn in fact grows slowly; as a result, it does

38

Mn = Cs1/2 for some constant C > 0. Condition (ii) requires the local equicontinuity at α0

in the (cid:96)1 norm of the class

{mj(τ, α) : τ ∈ T0, j ≤ 2p}.

We now establish that Assumptions B.1 and B.2 are satisﬁed for quantile regression

models.

Lemma B.1. Suppose that Assumptions 3.1 and 4.1 hold. Then Assumptions B.1 and B.2

are satisﬁed by the loss function for the quantile regression model, with Mn = Cs1/2 for some

constant C > 0.

B.1.1 Proof of Lemma B.1

Veriﬁcation of Assumption B.1 (i). It is straightforward to show that the loss function for

quantile regression is convex and satisﬁes the Liptschitz condition.

Veriﬁcation of Assumption B.1 (ii). Note that ρ(Y, t) = hγ(Y −t), where hγ(t) = t(γ−1{t ≤
0}). By (B.3) of Belloni and Chernozhukov (2011),

(cid:90) v

hγ(w − v) − hγ(w) = −v(γ − 1{w ≤ 0}) +

(1{w ≤ z} − 1{w ≤ 0})dz

(B.4)

where w = Y − X(τ0)T α0 and v = X(τ0)T (α − α0). Note that

0

E[v(γ − 1{w ≤ 0})|Q] = −E[X(τ0)T (α − α0)(γ − 1{U ≤ 0})|Q] = 0,

since P(U ≤ 0|X, Q) = γ. Let FY |X,Q denote the CDF of the conditional distribution Y |X, Q.

39

Then

E(cid:2)ρ(Y, X(τ0)T α) − ρ(Y, X(τ0)T α0)|Q(cid:3)
(cid:34)(cid:90) X(τ0)T (α−α0)
(cid:34)(cid:90) X(τ0)T (α−α0)

= E

0

(1{U ≤ z) − 1{U ≤ 0})dz

(cid:35)
(cid:12)(cid:12)(cid:12)Q

= E
≥ 0,

0

[FY |X,Q(X(τ0)T α0 + z|X, Q) − FY |X,Q(X(τ0)T α0|X, Q)]dz

(cid:35)
(cid:12)(cid:12)(cid:12)(cid:12)Q

where the last inequality follows immediately from the fact that FY |X,Q(·|X, Q) is the CDF.
Hence, we have veriﬁed Assumption B.1 (ii).

Veriﬁcation of Assumption B.1 (iii). Following the arguments analogous those used in (B.4)

of Belloni and Chernozhukov (2011), the mean value expansion implies:

(cid:20)

E(cid:2)ρ(Y, X(τ0)T α) − ρ(Y, X(τ0)T α0)|Q(cid:3)
(cid:40)(cid:90) X(τ0)T (α−α0)
(α − α0)T E(cid:2)X(τ0)X(τ0)T fY |X,Q(X(τ0)T α0|X, Q)|Q(cid:3) (α − α0)
(cid:40)(cid:90) X(τ0)T (α−α0)

zfY |X,Q(X(τ0)T α0|X, Q) +

= E

z2
2

˜fY |X,Q(X(τ0)T α0 + t|X, Q)dz

=

1
2
+E

(cid:41)
(cid:12)(cid:12)(cid:12)(cid:12)Q

0

0

z2
2

˜fY |X,Q(X(τ0)T α0 + t|X, Q)

(cid:21)

(cid:41)
(cid:12)(cid:12)(cid:12)(cid:12)Q

dz

for some intermediate value t between 0 and z. By condition (ii) of Assumption 4.1,

| ˜fY |X,Q(X(τ0)T α0 + t|X, Q)| ≤ C1

and fY |X,Q(X(τ0)T α0|X, Q) ≥ C2.

40

Hence, taking the expectation on {Q ≤ τ0} gives

E(cid:2)ρ(Y, X T β) − ρ(Y, X T β0)1{Q ≤ τ0}(cid:3)

≥ C2
2
≥ C2
4

E[(X T (β − β0))21{Q ≤ τ0}] − C1
6
E[|X T (β − β0)|21{Q ≤ τ0}],

E[|X T (β − β0)|31{Q ≤ τ0}]

where the last inequality follows from

C2
4

E[|X T (β − β0)|21{Q ≤ τ0}] ≥ C1
6

E[|X T (β − β0)|31{Q ≤ τ0}].

(B.5)

To see why (B.5) holds, note that by (4.5), for any nonzero β ∈ B(β0, r∗

QR),

E[|X T (β − β0)|21{Q ≤ τ0}]3/2
E[|X T (β − β0)|31{Q ≤ τ0}]

≥ r∗

QR

2C1
3C2

≥ 2C1
3C2

E[|X T (β − β0)|21{Q ≤ τ0}]1/2,

which proves (B.5) immediately. Thus, we have shown that Assumption B.1 (iii) holds for
r1(η) with η∗ = C2/4 and r∗ = r∗

QR deﬁned in (4.5) in Assumption 4.1. The case for r2(η) is

similar and hence is omitted.

Veriﬁcation of Assumption B.1 (iv). We again start from (B.4) but with diﬀerent choices of
(w, v) such that w = Y − X(τ0)T α0 and v = X T δ0[1{Q ≤ τ0}− 1{Q > τ0}]. Then arguments

similar to those used in verifying Assumptions B.1 (ii)-(iii) yield that for τ < τ0,

(cid:1)|Q = τ(cid:3)

(cid:1) − ρ(cid:0)Y, X T β0

E(cid:2)ρ(cid:0)Y, X T θ0
(cid:40)(cid:90) X T δ0
(cid:40)(cid:90) (cid:101)ε(X T δ0)
E(cid:2)(X T δ0)2|Q = τ(cid:3) ,
≥ (cid:101)ε2C3

≥ E

0

0

2

= E

zfY |X,Q(X T β0 + t|X, Q)dz

zfY |X,Q(X T β0 + t|X, Q)dz

(cid:41)
(cid:12)(cid:12)(cid:12)(cid:12)Q = τ
(cid:12)(cid:12)(cid:12)(cid:12)Q = τ

(cid:41)

(B.6)

(B.7)

(B.8)

(B.9)

41

where t is an intermediate value t between 0 and z. Thus, we have that

E(cid:2)(cid:0)ρ(cid:0)Y, X T θ0

(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:1) 1{τ < Q ≤ τ0}(cid:3) ≥ (cid:101)ε2C3

E(cid:2)(X T (β0 − θ0))2 1{τ < Q ≤ τ0}(cid:3) .

2

The case that τ > τ0 is similar.

Veriﬁcation of Assumption B.2. Note that that

mj(τ, α) = E[Xj(τ )(1{Y − X(τ )T α ≤ 0} − γ)].

Hence, mj(τ0, α0) = 0, for all j ≤ 2p. For condition (i) of Assumption B.2, for all j ≤ 2p,

|mj(τ, α0) − mj(τ0, α0)|
= |EXj(τ )[1{Y ≤ X(τ )T α0} − 1{Y ≤ X(τ0)T α0}]|
= |EXj(τ )[P(Y ≤ X(τ )T α0|X, Q) − P(Y ≤ X(τ0)T α0|X, Q)]|
≤ CE|Xj(τ )||(X(τ ) − X(τ0))T α0|
= CE|Xj(τ )||X T δ0(1{Q > τ} − 1{Q > τ0})|
≤ CE|Xj(τ )||X T δ0|(1{τ < Q < τ0} + 1{τ0 < Q < τ})
≤ C(P(τ0 < Q < τ ) + P(τ < Q < τ0)) sup
τ,τ(cid:48)∈T0
≤ C(P(τ0 < Q < τ ) + P(τ < Q < τ0)) sup
τ,τ(cid:48)∈T0

E(|Xj(τ )X T δ0||Q = τ(cid:48))

[E(|Xj(τ )|2||Q = τ(cid:48))]1/2[E(|X T δ0|2|Q = τ(cid:48))]1/2

≤ CM2K2|δ0|2|τ0 − τ|

for some constant C, where the last inequality follows from conditions (ii), (iii) and (v) of

Assumption 3.1. Therefore, we have veriﬁed condition (i) of Assumption B.2 with Mn =
CM2K2|δ0|2.

42

We now verify condition (ii) of Assumption B.2. For all j and τ in a neighborhood of τ0,

|mj(τ, α) − mj(τ, α0)| = |EXj(τ )(1{Y ≤ X(τ )T α} − 1{Y ≤ X(τ )T α0})|
= |EXj(τ )(P(Y ≤ X(τ )T α|X, Q) − P(Y ≤ X(τ )T α0|X, Q))|
≤ CE|Xj(τ )||X(τ )T (α − α0)| ≤ C|α − α0|1 max
j≤2p,i≤2p

E|Xj(τ )Xi(τ )|,

which implies the result immediately in view of Assumption 3.1. Finally, it is straightforward

to verify condition (iii) using Assumption 4.1 (iii).

B.2 When δ0 = 0

We now consider the case when δ0 = 0. In this case, τ0 is not identiﬁable, and there is
actually no structural change in the sparsity. If α0 is in the interior of A, then m(τ, α0) = 0
for all τ ∈ T .

For a constant η > 0, deﬁne

˜r1(η) ≡ sup

r

(cid:110)
r : E(cid:0)(cid:2)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0
≥ ηE[(X T (β − β0))21{Q ≤ τ}] for all β ∈ ˜B(β0, r, τ ) and for all τ ∈ T(cid:111)

(cid:1)(cid:3) 1{Q ≤ τ}(cid:1)

and

˜r2(η) ≡ sup

r

(cid:110)
r : E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T β0
≥ ηE[(X T (θ − β0))21{Q > τ}] for all θ ∈ ˜G(β0, r, τ ) and for all τ ∈ T(cid:111)

(cid:1)(cid:3) 1{Q > τ}(cid:1)

,

where ˜B(β0, r, τ ) and ˜G(β0, r, τ ) are deﬁned in (4.4).

Assumption B.3.

(i) Let Y denote the support of Y . There is a Liptschitz constant

43

L > 0 such that for all y ∈ Y, ρ(y,·) is convex, and

|ρ(y, t1) − ρ(y, t2)| ≤ L|t1 − t2|,∀t1, t2 ∈ R.

(ii) For all α ∈ A and for all τ ∈ T , almost surely,

E[ρ(Y, X(τ )T α) − ρ(Y, X T β0)|Q] ≥ 0,

(iii) There exist constants η∗ > 0 and r∗ > 0 such that ˜r1(η∗) ≥ r∗ and ˜r2(η∗) ≥ r∗.

(iv) E[ρ(Y, X(τ )T α)] is three times diﬀerentiable with respect to α, and there are universal
constants r > 0 and L > 0 such that for all β ∈ ˜B(β0, r, τ ), θ ∈ ˜G(β0, r, τ ), α =
(βT , θT − βT )T satisﬁes:

|mj(τ, α) − mj(τ, α0)| < L|α − α0|1 .

max
j≤2p

for all large n and for all τ ∈ T .

(v) α0 is in the interior of the parameter space A, and there are constants c1 an c2 > 0

such that

λmin

(cid:32)∂2E[ρ(Y, X T

J(β0)β0J )]

(cid:33)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∂3E[ρ(Y, X T

∂βJ ∂βT
J

J(β0)βJ )]

∂βi∂βj∂βk

> c1,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) < L.

sup

|αJ−α0J|1<c2,

max

i,j,k∈J(β0)

As in Lemma B.1, we now establish that Assumption B.3 is satisﬁed for quantile regression

models when δ0 = 0.

Lemma B.2. Suppose that Assumptions 3.1 and 4.1 hold. Then Assumption B.3 is satisﬁed.

44

B.2.1 Proof of Lemma B.2

Veriﬁcation of Assumption B.3 (i). This is the same as the veriﬁcation of Assumption B.1

(i).

Veriﬁcation of Assumption B.3 (ii). This can be veriﬁed exactly as in veriﬁcation of As-

sumption B.1 (ii) with α0 = β0 now.

Veriﬁcation of Assumption B.3 (iv). By the arguments identical to those used to verify As-

sumption B.1 (iii), we have that

E(cid:2)ρ(Y, X T β) − ρ(Y, X T β0)1{Q ≤ τ}(cid:3)

≥ C2
2
≥ C2
4

E[(X T (β − β0))21{Q ≤ τ}] − C1
6
E[|X T (β − β0)|21{Q ≤ τ}],

E[|X T (β − β0)|31{Q ≤ τ}]

where the last inequality follows from (4.7). This proves the case for ˜r1(η). The case for

˜r2(η) is similar and hence is omitted.

Veriﬁcation of Assumptions B.3 (iv) and (v). They can be veriﬁed similarly as in veriﬁca-
tion of Assumption B.2 in the proof of Lemma Lemma B.1. For all j and τ ∈ T ,

|mj(τ, α) − mj(τ, α0)| = |EXj(τ )(1{Y ≤ X(τ )T α} − 1{Y ≤ X(τ )T α0})|
= |EXj(τ )(P(Y ≤ X(τ )T α|X, Q) − P(Y ≤ X(τ )T α0|X, Q))|
≤ CE|Xj(τ )||X(τ )T (α − α0)| ≤ C|α − α0|1 max
j≤2p,i≤2p

E|Xj(τ )Xi(τ )|,

which implies condition B.3 (iv) in view of Assumption 3.1. It also is straightforward to

verify condition B.3 (v) using Assumption 4.1 (iii).

45

C Proofs of Theorems

Throughout the proofs, we deﬁne

n(cid:88)

(cid:104)

(cid:16)

νn (α, τ ) ≡ 1
n

ρ

Yi, Xi (τ )T α

Y, X (τ )T α

(cid:16)

(cid:17) − Eρ
(cid:16)
(cid:104)

.

(cid:17)(cid:105)
(cid:17) − Eρ

(cid:16)

Without loss of generality let νn (αJ , τ ) = n−1(cid:80)n

i=1

Y, XJ (τ )T αJ
In this section, we suppose that Assumptions B.1 and B.2 hold when δ0 (cid:54)= 0 and that

Yi, XiJ (τ )T αJ

i=1

ρ

(cid:17)(cid:105)

.

Assumption B.3 holds when δ0 = 0, respectively.

C.1 Useful Lemmas

For the positive constant K1 in Assumption 3.1 (i), deﬁne

(cid:114)

cnp ≡

2 log (4np)

n

+

K1 log (4np)

n

.

Let (cid:100)x(cid:101) denote the smallest integer greater than or equal to a real number x. The following

lemma bounds νn (α, τ ).

Lemma C.1. For any positive sequences m1n and m2n, and any (cid:101)δ ∈ (0, 1), there are con-
stants L1, L2 and L3 > 0 such that for an = L1cnp(cid:101)δ−1, bn = L2cnp(cid:100)log2 (m2n/m1n)(cid:101)(cid:101)δ−1, and
cn = L3n−1/2(cid:101)δ−1,

sup
τ∈T

|νn (α, τ ) − νn (α0, τ )| ≥ anm1n

sup

|α−α0|1≤m1n

sup
τ∈T

sup

m1n≤|α−α0|1≤m2n

|νn (α, τ ) − νn (α0, τ )|

|α − α0|1

≥ bn

and for any η > 0 and Tη = {τ ∈ T : |τ − τ0| ≤ η},

|νn (α0, τ ) − νn (α0, τ0)| ≥ cn|δ0|2

46

(cid:40)
(cid:40)

P

P

(cid:40)

P

sup
τ∈Tη

(cid:41)
(cid:41)

≤(cid:101)δ,
≤(cid:101)δ,

(cid:41)

√

η

≤(cid:101)δ.

(C.1)

(C.2)

(C.3)

Proof of (C.1): Let 1, ..., n denote a Rademacher sequence, independent of {Yi, Xi, Qi}i≤n.

By the symmetrization theorem (see, for example, Theorem 14.3 of B¨uhlmann and van de

Geer (2011)) and then by the contraction theorem (see, for example, Theorem 14.4 of

B¨uhlmann and van de Geer (2011)),

(cid:32)

E

sup
τ∈T
≤ 2E

(cid:32)
(cid:32)

≤ 4LE

(cid:33)

(cid:16)

|νn (α, τ ) − νn (α0, τ )|

sup

|α−α0|1≤m1n

sup
τ∈T

sup

|α−α0|1≤m1n

sup
τ∈T

sup

|α−α0|1≤m1n

n

ρ

Yi, Xi (τ )T α

iXi (τ )T (α − α0)

(cid:16)

Yi, Xi (τ )T α0

(cid:33)

(cid:17)(cid:105)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Note that

sup
τ∈T

sup

|α−α0|1≤m1n

n

= sup
τ∈T

sup

|α−α0|1≤m1n

iXi (τ )T (α − α0)

(αj − α0j)

1
n

iXij (τ )

≤

sup

|α−α0|1≤m1n

|αj − α0j| sup
τ∈T

max
j≤2p

iXij (τ )

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ m1n sup
τ∈T

max
j≤2p

iXij (τ )

n

i

i=1

i=1

(cid:104)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 2p(cid:88)
2p(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)

j=1

j=1

i=1

i=1

n

(cid:17) − ρ
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:33)

.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n(cid:88)

i=1

max
j≤2p

n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

i=1

n

For all ˜L > K1,

(cid:32)

E

sup
τ∈T

max
j≤2p

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

iXij (τ )

≤(1)

˜L log E

˜L−1 sup
τ∈T

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:32)
(cid:32)

(cid:18)

(cid:34)
(cid:34)

exp

exp

(C.4)

(cid:33)(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:33)(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)
(cid:19)(cid:21)

i=1

iXij (τ )

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

iXij (τ )

≤(2)

˜L log E

(cid:20)

≤(3)

˜L log

4np exp

˜L−1 max

τ∈{Q1,...,Qn} max
j≤2p

n

2( ˜L2 − ˜LK1)

,

where inequality (1) follows from Jensen’s inequality, inequality (2) comes from the fact that

47

Xij (τ ) is a step function with jump points on T ∩ {Q1, . . . , Qn}, and inequality (3) is by

Bernstein’s inequality for the exponential moment of an average (see, for example, Lemma

14.8 of B¨uhlmann and van de Geer (2011)), combined with the simple inequalities that

exp(|x|) ≤ exp(x) + exp(−x) and that exp(max1≤j≤J xj) ≤(cid:80)J

j=1 exp(xj). Then it follows

that

(cid:32)

E

n(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

n

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

sup
τ∈T

max
j≤2p

iXij (τ )

≤ ˜L log(4np)

+

1

2( ˜L − K1)

where the last equality follows by taking ˜L = K1 +(cid:112)n/[2 log(4np)]. Thus, by Markov’s

i=1

n

= cnp,

(C.5)

inequality,

(cid:40)

P

sup
τ∈T

sup

|α−α0|1≤m1n

|νn (α, τ ) − νn (α0, τ )| > anm1n

(cid:41)

−1 4Lm1ncnp =(cid:101)δ,

≤ (anm1n)

where the last equality follows by setting L1 = 4L.
Proof of (C.2): Recall that 1, ..., n is a Rademacher sequence, independent of {Yi, Xi, Qi}i≤n.

m1n≤|α−α0|1≤m2n

Note that

(cid:32)

E

sup
τ∈T

≤(1) 2E

≤(2) 2

≤(3) 4L

τ∈T

sup

sup
sup
k(cid:88)
(cid:32)
k(cid:88)

τ∈T

E

j=1

E

j=1

|νn (α, τ ) − νn (α0, τ )|

|α − α0|1
n(cid:88)
ρ

i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

n

i=1

sup

m1n≤|α−α0|1≤m2n

sup

2j−1m1n≤|α−α0|1≤2j m1n

sup
τ∈T

sup

2j−1m1n≤|α−α0|1≤2j m1n

Yi, Xi (τ )T α

Yi, Xi (τ )T α0

(cid:33)

(cid:16)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)

i=1

i=1

n

n

ρ

i

(cid:17) − ρ

(cid:16)
|α − α0|1
Yi, Xi (τ )T α

(cid:16)

Xi (τ )T (α − α0)

2j−1m1n

i

(cid:16)

(cid:17) − ρ
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:33)

,

2j−1m1n

(cid:17)



(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Yi, Xi (τ )T α0

(cid:17)



(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

where inequality (1) is by the symmetrization theorem, inequality (2) holds for some k ≡
(cid:100)log2 (m2n/m1n)(cid:101), and inequality (3) follows from the contraction theorem.

48

Next, the identical arguments showing (C.4) yield

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

n

n(cid:88)

i=1

Xi (τ )T (α − α0)

2j−1m1n

i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2 max

j≤2p

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

n

n(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

iXij (τ )

sup

2j−1m1n≤|α−α0|1≤2j m1n

uniformly in τ ∈ T . Then, as in the proof of (C.1), Bernstein’s and Markov’s inequalities

imply that

P

(cid:40)

sup
τ∈T

sup

m1n≤|α−α0|1≤m2n

|νn (α, τ ) − νn (α0, τ )|

|α − α0|1

> bn

(cid:41)

n 8Lkcnp =(cid:101)δ,

≤ b−1

where the last equality follows by setting L2 = 8L.

Proof of (C.3): As above, by the symmetrization and contraction theorems, we have that

(cid:33)

(cid:17) − ρ

(cid:16)

E

|νn (α0, τ ) − νn (α0, τ0)|

sup
τ∈Tη

(cid:32)
(cid:32)
(cid:32)

sup
τ∈Tη

≤ 2E

(cid:104)

(cid:16)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)

i=1

n

i

≤ 4LE
sup
τ∈Tη
≤ 4LC1(M2|δ0|2
√
n

n

i=1
2K2η)1/2

ρ

Yi, Xi (τ )T α0

Yi, Xi (τ0)T α0

iX T

i δ0 (1{Qi > τ} − 1{Qi > τ0})

(cid:17)(cid:12)(cid:12)(cid:12)(cid:35)(cid:33)

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

for some constant C1 < ∞, where the last inequality is due to Theorem 2.14.1 of van der

Vaart and Wellner (1996) with M2 in Assumption 3.1 (v) and K2 in Assumption 3.1 (ii).
Speciﬁcally, we apply the second inequality of this theorem to the class F = {f (, X, Q, τ ) =
X T δ0(1{Q > τ} − 1{Q > τ0}), τ ∈ Tη}. Note that F is a Vapnik-Cervonenkis class, which
has a uniformly bounded entropy integral and thus J(1,F) in their theorem is bounded, and
that the L2 norm of the envelope |iX T
i δ0|1{|Qi − τ0| < η} is proportional to the square root
of the length of Tη:

(E|iX T

i δ0|21{|Qi − τ0| < η})1/2 ≤ (2M2|δ0|2

2K2η)1/2.

49

This implies the last inequality with C1 being

√
2 times the entropy integral of the class F.

Then, by Markov’s inequality, we obtain (C.3) with L3 = 4LC1(M2K2)1/2.

C.2 Proof of Theorem 3.1

Deﬁne D(τ ) = diag(Dj(τ ) : j ≤ 2p); and also let D0 = D (τ0) and ˘D = D (˘τ ). It follows

from the deﬁnition of (˘α, ˘τ ) in (2.2) that

n(cid:88)

i=1

1
n

ρ(Yi, Xi(˘τ )T ˘α) + κn| ˘D ˘α|1 ≤ 1
n

n(cid:88)

i=1

ρ(Yi, Xi(τ0)T α0) + κn|D0α0|1.

(C.6)

From (C.6) we obtain the following inequality

R(˘α, ˘τ ) ≤ [νn(α0, τ0) − νn(˘α, ˘τ )] + κn|D0α0|1 − κn| ˘D ˘α|1
= [νn(α0, ˘τ ) − νn(˘α, ˘τ )] + [νn(α0, τ0) − νn(α0, ˘τ )]

(cid:16)|D0α0|1 − | ˘D ˘α|1

(cid:17)

.

+κn

(C.7)

(cid:2)(s/n)1/2 log n(cid:3) due to (C.3)

Note that the second component [νn(α0, τ0) − νn(α0, ˘τ )] = oP
of Lemma C.1 with taking Tη = T by choosing some suﬃciently large η > 0. Thus, we

focus on the other two terms in the following discussion. We consider two cases respectively:
|˘α − α0|1 ≤ |α0|1 and |˘α − α0|1 > |α0|1.

Suppose that |˘α − α0|1 ≤ |α0|1 . Then,

≤ 2 ¯D |α0|1 , and

(cid:12)(cid:12)(cid:12)1
≤(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12) ˘D ˘α
(cid:17)(cid:12)(cid:12)(cid:12) ≤ 3κn
(cid:16)|D0α0|1 − | ˘D ˘α|1

¯D |α0|1 .

(cid:12)(cid:12)(cid:12)κn

(cid:12)(cid:12)(cid:12) ˘Dα0

(cid:12)(cid:12)(cid:12)1

+

Applying (C.1) in Lemma C.1 with m1n = |α0|1, we obtain

|νn(α0, ˘τ ) − νn(˘α, ˘τ )| ≤ an |α0|1 ≤ κn |α0|1 w.p.a.1,

where the last inequality follows from the fact that an (cid:28) κn with κn satisfying (2.3). Thus,

50

the theorem follows in this case.

Now assume that |˘α − α0|1 > |α0|1. In this case, apply (C.2) of Lemma C.1 with m1n =

|α0|1 and m2n = 2M1p, where M1 is deﬁned in Assumption 3.1(iii), to obtain

|νn(α0, ˘τ ) − νn(˘α, ˘τ )|

|˘α − α0|1

≤ bn

with probability arbitrarily close to one for small enough(cid:101)δ. Since bn (cid:28) Dκn, we have

|νn(α0, ˘τ ) − νn(˘α, ˘τ )| ≤ κnD |˘α − α0|1 ≤ κn

Therefore,

R(˘α, ˘τ ) + oP

(cid:0)n−1/2 log n(cid:1) ≤ κn

≤ κn

w.p.a.1.

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:17)
(cid:16)|D0α0|1 − | ˘D ˘α|1
(cid:16)|D0α0|1 − | ˘D ˘αJ|1
(cid:17)

(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

+ κn

+ κn

(cid:12)(cid:12)(cid:12)1

,

where the last inequality follows from the fact that ˘α − α0 = ˘αJ C + (˘α − α0)J . Thus, the
theorem follows in this case as well.

C.3 Proof of Theorem 3.2

Deﬁne

(cid:0)R (α0, τ ) + 2ωn

(cid:1) /(ωnD),

¯D |α0|1

M∗ ≡ 4 max
τ∈Tn

where Tn ⊂ T will be speciﬁed below. For each τ , deﬁne

(cid:98)α(τ ) = argminα∈ARn(α, τ ) + ωn

2p(cid:88)

Dj(τ )|αj|.

(C.8)

(C.9)

j=1

51

It follows from the deﬁnition of (cid:98)α(τ ) in (C.9) that
n(cid:88)
ρ(Yi, Xi(τ )T(cid:98)α(τ )) + ωn|D(τ )(cid:98)α(τ )|1 ≤ 1

n(cid:88)

1
n

i=1

n

i=1

ρ(Yi, Xi(τ )T α0) + ωn|D(τ )α0|1.

(C.10)

Next, let

and ¯α (τ ) = t (τ )(cid:98)α (τ ) + (1 − t (τ )) α0. By construction, it follows that |¯α (τ ) − α0|1 ≤ M∗.

t (τ ) =

M∗

M∗ + |(cid:98)α (τ ) − α0|1

And also note that

|¯α (τ ) − α0|1 ≤ M∗/2 implies |(cid:98)α (τ ) − α0|1 ≤ M∗

(C.11)

since ¯α (τ ) − α0 = t (τ ) ((cid:98)α (τ ) − α0).
n(cid:88)

For each τ , (C.10) and the convexity of the following map

α (cid:55)→ 1
n

i=1

ρ(Yi, Xi(τ )T α) + ωn|D(τ )α|1

implies that

n(cid:88)

i=1

1
n

ρ(Yi, Xi(τ )T ¯α (τ )) + ωn|D(τ )¯α (τ )|1

ρ(Yi, Xi(τ )T(cid:98)α(τ )) + ωn|D(τ )(cid:98)α(τ )|1
n(cid:88)

ρ(Yi, Xi(τ )T α0) + ωn|D(τ )α0|1

(cid:35)
(cid:35)

(cid:34)

1
n

n(cid:88)
(cid:34)

i=1

≤ t (τ )

+ [1 − t (τ )]

1
n

i=1

(cid:34)

≤

1
n

n(cid:88)

i=1

(cid:35)

ρ(Yi, Xi(τ )T α0) + ωn|D(τ )α0|1

,

which in turn yields the following inequality

R(¯α(τ ), τ ) + ωn|D(τ )¯α(τ )|1 ≤ [νn(α0, τ ) − νn(¯α(τ ), τ )] + R(α0, τ ) + ωn|D(τ )α0|1.

(C.12)

52

Furthermore, by the triangle inequality, (C.12) can be written as

R(¯α(τ ), τ ) + ωnD |¯α(τ ) − α0|1 ≤ [νn(α0, τ ) − νn(¯α(τ ), τ )] + R(α0, τ ) + 2ωnD|α0|1.

(C.13)

Now let ZM = supτ∈Tn sup|α−α0|≤M |νn (α, τ ) − νn (α0, τ )| for each M > 0. Then, by Lemma
C.1, ZM∗ = op (ωnM∗) by the simple fact that log(np) ≤ 2 log(n ∨ p). Thus, in view of the
deﬁnition of M∗ in (C.8), the following inequality holds w.p.a.1,

R(¯α(τ ), τ ) + ωnD |¯α(τ ) − α0|1 ≤ ωnDM∗/2

(C.14)

uniformly in τ ∈ Tn.

We can repeat the same arguments for(cid:98)α(τ ) instead of ¯α(τ ) due to (C.11) and (C.14), to

obtain

R((cid:98)α(τ ), τ ) + ωnD |(cid:98)α(τ ) − α0|1 ≤ ωnDM∗ = O(ωns), w.p.a.1,

(C.15)

uniformly in τ ∈ Tn. It remains to show that there exists a set Tn such that(cid:98)τ ∈ Tn w.p.a.1

and the corresponding M∗ = O(s). We split the remaining part of the proof into two cases:
δ0 (cid:54)= 0 and δ0 = 0.
(Case 1: δ0 (cid:54)= 0)

Let

Tn =(cid:8)τ : |τ − τ0| ≤ Cn−1 log log n(cid:9)

for some constant C > 0. Note that we assume that if δ0 (cid:54)= 0, then

|(cid:98)τ − τ0| = OP (n−1),

53

which implies that(cid:98)τ ∈ Tn w.p.a.1. Furthermore, note that
(cid:1) − ρ(cid:0)Y, X T β0
(cid:1) − ρ(cid:0)Y, X T θ0

R (α0, τ ) = E(cid:0)(cid:2)ρ(cid:0)Y, X T θ0
+ E(cid:0)(cid:2)ρ(cid:0)Y, X T β0

(cid:1)(cid:3) 1{τ < Q ≤ τ0}(cid:1)
(cid:1)(cid:3) 1{τ0 < Q ≤ τ}(cid:1) .

(C.16)

Combining the fact that the objective function is Liptschitz continuous by Assumptions B.1

(i) with Assumption 3.1, we have that

|R (α0, τ )| ≤ L sup
τ∈Tn

sup
τ∈Tn

(cid:104)E(cid:0)|X T δ0||1{τ < Q ≤ τ0}(cid:1) + E(cid:0)|X T δ0||1{τ0 < Q ≤ τ}(cid:1)(cid:105)
(cid:1) .

= O(cid:0)|δ0|1 n−1 log log n(cid:1)
= o(cid:0)|δ0|1 ω2

n

Thus, M∗ = O (|α0|1) = O (s).
(Case 2: δ0 = 0) Redeﬁne M∗ with Tn = T as the maximum over the whole parameter
space for τ . Note that when δ0 = 0, we have that R (α0, τ ) = 0 and M∗ = O (|α0|1) = O (s).

Therefore, the desired result follows immediately.

C.4 Proof of Theorem 5.1

Remark C.1. We ﬁrst brieﬂy provide the logic behind the proof of Theorem 5.1 here.
Note that for all α ≡ (βT , δT )T ∈ R2p and θ ≡ β + δ, the excess risk has the following

decomposition: when τ1 < τ0,

R (α, τ1) = E(cid:0)(cid:2)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0
+ E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0
+ E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:3) 1{Q ≤ τ1}(cid:1)
(cid:1)(cid:3) 1{Q > τ0}(cid:1)
(cid:1)(cid:3) 1{τ1 < Q ≤ τ0}(cid:1) ,

(C.17)

54

and when τ2 > τ0,

R (α, τ2) = E(cid:0)(cid:2)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0
+ E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0
+ E(cid:0)(cid:2)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T θ0

(cid:1)(cid:3) 1{Q ≤ τ0}(cid:1)
(cid:1)(cid:3) 1{Q > τ2}(cid:1)
(cid:1)(cid:3) 1{τ0 < Q ≤ τ2}(cid:1) .

(C.18)

The key observations are that all the six terms in the above decompositions are non-negative,

and are stochastically negligible when taking α = ˘α, and τ1 = ˘τ if ˘τ < τ0 or τ2 = ˘τ if ˘τ > τ0.

This follows from the risk consistency of R(˘α, ˘τ ). Then, the identiﬁcation conditions for α0

and τ0 (Assumptions B.1 (ii)-(iv)), along with Assumption 4.5 (i), are useful to show that

the risk consistency implies the consistency of ˘τ .

Proof of Theorem 5.1. Recall from (C.18) that for all α = (βT , δT )T ∈ R2p and θ = β + δ,

the excess risk has the following decomposition: when τ > τ0,

R (α, τ ) = E(cid:0)(cid:2)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0
+ E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0
+ E(cid:0)(cid:2)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T θ0

(cid:1)(cid:3) 1{Q ≤ τ0}(cid:1)
(cid:1)(cid:3) 1{Q > τ}(cid:1)
(cid:1)(cid:3) 1{τ0 < Q ≤ τ}(cid:1) .

(C.19)

We split the proof into four steps.

Step 1: All the three terms on the right hand side (RHS) of (C.19) are nonnegative. As a

consequence, all the three terms on the RHS of (C.19) are bounded by R(α, τ ).

Proof of Step 1. Step 1 is implied by the condition that E[ρ(Y, X(τ0)T α)−ρ(Y, X(τ0)T α0)|Q] ≥
0 a.s. for all α ∈ A. To see this, the ﬁrst two terms are nonnegative by simply multiplying
E[ρ(Y, X(τ0)T α) − ρ(Y, X(τ0)T α0)|Q] ≥ 0 with 1{Q ≤ τ0} and 1{Q > τ} respectively. To
show that the third term is nonnegative for all β ∈ Rp and τ > τ0, set α = (β/2, β/2) in the

55

inequality 1{τ0 < Q ≤ τ}E[ρ(Y, X(τ0)T α) − ρ(Y, X(τ0)T α0)|Q] ≥ 0. Then we have that

1{τ0 < Q ≤ τ}E[ρ(Y, X T (β/2 + β/2)) − ρ(Y, X T θ0)|Q] ≥ 0,

which yields the nonnegativeness of the third term.

Step 2: Let a ∨ b = max(a, b) and a ∧ b = min(a, b). Prove:

E(cid:2)|X T (β − β0)|1{Q ≤ τ0}(cid:3) ≤ 1

(cid:20) 1

(cid:21)1/2

η∗r∗ R(α, τ ) ∨

η∗ R(α, τ )

.

Proof of Step 2. Recall that

(cid:110)
r : E(cid:0)(cid:2)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:3) 1{Q ≤ τ0}(cid:1)

r1(η) ≡ sup

r

≥ ηE[(X T (β − β0))21{Q ≤ τ0}] for all β ∈ B(β0, r)

For notational simplicity, write

E[(X T (β − β0))21{Q ≤ τ0}] ≡ (cid:107)β − β0(cid:107)2
q,

(cid:111)

.

and

F (δ) ≡ E(cid:0)(cid:2)ρ(cid:0)Y, X T (β0 + δ)(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:3) 1{Q ≤ τ0}(cid:1) .

Note that F (β − β0) = E(cid:0)(cid:2)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:3) 1{Q ≤ τ0}(cid:1), and β ∈ B(β0, r) if and

only if (cid:107)β − β0(cid:107)q ≤ r.

For any β, if (cid:107)β − β0(cid:107)q ≤ r1(η∗), then by the deﬁnition of r1(η∗), we have:

F (β − β0) ≥ η∗E[(X T (β − β0))21{Q ≤ τ0}].

If (cid:107)β − β0(cid:107)q > r1(η∗), let t = r1(η∗)(cid:107)β − β0(cid:107)−1

q ∈ (0, 1). Since F (·) is convex, and F (0) = 0,

56

we have F (β − β0) ≥ t−1F (t(β − β0)). Moreover, deﬁne

ˇβ = β0 + r1(η∗)

β − β0
(cid:107)β − β0(cid:107)q

,

then (cid:107) ˇβ − β0(cid:107)q = r1(η∗) and t(β − β0) = ˇβ − β0. Hence still by the deﬁnition of r1(η∗),

F (β − β0) ≥ 1
t

F ( ˇβ − β0) ≥ η∗

t

E[(X T ( ˇβ − β0))21{Q ≤ τ0}] = η∗r1(η∗)(cid:107)β − β0(cid:107)q.

Therefore, by Assumption B.1 (iii), and Step 1,

R(α, τ ) ≥ E(cid:0)(cid:2)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:3) 1{Q ≤ τ0}(cid:1)

≥ η∗E[(X T (β − β0))21{Q ≤ τ0}] ∧ η∗r∗{E[(X T (β − β0))21{Q ≤ τ0}]}1/2

≥ η∗(cid:0)E(cid:2)|X T (β − β0)|1{Q ≤ τ0}(cid:3)(cid:1)2 ∧ η∗r∗E(cid:2)|X T (β − β0)|1{Q ≤ τ0}(cid:3) ,

where the last inequality follows from Jensen’s inequality.

Step 3: For any (cid:48) > 0, there is an ε > 0 such that for all τ and α ∈ R2p, R(α, τ ) < ε
implies |τ − τ0| < (cid:48).

Proof of Step 3. We ﬁrst prove that, for any (cid:48) > 0, there is ε > 0 such that for all τ > τ0,
and α ∈ R2p, R(α, τ ) < ε implies that τ < τ0 + (cid:48).

Suppose that R(α, τ ) < ε. Applying the triangle inequality, for all β and τ > τ0,

E(cid:2)(cid:0)ρ(cid:0)Y, X T β0
(cid:1) − ρ(cid:0)Y, X T θ0
≤(cid:12)(cid:12)E(cid:2)(cid:0)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T θ0
+(cid:12)(cid:12)E(cid:2)(cid:0)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3)
(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3)(cid:12)(cid:12)
(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3)(cid:12)(cid:12) .

(C.20)

First, note that the ﬁrst term on the RHS of (C.20) is the third term on the RHS of (C.19),

hence is bounded by R(α, τ ) < ε.

We now consider the second term on the RHS of (C.20). Assumption 4.5 (i) implies, with

57

1 = (cid:101)C−1

1

C∗

C∗

2

(cid:17)

(cid:16)
1 − (cid:101)C1
E(cid:2)|X T β|1{Q > τ0}(cid:3) ≤ E(cid:2)|X T β|1{Q ≤ τ0}(cid:3) ≤ C∗

2 = (cid:101)C−1

1 − (cid:101)C2

> 0 and C∗

(cid:17)

(cid:16)

2

> 0, for all β ∈ Rp,

E(cid:2)|X T β|1{Q > τ0}(cid:3) .

1

(C.21)

It follows from the Lipschitz condition, Step 2, and (C.21) that

(cid:12)(cid:12)E(cid:2)(cid:0)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3)(cid:12)(cid:12) ≤ LE(cid:2)(cid:12)(cid:12)X T (β − β0)(cid:12)(cid:12) 1{τ0 < Q ≤ τ}(cid:3)
≤ LE(cid:2)(cid:12)(cid:12)X T (β − β0)(cid:12)(cid:12) 1{τ0 < Q}(cid:3)
E(cid:2)(cid:12)(cid:12)X T (β − β0)(cid:12)(cid:12) 1{Q ≤ τ0}(cid:3)
ε/(η∗r∗) ∨(cid:112)ε/η∗(cid:111)
(cid:110)

2

≤ LC∗−1
≤ LC∗−1
≡ C(ε).

2

Thus, we have shown that (C.20) is bounded by C(ε) + ε.

For any (cid:48) > 0, it follows from Assumptions B.1 (ii), B.1 (iv) and 4.4 (iii) (see also Remark

B.2) that there is a c > 0 such that if τ > τ0 + (cid:48),

cP (τ0 < Q ≤ τ0 + (cid:48)) ≤ cP (τ0 < Q ≤ τ )

≤ E(cid:2)(cid:0)ρ(cid:0)Y, X T β0

(cid:1) − ρ(cid:0)Y, X T θ0

(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3)

≤ C(ε) + ε.

Since ε (cid:55)→ C(ε) + ε converges to zero as ε converges to zero, for a given (cid:48) > 0 choose a
suﬃcient small ε > 0 such that C(ε) + ε < cP(τ0 < Q ≤ τ0 + (cid:48)), so that the above inequality
cannot hold. Hence we infer that for this ε, when R(α, τ ) < ε, we must have τ < τ0 + (cid:48).

By the same argument, if τ < τ0, then we must have τ > τ0 − (cid:48). Hence, R(α, τ ) < ε

implies |τ − τ0| < (cid:48).

Step 4: ˘τ

p−→ τ0.

Proof of Step 4. For the ε chosen in Step 3, consider the event {R(˘α, ˘τ ) < ε}, which occurs

58

w.p.a.1, due to Theorem 3.1. On this event, |˘τ − τ0| < (cid:48) by Step 3. Because (cid:48) is taken

arbitrarily, we have proved the consistency of ˘τ .

C.5 Proof of Theorem 5.2

The proof consists of several steps. First, we prove that ˘β and ˘θ are inside the neighbor-

hoods of β0 and θ0, respectively. Second, we obtain an intermediate convergence rate for ˘τ

based on the consistency of the risk and ˘τ . Finally, we use the compatibility condition to

obtain a tighter bound.
Step 1: For any r > 0, w.p.a.1, ˘β ∈ B(β0, r) and ˘θ ∈ G(θ0, r).

Proof of Step 1. Suppose that ˘τ > τ0. The proof of Step 2 in the proof of Theorem 5.1

implies that when τ > τ0,

E(cid:2)(X T (β − β0))21{Q ≤ τ0}(cid:3) ≤ R(α, τ )2

(η∗r∗)2 ∨ R(α, τ )
η∗

.

For any r > 0, note that R(˘α, ˘τ ) = oP (1) implies that the event R(˘α, ˘τ ) < r2 holds w.p.a.1.
Therefore, we have shown that ˘β ∈ B(β0, r).

We now show that ˘θ ∈ G(θ0, r). When τ > τ0, we have that

R(α, τ ) ≥(1) E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0
= E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0
− E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0
≥(2) η∗E(cid:2)|X T (θ − θ0)|21{Q > τ0}(cid:3) ∧ η∗r∗(cid:0)E(cid:2)|X T (θ − θ0)|21{Q > τ0}(cid:3)(cid:1)1/2
− E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0

(cid:1)(cid:3) 1{Q > τ}(cid:1)
(cid:1)(cid:3) 1{Q > τ0}(cid:1)
(cid:1)(cid:3) 1{τ0 < Q ≤ τ}(cid:1)
(cid:1)(cid:3) 1{τ0 < Q ≤ τ}(cid:1) ,

where (1) is from (C.18) and (2) can be proved using arguments similar to those used in the

59

proof of Step 2 in the proof of Theorem 5.1. This implies that

E(cid:2)(X T (θ − θ0))21{Q > τ0}(cid:3) ≤ ˜R(α, τ )2
where ˜R(α, τ ) ≡ R(α, τ ) + E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0

.

(η∗r∗)2 ∨ ˜R(α, τ )
η∗
(cid:1)(cid:3) 1{τ0 < Q ≤ τ}(cid:1). Thus, it suﬃces

to show that ˜R(˘α, ˘τ ) = oP (1) in order to establish that ˘θ ∈ G(θ0, r). Note that for some

constant C > 0,

E(cid:2)(ρ(Y, X T θ) − ρ(Y, X T θ0))1{τ0 < Q ≤ τ}(cid:3)
≤(1) LE(cid:2)|X T (θ − θ0)|1{τ0 < Q ≤ τ}(cid:3)
(cid:21)
(cid:27)

≤(2) L|θ − θ0|1E
≤(3) L|θ − θ0|1E
≤(4) C(τ − τ0)|θ − θ0|1E

| ˜Xj|1{τ0 < Q ≤ τ}

| ˜Xj| sup

(cid:26)(cid:20)

(cid:20)
(cid:20)

| ˜Xj|

max
j≤p

max
j≤p

˜x

max
j≤p

(cid:21)

+ 1

,

+ L|θ − θ0|1E [|Q|1{τ0 < Q ≤ τ}]

(cid:21)

P(τ0 < Q ≤ τ| ˜X = ˜x)

+ L|θ − θ0|1E [|Q|1{τ0 < Q ≤ τ}]

where (1) is by the Lipschitz continuity of ρ(Y,·), (2) is from the fact that |X T (θ − θ0)| ≤
|θ − θ0|1(maxj≤p | ˜Xj| + |Q|), (3) is by taking the conditional probability, and (4) is from

Assumption 4.4 (ii).

By the expectation-form of the Bernstein inequality (Lemma 14.12 of B¨uhlmann and

van de Geer (2011)), E[maxj≤p |Xj|] ≤ K1 log(p + 1) +(cid:112)2 log(p + 1). By (C.27), which will

be shown below, |˘θ − θ0|1 = OP (s). Hence by (C.23), when ˘τ > τ0,

|˘τ − τ0||˘θ − θ0|1E[max
j≤p

|Xj|] = OP (κns2 log p) = oP (1).

Note that when ˘τ > τ0, the proofs of (C.27) and (C.23) do not require ˘θ ∈ G(θ0, r), so there
is no problem of applying them here. This implies that ˜R(˘α, ˘τ ) = oP (1).

The same argument yields that w.p.a.1, ˘θ ∈ G(θ0, r) and ˘β ∈ B(β0, r) when ˘τ ≤ τ0; hence

it is omitted to avoid repetition.

60

Step 2: Let ¯c0(δ0) ≡ c0 inf τ∈T0
E[(X T δ0)2|Q = τ ], which is bounded away from zero and
bounded above due to Assumption 4.4 (iii) . Then ¯c0(δ0)|˘τ − τ0| ≤ 4R (˘α, ˘τ ) w.p.a.1. As a
result, |˘τ − τ0| = OP [κns/¯c0(δ0)].

Proof. For any τ0 < τ and τ ∈ T0, and any β ∈ B(β0, r), α = (β, δ) with arbitrary δ, for

some L, M > 0 which do not depend on β and τ,

(cid:12)(cid:12)E(cid:0)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0
(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:12)(cid:12)
≤(1) LE(cid:2)(cid:12)(cid:12)X T (β − β0)(cid:12)(cid:12) 1{τ0 < Q ≤ τ}(cid:3)
≤(2) M L(τ − τ0)E(cid:2)(cid:12)(cid:12)X T (β − β0)(cid:12)(cid:12) 1{Q ≤ τ0}(cid:3)
(cid:110)E(cid:104)(cid:0)X T (β − β0)(cid:1)2
1{Q ≤ τ0}(cid:105)(cid:111)1/2
≤(4) (M L(τ − τ0))2 / (4η∗) + η∗E(cid:104)(cid:0)X T (β − β0)(cid:1)2
1{Q ≤ τ0}(cid:105)
≤(5) (M L(τ − τ0))2 / (4η∗) + E(cid:2)(cid:0)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0

≤(3) M L(τ − τ0)

(cid:1)(cid:1) 1{Q ≤ τ0}(cid:3)

≤(6) (M L(τ − τ0))2 / (4η∗) + R(α, τ ),

where (1) follows from the Lipschitz condition on the objective function, (2) is by Assumption
4.5 (ii), (3) is by Jensen’s inequality, (4) follows from the fact that uv ≤ v2/ (4c) + cu2 for

any c > 0, (5) is from Assumption B.1 (iii), and (6) is from Step 1 in the proof of Theorem

5.1.

In addition,(cid:12)(cid:12)E(cid:2)(cid:0)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0
(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3)(cid:12)(cid:12)
(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3)
(cid:1) − ρ(cid:0)Y, X T θ0
≥(1) E(cid:2)(cid:0)ρ(cid:0)Y, X T β0
(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3)(cid:12)(cid:12)
−(cid:12)(cid:12)E(cid:2)(cid:0)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T θ0
(cid:1) − ρ(cid:0)Y, X T θ0
(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3) − R(α, τ )
≥(2) E(cid:2)(cid:0)ρ(cid:0)Y, X T β0
(cid:27)

(cid:26)

E[(X T δ0)2|Q = τ ]

(τ − τ0) − R(α, τ ),

≥(3) c0

inf
τ∈T0

61

where (1) is by the triangular inequality, (2) is from (C.18), and (3) is by Assumption B.1

(iv). Therefore, we have established that there exists a constant ˜C > 0, independent of

(α, τ ), such that

¯c0(δ0)(τ − τ0) ≤ ˜C(τ − τ0)2 + 2R(α, τ ).

(C.22)

Note that when 0 < (τ − τ0) < ¯c0(δ0)(2 ˜C)−1, (C.22) implies that

¯c0(δ0)(τ − τ0) ≤ ¯c0(δ0)

2

(τ − τ0) + 2R(α, τ ),

which in turn implies that τ−τ0 ≤ 4
(τ −τ0) ≤ 0, we have τ0−τ ≤ 4

¯c0(δ0)R(α, τ ). By the same argument, when −¯c0(δ0)(2 ˜C)−1 <
¯c0(δ0) R(α, τ ) for α = (β, δ), with any θ ∈ G(θ0, r) and arbitrary

β.

Hence when ˘τ > τ0, on the event ˘β ∈ B(β0, r), and ˘τ − τ0 < ¯c0(δ0)(2 ˜C)−1, we have

˘τ − τ0 ≤ 4

¯c0(δ0)

R(˘α, ˘τ ).

(C.23)

When ˘τ ≤ τ0, on the event ˘θ ∈ G(θ0, r), and τ0 − ˘τ < ¯c0(δ0)(2 ˜C)−1, we have τ0 − ˘τ ≤

4

¯c0(δ0) R(˘α, ˘τ ). Hence due to Step 1 and the consistency of ˘τ , we have

|˘τ − τ0| ≤ 4

¯c0(δ0)

R (˘α, ˘τ ) w.p.a.1.

(C.24)

This also implies |˘τ − τ0| = OP [κns/¯c0(δ0)] in view of the proof of Theorem 3.1.

(cid:16)|D0α0|1 −(cid:12)(cid:12)(cid:12) ˘Dα0

(cid:17)

(cid:12)(cid:12)(cid:12)1

Step 3: Deﬁne ν1n (τ ) ≡ νn (α0, τ ) − νn (α0, τ0) and cα ≡ κn

+ |ν1n (˘τ )|.

Then,

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)

(cid:12)(cid:12)(cid:12)1

1
2

κn

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

(cid:12)(cid:12)(cid:12)1

≤ cα + 2κn

R (˘α, ˘τ ) +

w.p.a.1.

(C.25)

62

Proof. Recall the following basic inequality in (C.7):

R(˘α, ˘τ ) ≤ [νn(α0, ˘τ ) − νn(˘α, ˘τ )] − ν1n (˘τ ) + κn

(cid:16)|D0α0|1 − | ˘D ˘α|1

(cid:17)

.

(C.26)

Now applying Lemma C.1 to [νn(α0, ˘τ )− νn(˘α, ˘τ )] with an and bn replaced by an/2 and bn/2,

we can rewrite the basic inequality in (C.26) by

(cid:12)(cid:12)(cid:12) ˘D ˘α

(cid:12)(cid:12)(cid:12)1

− 1
2

κn

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:12)(cid:12)(cid:12)1

− |ν1n (˘τ )| w.p.a.1.

Now adding κn
|α0j|1 − |˘αj|1 + |(˘αj − α0j)|1 = 0 for j /∈ J, we have that

on both sides of the inequality above and using the fact that

κn |D0α0|1 ≥ R (˘α, ˘τ ) + κn

(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:16)|D0α0|1 −(cid:12)(cid:12)(cid:12) ˘Dα0

κn
≥ R (˘α, ˘τ ) +

1
2

κn

(cid:12)(cid:12)(cid:12)1
(cid:17)
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:12)(cid:12)(cid:12)1

+ |ν1n (˘τ )| + 2κn

w.p.a.1.

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

(cid:12)(cid:12)(cid:12)1

Therefore, we have proved Step 3.

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

(cid:12)(cid:12)(cid:12)1

cα; (ii) κn

Step 4: Suppose that κn

We prove the remaining part of the steps by considering two cases: (i) κn

> cα. We ﬁrst consider Case (ii).

(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J
(cid:2)κ2
ns/¯c0(δ0)(cid:3) and |˘α − α0| = OP (κns) .

> cα. Then

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

(cid:12)(cid:12)(cid:12)1

≤

|˘τ − τ0| = OP

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J
(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

(cid:12)(cid:12)(cid:12)1
≥(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:12)(cid:12)(cid:12)1

6

Proof. By κn

> cα and the basic inequality (C.25) in Step 3,

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

(cid:12)(cid:12)(cid:12)1

=

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J c

(cid:12)(cid:12)(cid:12)1

+

,

(C.27)

which enables us to apply the compatibility condition in Assumption 4.2.

63

Recall that (cid:107)Z(cid:107)2 = (EZ 2)1/2 for a random variable Z. Note that for s = |J(α0)|0,

1
2

κn

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J
(cid:12)(cid:12)(cid:12)1
¯D(cid:13)(cid:13)X(˘τ )T (˘α − α0)(cid:13)(cid:13)2
(cid:13)(cid:13)X(˘τ )T (˘α − α0)(cid:13)(cid:13)2

R (˘α, ˘τ ) +
≤(1) 3κn
≤(2) 3κn
9κ2
≤(3)
n
2˜cφ2 +

¯D2s

s/φ

√

˜c
2

2 ,

(C.28)

where (1) is from the basic inequality (C.25) in Step 3, (2) is by the compatibility condition
(Assumption 4.2), and (3) is from the inequality that uv ≤ v2/(2˜c) + ˜cu2/2 for any ˜c > 0.

We will show below in Step 5 that there is a constant C0 > 0 such that

(cid:13)(cid:13)X(˘τ )T (˘α − α0)(cid:13)(cid:13)2

2 ≤ C0R(˘α, ˘τ ) + C0¯c0(δ0)|˘τ − τ0|, w.p.a.1.

(C.29)

Recall that by (C.24), ¯c0(δ0)|˘τ − τ0| ≤ 4R (˘α, ˘τ ). Hence, (C.28) with ˜c = (5C0)−1 implies

that

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)

(cid:12)(cid:12)(cid:12)1

¯D2s
≤ 9κ2
n
˜cφ2

.

R (˘α, ˘τ ) + κn

(C.30)

By (C.30) and (C.24), |˘τ − τ0| = OP [κ2
D(˘τ ) ≥ D w.p.a.1 by Assumption 3.1 (iv).

Step 5: There is a constant C0 > 0 such that(cid:13)(cid:13)X(˘τ )T (˘α − α0)(cid:13)(cid:13)2

ns/¯c0(δ0)]. Also, by (C.30), |˘α − α0| = OP (κns) since

2 ≤ C0R(˘α, ˘τ ) + C0¯c0(δ0)|˘τ −

τ0|, w.p.a.1.

Proof. Note that

(cid:13)(cid:13)X(τ )T (α − α0)(cid:13)(cid:13)2

2 ≤ 2(cid:13)(cid:13)X(τ )T α − X(τ0)T α(cid:13)(cid:13)2
+ 4(cid:13)(cid:13)X(τ0)T α − X(τ0)T α0
2 + 4(cid:13)(cid:13)X(τ0)T α0 − X(τ )T α0
(cid:13)(cid:13)2

2

(cid:13)(cid:13)2

2 .

(C.31)

We bound the three terms on the right hand side of (C.31). When τ > τ0, there is a constant

64

C1 > 0 such that

2

(cid:13)(cid:13)X(τ )T α − X(τ0)T α(cid:13)(cid:13)2
= E(cid:2)(X T δ)21{τ0 ≤ Q < τ}(cid:3)
(cid:90) τ
E(cid:2)(X T δ)2(cid:12)(cid:12)Q = t(cid:3) dFQ(t)
(cid:90) τ
E(cid:2)(X T δ0)2(cid:12)(cid:12)Q = t(cid:3) dFQ(t) + 2

≤ 2

=

τ0

τ0

≤ C1¯c0(δ0)(τ − τ0),

(cid:90) τ

τ0

E(cid:2)(X T (δ − δ0))2(cid:12)(cid:12)Q = t(cid:3) dFQ(t)

where the last inequality is by Assumptions 3.1, 4.4 (ii), 4.4 (iii), and 4.5 (ii).

Similarly, (cid:13)(cid:13)X(τ0)T α0 − X(τ )T α0

(cid:13)(cid:13)2
2 = E(cid:2)(X T δ0)21{τ0 ≤ Q < τ}(cid:3) ≤ C1¯c0(δ0)(τ − τ0).

Hence, the ﬁrst and third terms of the right hand side of of (C.31) are bounded by 6C1¯c0(δ0)(τ−

τ0).

To bound the second term, note that there exists a constant C2 > 0 such that

2

(cid:13)(cid:13)2

(cid:13)(cid:13)X(τ0)T α − X(τ0)T α0
=(1) E(cid:2)(X T (θ − θ0))21{Q > τ0}(cid:3) + E(cid:2)(X T (β − β0))21{Q ≤ τ0}(cid:3)
≤(2) (η∗)−1E(cid:2)(cid:0)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0
(cid:1)(cid:1) 1{Q > τ0}(cid:3)
+ (η∗)−1E(cid:2)(cid:0)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0
(cid:1)(cid:1) 1{Q ≤ τ0}(cid:3)
≤(3) (η∗)−1R(α, τ ) + (η∗)−1E(cid:2)(cid:0)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0
≤(4) (η∗)−1R(α, τ ) + (η∗)−1LE(cid:2)|X T (θ − θ0)|1{τ0 < Q ≤ τ}(cid:3)
(cid:90) τ
E(cid:2)|X T (θ − θ0)|(cid:12)(cid:12)Q = t(cid:3) dFQ(t)

=(5) (η∗)−1R(α, τ ) + (η∗)−1L
≤(6) (η∗)−1R(α, τ ) + C3(τ − τ0),

τ0

(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:3)

where (1) is simply an identity, (2) from Assumption B.1 (iii), (3) is due to (C.19): namely,

E(cid:2)(cid:0)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T θ0

(cid:1)(cid:1) 1{Q > τ}(cid:3) + E(cid:2)(cid:0)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:1) 1{Q ≤ τ0}(cid:3) ≤ R(α, τ ),

65

is by Assumptions 3.1 (ii) and 4.5 (ii). Therefore, we have shown that(cid:13)(cid:13)X(τ )T (α − α0)(cid:13)(cid:13)2

(4) is by the Lipschitz continuity of ρ(Y,·), (5) is by rewriting the expectation term, and (6)
2 ≤
C0R(α, τ ) + C0¯c0(δ0)(τ − τ0) for some constant C0 > 0. The case of τ ≤ τ0 can be proved

using the same argument. Hence, setting τ = ˘τ , and α = ˘α, we obtain the desired result.

Step 6: We now consider Case (i). Suppose that κn

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

(cid:12)(cid:12)(cid:12)1
(cid:2)κ2
ns/¯c0(δ0)(cid:3) and |˘α − α0| = OP (κns) .

≤ cα. Then

|˘τ − τ0| = OP

Proof. Recall that Xij is the jth element of Xi, where i ≤ n, j ≤ p. By Assumption 3.1 and

Step 2,

n(cid:88)

i=1

sup
1≤j≤p

1
n

|Xij|2 |1 (Qi < ˘τ ) − 1 (Qi < τ0)| = OP [κns/¯c0(δ0)] .

By the mean value theorem,

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)1

(cid:12)(cid:12)(cid:12)|D0α0|1 −(cid:12)(cid:12)(cid:12) ˘Dα0
(cid:32)
p(cid:88)
n(cid:88)
(cid:2)κ2
ns|J(δ0)|0/¯c0(δ0)(cid:3) .

j=1

i=1

4
n

κn

≤ κn

= OP

|Xij1{Qi > τ}|2

(cid:33)−1/2(cid:12)(cid:12)(cid:12)δ(j)

0

(cid:12)(cid:12)(cid:12) 1

n

n(cid:88)

i=1

|Xij|2 |1{Qi > ˘τ} − 1{Qi > τ0}|

(C.32)

Here, recall that τ is the right-end point of T and |J(δ0)|0 is the dimension of nonzero

elements of δ0.

Due to Step 2 and (C.3) in Lemma C.1,

(cid:34) |δ0|2(cid:112)¯c0(δ0)

(cid:35)

(κns/n)1/2

.

(C.33)

|ν1n (˘τ )| = OP

66

(C.34)

Thus, under Case (i), we have that, by (C.24), (C.25), (C.32), and (C.33),

¯c0(δ0)

4

|˘τ − τ0| ≤ κn
2
≤ 3κn

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:12)(cid:12)(cid:12)1
(cid:16)|D0α0|1 −(cid:12)(cid:12)(cid:12) ˘Dα0
(cid:12)(cid:12)(cid:12)1
(cid:17)
(cid:104)
s1/2 (κns/n)1/2(cid:105)
(cid:0)κ2
ns2(cid:1) + OP

+ R (˘α, ˘τ )

+ 3|ν1n (˘τ )|

,

where the last equality uses the fact that |J(δ0)|0/¯c0(δ0) = O(s) and |δ0|2/(cid:112)¯c0(δ0) = O(s1/2)

= OP

at most (both could be bounded in some cases).

Therefore, we now have an improved rate of convergence in probability for ˘τ from rn0,τ ≡
ns2 + s1/2(κns/n)1/2]. Repeating the arguments identical to those to prove

κns to rn1,τ ≡ [κ2

(C.32) and (C.33) yields that

(cid:12)(cid:12)(cid:12)|D0α0|1 −(cid:12)(cid:12)(cid:12) ˘Dα0

(cid:12)(cid:12)(cid:12) = OP [rn1,τ κns] and |ν1n (˘τ )| = OP

(cid:104)

s1/2 (rn1,τ /n)1/2(cid:105)

.

κn

Plugging these improved rates into (C.34) gives

¯c0(δ0)|˘τ − τ0| = OP

(cid:2)s1/2(κns)3/2/n1/2(cid:3) + OP
(cid:2)s3/4(κns)1/4/n3/4(cid:3)

(cid:0)κns3/2/n1/2(cid:1) + OP

(cid:2)s3/4(κns)1/4/n3/4(cid:3)

(cid:12)(cid:12)(cid:12)1
(cid:0)κ3
ns3(cid:1) + OP
(cid:0)κ2
ns3/2(cid:1) + OP

= OP
≡ OP (rn2,τ ),

(cid:0)κ2
ns3/2(cid:1) since

where the second equality comes from the fact that the ﬁrst three terms are OP
κns3/2 = o(1), κnn/s → ∞, and κn

n → ∞ in view of the assumption that κns2 log p = o(1).

√

Repeating the same arguments again with the further improved rate rn2,τ , we have that

ns5/4(cid:1) + OP
(cid:0)κ2

(cid:2)s7/8(κns)1/8/n7/8(cid:3) ≡ OP (rn3,τ ).

|˘τ − τ0| = OP

67

ns1+2−k(cid:17)

κ2

+ OP

(cid:104)
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)

s(2k−1)/2k(κns)1/2k/n(2k−1)/2k(cid:105) ≡ OP (rnk,τ ).
(cid:12)(cid:12)(cid:12) = oP (κns), which proves the desired result

ns). Finally, the

Then letting k → ∞ gives the desired result that ¯c0(δ0)|˘τ − τ0| = OP (κ2

Thus, repeating the same arguments k times yields

(cid:16)

¯c0(δ0)|˘τ − τ0| = OP

same iteration based on (C.34) gives
since D(˘τ ) ≥ D w.p.a.1 by Assumption 3.1 (iv).

C.6 Proof of Theorem 5.3

Proof of Theorem 5.3. The asymptotic property of (cid:101)τ is well-known in the literature (see
Lemma C.3 below for its asymptotic distribution). Speciﬁcally, we can apply Theorem 3.4.1
n (·), Mn (·) ≡
of van der Vaart and Wellner (1996) (by deﬁning the criterion Mn (·) ≡ R∗
n (·) = R(α0, τ ), the distance function d (τ, τ0) ≡ |τ − τ0|1/2, and φn (δ) ≡ δ) to char-
ER∗
acterize the convergence rate of (cid:101)τ , which results in the super-consistency in the sense that
(cid:101)τ − τ0 = Op(n−1). See e.g. Section 14.5 of Kosorok (2008).

Furthermore, it is worth noting that the same theorem also implies that if

n ((cid:98)τ ) − R∗

n (τ0)] − [Rn (˘α,(cid:98)τ ) − Rn (˘α, τ0)] = Op(r−2

n )

[R∗

(C.35)

for some sequence rn satisfying r2

nφn (r−1

n, then

n ) ≤ √
rnd ((cid:98)τ , τ0) = Op (1) .

68

This is because

n ((cid:98)τ ) = R∗

R∗

n ((cid:98)τ ) − [Rn (˘α,(cid:98)τ ) − Rn (˘α, τ0) + R∗
n ((cid:98)τ ) − [Rn (˘α,(cid:98)τ ) − Rn (˘α, τ0) + R∗
n ((cid:98)τ ) − R∗
(cid:1) + R∗
(cid:0)r−2

n (τ0) ,

n

n (τ0)] − [Rn (˘α,(cid:98)τ ) − Rn (˘α, τ0)]} + R∗

≤(1) R∗
=(2) {[R∗

=(3) Op

n (τ0)] + [Rn (˘α,(cid:98)τ ) − Rn (˘α, τ0) + R∗
n (τ0)]
n (τ0)] + [Rn (˘α, τ0) − Rn (˘α, τ0) + R∗

n (τ0)]

n (τ0)

where inequality (1) uses the fact that(cid:98)τ is a minimizer of Rn (˘α, τ ), equality (2) follows since

Rn (˘α, τ0) − Rn (˘α, τ0) + R∗

n (τ0) = R∗
Then, note that we can set r−2

n (τ0), and equality (3) comes from (C.35).

n = ansn log(np) with sn = 1 and an = κns log n due to
Lemma C.2 and the rate of convergence ˘α − α0 = Op (κns) given by Theorem 5.2. Next, we

will apply a chaining argument to obtain the convergence rate of(cid:98)τ by repeatedly verifying

the condition R∗

n(τ0) + Op(r−2

n ), with an iteratively improved rate rn. Applying

n((cid:98)τ ) ≤ R∗

Theorem 3.4.1 of van der Vaart and Wellner (1996) with rn = (an log(np))

−1/2 , we have

(cid:98)τ − τ0 = Op (an log(np)) = Op (κns log n log(np)) .

Next, we reset sn = κns (log n)2 log(np) and an = κns log n to apply Lemma C.2 again and
−1/2. It

then Theorem 3.4.1 of van der Vaart and Wellner (1996) with rn = (snan log(np))

follows that

(cid:98)τ − τ0 = Op

(cid:0)[κns]2 (log n)3 (log(np))2(cid:1) .

√

n since it should satisfy the constraint that r2

In the next step, we set rn =
√

n ) ≤
n as well. Then, we conclude that (cid:98)τ = τ0 + Op (n−1). Furthermore, in view of Lemma
C.2, (cid:98)τ = τ0 + Op (n−1) implies that the asymptotic distribution of n ((cid:98)τ − τ0) is identical
to n ((cid:101)τ − τ0) since each of them is characterized by the minimizer of the weak limit of

nφn (r−1

n (Rn (α, τ0 + tn−1) − Rn (α, τ0)) with α = ˘α and α = α0, respectively. That is, the weak

limits of the processes are identical due to Lemma C.2. Therefore, we have proved the ﬁrst

69

conclusion of the theorem. Lemma C.3 establishes the second conclusion.

Lemma C.2. Suppose that α ∈ An ≡ {α = (cid:0)βT , δT(cid:1)T : |α − α0|1 ≤ Kan} and τ ∈ Tn ≡
(cid:12)(cid:12)(cid:12){Rn (α, τ ) − Rn (α, τ0)} − {Rn (α0, τ ) − Rn (α0, τ0)}(cid:12)(cid:12)(cid:12) = Op [ansn log(np)] .

{|τ − τ0| ≤ Ksn} for some K < ∞ and for some sequences an and sn as n → ∞. Then,

sup

α∈An,τ∈Tn

Proof of Lemma C.2. Noting that

ρ(cid:0)Yi, X T

i δ1{Qi > τ}(cid:1) = ρ(cid:0)Yi, X T

i β(cid:1) 1{Qi ≤ τ} + ρ(cid:0)Yi, X T

i β + X T

i δ(cid:1) 1{Qi > τ} ,

i β + X T

we have, for τ > τ0,

Dn (α, τ ) := {Rn (α, τ ) − Rn (α, τ0)} − {Rn (α0, τ ) − Rn (α0, τ0)}

(cid:2)ρ(cid:0)Yi, X T
(cid:2)ρ(cid:0)Yi, X T

i β(cid:1) − ρ(cid:0)Yi, X T
i θ(cid:1) − ρ(cid:0)Yi, X T

i θ0

i β0

(cid:1)(cid:3) 1{τ0 < Qi ≤ τ}
(cid:1)(cid:3) 1{τ0 < Qi ≤ τ}

n(cid:88)
n(cid:88)

i=1

i=1

=

1
n
− 1
n

=: Dn1 (α, τ ) − Dn2 (α, τ ) .

However, the Lipschiz property of ρ yields that

|Dn1 (α, τ )| =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

n

n(cid:88)

i=1

(cid:2)ρ(cid:0)Yi, X T

i β(cid:1) − ρ(cid:0)Yi, X T
n(cid:88)

i β0

(cid:1)(cid:3) 1{τ0 < Qi ≤ τ}

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ L max

i,j

|Xij||β − β0|1

1
n

i=1

1{τ0 < Qi ≤ τ}

= Op [log(np) · an · sn] uniformly in (α, τ ) ∈ An × Tn,

fact that E(cid:12)(cid:12) 1

where log(np) term comes from the Bernstein inequality and the sn term follows from the

i=1 1{τ0 < Qi ≤ τ}(cid:12)(cid:12) = E1{τ0 < Qi ≤ τ} ≤ C · Ksn due to the boundedness
(cid:80)n

n

of the density of Qi around τ0. The other term Dn2 (α, τ ) can be bounded similarly. The

70

case of τ < τ0 can be treated analogously and hence details are omitted.

Lemma C.3. We have that n ((cid:101)τ − τ0) converges in distribution to the smallest minimizer
Proof of Lemma C.3. The convergence rate of(cid:101)τ is standard as commented in the beginning

of a compound Poisson process deﬁned in Theorem 5.3.

of the proof of Theorem 5.3 and thus details are omitted here. We present the characteriza-

tion of the asymptotic distribution for the given convergence rate n.

Recall that ρ (t, s) = ˙ρ (t − s) , where ˙ρ (t) = t (γ − 1{t ≤ 0}). Note that

=

=

=

i=1

n (τ )

nR∗

i β0 − X T

i β0 − X T

n(cid:88)
˙ρ(cid:0)Yi − X T
n(cid:88)
(cid:2) ˙ρ(cid:0)Ui − X T
n(cid:88)
(cid:2) ˙ρ(cid:0)Ui − X T
n(cid:88)
(cid:2) ˙ρ(cid:0)Ui + X T

i δ01{Qi > τ0}(cid:1)

i δ01{Qi > τ}(cid:1) − ˙ρ(cid:0)Yi − X T
i δ0 (1{Qi > τ} − 1{Qi > τ0})(cid:1) − ˙ρ (Ui)(cid:3) (1{τ < Qi ≤ τ0} + 1{τ0 < Qi ≤ τ})
(cid:1) − ˙ρ (Ui)(cid:3) 1{τ < Qi ≤ τ0}
(cid:1) − ˙ρ (Ui)(cid:3) 1{τ0 < Qi ≤ τ} .

i δ0

i=1

i=1

+

i δ0

i=1

Thus, the asymptotic distribution of n ((cid:101)τ − τ0) is characterized by the smallest minimizer of

(cid:26)

n(cid:88)

the weak limit of

(cid:27)
(cid:26)
n(cid:88)
(cid:1)− ˙ρ (Ui) and ˙ρ2i = ˙ρ(cid:0)Ui + X T
for |h| ≤ K for some large K, where ˙ρ1i = ˙ρ(cid:0)Ui − X T

τ0 < Qi ≤ τ0 +

< Qi ≤ τ0

Mn (h) =

(cid:27)

τ0 +

˙ρ1i1

˙ρ2i1

h
n

i=1

i δ0

i=1

h
n

+

(cid:1)−

i δ0

˙ρ (Ui). The weak limit of the empirical process Mn (·) is well developed in the literature, (see

e.g. Pons (2003); Kosorok and Song (2007); Lee and Seo (2008)) and the argmax continuous

mapping theorem by Seijo and Sen (2011b) yields the asymptotic distribution, namely the

smallest minimizer of a compound Poisson process, which is deﬁned in Theorem 5.3.

71

C.7 Proof of Theorem 5.4

Let (cid:98)D ≡ D ((cid:98)τ ). It follows from the deﬁnition of (cid:98)α in (2.5) that

n(cid:88)

i=1

1
n

ρ(Yi, Xi((cid:98)τ )T(cid:98)α) + ωn|(cid:98)D(cid:98)α|1 ≤ 1

n

ρ(Yi, Xi((cid:98)τ )T α0) + ωn|(cid:98)Dα0|1.

n(cid:88)

i=1

From this, we obtain the following inequality

R((cid:98)α,(cid:98)τ ) ≤ [νn(α0,(cid:98)τ ) − νn((cid:98)α,(cid:98)τ )] + R(α0,(cid:98)τ ) + ωn|(cid:98)Dα0|1 − ωn|(cid:98)D(cid:98)α|1.

(C.36)

by

ωn

ωn

(cid:12)(cid:12)(cid:12)1

− 1
2

As before, adding ωn

(cid:12)(cid:12)(cid:12)(cid:98)Dα0

(cid:12)(cid:12)(cid:12)1

− |R(α0,(cid:98)τ )| w.p.a.1.

Now applying Lemma C.1 to [νn(α0,(cid:98)τ ) − νn((cid:98)α,(cid:98)τ )], we rewrite the basic inequality in (C.36)

(cid:12)(cid:12)(cid:12)(cid:98)D ((cid:98)α − α0)
(cid:12)(cid:12)(cid:12)1
that |α0j|1 − |(cid:98)αj|1 + |((cid:98)αj − α0j)|1 = 0 for j /∈ J, we have that

(cid:12)(cid:12)(cid:12)(cid:98)D(cid:98)α
≥ R((cid:98)α,(cid:98)τ ) + ωn
(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12)(cid:98)D ((cid:98)α − α0)
(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12)(cid:98)D ((cid:98)α − α0)
≤ |R(α0,(cid:98)τ )|;
> |R(α0,(cid:98)τ )|. We ﬁrst consider case (ii). Recall that (cid:107)Z(cid:107)2 = (EZ 2)1/2

R ((cid:98)α,(cid:98)τ ) +
(cid:12)(cid:12)(cid:12)(cid:98)D ((cid:98)α − α0)J

(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12)(cid:98)D ((cid:98)α − α0)J

≤ |R(α0,(cid:98)τ )| + 2ωn

As in the proof of Theorem 5.2, we consider two cases: (i) ωn

(cid:12)(cid:12)(cid:12)(cid:98)D ((cid:98)α − α0)J

on both sides of the inequality above and using the fact

1
2

ωn

w.p.a.1.

(C.37)

(cid:12)(cid:12)(cid:12)1

(ii) ωn

(cid:12)(cid:12)(cid:12)1

for a random variable Z. It follows from the compatibility condition (Assumption 4.2) and

the same arguments as in (C.28) that

(cid:12)(cid:12)(cid:12)(cid:98)D ((cid:98)α − α0)J

(cid:12)(cid:12)(cid:12)1

ωn

¯D(cid:13)(cid:13)X((cid:98)τ )T ((cid:98)α − α0)(cid:13)(cid:13)2

√
s/φ

(cid:13)(cid:13)X((cid:98)τ )T ((cid:98)α − α0)(cid:13)(cid:13)2

2

n

¯D2s
2˜cφ2 +

˜c
2

≤ ωn
≤ ω2

(C.38)

for any ˜c > 0. Recall that ¯c0(δ0) ≡ c0 inf τ∈T0

E[(X T δ0)2|Q = τ ]. As in Step 5 of the proof of

72

Theorem 5.2, there is a constant C0 > 0 such that

w.p.a.1. Combining (C.37)-(C.39) with a suﬃciently small ˜c yields

(cid:13)(cid:13)X((cid:98)τ )T ((cid:98)α − α0)(cid:13)(cid:13)2

R ((cid:98)α,(cid:98)τ ) + ωn

2 ≤ C0R((cid:98)α,(cid:98)τ ) + C0¯c0(δ0)|(cid:98)τ − τ0|,
(cid:12)(cid:12)(cid:12)(cid:98)D ((cid:98)α − α0)
(cid:12)(cid:12)(cid:12)1

ns + |(cid:98)τ − τ0|(cid:1)

≤ C(cid:0)ω2

(C.39)

(C.40)

for some ﬁnite constant C > 0. Since |(cid:98)τ − τ0| = OP (n−1) by Theorem 5.3, the desired results

follow (C.40) immediately.

Now we consider case (i). In this case,

R ((cid:98)α,(cid:98)τ ) +

1
2

ωn

(cid:12)(cid:12)(cid:12)(cid:98)D ((cid:98)α − α0)

(cid:12)(cid:12)(cid:12)1

≤ 3|R(α0,(cid:98)τ )| .

As shown in the proof of Theorem 3.2, we have that

|R (α0,(cid:98)τ )| = OP

(cid:0)|δ0|1 n−1 log n(cid:1) = OP

ns(cid:1) .
(cid:0)ω2

(C.41)

(C.42)

Therefore, we obtain the desired results in case (i) as well by combining (C.42) with (C.41).

C.8 Proof of Theorems 5.5

We write αJ be a subvector of α whose components’ indices are in J(α0). Deﬁne ¯Qn(αJ ) ≡

(cid:101)Sn((αJ , 0)), so that

¯Qn(αJ ) =

n(cid:88)

i=1

1
n

ρ(Yi, XiJ ((cid:98)τ )T αJ ) + µn

(cid:88)

j∈J(α0)

wj(cid:98)Dj|αj|.

73

For notational simplicity, here we write (cid:98)Dj ≡ Dj((cid:98)τ ). When τ0 is identiﬁable, our argument

is conditional on

(cid:98)τ ∈ Tn =(cid:8)|τ − τ0| ≤ n−1 log n(cid:9) ,

whose probability goes to 1 due to Theorem 5.3.

We ﬁrst prove the following two lemmas. Deﬁne

¯αJ ≡ argmin

αJ

¯Qn(αJ ).

(C.43)

(C.44)

Lemma C.4. Suppose that M 2

o(n) and (cid:98)τ ∈ Tn if δ0 (cid:54)= 0; suppose that s4 log s = o(n) and (cid:98)τ is any value in T if δ0 = 0.

n(log n)2/(s log s) = o(n), s4 log s = o(n), s2 log n/ log s =

Then

(cid:32)(cid:114)

|¯αJ − α0J|2 = OP

(cid:113) s log s

(cid:33)

.

s log s

n

Proof of Lemma C.4. Let kn =
with probability at least 1 − ,

n . We ﬁrst prove that for any  > 0, there is C > 0,

inf

|αJ−α0J|2=Ckn

¯Qn(αJ ) > ¯Qn(α0J )

(C.45)

Once this is proved, then by the continuity of ¯Qn, there is a local minimizer of ¯Qn(αJ ) inside
B(α0J , Ckn) ≡ {αJ ∈ Rs : |α0J − αJ|2 ≤ Ckn}. Due to the convexity of ¯Qn, such a local

minimizer is also global. We now prove (C.45).

Write

n(cid:88)

i=1

1
n

lJ (αJ ) =

ρ(Yi, XiJ ((cid:98)τ )T αJ ), LJ (αJ , τ ) = E[ρ(Y, XJ (τ )T αJ )].

74

Then for all |αJ − α0J|2 = Ckn,

¯Qn(αJ ) − ¯Qn(α0J )
= lJ (αJ ) − lJ (α0J ) +

(cid:88)
≥ LJ (αJ ,(cid:98)τ ) − LJ (α0J ,(cid:98)τ )
(cid:125)
(cid:124)

(cid:123)(cid:122)

j∈J(α0)
−

(1)

wjµn(cid:98)Dj(|αj| − |α0j|)
|νn(αJ ,(cid:98)τ ) − νn(α0J ,(cid:98)τ )|
(cid:124)
(cid:123)(cid:122)
(cid:125)

|αJ−α0J|2≤Cδkn

sup

(2)

+

(cid:88)

(cid:124)

j∈J(α0)

µn(cid:98)Djwj(|αj| − |α0j|)
(cid:125)

(cid:123)(cid:122)

(3)

.

To analyze (1), note that |αJ − α0J|2 = Ckn and mJ (τ0, α0) = 0 and when δ0 = 0,

mJ (τ, α0J ) is free of τ . Then there is c3 > 0,

LJ (αJ ,(cid:98)τ ) − LJ (α0J ,(cid:98)τ )
≥ mJ (τ0, α0J )T (αJ − α0J ) + (αJ − α0J )T ∂2E[ρ(Y, XJ ((cid:98)τ )T α0J )]
−|mJ (τ0, α0J ) − mJ ((cid:98)τ , α0J )|2|αJ − α0J|2 − c3|α0J − αJ|3
(cid:18) ∂2E[ρ(Y, XJ ((cid:98)τ )T α0J )]
−(|mJ (τ0, α0J ) − mJ ((cid:98)τ , α0J )|2)|αJ − α0J|2 − c3s3/2|α0J − αJ|3
n − (|mJ (τ0, α0J ) − mJ ((cid:98)τ , α0J )|2)Ckn − c3s3/2C 3

≥ c1C 2
≥ Ckn(c1Ckn − Mnn−1 log n − c3s3/2C 2

|αJ − α0J|2

n) ≥ c1C 2
δ k2

≥ λmin

∂αJ ∂αT
J

∂αJ ∂αT
J

(cid:19)

n/3,

δ k3
n

 k2

 k2

2

1

2

(αJ − α0J )

where the last inequality follows from Mnn−1 log n < 1/3c1Ckn and c3s3/2C 2
These follow from the conditions M 2

n(log n)2/(s log s) = o(n) and s4 log s = o(n).

 k2

n < 1/3c1Ckn.

To analyze (2), by the symmetrization theorem and the contraction theorem (see, for ex-

ample, Theorems 14.3 and 14.4 of B¨uhlmann and van de Geer (2011)), there is a Rademacher
sequence 1, ..., n independent of {Yi, Xi, Qi}i≤n such that (note that when δ0 = 0, αJ = βJ ,

νn (αJ , τ ) ≡ 1
n

(cid:1) − Eρ(cid:0)Y, X T

(cid:1)(cid:3) ,

J(β0)iβJ

J(β0)βJ

n(cid:88)

(cid:2)ρ(cid:0)Yi, X T

i=1

75

Vn = E

sup
τ∈Tn

sup

|αJ−α0J|2≤Ckn

|νn(αJ , τ ) − νn(α0J , τ )|

(cid:33)

which is free of τ )

(cid:32)
(cid:32)
(cid:32)

≤ 2E

≤ 4LE

sup
τ∈Tn

sup

|αJ−α0J|2≤Ckn

sup
τ∈Tn

sup

|αJ−α0J|2≤Ckn

i[ρ(Yi, XiJ (τ )T αJ ) − ρ(Yi, XiJ (τ )T α0J )]

i(XiJ (τ )T (αJ − α0J ))

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

,

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

which is bounded by the sum of the following two terms, V1n + V2n, due to the triangle
√
inequality and the fact that |αJ − α0J|1 ≤ |αJ − α0J|2
s: when δ0 (cid:54)= 0 and τ0 is identiﬁable,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)

i=1

i=1

n

n

s

n

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)

i=1

i=1

n

n

log n
n2 ,

(cid:32)
(cid:32)

V1n = 4LE

≤ 4LE

sup
τ∈Tn

sup

|αJ−α0J|1≤Ckn

√

(cid:32)

√

s

|δJ−δ0J|1≤Ckn

sup

sup
τ∈Tn
√
sE
√
sC1 |J(δ0)|0

sup
τ∈Tn

max
j∈J(δ0)

(cid:114)

≤ 4LCkn

≤ 4LCkn

i(XiJ (τ ) − XiJ (τ0))T (αJ − α0J )

iX T

iJ(δ0)(1{Qi > τ} − 1{Qi > τ0})(δJ − δ0J )

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

iXij(1{Qi > τ} − 1{Qi > τ0})

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

due to the maximal inequality (for VC class indexed by τ and j); when δ0 = 0, V1n ≡ 0.

(cid:32)

V2n = 4LE

sup

(cid:32)

|αJ−α0J|1≤Ckn
√

√

max
j∈J(α0)

≤ 4LCkn

sE

iXiJ (τ0)T (αJ − α0J )

iXij(τ0)

≤ 4LCC2k2
n,

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)

i=1

i=1

n

n

s

due to the Bernstein’s moment inequality (Lemma 14.12 of B¨uhlmann and van de Geer

(2011) for some C2 > 0. Therefore,

Vn ≤ 4LCkn

√
sC1 |J(δ0)|0

(cid:114)

log n
n2 + 4LCC2k2

n < 5LCC2k2
n,

76

where the last inequality is due to s2 log n/ log s = o(n). Therefore, conditioning on the

event (cid:98)τ ∈ Tn when δ0 (cid:54)= 0, or for (cid:98)τ ∈ T when δ0 = 0, with probability at least 1 − ,
In addition, note that P (maxj∈J(α0) |wj| = 0) = 1, so (3) = 0 with probability approach-

 5LC2Ck2
n.

(2) ≤ 1

ing one. Hence

inf

|αJ−α0J|2=Ckn

 k2
¯Qn(αJ ) − ¯Qn(α0J ) ≥ c1C 2
n
3

− 1


5LC2Ck2

n > 0.

. By the continuity of ¯Qn, there is a local minimizer
The last inequality holds for C > 15LC2
c1
of ¯Qn(αJ ) inside {αJ ∈ Rs : |α0J − αJ|2 ≤ Ckn}, which is also a global minimizer due to

the convexity.

On R2p, recall that

n(cid:88)

i=1

1
n

Rn(τ, α) =

ρ(Yi, Xi(τ )T α).

For ¯αJ = ( ¯βJ(β0), ¯δJ(δ0)) ≡ ( ¯βJ , ¯δJ ) in the previous lemma, deﬁne

¯α = ( ¯βT

J , 0T , ¯δT

J , 0T )T .

Without introducing confusions, we also write ¯α = (¯αJ , 0) for notational simplicity. This

notation indicates that ¯α has zero entries on the indices outside the oracle index set J(α0).

We prove the following lemma.

Lemma C.5. With probability approaching one, there is a random neighborhood of ¯α in R2p,

denoted by H, so that ∀α = (αJ , αJ c) ∈ H, if αJ c (cid:54)= 0, we have (cid:101)Sn(αJ , 0) < (cid:101)Qn(α).

Proof of Lemma C.5. Deﬁne an l2-ball, for rn ≡ µn/ log n,

H = {α ∈ R2p : |α − ¯α|2 < rn/(2p)}.

Then supα∈H |α − ¯α|1 = supα∈H(cid:80)

l≤2p |αl − ¯αl| < rn. Consider any τ ∈ Tn. For any α =

77

(αJ , αJ c) ∈ H, write

Rn(τ, αJ , 0) − Rn(τ, α)
= Rn(τ, αJ , 0) − ERn(τ, αJ , 0) + ERn(τ, αJ , 0) − Rn(τ, α) + ERn(τ, α) − ERn(τ, α)
≤ ERn(τ, αJ , 0) − ERn(τ, α) + |Rn(τ, αJ , 0) − ERn(τ, αJ , 0) + ERn(τ, α) − Rn(τ, α)|
≤ ERn(τ, αJ , 0) − ERn(τ, α) + |νn(αJ , 0, τ ) − νn(α, τ )|.

Note that |(αJ , 0) − ¯α|2

2 = |αJ − ¯αJ|2

2 ≤ |αJ − ¯αJ|2

2 + |αJ c − 0|2

2 = |α − ¯α|2

implies (αJ , 0) ∈ H. In addition, by deﬁnition of ¯α = (¯αJ , 0) and |¯αJ − α0J|2 = OP (
(Lemma C.4), we have |¯α − α0|1 = OP (s

n ), which also implies

(cid:113) log s

(cid:113) s log s

2. Hence α ∈ H
n )

(cid:32)

(cid:114)

s

(cid:33)

|α − α0|1 = OP

sup
α∈H

log s

n

+ rn,

where the randomness in supα∈H |α − α0|1 comes from that of H.

By the mean value theorem, there is h in the segment between α and (αJ , 0),

ERn(τ, αJ , 0) − ERn(τ, α) = Eρ(Y, XJ (τ )T αJ ) − Eρ(Y, XJ (τ )T αJ + XJ c(τ )T αJ c)

∂Eρ(Y, X(τ )T h)

αj ≡ (cid:88)
= − (cid:88)
. Hence, ERn(τ, αJ , 0) − ERn(τ, α) ≤(cid:80)

j /∈J(α0)

∂αj

j /∈J(α0)

mj(τ, h)αj

where mj(τ, h) = − ∂Eρ(Y,X(τ )T h)

j /∈J |mj(τ, h)||αj|.
Because h is on the segment between α and (αJ , 0), so h ∈ H. So for all j /∈ J(α0),

∂αj

|mj(τ, h)| ≤ sup
α∈H

|mj(τ, α)| ≤ sup
α∈H

|mj(τ, α) − mj(τ, α0)| + |mj(τ, α0) − mj(τ0, α0)|.

We now argue that we can apply Assumption B.2 (ii). Let

cn ≡ s(cid:112)(log s) /n + rn.

78

For any  > 0, there is C > 0, with probability at last 1−, supα∈H |α−α0|1 ≤ Ccn. ∀α ∈ H,
write α = (β, δ) and θ = β + δ. On the event |α − α0|1 ≤ Ccn, we have |β − β0|1 ≤ Ccn
1 maxi,j≤p E|XiXj| < r2,
and |θ − θ0|1 ≤ Ccn. Hence E[(X T (β − β0))21{Q ≤ τ0}] ≤ |β − β0|2
yielding β ∈ B(β0, r). Similarly, θ ∈ G(θ0, r). Therefore, by Assumption B.2 (ii), with
probability at least 1 − , (note that neither C, L nor cn depend on α)

max
j /∈J(α0)

sup
τ∈Tn

sup
α∈H

|mj(τ, α) − mj(τ, α0)| ≤ L sup
α∈H

|α − α0|1 ≤ L(Ccn),

|mj(τ, α0) − mj(τ0, α0)| ≤ Mnn−1 log n.

max
j≤2p

sup
τ∈Tn

In particular, when δ0 = 0, mj(τ, α0) = 0 for all τ . Therefore, when δ0 (cid:54)= 0,

sup
j /∈J(α0)

sup
τ∈Tn

|mj(τ, h)| = OP (cn + Mnn−1 log n) = oP (µn);

when δ0 = 0, supj /∈J(α0) supτ∈T |mj(τ, h)| = OP (cn) = oP (µn).

Let 1, ..., n be a Rademacher sequence independent of {Yi, Xi, Qi}i≤n. Then by the

symmetrization and contraction theorems,

(cid:19)

|νn(αJ , 0, τ ) − νn(α, τ )|

(cid:18)

E

sup
τ∈T
≤ 2E

≤ 4LE

≤ 4LE

sup
τ∈T

n

(cid:32)
(cid:32)
(cid:32)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
n(cid:88)

i=1

i=1

n

sup
τ∈T

n

sup
τ∈T

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:88)

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

i[ρ(Yi, XiJ (τ )T αJ ) − ρ(Yi, Xi(τ )T α)]

i[XiJ (τ )T αJ − Xi(τ )T α]

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)max

(cid:33) (cid:88)

j /∈J(α0)

iXi(τ )

|αj| ≤ 2ωn

|αj|,

j /∈J(α0)

where the last equality follows from (C.5).

Thus uniformly over α ∈ H, Rn(τ, αJ , 0)− Rn(τ, α) = oP (µn)(cid:80)

j /∈J(α0) |αj|. On the other

hand,

(cid:88)

wjµn(cid:98)Dj|αj| −(cid:88)

wjµn(cid:98)Dj|αj| =

(cid:88)

µnwj(cid:98)Dj|αj|.

j∈J(α0)

j /∈J(α0)

j

79

Also, w.p.a.1, wj = 1 and (cid:98)Dj ≥ D for all j /∈ J(α0). Hence with probability approaching
one, (cid:101)Qn(αJ , 0) − (cid:101)Qn(α) equals
Rn((cid:98)τ , αJ , 0) +

(cid:98)Djwjλn|αj| − Rn((cid:98)τ , α) −(cid:88)

(cid:98)Djwjωn|αj| ≤ −D

|αj| < 0.

(cid:88)

(cid:88)

j /∈J(α0)

µn
2

j∈J(α0)

j≤2p

Proof of Theorem 5.5. Conditions in Lemmas C.4 and C.5 are expressed in terms of Mn.

By Lemma B.1, we verify that in quantile regression models, Mn = Cs1/2 for some C > 0.

Then all the required conditions in Lemmas C.4 and C.5 are satisﬁed by the conditions

imposed in Theorem 5.5.

By Lemmas C.4 and C.5, w.p.a.1, for any α = (αJ , αJ c) ∈ H,

(cid:101)Sn(¯αJ , 0) = ¯Qn(¯αJ ) ≤ ¯Qn(αJ ) = (cid:101)Sn(αJ , 0) ≤ (cid:101)Sn(α).

Hence (¯αJ , 0) is a local minimizer of (cid:101)Sn, which is also a global minimizer due to the convexity.
This implies that w.p.a.1, (cid:101)α = ((cid:101)αJ ,(cid:101)αJ c) satisﬁes: (cid:101)αJ c = 0, and (cid:101)αJ = ¯αJ , so

|(cid:101)αJ − α0J|2 = OP

(cid:32)(cid:114)

(cid:33)

s log s

n

|(cid:101)αJ − α0J|1 = OP

,

(cid:32)

(cid:114)

s

log s

n

(cid:33)

.

C.9 Proof of Theorem 5.6

Recall that by Theorems 5.3 and 5.5, we have

(cid:32)(cid:114)

(cid:33)

|(cid:101)αJ − α0J|2 = OP

s log s

n

and |(cid:98)τ − τ0| = OP (n−1),

(C.46)

80

and the set of regressors with nonzero coeﬃcients is recovered w.p.a.1. Hence we can restrict

ourselves on the oracle space J(α0). In view of (C.46), deﬁne rn ≡(cid:112)n−1s log s and sn. Let

n (αJ , τ ) ≡ 1
R∗
n

ρ(cid:0)Yi, XiJ (τ )T αJ

(cid:1) ,

n(cid:88)

i=1

where αJ ∈ An ≡ {αJ : |αJ − α0J|2 ≤ Krn} ⊂ Rs and τ ∈ Tn ≡ {τ : |τ − τ0| ≤ Ksn} for
some K < ∞, where K is a generic ﬁnite constant.

The following lemma is useful to establish that α0 can be estimated as if τ0 were known.

∂α E(cid:2)ρ(cid:0)Y, X T α(cid:1)|Q = t(cid:3) exists for

∂

Lemma C.6 (Asymptotic Equivalence). Assume that

all t in a neighborhood of τ0 and all its elements are continuous and bounded. Suppose that

s3(log s)(log n) = o (n). Then

(cid:0)n−1(cid:1) .

n (αJ ,(cid:98)τ ) can be charac-

R∗

sup

αJ∈An,τ∈Tn

|{R∗

n (αJ , τ ) − R∗

n (αJ , τ0)} − {R∗

n (α0J , τ ) − R∗

n (α0J , τ0)}| = oP

This lemma implies that the asymptotic distribution of argminαJ

terized by(cid:98)α∗
consistency that the asymptotic distribution of(cid:101)αJ is equivalent to that of(cid:98)α∗

R∗
n (αJ , τ0). It then follows immediately from the variable selection

J ≡ argminαJ

J . Therefore, we

have proved the theorem.

Proof of Lemma C.6. Noting that

ρ(cid:0)Yi, X T

i β + X T

i δ1{Qi > τ}(cid:1) = ρ(cid:0)Yi, X T

i β(cid:1) 1{Qi ≤ τ} + ρ(cid:0)Yi, X T

i β + X T

i δ(cid:1) 1{Qi > τ} ,

81

we have, for τ > τ0,

Dn (α, τ )
≡ {Rn (α, τ ) − Rn (α, τ0)} − {Rn (α0, τ ) − Rn (α0, τ0)}

i β(cid:1) − ρ(cid:0)Yi, X T

(cid:1)(cid:3) 1{τ0 < Qi ≤ τ}

(cid:2)ρ(cid:0)Yi, X T
(cid:2)ρ(cid:0)Yi, X T

n(cid:88)
n(cid:88)

i=1

i=1

=

1
n
− 1
n

i β0

i δ(cid:1) − ρ(cid:0)Yi, X T

i β + X T

i β0 + X T

i δ0

(cid:1)(cid:3) 1{τ0 < Qi ≤ τ}

=: Dn1 (α, τ ) − Dn2 (α, τ ) .

To prove this lemma, we consider empirical processes

Gnj (αJ , τ ) ≡ √

n (Dnj (αJ , τ ) − EDnj (αJ , τ )) ,

(j = 1, 2),

and apply the maximal inequality in Theorem 2.14.2 of van der Vaart and Wellner (1996).

First, for Gn1 (αJ , τ ), we consider the following class of functions indexed by (βJ , τ ):

Fn ≡ {(cid:0)ρ(cid:0)Yi, X T

iJ βJ

(cid:1) − ρ(cid:0)Yi, X T

iJ β0J

(cid:1)(cid:1) 1 (τ0 < Qi ≤ τ ) : |βJ−β0J|2 ≤ Krn and |τ − τ0| ≤ Ksn}.

Note that the Lipschitz property of ρ yields that

(cid:1)(cid:12)(cid:12) 1{τ0 < Qi ≤ τ} ≤(cid:12)(cid:12)X T

iJ

(cid:12)(cid:12)2 |βJ − β0J|21{|Qi − τ0| ≤ Ksn} .

Thus, we let the envelope function be Fn(XiJ , Qi) ≡ |XiJ|2 Krn1{|Qi − τ0| ≤ Ksn} and

iJ βJ

(cid:12)(cid:12)ρ(cid:0)Yi, X T
(cid:1) − ρ(cid:0)Yi, X T
note that its L2 norm is O(cid:0)√

iJ β0J

To compute the bracketing integral

√
sn

srn

(cid:1) .
(cid:90) 1
(cid:113)

0

82

J[] (1,Fn, L2) ≡

1 + log N[] (ε(cid:107)Fn(cid:107)L2,Fn, L2)dε,

note that its 2ε bracketing number is bounded by the product of the ε bracketing num-

bers of two classes Fn1 ≡ (cid:8)ρ(cid:0)Yi, X T

iJ βJ

(cid:1) − ρ(cid:0)Yi, X T

iJ β0

(cid:1) : |βJ − β0J|2 ≤ Krn

(cid:9) and Fn2 ≡

{1 (τ0 < Qi ≤ τ ) : |τ − τ0| ≤ Ksn} by Lemma 9.25 of Kosorok (2008) since both classes are
bounded w.p.a.1 (note that w.p.a.1, |XiJ|2 Krn < C < ∞ for some constant C). That is,

N[] (2ε(cid:107)Fn(cid:107)L2,Fn, L2) ≤ N[] (ε(cid:107)Fn(cid:107)L2,Fn1, L2) N[] (ε(cid:107)Fn(cid:107)L2,Fn2, L2) .

Let Fn1(XiJ ) ≡ |XiJ|2 Krn and ln(XiJ ) ≡ |XiJ|2. Note that by Theorem 2.7.11 of van der

Vaart and Wellner (1996), the Lipschitz property of ρ implies that

N[] (2ε(cid:107)ln(cid:107)L2,Fn1, L2) ≤ N (ε,{βJ : |βJ − β0J|2 ≤ Krn},| · |2),

which in turn implies that, for some constant C,

N[] (ε(cid:107)Fn(cid:107)L2,Fn1, L2) ≤ N
≤ C

,{βJ : |βJ − β0J|2 ≤ Krn},| · |2

(cid:18)√

(cid:19)s

,

ns
ε

= C

(cid:18) ε(cid:107)Fn(cid:107)L2
(cid:19)s
(cid:18) √
2(cid:107)ln(cid:107)L2
√

s
sn

ε

(cid:19)

√
where the last inequality holds since a ε-ball contains a hypercube with side length ε/
s in
the s-dimensional Euclidean space. On the other hand, for the second class of functions Fn2
with the envelope function Fn2(Qi) ≡ 1{|Qi − τ0| ≤ Ksn}, we have that
√
√
C

N[] (ε(cid:107)Fn(cid:107)L2,Fn2, L2) ≤ C

=

=

n

,

√
C
srn

ε

εs

log s

for some constant C. Combining these results together yields that

N[] (ε(cid:107)Fn(cid:107)L2,Fn, L2) ≤ C 2√
√
log s

εs

n

ns
ε

(cid:19)s ≤ C 2ε−s−1n(s+1)/2

√
sn
ε(cid:107)Fn(cid:107)L2
(cid:18)√

83

for all suﬃciently large n. Then we have that

J[] (1,Fn, L2) ≤ C 2((cid:112)s log n +

√

s)

for all suﬃciently large n. Thus, by the maximal inequality in Theorem 2.14.2 of van der

Vaart and Wellner (1996),

n−1/2 E sup
An×Tn

|Gn1 (αJ , τ )| ≤ O

√

(cid:104)
sn((cid:112)s log n +
n−1/2√
(cid:104) s
(cid:112)log s((cid:112)s log n +
= o(cid:0)n−1(cid:1) ,

= O

n3/2

srn

√
s)

√

(cid:105)

(cid:105)

s)

where the last equality follows from the restriction that s3(log s)(log n) = o (n). Identical
arguments also apply to Gn2 (αJ , τ ).

∂α E(cid:2)ρ(cid:0)Y, X T α(cid:1)|Q = t(cid:3) exists

Turning to EDn (α, τ ) , note that by the condition that ∂

for all t in a neighborhood of τ0 and all its elements are continuous and bounded, we have
that for some mean value ˜βJ between βJ and β0J ,

(cid:1)(cid:1) 1{τ0 < Q ≤ τ}(cid:12)(cid:12)
(cid:21)

1{τ0 < Q ≤ τ}

J β0J

(cid:12)(cid:12)(cid:12)(cid:12)

(β − β0)

ρ

=

J βJ

Y, X T ˜βJ

(cid:12)(cid:12)E(cid:0)ρ(cid:0)Y, X T
(cid:1) − ρ(cid:0)Y, X T
(cid:12)(cid:12)(cid:12)(cid:12)E
(cid:20) ∂
(cid:17)|Q
(cid:16)
E(cid:104)
(cid:105)
(cid:20) s3/2
(cid:112)log s
= o(cid:0)n−1(cid:1) ,

= O (srnsn)

(cid:21)

= O

n3/2

∂β

where the last equality follows from the restriction that s3(log s) = o (n). Since the same
holds for the other term in EDn, sup|EDn (α, τ )| = o (n−1) as desired.

84

C.10 Proof of Theorem 5.7

By deﬁnition,

n(cid:88)

i=1

1
n

ρ(Yi, Xi((cid:98)τ )T(cid:101)α) + µn|W(cid:98)D(cid:101)α|1 ≤ 1

n

n(cid:88)

i=1

ρ(Yi, Xi((cid:98)τ )T α0) + µn|W(cid:98)Dα0|1.

where W = diag{w1, ..., w2p}. From this, we obtain the following inequality

R((cid:101)α,(cid:98)τ ) + µn|W(cid:98)D(cid:101)α|1 ≤ |νn(α0,(cid:98)τ ) − νn((cid:101)α,(cid:98)τ )| + R(α0,(cid:98)τ ) + µn|W(cid:98)Dα0|1.

Now applying Lemma C.1 yields, when (cid:112)log(np)/n = o(µn) (which is true under the as-
2µn|(cid:98)D(α0 −(cid:101)α)|1.
sumption that ωn (cid:28) µn), we have that w.p.a.1, |νn(α0, ˆτ ) − νn((cid:101)α,(cid:98)τ )| ≤ 1
µn|(cid:98)D(α0 −(cid:101)α)|1 + R(α0,(cid:98)τ ) + µn|W(cid:98)Dα0|1.

R((cid:101)α,(cid:98)τ ) + µn|W(cid:98)D(cid:101)α|1 ≤ 1

Hence on this event,

2

Note that maxj wj ≤ 1, so for ∆ :=(cid:101)α − α0,
R((cid:101)α,(cid:98)τ ) + µn|(W(cid:98)D∆)J c|1 ≤ 3

2

µn|(cid:98)D∆J|1 +

µn|(cid:98)D∆J c|1 + R(α0,(cid:98)τ ).

1
2

By Theorem 5.2, maxj /∈J |(cid:98)αj| = OP (ωns). Hence for any  > 0, there is C > 0, maxj /∈J |(cid:98)αj| ≤
Cωns < µn with probability at least 1 − . On the event maxj /∈J |(cid:98)αj| ≤ Cωns < µn, by

deﬁnition, wj = 1 ∀j /∈ J. Hence on this event,

1
2

R((cid:101)α,(cid:98)τ ) +
2µn|(cid:98)D∆J|1 ≤ R(α0,(cid:98)τ )

We now consider two cases: (i) 3

µn|(cid:98)D∆J|1 + R(α0,(cid:98)τ ).

µn|((cid:98)D∆)J c|1 ≤ 3
2µn|(cid:98)D∆J|1 ≤ R(α0,(cid:98)τ ); (ii) 3

2µn|(cid:98)D∆J|1 > R(α0,(cid:98)τ ).

2

case 1: 3

We have: for C = 14D−1/3, µn|∆|1 ≤ CR(α0,(cid:98)τ ). If(cid:98)τ > τ0, for τ =(cid:98)τ in the inequalities

85

below,

R(α0,(cid:98)τ ) = E(ρ(Y, X T β0) − ρ(X T θ0))1{τ0 < Q < τ} ≤ LE|X T δ0|1{τ0 < Q < τ}

E|Xj|1{τ0 < Q < τ} ≤ L|δ0|1 max
j≤p

sup

q

E(|Xj||Q = q)P (τ0 < Q < τ )

≤ L|δ0|1 max
j≤p
≤ Cs(τ − τ0).

The case for τ ≤ τ0 follows from the same argument. Hence µn|∆|1 ≤ C|(cid:98)τ − τ0|s.

case 2: 3

Then by the compatibility property,

2µn|(cid:98)D∆J|1 > R(α0,(cid:98)τ )
R((cid:101)α,(cid:98)τ ) +

1
2

µn|((cid:98)D∆)J c|1 ≤ 3µn|(cid:98)D∆J|1 ≤ 3µn

s(cid:107)X(τ0)∆(cid:107)2/(cid:112)φ.

√

¯D

The same argument as that of Step 5 in the proof of Theorem 5.2 yields

(cid:107)X(τ0)∆(cid:107)2

2 ≤ CR((cid:101)α,(cid:98)τ ) + C|(cid:98)τ − τ0|

for some generic constant C > 0. This implies, for some generic constant C > 0,

It follows that R((cid:101)α,(cid:98)τ ) ≤ C(µ2

ns + |(cid:98)τ − τ0|). Hence

R((cid:101)α,(cid:98)τ )2 ≤ µ2
ns + |(cid:98)τ − τ0|), and (cid:107)X(τ0)∆(cid:107)2
1 ≤ Cs(cid:107)X(τ0)∆(cid:107)2

nsC(R((cid:101)α,(cid:98)τ ) + |(cid:98)τ − τ0|).
2 ≤ C(µ2
ns2 + |(cid:98)τ − τ0|s).

2 ≤ C(µ2

|∆|2

Combining both cases, we reach:

|(cid:101)α − α0|2

1 ≤ C(µ2

ns2 + |(cid:98)τ − τ0|s +

|(cid:98)τ − τ0|2s2),

1
µ2
n

which gives the desired result since the ﬁrst term µ2

ns2 dominates the other two terms.

86

C.11 Proof of Theorem 6.1

If δ0 = 0, τ0 is non-identiﬁable. In this case, we decompose the excess risk in the following

way:

R (α, τ ) = E(cid:0)(cid:2)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0
+ E(cid:0)(cid:2)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:3) 1{Q ≤ τ}(cid:1)
(cid:1)(cid:3) 1{Q > τ}(cid:1) .

(C.47)

We split the proof into three steps.
Step 1: For any r > 0, we have that w.p.a.1, ˘β ∈ ˜B(β0, r, ˘τ ) and ˘θ ∈ ˜G(β0, r, ˘τ ).

Proof of Step 1. As in the proof of Step 1 in the proof of Theorem 5.2, Assumption B.3 (iv)

implies that

E(cid:2)(X T (β − β0))21{Q ≤ τ}(cid:3) ≤ R(α, τ )2

(η∗r∗)2 ∨ R(α, τ )
η∗

.

For any r > 0, note that R(˘α, ˘τ ) = oP (1) implies that the event R(˘α, ˘τ ) < r2 holds w.p.a.1.
Therefore, we have shown that ˘β ∈ ˜B(β0, r, ˘τ ). The other case can be proved similarly.

Step 2 : Suppose that δ0 = 0. Then

(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

(cid:12)(cid:12)(cid:12)1

≤ 2κn

w.p.a.1.

(C.48)

R (˘α, ˘τ ) +

1
2

κn

Proof. The proof of this step is similar to that of Step 3 in the proof of Theorem 5.2. Since

(˘α, ˘τ ) minimizes the (cid:96)1-penalized objective function in (2.2), we have that

n(cid:88)

i=1

1
n

n(cid:88)

i=1

ρ(Yi, Xi(˘τ )T ˘α) + κn| ˘D ˘α|1 ≤ 1
n

ρ(Yi, Xi(˘τ )T α0) + κn| ˘Dα0|1.

(C.49)

When δ0 = 0, ρ(Y, X(˘τ )T α0) = ρ(Y, X(τ0)T α0). Using this fact and (C.49), we obtain the

87

following inequality

R(˘α, ˘τ ) ≤ [νn(α0, ˘τ ) − νn(˘α, ˘τ )] + κn| ˘Dα0|1 − κn| ˘D ˘α|1.

(C.50)

As in Step 3 in the proof of Theorem 5.2, we apply Lemma C.1 to [νn(α0, ˘τ ) − νn(˘α, ˘τ )]

with an and bn replaced by an/2 and bn/2. Then we can rewrite the basic inequality in

(C.50) by

Now adding κn
|α0j|1 − |˘αj|1 + |(˘αj − α0j)|1 = 0 for j /∈ J, we have that w.p.a.1,

on both sides of the inequality above and using the fact that

≥ R (˘α, ˘τ ) + κn

κn

(cid:12)(cid:12)(cid:12) ˘Dα0
(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

2κn

(cid:12)(cid:12)(cid:12)1

(cid:12)(cid:12)(cid:12) ˘D ˘α

(cid:12)(cid:12)(cid:12)1

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)

(cid:12)(cid:12)(cid:12)1

− 1
2

κn

w.p.a.1.

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:12)(cid:12)(cid:12)1

.

≥ R (˘α, ˘τ ) +

1
2

κn

Therefore, we have obtained the desired result.

Step 3 : Suppose that δ0 = 0. Then

R (˘α, ˘τ ) = OP (κ2

ns) and

|˘α − α0| = OP (κns) .

Proof. By Step 2,

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

(cid:12)(cid:12)(cid:12)1

≥(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)

(cid:12)(cid:12)(cid:12)1

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J

(cid:12)(cid:12)(cid:12)1

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J c

(cid:12)(cid:12)(cid:12)1

,

+

=

(C.51)

4

which enables us to apply the compatibility condition in Assumption 4.2.

88

Recall that (cid:107)Z(cid:107)2 = (EZ 2)1/2 for a random variable Z. Note that for s = |J(α0)|0,

1
2

κn

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)
(cid:12)(cid:12)(cid:12)1
(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)J
(cid:12)(cid:12)(cid:12)1
¯D(cid:13)(cid:13)X(˘τ )T (˘α − α0)(cid:13)(cid:13)2
(cid:13)(cid:13)X(˘τ )T (˘α − α0)(cid:13)(cid:13)2

R (˘α, ˘τ ) +
≤(1) 2κn
≤(2) 2κn
4κ2
≤(3)
n
2˜cφ2 +

¯D2s

s/φ

√

˜c
2

2 ,

(C.52)

where (1) is from the basic inequality (C.48) in Step 2, (2) is by the compatibility condition
(Assumption 4.2), and (3) is from the inequality that uv ≤ v2/(2˜c) + ˜cu2/2 for any ˜c > 0.

Note that(cid:13)(cid:13)X(τ )T α − X(τ )T α0

(cid:13)(cid:13)2

2

=(1) E(cid:2)(X T (θ − β0))21{Q > τ}(cid:3) + E(cid:2)(X T (β − β0))21{Q ≤ τ}(cid:3)
≤(2) (η∗)−1E(cid:2)(cid:0)ρ(cid:0)Y, X T θ(cid:1) − ρ(cid:0)Y, X T β0
+ (η∗)−1E(cid:2)(cid:0)ρ(cid:0)Y, X T β(cid:1) − ρ(cid:0)Y, X T β0

(cid:1)(cid:1) 1{Q > τ}(cid:3)
(cid:1)(cid:1) 1{Q ≤ τ}(cid:3)

≤(3) (η∗)−1R(α, τ ),

where (1) is simply an identity, (2) from Assumption B.3 (iv) , and (3) is due to (C.47).
Hence, (C.52) with ˜c = η∗ implies that

(cid:12)(cid:12)(cid:12) ˘D (˘α − α0)

(cid:12)(cid:12)(cid:12)1

R (˘α, ˘τ ) + κn

≤ 4κ2

n

¯D2s
η∗φ2

.

(C.53)

Therefore, R (˘α, ˘τ ) = OP (κ2

ns). Also, |˘α − α0| = OP (κns) since D(˘τ ) ≥ D w.p.a.1 by

Assumption 3.1 (iv).

89

C.12 Proof of Theorem 6.2

When τ0 is not identiﬁable (δ0 = 0), (cid:98)τ obtained in the second-step estimation can be

any value in T . Note that Lemmas C.4 and C.5 are stated and proved for this case as

well. Similar to the proof of Theorem 5.5, by Lemma B.1, in quantile regression models,

Mn = Cs1/2 for some C > 0. Hence all the required conditions in Lemmas C.4 and C.5 are

satisﬁed by the conditions imposed in Theorem 6.2. Then by Lemmas C.4 and C.5, w.p.a.1,
for any α = (αJ , αJ c) ∈ H,

(cid:101)Sn(¯αJ , 0) = ¯Qn(¯αJ ) ≤ ¯Qn(αJ ) = (cid:101)Sn(αJ , 0) ≤ (cid:101)Sn(α).

Hence (¯αJ , 0) is a local minimizer of (cid:101)Sn, which is also a global minimizer due to the convexity.
This implies that w.p.a.1, (cid:101)α = ((cid:101)αJ ,(cid:101)αJ c) satisﬁes: (cid:101)αJ c = 0, and (cid:101)αJ = ¯αJ , so

(cid:32)(cid:114)

(cid:33)

s log s

n

|(cid:101)αJ − α0J|1 = OP

,

(cid:32)

(cid:114)

s

(cid:33)

.

log s

n

|(cid:101)αJ − α0J|2 = OP

References

Belloni, A. and Chernozhukov, V. (2011). (cid:96)1-penalized quantile regression in high

dimensional sparse models. Annals of Statistics 39 82–130.

Bickel, P., Ritov, Y. and Tsybakov, A. (2009). Simultaneous analysis of Lasso and

Dantzig selector. Annals of Statistics 37 1705–1732.

Bradic, J., Fan, J. and Wang, W. (2011). Penalized composite quasi-likelihood for
ultrahigh dimensional variable selection. Journal of the Royal Statistical Society: Series
B (Statistical Methodology) 73 325–349.

B¨uhlmann, P. and van de Geer, S. (2011). Statistics for high-dimensional data, methods,

theory and applications. Springer, New York.

Callot, L., Caner, M., Kock, A. B. and Riquelme, J. A. (2016). Sharp threshold
detection based on sup-norm error rates in high-dimensional models. Journal of Business
& Economic Statistics , forthcoming.

Card, D., Mas, A. and Rothstein, J. (2008). Tipping and the dynamics of segregation.

Quarterly Journal of Economics 177–218.

90

Chan, K.-S. (1993). Consistency and limiting distribution of the least squares estimator of

a threshold autoregressive model. Annals of Statistics 21 520–533.

Chan, N. H., Ing, C.-K., Li, Y. and Yau, C. Y. (2016). Threshold estimation via group
orthogonal greedy algorithm. Journal of Business & Economic Statistics , forthcoming.

Chan, N. H., Yau, C. Y. and Zhang, R.-M. (2014). Group LASSO for structural break

time series. Journal of the American Statistical Association 109 590–599.

Cho, H. and Fryzlewicz, P. (2015). Multiple-change-point detection for high dimensional
time series via sparsiﬁed binary segmentation. Journal of the Royal Statistical Society:
Series B (Statistical Methodology) 77 475–507.

Ciuperca, G. (2013). Quantile regression in high-dimension with breaking. Journal of

Statistical Theory and Applications 12 288–305.

Enikeeva, F. and Harchaoui, Z. (2013). High-dimensional change-point detection with

sparse alternatives. arXiv preprint http://arxiv.org/abs/1312.1900.

Fan, J., Fan, Y. and Barut, E. (2014). Adaptive robust variable selection. Annals of

Statistics 42 324–351.

Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its

oracle properties. Journal of the American Statistical Association 96 1348–1360.

Frick, K., Munk, A. and Sieling, H. (2014). Multiscale change point inference. Journal

of the Royal Statistical Society: Series B (Statistical Methodology) 76 495–580.

Hansen, B. E. (2000). Sample splitting and threshold estimation. Econometrica 68 575–

603.

He, X. and Shao, Q.-M. (2000). On parameters of increasing dimensions. Journal of

Multivariate Analysis 73 120–135.

Koenker, R. and Bassett, G. (1978). Regression quantiles. Econometrica 33–50.

Kosorok, M. R. (2008). Introduction to Empirical Processes and Semiparametric Infer-

ence. Springer, New York.

Kosorok, M. R. and Song, R. (2007). Inference under right censoring for transformation
models with a change-point based on a covariate threshold. Annals of Statistics 35 957–
989.

Lee, S. and Seo, M. H. (2008). Semiparametric estimation of a binary response model
with a change-point due to a covariate threshold. Journal of Econometrics 144 492–499.

Lee, S., Seo, M. H. and Shin, Y. (2011). Testing for threshold eﬀects in regression models.

Journal of the American Statistical Association 106 220–231.

91

Lee, S., Seo, M. H. and Shin, Y. (2016). The lasso for high dimensional regression with
a possible change point. Journal of the Royal Statistical Society: Series B (Statistical
Methodology) 78 193–210.

Leonardi, F. and B¨uhlmann, P. (2016). Computationally eﬃcient change point detection
for high-dimensional regression. arXiv preprint arXiv:1601.03704 http://arxiv.org/
abs/1601.03704.

Li, D. and Ling, S. (2012). On the least squares estimation of multiple-regime threshold

autoregressive models. Journal of Econometrics 167 240–253.

Loh, P.-L. and Wainwright, M. J. (2013). Regularized M -estimators with nonconvexity:
Statistical and algorithmic theory for local optima. In Advances in Neural Information
Processing Systems 26 (C. Burges, L. Bottou, M. Welling, Z. Ghahramani and K. Wein-
berger, eds.). Curran Associates, Inc., 476–484.

Lov´asz, L. and Vempala, S. (2007). The geometry of logconcave functions and sampling

algorithms. Random Structures & Algorithms 30 307–358.

Negahban, S. N., Ravikumar, P., Wainwright, M. J. and Yu, B. (2012). A uniﬁed
framework for high-dimensional analysis of M -estimators with decomposable regularizers.
Statistical Science 27 538–557.

Pons, O. (2003). Estimation in a Cox regression model with a change-point according to a

threshold in a covariate. Annals of Statistics 31 442–463.

Raskutti, G., Wainwright, M. and Yu, B. (2011). Minimax rates of estimation for
high-dimensional linear regression over (cid:96)q-balls. IEEE Transactions on Information Theory
57 6976–6994.

Seijo, E. and Sen, B. (2011a). Change-point in stochastic design regression and the

bootstrap. Annals of Statistics 39 1580–1607.

Seijo, E. and Sen, B. (2011b). A continuous mapping theorem for the smallest argmax

functional. Electronic Journal of Statistics 5 421–439.

Tong, H. (1990). Non-linear time series: a dynamical system approach. Oxford University

Press.

van de Geer, S. A. (2008). High-dimensional generalized linear models and the lasso.

Annals of Statistics 36 614–645.

van der Vaart, A. and Wellner, J. (1996). Weak convergence and empirical processes.

Springer, New York.

Wang, L. (2013). The L1 penalized LAD estimator for high dimensional linear regression.

Journal of Multivariate Analysis 120 135–151.

Wang, L., Wu, Y. and Li, R. (2012). Quantile regression for analyzing heterogeneity in

ultra-high dimension. Journal of the American Statistical Association 107 214–222.

92

Zou, H. and Li, R. (2008). One-step sparse estimations in non concave penalized likelihood

models. Annals of Statistics 36 1509–1533.

93

