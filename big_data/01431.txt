Normalization Propagation: A Parametric Technique for Removing Internal

Covariate Shift in Deep Networks

Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, Venu Govindaraju
GOVIND}@BUFFALO.EDU
SUNY Buffalo

{DEVANSHA, YINGBOZH, URALAKOTA,

6
1
0
2

 
r
a

M
9

 

 
 
]
L
M

.
t
a
t
s
[
 
 

2
v
1
3
4
1
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

While the authors of Batch Normalization (BN)
identify and address an important problem in-
volved in training deep networks– Internal Co-
variate Shift– the current solution has multi-
ple drawbacks. For instance, BN depends on
batch statistics for layerwise input normaliza-
tion during training which makes the estimates of
mean and standard deviation of input (distribu-
tion) to hidden layers inaccurate due to shifting
parameter values (specially during initial train-
ing epochs). Another fundamental problem with
BN is that it cannot be used with batch-size 1
during training. We address these (and other)
drawbacks of BN by proposing a non-adaptive
normalization technique for removing covariate
shift, that we call Normalization Propagation.
Our approach does not depend on batch statis-
tics, but rather uses a data-independent para-
metric estimate of mean and standard-deviation
in every layer thus being faster compared with
BN. We exploit the observation that the pre-
activation before Rectiﬁed Linear Units follow
a Gaussian distribution in deep networks, and
that once the ﬁrst and second order statistics of
any given dataset are normalized, we can for-
ward propagate this normalization without the
need for recalculating the approximate statistics
(using data) for any of the hidden layers.

1. Introduction and Motivation
Ioffe & Szegedy (2015) identiﬁed an important problem in-
volved in training deep networks, viz., Internal Covariate
Shift.
It refers to the problem of shifting distribution of
the input of every hidden layer in a deep neural network.

This idea is borrowed from the concept of covariate shift
(Shimodaira, 2000), where this problem is faced by a sin-
gle input-output learning system. Consider the last layer of
a deep network being used for classiﬁcation; this layer es-
sentially tries to learn P (Y |X), where Y is the class label
random variable (r.v.) and X is the layer input r.v. How-
ever, learning a ﬁxed P (Y |X) becomes a problem if P (X)
changes continuously. As a result, this slows down training
convergence.
Batch Normalization (BN) addresses this problem by nor-
malizing the distribution of every hidden layers’ input. In
order to do so, it approximates the pre-activation mean and
standard deviation (std) using mini-batch statistics at each
iteration of training and uses these estimates to normalize
the input to the next layer. While this approach leads to
a signiﬁcant performance jump by addressing internal co-
variate shift, it’s estimates of mean and std of hidden layer
input rely on mini-batch statistics which makes these es-
timates an approximation (specially during initial training
iterations). This is because the mini-batch statistics of input
to hidden layers depends on the output from previous lay-
ers, which in turn depend on the previous layer parameters
that keep shifting during training. Validation and testing
get affected for the same reason since they use a running
average estimate of these statistics. Finally, due to involve-
ment of batch statistics, BN is inapplicable with batch-size
1.
In this paper, we propose a parametric normalization tech-
nique for addressing internal covariate shift that does not
depend on batch statistics for normalizing the input to hid-
den layers and is less severely affected by the problem of
shifting parameters during training. In fact, we show that
it is unnecessary to explicitly calculate mean and standard-
deviation from mini-batches for normalizing the input to
hidden layers. Instead, a data independent estimate of these
normalization components are available in closed form for
every hidden layer, assuming the pre-activation values fol-
low Gaussian distribution and that the weight matrix of hid-
den layers are roughly incoherent. We show how to forward
propagate the normalization property (of the data distribu-

Normalization Propagation

tion) to all hidden layers by exploiting the knowledge of
the distribution of the pre-activation values (Gaussian) and
some algebraic manipulations. Hence we call our approach
Normalization Propagation.

2. Background
It has long been known in Deep Learning community that
input whitening and decorrelation helps in speeding up the
training process (LeCun et al., 2012). In fact, it is explic-
itly mentioned in (LeCun et al., 2012) that this whitening
should be performed before every layer so that the input
to the next layer has zero mean. From the perspective of
Internal Covariate Shift, what is required for the network
to learn a ﬁxed hypothesis P (Y |X) at any given layer
at every point in time during training, is for the distribu-
tion P (X) of the input to that layer to be ﬁxed. While
whitening could be used for achieving this task at every
layer, it would be a very expensive choice (cubic order of
input size) since whitening dictates computing the Singu-
lar Value Decomposition (SVD) of the input data matrix.
However, Desjardins et al. (2015) suggest to overcome this
problem by approximating this SVD by: a) using train-
ing mini-batch data to compute this SVD; b) computing
it every few number of iteration and relying on the assump-
tion that this SVD approximately holds for the iterations
in between. In addition, each hidden layer’s input is then
whitened by re-parametrizing a subset of network parame-
ters that are involved in gradient descent. As mentioned in
Ioffe & Szegedy (2015), this re-parametrizing may lead to
effectively cancelling/attenuating the effect of the gradient
update step since these two operations are done indepen-
dently.
Batch Normalization (Ioffe & Szegedy, 2015) addresses
both the above problems. First, they propose a strategy for
normalizing the data at hidden layers such that the gradient
update step accounts for this normalization. Secondly, this
normalization is performed for units of each hidden layer
independently (thus avoiding whitening) using mini-batch
statistics. Speciﬁcally, this is achieved by normalizing the
pre-activation u = WT x of all hidden layers as,

ui − EB[ui]

(cid:112)varB(ui)

ˆui =

(1)

3. Normalization Propagation (NormProp)

Derivation

We will now describe the idea behind NormProp. At a
glance the problem at hand seems cyclic because estimat-
ing the mean and standard deviation of the input distribu-
tion to any hidden layer requires the input distribution of
it’s previous layer (and hence it’s parameters) to be ﬁxed to
the optimal value before hand. However, as we will now
show, we can side-step this naive approach and get an ap-
proximation of the true unbiased estimate using the knowl-
edge that the pre-activation to every hidden layer follows
a Gaussian distribution and some algebraic manipulation
over the properties of the weight matrix. For the derivation
below, we will focus on networks with ReLU activation,
and later discuss how to extend our algorithm to other acti-
vation functions.

3.1. Data Normalization

Real world data generally follows Gaussian like distribu-
tion. Therefore, consider a data distribution X in Rn such
that all the samples are normalized, i.e.,
Ex∈X [x] = 0
j ] = 1 ∀ j ∈ {1, . . . , n}

Ex∈X [x2

(2)

Then our goal is to ﬁnd a way to propagate this normaliza-
tion to all the hidden layers without the need of explicit data
dependent normalization. Depending on whether this input
is passed through a convolutional layer or a fully connected
layer, a part of the input or the entire input gets multiplied
to a weight matrix. Irrespective of the case, lets use x to de-
note this input for ease of notation; which can thus be the
entire data vector or a subset of it’s element depending on
the case. The pre-activation is given by u (cid:44) Wx, where
W ∈ Rm×n and m is the number of ﬁlters (we will ignore
bias for now). As also claimed by Ioffe & Szegedy (2015);
Hyv¨arinen & Oja (2000), we assume the pre-activation (u)
has a Gaussian form. So if we ensure that the pre-activation
of the ﬁrst hidden layer is a normalized Gaussian, then the
hidden layer’s output (ReLU(u)) will be a Rectiﬁed Gaus-
sian distribution. As mentioned in section 2, we can choose
to directly normalize the post-activation output ReLU(u).
However, as we will now show, it is easier to ﬁnd closed
form estimates for normalizing u instead.

where ui denotes the ith element of u and the expecta-
tion/variance is calculated over the training mini-batch B.
Notice since W is a part of this normalization, it becomes
a part of the gradient descent step as well. However, a
problem common to both the above approaches is that of
shifting network parameters and mini-batch statistics upon
which their approximation of input normalization for hid-
den layers depends.

3.2. Mean and Standard-deviation Normalization for

First Hidden Layer

Clearly, since the input data x is mean subtracted, the pre-
activation to the ﬁrst hidden layer u also has zero mean
from linearity,i.e., Ex∈X [u] = 0. Now we want to ensure
the variance of u is 1; this is the tricky part. Let the co-
variance matrix of u be denoted by Σ. Then the following

Normalization Propagation

proposition bounds how far Σ is from a canonical distribu-
tion.
Proposition 1. (Canonical Error Bound) Let u = Wx
where x ∈ Rn and W ∈ Rm×n such that Ex[x] = 0 and
Ex[xxT ] = σ2I (I is the identity matrix) . Then the co-
variance matrix of u is approximately canonical satisfying,

(cid:32) m(cid:88)

i=1

σ2

α

min

(cid:0)(cid:107)Wi(cid:107)2

2

(cid:107)Σ − diag (α)(cid:107)F ≤

(cid:0)1 − (cid:107)Wi(cid:107)2
(cid:1)(cid:1)
1/2

2(cid:107)Wj(cid:107)2

2

2

m(m − 1)µ2(cid:107)Wi(cid:107)2

(3)

m(cid:88)

i,j=1;i(cid:54)=j

+

where Σ = Eu[(u−Eu[u])(u−Eu[u])T ] is the covariance
matrix of u, µ is the coherence of the rows of W, α ∈
Rm is the closest approximation of the covariance matrix
to a canonical ellipsoid and diag(.) diagonalizes a vector
to a diagonal matrix. The corresponding optimal α∗
i =
σ2(cid:107)W(cid:107)2

2 ∀i ∈ {1, . . . , m}.

2

The above proposition tells us two things. First, the co-
variance matrix of the pre-activation Σ is approximately
canonical (diagonal covariance matrix) with a bounded er-
ror and that this error can be controlled by certain proper-
ties of the weight matrix W. Second, if we want to nor-
malize each element of the vector u to have unit standard-
deviation, then our best bet is to divide each ui by the cor-
responding weight length (cid:107)Wi(cid:107)2. This is because the clos-
i = (cid:107)Wi(cid:107)2
est estimate of a diagonal variance for Σ is α∗
(σ = 1 in our case).
Notice the error bound has two distinct terms, while the
second term depends on both length and coherence be-
tween weight vectors, the ﬁrst term depends only on the
weight lengths. For any dictionary (weight matrix W),
while the second term in the above error bound only takes
non-negative values, the ﬁrst term can take both negative
as well as positive values depending on the weight length.
However, due to complex interaction between these two
terms during training, it is hard to choose a ﬁxed value that
optimally minimizes the error bound. In our approach, we
need to normalize each element of the vector u to have unit
standard-deviation which is achieved by dividing each ui
by (cid:107)Wi(cid:107)2. Notice this automatically makes each hidden
weight vector to effectively have unit (cid:96)2 length. As a re-
sult, the ﬁrst term in the above error bound automatically
becomes zero and the error bound only depends on the co-
herence of W. On the other hand, it is generally observed
that useful ﬁlters that constitute a good representation of
real world data are roughly incoherent (Wright et al., 2010;
Makhzani & Frey, 2013); thus ensuring the second term is
also small thereby minimizing the overall error bound.

(cid:0)1 − 1

(cid:1)

At this point, we have normalized the pre-activation u
to have zero mean and unit variance (divide each pre-
activation element by corresponding (cid:107)Wi(cid:107)2). As a result,
the output of the ﬁrst hidden layer (ReLU(u)) is Rectiﬁed
Gaussian distribution. Notice that the above bound ensures
the dimensions of u and hence (ReLU(u)) are roughly un-
correlated. Thus, if we subtract the distribution mean from
ReLU(u) and divide by it’s standard deviation, we will
have reduced the dynamics of the second layer to be iden-
tical to that of the ﬁrst layer. The mean and standard devi-
ation of the aforementioned Rectiﬁed Gaussian is,
Remark 1. (Post-ReLU distribution) Let X ∼ N (0, 1)
and Y = max(0, X). Then E[Y ] = 1√
and var(Y ) =

π

1
2
Hence in order to normalize the post-activation ReLU(u)
to have zero mean and unit standard, the above calculated
values can be used. Finally, in the case of Pooling (in Conv-
Nets), we essentially take a block of post-activated units
and take average or maximum of these values. If we con-
sider each such unit to be independent then the distribution
after pooling, will have a different mean and standard de-
viation. However, in reality, each of these units are highly
correlated since they involve computation over either over-
lapping or spatially close patches. Therefore, we found that
the distribution statistics do not get affected signiﬁcantly
and hence we do not recompute mean and standard devia-
tion post-pooling.

2π

3.3. Propagation to Higher Layers

With the above two operations, the dynamics of the sec-
ond hidden layer become identical to that of the ﬁrst hid-
den layer. By induction, repeating these two operations for
every layer, viz.–1) divide every hidden layer’s pre-ReLU-
activation by it’s corresponding (cid:107)Wi(cid:107)2, where W is the
corresponding layer’s weight matrix, 2) subtract and divide

(cid:1) (respectively) from every hidden

(cid:112)1/2π and

(cid:0)1 − 1

(cid:113) 1

layer’s post-ReLU-activation– we ensure that the input to
every layer is a canonical distribution. While training, all
these normalization operations are back-propagated.

2

π

3.4. Effect of NormProp on Jacobian of Hidden Layers

It has been discussed in Saxe et al. (2013); Ioffe & Szegedy
(2015) that Jacobian of hidden layers with singular values
close to one improves training speed in deep networks. Al-
though the authors of BN suggest that their approach in-
tuitively achieves this condition, we will now show more
rigorously that NormProp (approximately) indeed achieves
this condition.
Let l ∈ Rm be a vector such that the ith element of l is
given by li = 1/(cid:107)Wi(cid:107)2. The output of a hidden unit using
ReLU((Wx) (cid:12) l) − c2/c1
NormProp is given by o = 1
c1

Normalization Propagation

(cid:113) 1

(cid:0)1 − 1

(cid:1) and

2π

where W ∈ Rm×n, x ∈ Rn, c1 =
c2 = 1√
. Let ˜W be such that the ith row of ˜W
equals Wi/(cid:107)Wi(cid:107)2. Thus the output can be rewritten as
ReLU( ˜Wx) − c2/c1. Let J denote the Jacobian of
o = 1
c1
this output with respect to the previous layer input x. Then
the ith row of J is given by

π

2

4. NormProp: Implementation Details
We have all the ingredients required to ﬁlter out the steps
for Normalization Propagation for training any deep neu-
ral network with ReLU activation though out it’s hidden
layers. Like BN, NormProp can be used alongside any
optimization algorithm (eg. Stochastic Gradient Descent
with/without momentum) for training deep networks.

4.1. Normalize Data

Since the core idea of NormProp is to propagate the data
normalization through hidden layers, we offer two alterna-
tive choices either one of which can be used for normaliz-
ing the input to a NormProp network. As we will describe,
both options are justiﬁed in their respective scenario.
1. Global Data Normalization: In cases when the entire
dataset – approximately representing the true data distribu-
tion – is available at hand, we compute the global mean
and standard deviation for each feature element. Then
the ﬁrst step for NormProp is to subtract element-wise
mean calculated over the entire dataset from each sample.
Similarly divide each feature element by the element-wise
standard-deviation. Ideally it is required by NormProp that
all input dimensions be statistically uncorrelated, a prop-
erty achieved by whitening for instance, but we suggest
element-wise normalization as an approximation since it is
computationally cheaper.Notice this precludes the dilemma
of what range the input should be scaled to before passing
through the network.
2. Batch Data Normalization: In many real world scenario,
streaming data is available and thus it is not possible to
compute an unbiased estimate of global mean and standard
deviation at any given point in time. In such cases, we pro-
pose to instead batch-normalize every mini-batch training
data fed to the network. Again, we perform the normal-
ization of each feature element independently for compu-
tational purposes. Notice this normalization is only per-
formed at the data level, all hidden layers are still normal-
ized by the NormProp strategy which is both more accu-
rate and computationally cheaper compared to the Batch
Normalization strategy (Ioffe & Szegedy, 2015) because
NormProp is not affected by shifting model parameters
as compared to BN. Moreover, Batch Data Normalization
also serves as a regularization since each data sample gets
a different representation each time depending on the mini-
batch it comes with. Thus by using the Batch Data Normal-
ization strategy we actually beneﬁt from the regularization
aspect of BN but also overcome it’s drawbacks by com-
puting the hidden layer mean and standard-deviation more
accurately. Notice this strategy is most effective when the
incoming data is well shufﬂed.

∂ReLU( ˜Wix)

∂ ˜Wix

Ji (cid:44) 1
c1

∂ ˜Wix
∂ReLU( ˜Wix)

∂x

˜Wi

∂ ˜Wix

(4)

=

1
c1

where ˜Wi denotes the ith row of ˜W. Let 1x ∈ Rn be an
indicator vector such that the ith element of 1x is given by

1xi

(cid:44) ∂ReLU( ˜Wix)

∂ ˜Wix

= 1( ˜Wix > 0)

(5)

where 1(.) is the indicator operator. Let M1x ∈ Rm×n be
a matrix such that every column of M1x is occupied by the
vector 1x. Then the entire Jacobian matrix can be written
(M1x (cid:12) ˜W). In order to analyze the singular
as J = 1
c1
values of J, we want to calculate JJT . From proposition 1,
the covariance matrix Σ of the pre-activation ˜Wx is given
by Σ = σ ˜W ˜WT , where σ = 1. Since the length of each
˜Wi is 1, Σ, and (therefore) ˜W ˜WT is approximately an
identity matrix if the rows of ˜W are incoherent. Thus,

JJT =

≈ 1
c2
1

(M1x (cid:12) ˜W)( ˜WT (cid:12) MT
1x )

1
c2
1
diag(1x (cid:12) 1x) =

1
c2
1

diag(1x)

(6)

Finally taking an expectation of JJT over the distribution
of x which is Normal, we get,

Ex[JJT ] ≈ 1
c2
1

(cid:90)

=

1
c2
1

Ex[diag(1x)]

(7)

diag(1x)p(x)d(x)

where p(.) denotes the density of x– Normal distribution.
From the deﬁnition of 1x, it is straight forward to see that
the result of the integral is a matrix with it’s diagonal equal
to a vector of 0.5, hence, Ex[JJT ] ≈ 0.5
I ≈ 1.47I, where
I is the identity matrix. Thus the singular values of the

Jacobian are (cid:112)(1.47) = 1.21 which, being close to 1,

c2
1

approximately achieves dynamical isometry (Saxe et al.,
2013) and should thus prevent the problem of exploding
or diminishing gradients while training deep networks sug-
gesting faster convergence. In the next section, we will use
this value during the practical implementation of Norm-
Prop for improving the Jacobian to be approximately 1.

Normalization Propagation

4.2. Initialize Network Parameters

We use Normalized Initialization (Glorot & Bengio, 2010)
for setting the initial values of all the weight matrices, both
fully connected and convolutional.

4.3. Propagate Normalization

Similar to BN, we also make use of gradient-based-
learnable scaling and bias parameters γ and β during im-
plementation. We will now describe our normalization in
detail for both fully connected and convolutional layers.

4.3.1. FULLY CONNECTED LAYERS

Consider any fully connected layer characterized by a
weight matrix W ∈ Rm×n, bias β ∈ Rm, scaling γ ∈ Rm,
input x ∈ Rn and activation ReLU. Here m denotes the
number of ﬁlters and n denotes the input dimension. Then
without NormProp, the ith output unit oi of the hidden
layer would traditionally be:

Now in the case of NormProp, the output oi becomes,

oi = ReLU(WT

i x + βi)

(cid:34)

1(cid:113) 1
(cid:0)1 − 1

2

π

(cid:1)

oi =

ReLU

(cid:18) γi(WT

i x)
1.21(cid:107)Wi(cid:107)2

(cid:19)

+ βi

−

(8)

(cid:114) 1

(cid:35)

2π

(9)

Here we divide the pre-activation by 1.21 in order to make
the Jacobian close to one as suggested by our analysis in
section 3.4. Thus we call this number the Jacobian fac-
tor. We found dividing the pre-activation by Jacobian fac-
tor helps signiﬁcantly while training with larger learning
rates without diverging.

4.3.2. CONVOLUTIONAL LAYERS

Consider any convolutional layer characterized by a ﬁlter
matrix W ∈ Rm×d×h×w, bias β ∈ Rm, scaling γ ∈ Rm,
input x ∈ Rd×L×B and activation ReLU along with any
arbitrary choice of stride-size. Here, m denotes the number
of ﬁlters, d–depth, h–height, w– width for input/ﬁlters and
L,B– height and width of image. Then without NormProp,
the ith output feature map oi of the hidden layer using the
ith ﬁlter Wi ∈ Rd×h×w would traditionally be:

oi = ReLU(Wi∗x + βi)

(10)
where ∗ denotes the convolution operation. Now in the case
of NormProp, the output feature map oi becomes,

(cid:34)

1(cid:113) 1
(cid:0)1 − 1

2

π

(cid:1)

oi =

ReLU

(cid:18) γi(Wi∗x)

1.21(cid:107)Wi(cid:107)F

(cid:19)

+ βi

−

(cid:114) 1

(cid:35)

2π

Figure 1. Effect of Global vs. Batch Data Normalization on
NormProp. Performance of NormProp is unaffected by the choice
of data normalization strategy.

(11)

where each element of γi is multiplied to all outputs from
the same corresponding ﬁlter and similarly all the scalars
as well as the bias vector are broadcasted to all the dimen-
sions. Pooling is done after this normalization process the
same way as done traditionally.

4.4. Training

The network is trained using Back Propagation. While do-
ing so, all the normalizations also get back-propagated at
every layer.
Optimization: We use Stochastic Gradient Descent with
momentum (set to 0.9) for training. Data shufﬂing also
leads to performance improvement (this however, is true in
general while training deep networks).
Learning Rate: We found learning speeds up by reducing
the learning rate by half whenever the training error starts
saturating. Also, we found larger initial learning rate for
larger batch size improves performance.
Constraints: After every training iteration, we scale each
hidden weight-vector/ﬁlter-map to have unit (cid:96)2 length, i.e.,
we use (cid:96)2 constraint on all hidden weights, both convo-
lutional and fully connected. This is done because the
scale of weight vectors do not affect network represen-
tation, so constraining the weights reduces the parameter
search space.
Regularizations: We use weight decay along with the loss
function; we found a small coefﬁcient value of 0.0005 −
0.005 is necessary during training. We found Dropout does
not help during training; we believe this might be because
Dropout changes the distribution of output of the layer it is
applied, which affects NormProp. We believe this is also
the reason Dropout doesn’t improve performance for BN .

4.5. Validation and Testing

Validation and test procedures are identical in our case
in contrast with BN because of parametric normalization.

1020304050epochs657075808590validation accuracyGlobal (batch size 50)Batch (batch size 50)Global (batch size 100)Batch (batch size 100)Global vs. Batch Data NormalizationNormalization Propagation

Figure 2. Evolution of hidden layer input distribution mean (over validation set) of a randomly chosen unit for each hidden layer of a 9
layer convnet trained on CIFAR-10. NormProp achieves both more stable distribution for lower layers as well as overall convergence
closer to zero compared to BN. Hidden layer input distribution without normalization is incomparable due to extreme variations.
While validation/testing, each sample is ﬁrst normalized
using mean and standard deviation which are calculated de-
pending on how the train data is normalized during train-
In case we use Global Data Normalization during
ing.
training, we simply use the same global estimate of mean
and standard deviation to normalize each test/validation
sample. On the other hand, if Batch Data Normalization
is used during training, a running estimate of mean and
standard deviation is maintained during training which is
then used to normalize every test/validation sample. Fi-
nally, the input is forward propagated though the network
with learned parameters using the same strategy described
in section 4.3.

Notice the distribution mean and standard deviation de-
pends on the parameter a and thus will be involved in the
normalization process. In case of non-parameter based ac-
tivations (eg. Tanh, Sigmoid), one can either choose to an-
alytically compute the statistics (like we did for ReLU) or
compute these values empirically by simulation since the
input distribution to the activation is a ﬁxed Normal distri-
bution. Thus NormProp is a general framework which can
be extended to any activation function of choice.

4.6. Extension to other Activation Functions

Even though our paper shows how to overcome the prob-
lem of Internal Covariate Shift speciﬁcally for networks
with ReLU activation throughout, we have in essence pro-
posed a general framework for propagating normalization
done at data level to all hidden layers. All that is needed
for extending NormProp to other activation functions is to
compute the distribution mean (c2) and standard deviation
(c1) of output after the activation function of choice, simi-
lar to what is shown in remark 1. Thus the general form of
output for any given activation σ(.) becomes1 (shown for
convolution layer as an example),

(cid:20)

σ

(cid:18) γi(Wi∗x)

(cid:107)Wi(cid:107)F

oi =

1
c1

(cid:19)

(cid:21)

+ βi

− c2

5. Empirical Results and Observations
We want to verify the following: a) performance compari-
son of NormProp when using Global Data Normalization
vs. Batch Data Normalization; b) NormProp alleviates the
problem of Internal Covariate Shift more accurately com-
pared to BN; c) thus, convergence stability of NormProp
is better than BN; d) effect of batch-size on the behaviour
of NormProp, specially batch-size 1 (BN not applicable).
Finally we report classiﬁcation result on various datasets
using NormProp and BN.
Datasets: We use the following datasets,
1) CIFAR-10 (Krizhevsky, 2009)– It consists of 60, 000
32 × 32 real world color images in 10 classes split into
50, 000 train and 10, 000 test images. We use 5000 images
from train set for validation and remaining for training.
2) CIFAR-100– It has the same number of train and test
samples as CIFAR-10 but it has 100 classes. For training,
we use hyperparameters same as those for CIFAR-10.
3) SVHN (Netzer et al., 2011)– It consists of 32× 32 color
images of house numbers collected by Google Street View.
It has 73, 257 train images, 26, 032 test images and an ad-
ditional 5, 31, 131 train images. Similar to the protocol in
(Goodfellow et al., 2013), we select 400 samples per class
from the train set and 200 samples per class from the extra
set as validation and use the remaining images of the train
and extra sets for training.
Experimental Protocols (For experiments in sections
5.1 through 5.4): We use CIFAR-10 with the following
Network in Network (Lin et al., 2014) architecture2

(12)

(13)

This activation can be both parameter based or ﬁxed. For
instance, a parameter based activation is Parametric ReLU
(PReLU, He et al. (2015)) (with parameter a) given by,

(cid:26) x

PReLUa(x) =

if x > 0
ax if x ≤ 0

Then the post PReLU distribution statistics is given by,
Remark 2. Let X ∼ N (0, 1) and Y = PReLUa(X). Then
(1 + a2) − (1−a)2
E[Y ] = (1−a) 1√

and var(Y ) = 1
2

π

(cid:17)

(cid:16)

2π

1Using the appropriate Jacobian Factor allows the use of larger learning rate;

however, NormProp works without it as well.

2We use the following shorthand for a) conv layer: C(number of ﬁlters, ﬁlter

size, stride size, padding); b) pooling: P(kernel size, stride, padding, pool mode)

1020304050epochs0.50.00.51.01.52.0hidden unit input meanlayer 2layer 3layer 4layer 5layer 6layer 7layer 8layer 9NormProp1020304050epochs0.00.51.01.52.0hidden unit input meanlayer 2layer 3layer 4layer 5layer 6layer 7layer 8layer 9Batch Normalization1020304050epochs020406080100120140160hidden unit input meanlayer 2layer 3layer 4layer 5layer 6layer 7layer 8layer 9No NormalizationNormalization Propagation

In order to study the effect of normalization by NormProp
and BN on hidden layers, we train two separate networks
using each strategy and an additional network without any
normalization as our baseline. After every training epoch,
we record the mean of the input distribution (over the val-
idation set) to a single randomly chosen (but ﬁxed) unit
in each hidden layer. We use batch size 50 and an initial
learning rate of 0.05 for NormProp and BN, and 0.0001
for training the network without any normalization (larger
learning rates cause divergence). For the 9 layer convo-
lutional networks we train, the input mean to the last 8
layers against training epoch are shown in ﬁgure 2. There
are three important observations in these ﬁgures: a) Norm-
Prop achieves signiﬁcantly more stable input distribution
for lower hidden layers compared to BN, thus facilitating
the learning of good lower level representation; b) the input
distribution for all hidden layers converge after 32 epochs
for NormProp. On the other hand, the input distribution
to the second layer for BN remains un-converged even af-
ter 50 epochs; c) on an average, input distribution to all
layers converge closer to zero for NormProp (avg. 0.19)
as compared to BN (avg. 0.33). Finally the performance
of the network trained without any normalization is in-
comparable to the normalized ones due to large variations
in the hidden layer input distribution (especially the lower
layers). This experiment also serves to show the Canon-
ical Error Bound (proposition 1) is small since the input
statistics to hidden layers are roughly preserved.

5.3. Convergence Stability of NormProp vs. BN

As a result of alleviating Internal Covariate Shift more
accurately as compared to BN, NormProp is expected to
achieve a more stable convergence. We conﬁrm this intu-
ition by recording the validation accuracy while the net-
work is being trained. We use batch size 50 and initial
learning rates 0.05. The plot is shown in ﬁgure 35. Clearly
NormProp achieves a more stable convergence in general,
but specially during initial training. This is because of more
stable input distribution (specially to lower hidden layers)
achieved by NormProp as well as a more accurate estimate
of layer input distribution statistics resulting from it during
initial training.

5.4. Effect of Batch-size on NormProp

We want to see the effect of batch-size used during training
with NormProp. Since it is also possible to train with batch
size 1 (using Global Data Normalization at data layer), we
compare the validation performance of NormProp during
training for various batch sizes including 1. The plots are
shown in ﬁgure 4. The performance of NormProp is largely
unaffected by batch size although lower batch sizes seem to

5We observed identical trends on SVHN and CIFAR-100.

Figure 3. NormProp vs. BN convergence stability. Clearly
NormpProp avoids Internal Covariate shift more accurately re-
sulting in a more stable convergence (specially during initial train-
ing). Caps on markers show 95% conﬁdence interval.
C(192, 5, 1, 2) − C(160, 1, 1, 0) − P (3, 2, 1, max) −
C(96, 1, 1, 0) − C(192, 5, 1, 2) − C(192, 1, 1, 0) −
P (3, 2, 1, avg) − C(192, 1, 1, 0) − C(192, 5, 1, 0) −
C(192, 1, 1, 2) − C(10, 1, 1, 0) − P (8, 8, 0, avg).
For
any speciﬁed initial learning rate, we reduce it by half
every 10 epochs. We use Stochastic Gradient Descent
with momentum 0.9. We use test set during validation for
convergence analysis.

5.1. Global vs. Batch Data Normalization

Since we offer two alternate ways to normalize data (sec-
tion 4.1) fed to a NormProp network, we evaluate both
strategies with different batch sizes to see the difference
in performance. We use batch sizes3 50 and 100 using ini-
tial learning rates 0.05 and 0.08 respectively. The results
are shown in ﬁgure 1. The performance 4 using both strate-
gies is very similar for both batch sizes, converging in only
30 epochs. This shows the robustness and applicability of
NormProp in both streaming data as well as block data sce-
nario. However, since Batch Data Normalization strategy
is a more practical choice, we stick to this strategy for the
rest of the experiments when using NormProp.

5.2. NormProp vs. BN– Internal Covariate Shift

The fundamental goal of our paper (as well as that of Batch
Normalization Ioffe & Szegedy, 2015) is to alleviate the
problem of Internal Covariate Shift. This implies prevent-
ing the distribution of hidden layer inputs from shifting
while the network is being trained. In deep networks, the
features generated by higher layers are completely depen-
dent on the lower features since all the information in data
is propagated from lower to higher layers. Thus the prob-
lem of Internal Covariate Shift in lower hidden layers is
expected to affect the overall performance more severely
as compared to the same problem in higher layers.

3Notice this batch size has nothing to do with the data normalization strate-
gies in discussion. Different batch sizes are used only for adding more variation in
experiments.

4Even though the numbers are very close, the best accuracy of 90.35% is

achieved by Batch Data Normalization using batch size 50.

1020304050epochs6065707580859095validation accuracyNormPropBNNormProp vs. BN Convergence (batch-size 50)Normalization Propagation

Table 1. Performance comparison of NormProp (ours) with Batch
Normalization along with other State-of-the-art methods on vari-
ous datasets.

Test Error (%)

Datasets and Methods
CIFAR-10

without data augmentation

NormProp
Batch Normalization
NIN + ALP units (Agostinelli et al., 2015)
NIN (Lin et al., 2014)
DSN (Lee et al., 2015)
Maxout (Goodfellow et al., 2013)

with data augmentation

NormProp
Batch Normalization
NIN + ALP units (Agostinelli et al., 2015)
NIN (Lin et al., 2014)
DSN (Lee et al., 2015)
Maxout (Goodfellow et al., 2013)

without data augmentation

NormProp
Batch Normalization
NIN + ALP units (Agostinelli et al., 2015)
NIN (Lin et al., 2014)
DSN (Lee et al., 2015)
Maxout (Goodfellow et al., 2013)

with data augmentation

NormProp
Batch Normalization
NIN + ALP units (Agostinelli et al., 2015)
NIN (Lin et al., 2014)
DSN (Lee et al., 2015)
Maxout (Goodfellow et al., 2013)

NormProp
Batch Normalization
NIN + ALP units (Agostinelli et al., 2015)
NIN (Lin et al., 2014)
DSN (Lee et al., 2015)
Maxout (Goodfellow et al., 2013)

9.11
9.41
9.59
10.47
9.69
11.68

7.47
7.25
7.51
8.81
7.97
9.38

32.19
35.32
34.40
35.68
34.57
38.57

29.24
30.26
30.83

-
-
-

1.88
2.25

-

2.35
1.92
2.47

CIFAR-100

SVHN

Figure 4. Effect training batch-size on NormProp. NormProp
achieves slightly better performance with decreasing batch-size.
Caps on markers show 95% conﬁdence interval.
yield better performance.

5.5. Results on various Datasets

We evaluate NormProp and BN on CIFAR-10, CIFAR-100
and SVHN datasets, but also report existing state-of-the-art
(SOTA) results. For all the datasets and both methods, we
use the same architecture as mentioned in the experimen-
tal protocol above except for CIFAR-100, the last convolu-
tional layer is C(100, 1, 1, 0) instead of C(10, 1, 1, 0). For
CIFAR datasets we use batch size 50 and an initial learn-
ing rate of 0.05 and reduce it by half after every 25 epochs
and train for 200 epochs. Since SVHN is a much larger
dataset, we only train for 25 epochs with batch size 100
and an initial learning rate of 0.08 and reduce it by half
after every 5 epochs. We use Stochastic gradient descent
with momentum (0.9). For CIFAR-10 and CIFAR-100, we
train using both without data augmentation and with data
augmentation (horizontal ﬂipping only); and no data aug-
mentation for SVHN. We did not pre-process any of the
datasets. The results are shown in table 1. We ﬁnd Norm-
Prop consistently achieves either better or competitive per-
formance compared to BN, but also beats existing SOTA
results.

5.6. Training Speed

Since there is no need for estimating the running average
values of input mean and standard deviation for hidden lay-
ers for NormProp algorithm, it expected to be faster com-
pared to Batch Normalization. So we record the time taken
for NormProp and BN for 1 epoch of training on CIFAR-
10 dataset using the experimental protocol used for above
experiments. On an NVIDIA GeForce GTX Titan X GPU
with Intel i7-3930K CPU and 32GB Ram machine, Norm-
Prop takes ∼ 84 sec while BN takes ∼ 96 sec.

6. Conclusion
We have proposed a novel algorithm for addressing the
problem of Internal Covariate Shift involved during train-
ing deep neural networks that overcomes several draw-
backs of Batch Normalization (BN). Speciﬁcally, we pro-

pose a parametric approach (NormProp) that avoids esti-
mating the mean and standard deviation of hidden layers’
input distribution using input data mini-batch statistics (that
involve shifting network parameters), which make it inac-
curate (specially during initial training period when param-
eters change drastically). Instead, NormProp relies on nor-
malizing the statistics of the given dataset and condition-
ing the weight matrix which ensures normalization done for
the dataset is propagated to all hidden layers. Thus Norm-
Prop does not need to compute and maintain a moving av-
erage estimate of batch statistics of hidden layer inputs for
validation/test phase. This also enables the use of batch
size 1 for training, which overcomes another fundamental
drawback of BN. Although we have shown how to apply
NormProp in detail for networks with ReLU activation, we
have discussed (section 4.6) how to extend it for other ac-
tivations as well. We have empirically shown NormProp
achieves more stable convergence and hidden layer input
distribution during training, and better/competitive classiﬁ-
cation performance compared with BN while being faster
by omitting the need to compute a running estimate of hid-
den layer input mean/std. In conclusion, our approach is
applicable alongside any activation function and cost ob-
jectives for improving training convergence.

1020304050epochs405060708090100validation accuracybatch-size 1batch-size 50batch-size 100batch-size 250Effect of Batch-size on NormPropNormalization Propagation

Saxe, Andrew M, McClelland, James L, and Ganguli,
Surya. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks. arXiv preprint
arXiv:1312.6120, 2013.

Shimodaira, Hidetoshi. Improving predictive inference un-
der covariate shift by weighting the log-likelihood func-
tion. Journal of statistical planning and inference, 90
(2):227–244, 2000.

Wright, John, Ma, Yi, Mairal, Julien, Sapiro, Guillermo,
Huang, Thomas S, and Yan, Shuicheng. Sparse repre-
sentation for computer vision and pattern recognition.
Proceedings of the IEEE, 98(6):1031–1044, 2010.

References
Agostinelli, Forest, Hoffman, Matthew, Sadowski, Peter,
and Baldi, Pierre. Learning activation functions to im-
prove deep neural networks. In ICLR, 2015.

Desjardins, Guillaume, Simonyan, Karen, Pascanu, Raz-
In Advances in
van, et al. Natural neural networks.
Neural Information Processing Systems, pp. 2062–2070,
2015.

Glorot, Xavier and Bengio, Yoshua. Understanding the
difﬁculty of training deep feedforward neural networks.
In International conference on artiﬁcial intelligence and
statistics, pp. 249–256, 2010.

Goodfellow, Ian J., Warde-farley, David, Mirza, Mehdi,
Courville, Aaron, and Bengio, Yoshua. Maxout net-
works. In In ICML, 2013.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Delving deep into rectiﬁers: Surpassing human-
In Pro-
level performance on imagenet classiﬁcation.
ceedings of the IEEE International Conference on Com-
puter Vision, pp. 1026–1034, 2015.

Hyv¨arinen, Aapo and Oja, Erkki. Independent component
analysis: algorithms and applications. Neural networks,
13(4):411–430, 2000.

Ioffe, Sergey and Szegedy, Christian. Batch normalization:
Accelerating deep network training by reducing internal
covariate shift. In Bach, Francis R. and Blei, David M.
(eds.), ICML, volume 37 of JMLR Proceedings, pp. 448–
456. JMLR.org, 2015.

Krizhevsky, Alex. Learning Multiple Layers of Features

from Tiny Images. Technical report, 2009.

LeCun, Yann A, Bottou, L´eon, Orr, Genevieve B, and
M¨uller, Klaus-Robert. Efﬁcient backprop. In Neural net-
works: Tricks of the trade, pp. 9–48. Springer, 2012.

Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang,
Zhengyou, and Tu, Zhuowen. Deeply-supervised nets.
In ICML, 2015.

Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in

network. In ICLR, 2014.

Makhzani, Alireza and Frey, Brendan. k-sparse autoen-

coders. CoRR, abs/1312.5663, 2013.

Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,
Alessandro, Wu, Bo, and Ng, Andrew Y. Reading dig-
its in natural images with unsupervised feature learning.
In NIPS workshop on deep learning and unsupervised
feature learning, volume 2011, pp. 5. Granada, Spain,
2011.

Appendices

Normalization Propagation

(cid:118)(cid:117)(cid:117)(cid:116) m(cid:88)

i=1

m(cid:88)

i,j=1;i(cid:54)=j

A. Proofs
Proposition 1. Let u = Wx where x ∈ Rn and W ∈ Rm×n such that Ex[x] = 0 and Ex[xxT ] = σ2I (I is the identity
matrix) . Then the covariance matrix of u is approximately canonical satisfying,

(cid:107)Σ − diag (α)(cid:107)F ≤ σ2

min

α

((cid:107)Wi(cid:107)2

2 (1 − (cid:107)Wi(cid:107)2

2)) +

m(m − 1)µ2(cid:107)Wi(cid:107)2

2(cid:107)Wj(cid:107)2

2

(14)

where Σ = Eu[(u − Eu[u])(u − Eu[u])T ] is the covariance matrix of u, µ is the coherence of the rows of W, α ∈ Rm is
the closest approximation of the covariance matrix to a canonical ellipsoid and diag(.) diagonalizes a vector to a diagonal
matrix. The corresponding optimal α∗

2 ∀i ∈ {1, . . . , m}.

i = σ2(cid:107)W(cid:107)2

Proof. Notice that,

Eu[u] = WEx[x] = 0

On the other hand, the covariance of u is given by,

Σ = Eu[(u − Eu[u])(u − Eu[u])T ] = Ex[(Wx − WEx[x])(Wx − WEx[x])T ]
= Ex[W(x − Ex[x])(x − Ex[x])T WT ]
= WEx[(x − Ex[x])(x − Ex[x])T ]WT

(15)

(16)

Since x has spherical covariance, the off-diagonal elements of Ex[(x − Ex[x])(x − Ex[x])T ] are zero and the diagonal
elements are the variance of any individual unit, since all units are identical. Thus,

Eu[(u − Eu[u])(u − Eu[u])T ] = σ2WWT

Thus,

(cid:107)Σ − diag (α)(cid:107)2

F = tr(cid:0)(σ2WWT − diag (α))(σ2WWT − diag (α))T(cid:1)
= tr(cid:0)σ4WWT WWT + diag (α2) − 2σ2 diag (α)WWT(cid:1)
(cid:1)
m(cid:88)
(cid:1)

i − 2σ2αi(cid:107)Wi(cid:107)2

= σ4(cid:107)WWT(cid:107)2

m(cid:88)
m(cid:88)

m(m − 1)µ2(cid:107)Wi(cid:107)2

i − 2σ2αi(cid:107)Wi(cid:107)2

(cid:0)α2
(cid:0)α2

2(cid:107)Wj(cid:107)2

F +

2 +

i=1

2

2

i=1

(cid:0)(cid:107)Wi(cid:107)2

2

(cid:1) +

m(cid:88)

i=1

≤ σ4

α2 in the above equation denotes element-wise square of elements of α. Finally minimizing w.r.t αi ∀i ∈ {1, . . . , m},
leads to α∗

2. Substituting this into equation 18, we get,

i = σ2(cid:107)W(cid:107)2

(cid:107)Σ − diag (α)(cid:107)2

F ≤ σ4

(cid:0)(cid:107)Wi(cid:107)2

2

(cid:0)1 − (cid:107)Wi(cid:107)2

2

(cid:1)(cid:1) +

m(cid:88)

i,j=1;i(cid:54)=j

m(m − 1)µ2(cid:107)Wi(cid:107)2

i,j=1;i(cid:54)=j

 m(cid:88)

i=1



2

2(cid:107)Wj(cid:107)2
(cid:1)

(cid:0)1 − 1

π

Remark 1. Let X ∼ N (0, 1) and Y = max(0, X). Then E[Y ] = 1√

2π

and var(Y ) = 1
2

Proof. For the deﬁnition of X and Y , we have,

E[Y ] =

1
2

.0 +

1
2

E[Z] =

E[Z]

1
2

(17)

(18)

(19)

(20)

where Z is sampled from a Half-Normal distribution such that Z = |X|; thus E[Z] =
In order to compute variance, notice that E[Y 2] = 0.5E[Z 2]. Then,

Normalization Propagation

(cid:113) 2

π leading to the claimed result.

var(Y ) = E[Y 2] − E[Y ]2 = 0.5E[Z 2] − 1
4

E[Z]2 = 0.5(var(Z) + E[Z]2) − 1
4

E[Z]2

Substituting var(Z) = 1 − 2
Remark 2. Let X ∼ N (0, 1) and Y = PReLUa(X). Then E[Y ] = (1 − a) 1√

π yields the claimed result.

2π

and var(Y ) = 1
2

(21)

(cid:17)

(cid:16)

(1 + a2) − (1−a)2

π

Proof. For the deﬁnition of X and Y , half the mass of Y is concentrated on R+ with Half-Normal distribution, while the
other half of the mass is concentrated on R−sign(a)with Half-Normal distribution scaled with |a|. Thus,

E[Y ] = − a
2

E[Z] +

1
2

E[Z] = (1 − a)

E[Z]

1
2

(cid:113) 2

(22)

where Z is sampled from a Half-Normal distribution such that Z = |X|; thus E[Z] =
Similarly in order to compute variance, notice that E[Y 2] = 0.5E[(aZ)2] + 0.5E[Z 2] = 0.5E[Z 2](1 + a2). Then,

π leading to the claimed result.

var(Y ) = E[Y 2] − E[Y ]2 = 0.5E[Z 2](1 + a2) − (1 − a)2 1
4
= 0.5(1 + a2)(var(Z) + E[Z]2) − (1 − a)2 1
4

E[Z]2

E[Z]2

Substituting var(Z) = 1 − 2

π yields the claimed result.

(23)

