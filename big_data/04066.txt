6
1
0
2

 
r
a

 

M
3
1

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
6
6
0
4
0

.

3
0
6
1
:
v
i
X
r
a

Local circular law for the product of a deterministic matrix

with a random matrix

Haokai Xi ˚1, Fan Yang :1 and Jun Yin ;1

1Department of Mathematics, University of Wisconsin-Madison

March 15, 2016

Abstract

It is well known that the spectral measure of eigenvalues of a rescaled square non-Hermitian
random matrix with independent entries satisﬁes the circular law. We consider the product T X,
where T is a deterministic N ˆ M matrix and X is a random M ˆ N matrix with independent
entries having zero mean and variance pN ^ Mq´1. We prove a local general circular law for the
empirical spectral distribution (ESD) of T X at any point z away from the unit circle under the
assumptions that N „ M , the matrix entries Xij have suﬃciently high moments and EX 3
ij “ 0.
More precisely, if z satisﬁes ||z| ´ 1| ě τ for arbitrarily small τ ą 0, the ESD of T X converges
to ˜χDpzqdApzq, where ˜χD is a rotation-invariant function determined by the singular values of T
and dA denotes the Lebesgue measure on C. Furthermore, the local circular law is valid around
z up to scale pN ^ Mq´1{2` for any  ą 0.

1

Introduction

Circular law for non-Hermitian random matrices. The study of the eigenvalue spectral of non-
Hermitian random matrices goes back to the celebrated paper [19] by Ginibre, where he calculated
the joint probability density for the eigenvalues of non-Hermitian random matrix with independent
complex Gaussian entries. The joint density distribution is integrable with an explicit kernel (see
[19, 28]), which allowed him to derive the circular law for the eigenvalues. For the Gaussian random
matrix with real entries, the joint distribution of the eigenvalues is more complicated but still
integrable, which leads to the proof of the circular law as well [6, 10, 18, 35].

For the random matrix with non-Gaussian entries, there is no explicit formula for the joint
distribution of the eigenvalues. However, in many cases the eigenvalue spectrum of the non-Gaussian
random matrices behaves similarly to the Gaussian case as N Ñ 8, known as the universality
phenomena. A key step in this direction is made by Girko in [20], where he partially proved the
circular law for non-Hermitian matrices with independent entries. The crucial insight of the paper is
the Hermitization technique, which allowed Girko to translate the convergence of complex empirical

˚E-mail: haokai@math.wisc.edu.
:E-mail: fyang75@math.wisc.edu.
;E-mail: jyin@math.wisc.edu. Partially supported by NSF Career Grant DMS-1552192 and Sloan fellowship.

1

measures of a non-Hermitian matrix into the convergence of logarithmic transforms for a family of
Hermitian matrices, or, to be more precise,

“

‰

Tr logrpX ´ zq:pX ´ zqs “ log

detppX ´ zq:pX ´ zqq

,

(1.1)
with X being the random matrix and z P C. Due to the singularity of the log function at 0, the small
eigenvalues of pX ´ zq:pX ´ zq play a special role. The estimate on the smallest singular value of
X´z was not obtained in [20], but the gap was remedied later in a series of paper. Bai [1, 2] analyzed
the ESD of pX ´ zq:pX ´ zq through its Stieltjes transform and handled the logarithmic singularity
by assuming bounded density and bounded high moments for the entries of X. Lower bounds on
the smallest singular values were given by Rudelson and Vershynin [31, 32], and subsequently by
Tao and Vu [36], Pan and Zhou [30] and G˝otze and Tikhomirov [21] under weakened moments and
smoothness assumptions. The ﬁnal result was presented in [38], where the circular is proved under
the optimal L2 assumptions. These papers considered the circular law in the global regime, which
gives the convergence of ESD on subsets containing ηN eigenvalues for some small constant η ą 0.
Later in a series of papers [7, 8, 39], Bourgade, Yau and Yin proved the local version of the circular
law up to the optimal scale N´1{2` under the assumption that the distributions of the matrix
entries satisfy a uniform sub-exponential decay condition. In [37], the local universality was proved
by Tao and Vu under the assumption of ﬁrst four moments matching the moments of the Gaussian
random variable.

Figure 1: The eigenvalue distribution of the product T X of a deterministic N ˆ M matrix T with a
Gaussian random M ˆ N matrix X. The entries of X have zero mean and variance pN ^ Mq´1, and
T T : has 0.5pN ^ Mq eigenvalues as 2{17 and 0.5pN ^ Mq eigenvalues as 32{17. (a) N “ M “ 1000.
(b) N “ 1000, M “ 2000. (c) N “ 1500, M “ 750.

In this paper, we study the eigenvalue spectrum of the product T X of a deterministic N ˆ M
matrix T with a random M ˆ N matrix X. In Figure 1, we plot the eigenvalue distribution of T X
when T have two distinct singular values (except the possible trivial zero singular values). The goal
of this paper is to prove a local circular law for the ESD of T X at any point z away from the unit
circle. Following the idea in [7], the key ingredients for the proof are (a) the upper bound for the
largest singular value of T X ´ z, (b) the lower bound for the least singular value of T X ´ z, and
(c) rigidity of the singular values of T X ´ z. The upper bound for the largest singular value can be
obtained by controlling the norm of T X ´ z through a standard large deviation estimate (see e.g.
[9, 27, 33] and (2.62)). The lower bound problem for the least singular value of T X ´ z has been
discussed in e.g.
[32] and [36] (see also Lemma 2.24). Thus the bulk of this paper is devoted to
establish (c).

2

()a()b()cBasic ideas. To obtain the rigidity property of the singular values of T x ´ z, as in [7] we study
the ESD of Q “ pT X ´ zq:pT X ´ zq using Stieltjes transform. We normalize X so that its entries
have zero mean and variance pN ^ Mq´1. Then Q is an N ˆ N Hermitian matrix with eigenvalues
being typically of order 1. We denote its resolvent by Rpwq :“ pQ ´ wq´1, where w “ E ` iη is a
spectral parameter with positive imaginary part η. Then Stieltjes transform of the ESD is equal to
N´1Tr Rpwq, and we have the convergence estimate

N´1Tr Rpwq « mcpwq

(1.2)

with high probability for large N . Here mc is the Stieltjes transform of the asymptotic eigenvalue
density, and the convergence in (1.2) is referred to as the averaged law. By taking the imaginary part
of (1.2), it is easy to see that a control of the Stieltjes transform yields a control of the eigenvalue
density on a small scale of order η around E (which contains an order ηM eigenvalues). A local law is
an estimate of the form (1.2) for all η " N´1. Such local laws have been a cornerstone of the modern
random matrix theory. In [16], a local law was ﬁrst derived for Wigner matrices. Subsequently in
[7], a local law for the resolvent of pX ´ zq:pX ´ zq was established to prove the local circular law
of X up to the scale N´1{2`.
In generalizing the proof in [7] to the Q “ pT X ´ zq:pT X ´ zq case, a main diﬃculty is that
the entries of T X are not independent. We use a new comparison method invented in [24], which
roughly states that if the local laws hold for Rpwq with Gaussian X, then the local laws hold in the
case of a general X. For deﬁniteness, we assume N “ M for now, and T is a square matrix with
singular decomposition T “ U DV . For a Gaussian X ” X Gauss, we have V X GaussU d“ ¯X Gauss,
where ¯X is also an N ˆ N Gaussian random matrix. Then we have that for the determinant in (1.1),

detpT X Gauss ´ zq“ detpU DV X Gauss ´ zq d“ detpD ¯X Gauss ´ zq.

The problem is now reduced to the study of the singular values of D ¯X Gauss ´ z, which has inde-
pendent entries. Notice the entries of D ¯X Gauss are not identically distributed. This issue usually
can be handled, e.g. as in [14], where a local law was obtained for generalized Wigner matrices with
non-identically distributed entries.
To use the comparison method invented in [24], it turns out the averaged local law from (1.2)
is not suﬃcient. We has to control not only the trace of Rpwq, but also the matrix Rpwq itself
by showing that Rpwq is close to some deterministic matrix Πpwq, provided that η " N´1. This
closeness can be established in the sense of individual matrix entries Rijpwq « Πijpwq (see e.g.
[7, 17]). We call such an estimate an entrywise local law. More generally, in [4, 25] the following
closeness was established for generalized matrix entries:

xv, Rpwquy « xv, Πpwquy, η " N´1, @|v|,|u| “ 1.

(1.3)

We call the estimate in (1.3) an anisotropic local law. (If Π is a scalar matrix, (1.3) is also referred
to as an isotropic local law, in the sense that Rpwq is approximately isotropic for large N .) To use
the method in [24], we need to prove the anisotropic local law for R.

Here we outline the three steps to establish the anisotropic local law: (A) the entrywise local law
and averaged local law when T is diagonal (Theorem 2.19); (B) the anisotropic local law when T is
diagonal (Theorem 2.19); (C) the anisotropic local law and averaged local law when T is a general
(rectangle) matrix (Theorem 2.20). We basically adapt the methods of [4] and [7] in performing
Steps (A) and (B). However we present the details of the proof, since there are still many technical
diﬃculties to be handled in our setting. As remarked above, (B) implies the anisotropic local law
with a Gaussian X and a general T . Based on this fact we perform Step (C) using a self-consistent
comparison argument in [24].

3

Conventions. The fundamental large parameter is N and we assume that M is comparable to N
(see (2.1)). All quantities that are not explicitly constant may depend on N , and we usually omit
N from our notation. We use C to denote a generic large positive constant, which may depend on
ﬁxed parameters and whose value may change from one line to the next. Similarly, we use  or δ to
denote a generic small positive constant. If a constant depend on a quantity a, we use Cpaq or Ca
to indicate this dependence. We use τ ą 0 in various assumptions to denote a positive constant that
may be chosen arbitrarily small. All constants C, c and  may depend on τ ; we neither indicate nor
track this dependence in this paper.
For any (complex) matrix A, we use A: to denote its conjugate transpose, AT the transpose,
}A} the operator norm and }A}HS the Hilbert-Schmidt norm. We use the notation v “ pviqn
i“1 for
a vector in Cn, and denote its Euclidean norm by |v| ” }v}2. We usually write the n ˆ n identity
matrix Ik as 1 without causing any confusions.
For two quantities AN and BN ą 0 depending on N , we use the notation AN “ OpBNq to
mean |AN| ď CBN for some positive constant C, AN „ BN to mean C´1BN ď |AN| ď CBN for
some positive constant C, AN “ opBNq to mean |AN| ď cN BN for some positive constant cN Ñ 0
as N Ñ 8.
If AN is a matrix, we use the notations AN “ OpBNq and AN “ opBNq to mean
}AN} “ OpBNq and }AN} “ opBNq, respectively.

Acknowledgements. The third author would like to thank Terence Tao, Mark Rudelson and
Roman Vershynin for fruitful discussions and valuable suggestions.

2 The main results

In this section, we state and prove the main results of this paper, namely, the local general circular
law for the product T X of a deterministic matrix T and a random matrix X. In Section 2.1, we
deﬁne our model and list our main assumptions.
In Section 2.2, we ﬁrst deﬁne the asymptotic
eigenvalue density ρ2c of Q “ pT X ´ zq:pT X ´ zq and explain its basic structure. Then we state
the main theorems—Theorems 2.6 and 2.7—of this paper, which give a quantitative description of
the eigenvalue distribution of T X. Their proofs depend crucially on local estimates of the resolvent
of Q, which are presented in Section 2.3. In Section 2.4, we prove Theorems 2.6 and 2.7 using the
results in Section 2.3.

2.1 Deﬁnition of the model
In this paper, we want to understand the local statistics of the eigenvalues of matrix T X ´ zI, where
T is a deterministic N ˆ M matrix, X is a random M ˆ N matrix, z P C and I is the identity
operator. We assume M „ N , i.e.

(2.1)
for φ :“ M{N and a small constant τ ą 0. We assume that the entries Xiµ of X are independent
(not necessarily identically distributed) random variables satisfying

τ ď φ ď τ´1

for all 1 ď i ď M, 1 ď µ ď N . If Xiµ is a complex random variable, we assume in addition that

(2.2)

E Xiµ “ 0, E|Xiµ|2 “

1

N ^ M

EX 2

iµ “ 0 if Xiµ P C.

(2.3)

4

We assume that for all p P N, there is an N -independent constant Cp such that

E|?

N ^ M Xij|p ď Cp

(2.4)

for all 1 ď i ď M, 1 ď µ ď N . We deﬁne Σ :“ T T :, and assume the eigenvalues of Σ satisfy that

and all other eigenvalues are 0. We can normalize T by multiplying a scalar such that

τ´1 ě σ1 ě σ2 ě ¨¨¨ ě σN^M ě τ

N^Mÿ

i“1

1

N ^ M

σi “ 1.

(2.5)

(2.6)

We summarize our basic assumptions here for future reference.

Assumption 2.1. We suppose that (2.1), (2.2), (2.3), (2.4), (2.5) and (2.6) hold.

2.2 The main theorems

In this section, we state the local circular law for the random matrix T X with T and X satisfying
Assumpation 2.1. The main results are Theorems 2.6 and 2.7.

We ﬁrst deﬁne the self-consistent equations and explain some basic properties of their solutions.
The solutions give the asymptotic eigenvalue density that will appear in the main theorems. Deﬁne

N^Mÿ

i“1

ρΣ :“

1

N ^ M

δσi

(2.7)

as the empirical spectral density of Σ. Let n :“ |supp ρΣ| be the number of distinct nonzero
eigenvalues of Σ, which are denoted as

(2.8)
Let li :“ pN ^ MqρΣptsiuq be the multiplicity of si. By (2.6), li and si satisﬁes the normalization
conditions

τ´1 ě s1 ą s2 ą ¨¨¨ ą sn ě τ.
nÿ
nÿ

1

N ^ M

i“1

li “ 1,

1

N ^ M

i“1

lisi “ 1.

For each w P C` :“ tw P C : Im w ą 0u, we deﬁne the self-consistent equations of pm1, m2q as

,

1
m2
m1 “ 1
N
nÿ

“ ´w ´ wm1 ` |z|2
1 ` m1

«
nÿ
´w p1 ` sim2q ` |z|2
1 ` m1
¨˝1 ` si
»–´w

i“1

.

ﬀ´1
˛‚` |z|2

lisi

1

´wp1 ` m1q ` |z|2
1`m1

1 ` m1

ﬁﬂ´1

m1 “ 1
N

lisi

i“1

Notice that if we plug (2.10) into (2.11), we can get the self-consistent equation for m1 only,

(2.9)

(2.10)

(2.11)

.

(2.12)

The following lemma states that the solution to (2.12) in C` is unique if z is away from the unit
circle.

5

Lemma 2.2. Fix z P C such that |z| ‰ 1. For each w P C`, there exists at most one analytic
function m1 ” m1c,z,Σpwq : C` Ñ C` such that (2.12) holds and wm1c,z,Σpwq P C`. Moreover,
m1c,z,Σ,Npwq is the Stieltjes transform of a positive integrable function ρ1c with compact support in
r0,8q.

We shall abbreviate m1cpwq :“ m1c,z,Σpwq. We also deﬁne m2cpwq :“ m2c,z,Σpwq by taking
m1 “ m1cpwq in (2.10). Obviously, m2c is also an analytic function of w. Furthermore, for any
w P C` we have m2cpwq, wm2cpwq P C` by using (2.10) and m1c, wm1c P C`. Since m1cpwq is an
analytic function of w, Im m1c is a harmonic function that is uniquely determined by its boundary
value on R, that is,

(2.13)
It is easy to see that ρ1c ě 0 and supppρ1cq Ă r0,8q by the condition m1c, wm1c P C`. Similarly,
we can deﬁne

lim
ηŒ0

Im m1cpx ` iηq, x P R.

(2.14)
By (2.10), ρ2cpxq ě 0 and supp ρ2c “ supp ρ1c. We shall call ρ2c the asymptotic eigenvalue density
(of Q “ pT X ´ zq:pT X ´ zq for a reason that will be made clear during the proof). Given ρ1cpxq
for x P R, Im m1cpwq can be expressed explicitly using Stieltjes transform,

lim
ηŒ0

Im m2cpx ` iηq, x P R.

ρ1cpxq “ 1
π

ρ2cpxq “ 1
π

ż

Then Re m1c is determined up to a constant. Using (2.10) and Impwm2cq ě 0, we get

Im m1cpwq “ Im

R

«
´w p1 ` sim2cq ` |z|2
1 ` m1c

dx.

ρ1cpxq
x ´ w
ﬀ

Im

ď ´Im w.

Plugging it into (2.11) gives |m1c| ď 1{Im w Ñ 0 as Im w Ñ 8. Thus m1cpwq is indeed the Stieltjes
transform of ρ1c,

ż
ż
R
By a similar argument, m2cpwq can be expressed as

m1cpwq “

m2cpwq “

ρ1cpxq
x ´ w

ρ2cpxq
x ´ w

R

dx.

dx.

(2.15)

(2.16)

Proof of Lemma 2.2. The fact that ρ1c has compact support follows from Lemma 2.3, and ρ1c being
integrable follows from Proposition 2.15. It remains to show that for ﬁxed w P C` and |z| ‰ 1,
there exists a unique m1cpwq P C` satisfying equation (2.12). This can be proved using a ﬁxed
point argument in C`. The proof of case 1 of Lemma 3.11 can be repeated here after some minor
modiﬁcations. Notice in the proof, η :“ Im w, |z| and |z| ´ 1 are taken as constants of order 1.
Remark: The statements of Proposition 2.15 and Lemma 3.11 contain some regularity assumptions
introduced in Deﬁnition 2.4. However, the results used in proving Lemma 2.2 are obtained without
using these assumptions.

We now establish the basic structures of ρ1c and ρ2c. This can be done by studying the solution
m1cpwq to the self-consistent equation (2.12) when w P p0,8q. Here we extend the deﬁnition of m1c
continuously down to the real axis by setting
m1cpxq “ lim
ηŒ0

m1cpx ` iηq, x P R.

6

As a convention, for w P C`, we take
m :“ ?

?
wp1 ` m1q, mc :“ ?

wp1 ` m1cq.

w to be the branch with positive imaginary part. Deﬁne

Equation (2.12) then becomes

fp?

w, mq “ 0,

where

fp?

w, mq “ ´?

w ` m ` 1
N

?

lisi

nÿ

i“1

The following lemma gives the basic structure of ρ1c (recall that ρ2c “ ρ1c).
Lemma 2.3. Fix τ ď

ˇˇ ď τ´1. The support of ρ1c is a union of connected components:

ˇˇ|z|2 ´ 1

w|z|2m ` |z|4 .

mpm2 ´ |z|2q
wm3 ´ psi ` |z|2qm2 ´ ?
˜ ď

¸
re2k, e2k´1s

1ďkďL

X p0,8q,

(2.17)

(2.18)

(2.19)

supp ρ1c X p0,`8q “

where L ” Lpnq P N and C1τ´1 ě e1 ě e2 ě . . . ě e2L ě 0 for some absolute constant C1. If
|z|2 ď 1 ´ τ , we have e2L “ 0, while if 1 ` τ ď |z|2 ď 1 ` τ´1, there is a constant pτq ą 0 so that
e2L ě pτq. Moreover, for every ei ą 0, there exists a unique mcpeiq such that

Bmfp?

ei, mcpeiqq “ 0.

(2.20)

We shall call the ei’s as the edges of ρ1c. Lemma 2.3 is proved in Appendix A through an
elementary analysis of f .
For any w P p0,8q and for each i, the cubic polynomial
w|z|2m`|z|4
in (2.18) has three distinct roots aipwq ą 0, bipwq ą 0 and ´cipwq ă 0 (see Lemma A.1). Our next
assumption on ρΣ and |z| takes the form of the following regularity conditions.
Deﬁnition 2.4. (Regularity) Fix τ ď

wm3 ´psi `|z|2qm2 ´?
ˇˇ ď τ´1 and a small constant  ą 0.

ˇˇ|z|2 ´ 1

?

(i) We say that the edge ek ‰ 0, k “ 1, . . . , 2L, is regular if

t|mcpekq ´ aipekq|,|mcpekq ´ bipekq|,|mcpekq ` cipekq|u ě ,

(2.21)

min
1ďiďn

and

ek, mcpekqq ě .
In the case |z|2 ď 1 ´ τ , we always call e2L “ 0 a regular edge.
(ii) We say that the bulk components re2k, e2k´1s is regular if for any ﬁxed τ1 ą 0 there exists a
constant c “ cpτ, τ1q ą 0 such that the density of ρ1c in re2k ` τ1, e2k´1 ´ τ1s is bounded from below
by c.

(2.22)

mfp?
B2

Remark 1: The edge regularity conditions (i) has previously appeared (may be in slightly diﬀerent
forms) in several works on sample covariance matrices and Wigner matrices [3, 11, 23, 24, 26, 29].
The conditions (2.21) and (2.22) guarantees a regular square-root behavior of the spectral density
ρ1c near ek and ensures that the gap in the spectrum of ρ1c adjacent to ek does not close for large
N (see Lemma A.5). The bulk regularity condition (ii) was introduced in [24]. It imposes a lower
bound on the density of eigenvalues away from the edges. Without it, one can have points in the
interior of supp ρ1c with an arbitrarily small density and our arguments would fail.

7

´

¯

´

¯

Remark 2: The regularity conditions in Deﬁnition 2.4 are stable under perturbations of |z| and ρΣ. In
particular, ﬁx ρΣ, suppose the regularity conditions are satisﬁed at z “ z0 with τ ď ||z0|2 ´ 1| ď τ´1.
Then there exists a suﬃciently small c ą 0 such that the regularity conditions hold uniformly in
z P tz : ||z| ´ |z0|| ď cu. For a detailed discussion, see the remark at the end of Section A.3.

We will use the following notion of stochastic domintaion, which was ﬁrst introduced in [12]
and subsequently used in many works on random matrix theory, such as [4, 5, 7, 13, 14, 24]. It
simpliﬁes the presentation of the results and their proofs by systematizing statements of the form
“ξ is bounded by ζ with high probability up to a small power of N ”.

Deﬁnition 2.5 (Stochastic domintaion).

(i) Let
ξpNqpuq : N P N, u P UpNq

ξ “

be two families of nonnegative random variables, where UpNq is possibly N -dependent parameter
set. We say that ξ is stochastically dominated by ζ, uniformly in u, if for all (small)  ą 0
and (larget) D ą 0, we have

,

ζ “

ζpNqpuq : N P N, u P UpNq
”
ı
ξpNqpuq ą N ζpNqpuq

ď N´D

P

sup
uPUpNq

for large enough N ě N0p, Dq. Throughout this paper the stochastic domination will always
be uniform in all parameters that are not explicitly ﬁxed (such as matrix indices, and w and z
that take values in some compact sets). Note that N0p, Dq may depend on quantities that are
explicitly constant, such as τ and Cp in (2.1), (2.4) and (2.5).

(ii) If ξ is stochastically dominated by ζ, uniformly in u, we use the notation ξ ă ζ. Moreover, if

for some complex family ξ we have |ξ| ă ζ we also write ξ “ Oăpζq.

(iii) We extend the deﬁnition of Oăp¨q to matrices in the weak operator sense as follows. Let A be
a family of complex square random matrices and ζ a family of nonnegative random variables.
Then we use A “ Oăpζq to mean }A} ă ζ, where }A} is the operator norm of A.

In the following, we denote the eigenvalues of T X as µj, 1 ď j ď N . We are now ready to state

the main theorems, i.e. the local general circular law for T X.
Theorem 2.6 (Local circular law for T X when N “ M ). Suppose Assumption 2.1 holds and
N “ M . Assume that for any N , we have τ ď ||z0|2 ´ 1| ď τ´1 (z0 can depend on N ). Suppose π
and |z0| are such that all the edges and bulk components of ρ1c are regular in the sense of Deﬁnition
2.4. Assume in addition that the entries of X have vanishing third moments,

(2.23)
for 1 ď i ď M, 1 ď µ ď N . Let F be a smooth non-negative function which may depend on N , such
that }F}8 ď C, }F 1}8 ď N C and Fpzq “ 0 for |z| ě C, for some constant C independent of N . Let
Fz0pzq “ N 2aFpN apz ´ z0qq be the approximate delta function around z0. Then for any a P p0, 1{2s,

EX 3

iµ “ 0,

Nÿ

j“1

1
N

where

ż

ż 8

0

8

Fz0pµjq ´ 1
π

Fz0pzq ˜χDpzqdApzq ă N´1`2a}∆F}L1 ,

with ρ2c ” ρ2c,z,Σ deﬁned in (2.14).

˜χDpzq :“ 1
4

plog xq∆zρ2cpx, zqdx

(2.24)

(2.25)

ż

Kÿ

j“1

Theorem 2.7 (Local circular laws for T X when N ‰ M ). Suppose that N ‰ M and the assumptions
of Theorem 2.6 hold. Assume in addition that the entries of X have a density bounded by N C for
some C ą 0. Let Fz0pzq “ K 2aFpK apz ´ z0qq for K :“ N ^ M . Then T X has pN ´ Kq trivial zero
eigenvalues, and for the other eigenvalues µj, 1 ď j ď K,

1
K

Fz0pµjq ´ 1
π

Fz0pzq ˜χDpzqdApzq ă K´1`2a}∆F}L1 ,

(2.26)

for any a P p0, 1{2s.
Remark 1: The ˜χD in (2.25) deﬁnes the local law for the distribution of the eigenvalues of T X. When
T is the identity matrix, ˜χD becomes the indicator function χD on the unit disk D, and we get the
famous local circular law for X [7]. For a more general T , we do not have a good understanding of the
properties of ˜χD so far. According to our numerical simulation, it seems that ˜χD would concentrate
in D (see Figure 1), but we have not got an analytic proof for this conjecture. This will be one of
the topics of our future study. Also, in this two theorems we assume that z is strictly away from the
unit circle. In future works we will try to extend our proof to include the |z ´ 1| “ op1q case.
Remark 2: As explained in the Introduction, the basic strategy of this paper is ﬁrst to prove an
anisotropic local law for the resolvent of Q when X is Gaussian, and then to get the anisotropic
local law in the case of a general X through comparison with a Gaussian random matrix. The
vanishing third moment assumption (2.23) will be used in the comparison arguments (see Theorem
2.20 and its proof in Section 6). In future works we will remove this assumption by improving our
comparison arguments. In Theorem 2.7, we have an extra bounded density condition. This is only
used in Lemma 2.24, which gives a lower bound for the smallest singular value of T X ´ z. Thus it
can be removed from the assumptions if we have a stronger result about the smallest singular value.

We conclude this section with two examples verifying the regularity conditions of Deﬁnition 2.4.

Example 2.8 (Bounded number of distinct eigenvalues). We suppose that n is ﬁxed, and that
s1, . . . , sn and ρΣpts1uq, . . . , ρΣptsnuq all converge as N Ñ 8. We suppose that limN ek ą limN ek`1
ek, mcpekqq ‰ 0. Then it is easy to check that
for all k, and furthermore for all ek we have B2
all the edges and bulk components are regular in the sense of Deﬁnition 2.4 for small enough .
Example 2.9 (Continuous limit). We suppose ρΣ is supported in some interval ra, bs Ă p0,8q, and
that ρΣ converges in distribution to some measure ρ8 that is absolutely continuous and whose density
satisﬁes τ ď dρ8pEq{dE ď τ´1 for E P ra, bs. Then there are only a small number of connected
components for supp ρ1c (whose value is independent of n), and all the edges and bulk components
are regular. See the remark at the end of Section A.1.

mfp?

2.3 Hermitization and local laws for resolvents

In the rest of this section, we prove Theorems 2.6 and 2.7. In the following, we will use the notation

Y ” Yz :“ T X ´ zI,

where I is the identity matrix. Following Girko’s Hermitization technique [20], the ﬁrst step in
proving the local circular law is to understand the local statistics of eigenvalues of Y :Y . In this
,
subsection, we present the key estimates concerning the resolvents
which show that Y Y : and Y :Y have asymptotic eigenvalue density ρ2c. These results will be used
later to prove the local circular law of T X.

Y :Y ´ w

Y Y : ´ w

and

`

˘´1

`

(2.27)

˘´1

9

Our local estimates on the resolvents take the form of local laws. They can be formulated in a

simple, uniﬁed fashion using an 2N ˆ 2N block matrix, which is a linear function of X.
Deﬁnition 2.10 (Index sets). We deﬁne the index sets
I 1 :“ t1, ..., Nu,
We will consistently use the latin letters i, j P I 1 or IM
label the indices of the matrices according to

1 YI 2 .
1 , greek letters µ, ν P I 2, and s, t P I. We

I 2 :“ tN ` 1, ..., 2Nu,

1 :“ t1, . . . , Mu,
IM

I :“ I 1 YI 2,

IM :“ IM

X “ pXiµ : i P IM
When M “ N , we always identify IM
¯i :“ i ` N P I2 and ¯µ :“ µ ´ N P I1.
Deﬁnition 2.11 (Groups). For an I ˆ I matrix A, we deﬁne the 2 ˆ 2 matrices Arijs as

1 , µ P I2q, T “ pTij : i P I1, j P IM
1 q.
1 with I1. For i P I1 and µ P I2, we introduce the notations

ˆ

˙

Arijs “

Aij Ai¯j
A¯ij A¯i¯j

.

(2.28)

We shall call Arijs a diagonal group if i “ j, and an oﬀ-diagonal group otherwise .
Deﬁnition 2.12 (Linearizing block matrix). For w :“ E ` iη P C`, we deﬁne the I ˆI matrix

ˆ

˙

Hpwq ” HpT, X, z, wq :“

´wI
w1{2Y
w1{2Y : ´wI

?

,

(2.29)

w is taken to be the branch with positive imaginary part. Deﬁne

where Y is deﬁned in (2.27) and
the I ˆ I matrix

as well as the I1 ˆ I1 and I2 ˆ I2 matrices
Y Y : ´ w

GLpwq “

Gpwq ” GpT, X, z, wq :“ Hpwq´1,
`

`

˘´1

˘´1

.

Y :Y ´ w

ˆ

Throughout the following, we frequently omit the argument w from our notations.

By Schur’s complement formula, it is easy to see that
Gpwq “

w´1{2Y :GL w´1Y :GLY ´ w´1I

w´1{2GLY

“

GL

w´1Y GRY : ´ w´1I w´1{2Y GR

w´1{2GRY :

GR

(2.32)
Therefore a control of G immediately yields controls of the resolvents GL and GR.
In the following, we consider the N ď M case (i.e. when T is a “wide” matrix). The N ą M
case, as we will see, will be built easily upon N ď M case. Now we introduce a deterministic matrix
Π, which will be proved to be close to G with high probability.
Deﬁnition 2.13 (Deterministic limit of G). Suppose N ď M and T has a singular decomposition

, GRpwq “
ˆ
˙

(2.30)

(2.31)

˙

.

(2.33)

where D “ diagpd1, d2, . . . , dNq is an N ˆ N diagonal matrix. Deﬁne πrisc to be the 2 ˆ 2 matrix

`

˘´1 “

πrisc

¯D “ pD, 0q,

T “ U ¯DV,
ˆ
´wp1 ` |di|2m2cq

´w1{2 ¯z

˙
´wp1 ` m1cq

´w1{2z

10

.

(2.34)

‰´1
ÿ

µPI2

`

˘

¯ΣG

µµ ,

˙

.

(2.35)

(2.36)

(2.37)

(2.38)

Let Πd be the 2N ˆ 2N matrix with pΠdqriis “ πrisc and all other entries being zero. Deﬁne
w´1{2zApΣq

ˆ
´p1 ` m1cqApΣq

˙

ˆ

˙

ˆ

Π ” ΠpΣ, z, wq :“

Πd

U:
0
0 U:

“

w´1{2 ¯zApΣq

´p1 ` m2cΣqApΣq

˙

,

where Σ “ T T : and ApΣq “
Deﬁnition 2.14 (Averaged variables). Suppose N ď M . Deﬁne the averaged random variables

wp1 ` m2cΣqp1 ` m1cq ´ |z|2

.

U 0
0 U

“

m1 :“ 1
N

where

˘

¯ΣG

ÿ

`

iPI1

ii , m2 :“ 1
ˆ
˙

N

Σ 0
0
I

.

¯Σ :“

Deﬁne πris to be the 2 ˆ 2 matrix such that

ˆ

˘´1 “

`

πris

´wp1 ` |di|2 m2q

´w1{2 ¯z

´w1{2z

´wp1 ` m1q

Remark: Note that under the above deﬁnition we have
Tr GR “ 1
N
˘

m2 “ 1
N
ÿ
`

Tr GL,

ÿ

`

˘

which is the Stieltjes transform of the empirical eigenvalue density of Y Y : and Y :Y . Moreover, we
will see from the proof that m1,2c are the almost sure limits of m1,2 as N Ñ 8 with

m1c “ 1
N

iPI1

¯ΣΠ

ii , m2c “ 1

N

µPI2

¯ΣΠ

µµ .

(2.39)

Hence we also have πris Ñ πrisc as N Ñ 8.

Ť

The following two propositions summarize the properties of ρ1,2c and m1,2c that we will need to
understand the main results in this section. They will be proved in Appendix A. In Fig. 2 we plot
ρ2c for the example from Fig. 1 in the cases |z| ą 1 and |z| ă 1, respectively.
Proposition 2.15 (Basic properties of ρ1c and ρ2c). Fix  ą 0. The density ρ1c is compactly
supported in r0,8q and the following properties regarding ρ1c hold.
1ďkďLpnqre2k, e2k´1s where e1 ě e2 ě . . . ě e2L ě 0. If 1 ` τ ď |z|2 ď
(i) The support of ρ1c is
1 ` τ´1, then e2L ě ; if |z|2 ď 1 ´ τ , then e1 “ 0.
(ii) Suppose re2k, e2k´1s is a regular bulk component. For any τ1 ą 0, if x P re2k ` τ1, e2k´1 ´ τ1s,
then ρ1cpxq „ 1.
(iii) Suppose ej is a nonzero regular edge. If j is even, then ρ1cpxq „ ?
x ´ ej as x Ñ ej from
above. Otherwise if j is odd, then ρ1cpxq „ ?
ej ´ x as x Ñ ej from below.
(iv) If |z|2 ď 1 ´ τ , then ρ1cpxq „ x´1{2 as x Œ e2L “ 0.
The same results also hold for ρ2c. In addition, ρ2c is a probability density. All of the estimates

in this proposition are uniform in τ ď ||z|2 ´ 1| ă τ´1.

11

Figure 2: The densities ρ2cpx, zq when |z| “ 0.5, 0.75, 1.2, 1.5 for the example from Fig. 1, where
ρΣ “ 0.5δ?

?
2{17 ` 0.5δ4

2{17.

Proposition 2.16. The preceding proposition implies that, uniformly in w in any compact set of
C`,
(2.40)
Moreover, if 1 ` τ ď |z|2 ď 1 ` τ´1, then |m1,2cpwq| „ 1 for w in any compact set of C`; if
|z|2 ď 1 ´ τ , then |m1,2cpwq| „ |w|´1{2 for w in any compact set of C`.

|m1,2cpwq| “ Op|w|´1{2q.

We will consistently use the notation E ` iη for the spectral parameter w. In this paper, we
regard the quantities Epwq and ηpwq as functions of w and usually omit the argument w. In the
following we would like to deﬁne the fundamental spectral domain of w. For later purpose, it is also
helpful to consider the subdomains corresponding to the regular edges and bulks in Deﬁnition 2.4.

Deﬁnition 2.17 (Spectral domains). The spectral parameter w will always lie in the fundamental
domain

D ” Dpτ, Nq :“ tw P C` : 0 ď E ď τ´2, N´1`τ|m1c|´1 ď η ď τ´1u.

Given a regular edge ek, we deﬁne the subdomain

k ” De
De

kpτ, τ1, Nq :“ tw P Dpτ, Nq : |E ´ ek| ď τ1, E ě 0u.
Corresponding to a regular bulk component re2k, e2k´1s, we deﬁne the subdomain
kpτ, τ1, Nq :“ tw P Dpτ, Nq : E P re2k ` τ1, e2k´1 ´ τ1su.

k ” Db
Db

For the component outside supp ρ1c, we deﬁne the subdomain

Do ” Dopτ, τ1, Nq :“ tw P Dpτ, Nq : distpE, supp ρ1cq ě τ1u.

12

(2.41)

(2.42)

(2.43)

(2.44)

|=0.5z||=0.75z||=1.2z||=1.5z|We call a subset S ” Spτ, Nq Ă Dpτ, Nq a spectral domain if for each w P S we have

tu P D : Re u “ Re w, Im u ě Im wu Ă S .

(2.45)

We call S a regular domain if it is a regular De

k domain, a regular Db

k domain or a Do domain.

Remark: In the deﬁnition of D, we have suppressed the explicit w-dependence. Notice that when
|z|2 ă 1 ´ τ , since |m1c| „ |w|´1{2 as w Ñ 0, we allow η „ |w| „ N´2`2τ in D. In the deﬁnition of
k, the condition E ě 0 is added to take into account of the edge at 0 when |z|2 ď 1 ´ τ . For a
De
nonzero edge, this condition is redundant.

Now we are prepared to state the key theorems of this paper, namely various local laws satisﬁed
by G deﬁned in (2.30). We ﬁrst deﬁne the three kinds of local laws that will be used in this paper.
Let

d

Ψ ” Ψpwq :“

Impm1c ` m2cq

N η

` 1
N η

(2.46)

be the deterministic control parameter.
Deﬁnition 2.18 (Local laws). Suppose N ď M . Recall G ” GpT, X, z, wq deﬁned in (2.30) and
Π ” ΠpΣ, z, wq deﬁned in (2.35). Let S Ď D be a spectral domain.

(i) We say that the entrywise local law holds with parameters pT, X, z, Sq if

rGpT, X, z, wq ´ ΠpΣ, z, wqsst ă Ψpwq

(2.47)

uniformly in w P S and s, t P I.

(ii) We say that the anisotropic local law holds with parameters pT, X, z, Sq if

}GpT, X, z, wq ´ ΠpΣ, z, wq} ă Ψpwq

(2.48)
uniformly in w P S. Here the law is anisotropic in the sense that Π is not a multiple of the identity
matrix.

(iii) We say that the averaged local law holds with parameters pT, X, z, Sq if

|m2pT, X, z, wq ´ m2cpΣ, z, wq| ă 1
N η

(2.49)

uniformly in w P S.

The local laws for G in the case of a general T will be built upon the following result in the case

of a square diagonal T .
Theorem 2.19 (Local laws when T is diagonal). Fix τ ď ||z|2 ´ 1| ď τ´1. Suppose Assumption
2.1 holds and N “ M . Suppose T ” D :“ diagpd1, ..., dNq is a diagonal square matrix. Let S Ă D
be a regular domain. Then the entrywise local law, anisotropic local law and averaged local law hold
with parameters pD, X, z, Sq.

Remark: Notice that the vanishing third moment assumption (2.23) is not required for this theorem.
Also our entrywise local law is not optimal compared to the one obtained in [7, Theorem 3.4]. We
can improve our estimates up to the optimal scale with some extra work by assuming a uniform
subexponential decay of the matrix elements Xiµ. However, we do not pursue such improvement in
this paper.

13

Now suppose that N ď M and T is an N ˆ M matrix (call it a wide matrix) such that the
eigenvalues of Σ satisfy (2.5) and (2.6). Consider the singular decomposition T “ U ¯DV , where U
is an N ˆ N unitary matrix, V is an M ˆ M unitary matrix and ¯D “ pD, 0q is an N ˆ M matrix
such that D “ diagpd1, d2, . . . , dNq. Then we have
˙

T X ´ z “ U DV1X ´ z,

where V1 is an N ˆ M matrix and V2 is an pM ´ Nq ˆ M matrix deﬁned through V “
. If
X “ X Gauss is Gaussian, then V1X Gauss d“ ¯X GaussU: with ¯X being an N ˆ N Gaussian random
matrix. Then by the deﬁnition of G in (2.30),

ˆ

(2.50)

V1
V2

ˆ

˙

˙

ˆ

U:
0
0 U:

GpT, X Gauss, z, wq d“

U 0
0 U

GpD, ¯X Gauss, z, wq

.

(2.51)

Since the anisotropic local law holds for GpD, ¯X Gauss, z, wq by Theorem 2.19, we get immediately
the anisotropic local law for GpT, X Gauss, z, wq. The next theorem states that the anisotropic local
law holds for general T and X provided that the anisotropic local law holds for Gaussian X and
diagonal T .
Theorem 2.20 (Anisotropic local law when N ď M ). Fix τ ď ||z|2 ´ 1| ď τ´1. Suppose Assump-
tion 2.1 holds and N ď M . Assume in addition that (2.23) holds. Let S Ă D be a regular domain.
Let T “ U ¯DV be a singular decomposition of T , where ¯D “ pD, 0q with D “ diagpd1, d2, . . . , dNq.
(i) If the anisotropic local law holds with parameters p ¯D, X Gauss, z, Sq, then the anisotropic local
law holds with parameters pT, X, z, Sq.
(ii) If the anisotropic local law and averaged local law hold with parameters p ¯D, X Gauss, z, Sq,
then the averaged local law holds with parameters pT, X, z, Sq.
˙
Finally we turn to the N ą M case. Suppose T “ U ¯DV is a singular decomposition of T (call it
a tall matrix), where U is an N ˆN unitary matrix, V is an M ˆM unitary matrix and ¯D “
is an N ˆ M matrix such that D “ diagpd1, d2, . . . , dMq. We denote U “ pU1, U2q, where U1 has size
N ˆ M and U2 has size N ˆ pN ´ Mq. Following Girko’s idea of Hermitization [20], to prove the
local circular law in Theorem 2.7 when N ą M , it suﬃces to study detpT X ´ zq (see (2.53) below),
for which we have
detpT X ´ zq “ detp ¯DV XU ´ zq “ det

“ detpDV XU1 ´ zqp´zqN´M

DV XU1 ´ z DV XU2

ˆ

ˆ

˙

D
0

“ detpXU1DV ´ zqp´zqN´M “ detpV T DT U T

0

´z
1 X T ´ zqp´zqN´M .

(2.52)

Comparing with (2.50), we see that this case is reduced to the previous one with a wide T , while
the only diﬀerence is that the extra p´zqN´M term corresponds to the N ´ M zero eigenvalues of
T X. Thus we immediately have the following claim.
Claim 2.21. The N ă M case of Theorem 2.7 implies the N ą M case of Theorem 2.7.

Hence without loss of generality, we always assume that N ď M throughout the following.

14

2.4 Proof of theorems 2.6 and 2.7
Now we prove Theorems 2.6 and 2.7. By Claim 2.21, it suﬃces to assume N ď M . The main tool
will be Theorem 2.20. Our proof follows basically the section 5 of [7].

The following lemma collects basic properties of stochastic domination ă, which will be used

tacitly during the proof and throughout this paper.
Lemma 2.22. (i) Suppose that ξpu, vq ă ζpu, vq uniformly in u P U and v P V . If |V | ď N C for
some constant C, then

ÿ

ÿ

ξpu, vq ă

ζpu, wq

vPV

vPV

uniformly in u.

(ii) If ξ1puq ă ζ1puq uniformly in u P U and ξ2puq ă ζ2puq uniformly in u P U , then

ξ1puqξ2puq ă ζ1puqζ2puq

uniformly in u P U .
that Eξpuq2 ď N C for all u. Then if ξpuq ă Ψpuq uniformly in u, we have

(iii) Suppose that Ψpuq ě N´C is deterministic and ξpuq is a nonnegative random variable such

Eξpuq ă Ψpuq

uniformly in u.

Proof. See e.g. Lemma 3.2 of [4].

We follow Girko’s Hermitization technique [20], which can be reformulated as the following (see

e.g. [22]): we have for any smooth function g,

Nÿ

i“1

1
N

gpµjq “ 1
4πN

“ 1
4πN

ż
ż

∆gpzq Nÿ

j“1
∆gpzq log

logpµj ´ zqp¯µj ´ ¯zqdApzq

ˇˇ dApzq “ 1

4πN

ż

∆gpzq Nÿ

j“1

log λjpzqdApzq, (2.53)

ˇˇdetpY pzqY :pzqq
ż

Fz0pµjq “ N´1`2a
ż

4π

γjpzq

Nÿ

i“1

1
N

where 0 ď λ1 ď λ2 ď . . . ď λN are the ordered eigenvalues of Y pzqY :pzq. For g “ Fz0, we use the
new variable ξ “ N apz ´ z0q to write the above equation as

p∆Fqpξq Nÿ

j“1

log λjpzqdApξq.

Now deﬁne the classical location γjpzq of the j-th eigenvalue of Y pzqY :pzq by

ρ2cpxqdx “ j
N
ż
Using Proposition 2.15, we have that for any δ ą 0

0

log γjpzq ´ N

plog xqρ2cpx, zqdx

ˇˇˇˇˇ ď Nÿ

j“1

ˇˇˇˇˇ Nÿ

j“1

ż 8

0

, 0 ď j ď N.

γjpzq
γj´1pzq

N

15

|log γjpzq ´ log x| ρ2cpx, zqdx ď N δ (2.56)

(2.54)

(2.55)

for large enough N . Suppose we haveˇˇˇˇˇÿ
Nÿ

˜

then

ż

ˇˇˇˇˇ

Nÿ

i“1

1
N

p∆Fqpξq
ż

j“1

ż
Fz0pµjq “ N 2a
4π
“ 1
4π

p∆Fqpξq

ż 8

0

0

Plugging (2.56) and (2.58) into (2.54), we get

ˇˇˇˇˇ ă 1,
¸

log γj

log γjpzq

dApξq

j

j

log λj ´

ÿ
log λjpzq ´ Nÿ
ż 8

j“1

plog xqρ2cpx, zqdxdApξq ` OăpN´1`2a}∆F}L1q

Fpξq

plog xq∆zρ2cpx, zqdxdApξq ` OăpN´1`2a}∆F}L1q.

ˇˇˇˇˇ ă }∆F}L1 .

(2.57)

(2.58)

This proves Theorems 2.6 and 2.7.

Now it remains to prove (2.57). We need the following rigidity estimate which is a consequence

of Theorem 2.20.
Lemma 2.23. Fix a small  ą 0. For any N  ď j ď N ´ N , we have in the case |z|2 ď 1 ´ τ ,

|λj ´ γj|

γj

ă j´1pN ` 1 ´ jq´1{3N 1{3,

and in the case |z|2 ě 1 ` τ ,

|λj ´ γj|

γj

ă pmintj, N ` 1 ´ juq´1{3 N´2{3.

(2.59)

(2.60)

Proof. The proof is almost the same as the proof of Lemma 5.1 in [7]. Theorem 2.20 will be used
crucially during the proof.

From (2.59) and (2.60), we have for N  ď j ď N ´ N ,

#
j´1pN ` 1 ´ jq´1{3N 1{3
pmintj, N ` 1 ´ juq´1{3 N´2{3
ÿ

|log λj ´ log γj| ă 1.

N ďjďN´N 

if |z|2 ď 1 ´ τ,
if 1 ` τ ď |z|2 ď 1 ` τ´1.

(2.61)

|log λj ´ log γj| ď C

|λj ´ γj|

γj

ă

Using theses bounds we get

Through a standard large deviation estimate, we have the following bound for the norm of X (see
e.g. [9, 27, 33], we have that

(2.62)
where c0, C0 ą 0 are constants (here we use the form in [33, Proposition 2.4]). Since λj ď }Y }2 ď
p}T}}X} ` |z|q2 and log γj “ Op1q for j ě N ´ N  by Proposition 2.15, we conclude that

Pp}X} ą tq ď e´c0t2N for t ą C0,

ÿ

jąN´N 

|log λj ´ log γj| ă N .

(2.63)

16

Finally for j ď N , the following Lemma 2.24 holds concerning the smallest singular value of

T X ´ z. It implies that

|log λj| ă N 

(2.64)

holds uniformly for z in any ﬁxed compact set. Using Proposition 2.15, we can also check that

ÿ
ÿ

jăN 

jăN 

ÿ

1ďjďN

|log γj| ă N 

|log λj ´ log γj| ă N 

for large enough N . Combining (2.61)-(2.65), we get for any  ą 0,

(2.65)

(2.66)

for large enough N . This implies (2.57) and hence concludes the proof of Theorems 2.6 and 2.7.
Lemma 2.24 (Lower bound on the smallest singular value). If N ă M and the entries of X have
a density bounded by N C1 for some C1 ą 0, then

holds uniformly for z in any ﬁxed compact set.
bounded density condition.

Proof. To prove (2.67), we need to prove that

| log λ1pzq| ă 1

(2.67)
If N “ M , the bound (2.67) holds without the
¯

´
λ1pzq ď e´N 

P

ď N´C

(2.68)
for any , C ą 0. In the case N “ M without the bounded density assumption, we have λ1pzq ě
1pzq, where λ1
1pzq is the smallest singular values of X ´ T ´1z. Following [32] or [36, Theorem 2.1],
τ λ1
we have the bound | log λ1
Now we turn to the case N ă M with the entries of X having a density bounded by N C1. By

1pzq| ă 1, which further proves (2.67).

(2.50) we have that

T X ´ z “ U DpV1X ´ D´1U´1zq “: U D ¯Y pzq.

Hence it suﬃces to control the smallest singular value of ¯Y pzq, call it ¯λ1pzq. Notice the columns
¯Y1, . . . , ¯YN of ¯Y pzq are independent vectors. Using the variational characterization

¯λ1pzq “ min|u|“1
`

} ¯Y pzqu}2,
˘

¯Yk, spant ¯Yl, l ‰ ku

ˇˇx ¯Yk, uky

ˇˇ ,

we get

¯λ1pzq ě N´1{2 min
1ďkďN

(2.69)
where uk is the unit normal vector of spantYl, l ‰ ku and hence is independent of Yk. By conditioning
on uk, we get immediately

dist

“ N´1{2 min
1ďkďN

Pp¯λ1pzq ď N´Dq ď CN´D`C1`3{2,

(2.70)

which is a much stronger result than (2.68). Here we have used Theorem 1.2 of [34] to conclude that
x ¯Yk, uky for ﬁxed uk also has density bounded by CN C1.

17

2.5 Outline of the paper

The rest of this paper is devoted to the proof of Theorems 2.19 and 2.20. In Section 3, we collect the
basics tools that we shall use throughout the proof, including basic identities and estimates for the
matrix G, and the stability of the self-consistent equation (2.12). In Section 4, we perform step (A)
of the proof by proving the entrywise local law and averaged local law in Theorem 2.19 under the
assumption that T is diagonal. We ﬁrst prove a weak version of the entrywise local law in Sections
4.1-4.3, and then improve the weak law to the strong entrywise local law and averaged local law in
Sections 4.4-4.5. In Section 5, we perform step (B) of the proof by proving the anisotropic local law
in Theorem 2.19 with the entrywise local law proved in Section 4. Finally in Section 6 we ﬁnish
the step (C) of the proof. Using Theorem 2.19 proved in Sections 4 and 5, we prove Theorem 2.20
under the assumption (2.23) by using a self-consistent comparison method.

The ﬁrst part of the Appendix A establishes the basic properties of ρ1,2c stated in Lemma 2.3
and Proposition 2.15. In Sections A.2 and A.3, we establish some key estimates on m1,2c and the
stability of the self-consistent equation (2.12) on each subdomain Db
k separately, under
the regularity assumptions in Deﬁnition 2.4.

k, Do and De

3 Basic tools

The rest of this paper is devoted to prove Theorems 2.19 and 2.20. In this preliminary section, we
collect various identities and estimates that we shall use throughout this paper. At the end of this
section, we state a lemma about the stability of the self-consistent equation (2.12).
Deﬁnition 3.1 (Minors). For J Ă I, we deﬁne the minor HpJq :“ tHst : s, t P IzJu, and corre-
spondingly GpJq :“ pHpJqq´1. Denote rJs :“ ts P I : s P J or ¯s P Ju. We also use the notations
HrJs :“ tHst : s, t P IzrJsu and GrJs :“ pHrJsq´1. We abbreviate ptsuq ” psq, pts, tuq ” pstq,
rtsus ” rss and rts, tus “ rsts.

Notice that by the deﬁnitions, we have H

pJq
st “ 0 and G

pJq
st “ 0 if s P J or t P J.

Lemma 3.2. (Resolvent identities).

(i) We have

Gpwq “

“

ˆ
ˆ
´

1
Gii

Y GpiqY :
For i ‰ j P I1 and µ ‰ ν P I2, we have

(ii) For i P I1 and µ P I2, we have
“ ´w ´ w
¯
´
¯

Gij “ ´w1{2Gii
´

Y Gpiq

ij

and
Gµν “ ´w1{2Gµµ

Y :Gpµq

µν

“ ´w1{2Gjj

“ ´w1{2Gνν

18

˙
˙

.

´

´

w´1{2GLY

GL

w´1{2Y :GL w´1Y :GLY ´ w´1I
w´1Y GRY : ´ w´1I w´1{2Y GR
´

w´1{2GRY :

¯

GR

¯

.

µµ

Y :GpµqY

,

ii

1

Gµµ

´
´

“ ´w ´ w
¯
¯

ij

GpjqY :

piq
“ wGiiG
jj

Y GpijqY :

(3.1)

(3.2)

(3.3)

¯

ij

¯

GpνqY

“ wGµµGpµq

νν

Y :GpµνqY

µν

.

(3.4)

µν

(iii) For i P I1 and µ P I2, we have

Giµ “ ´w1{2Gii
Gµi “ ´w1{2Gii

´
´

¯
¯

iµ

´
´
“ ´w1{2Gµµ
“ ´w1{2Gµµ

Y Gpiq
GpiqY :

Furthermore we have

and

Giµ “ GiiGpiq

µµ

Gµi “ GµµG

pµq
ii

ˆ
ˆ

µi

´
´w1{2Yiµ ` w
´

´w1{2Y

:
µi ` w

Y GpiµqY

Y :GpµiqY :

,

˙

iµ

¯

.

µi

¯

¯

iµ

,

GpµqY
Y :Gpµq

.

µi

˙

¯

(iv) For s, t, r P I and s, t ‰ r P I,

and

G

prq
st “ Gst ´ GsrGrt
Grr

,

1
Gss

“ 1
prq
G
ss

´ GsrGrs
prq
ss Grr

GssG

.

(v) All of the above identities hold for GpJq instead of G for J Ă I.

Proof. All these identities can be proved using Schur’s complement formula. They have been previ-
ously derived and summarized e.g. in [14, 15, 17].

(3.5)

(3.6)

(3.7)

(3.8)

(3.9)

(3.10)

(3.11)

(3.12)

(3.13)

(3.14)

(3.15)

Lemma 3.3. (Resolvent identities for Grijs groups).
(i) For i P I1, we have

For i ‰ j P I1, we have

G´1riis “ Hriis ´
ÿ

ÿ

k,l‰i

HriksG

ris
rklsHrlis.

ÿ
ÿ

k‰j

Grijs “ ´Griis

HriksG

ris
rkjs “ ´
ris
“ ´GriisHrijsG
rjjs ` Griis

k‰i

rjs
riksHrkjsGrjjs

G

HriksG

ri,js
rkls HrljsG

ris
rjjs.

k,lRti,ju

(ii) For i, j, k P I1 and i, j ‰ k P I,
´

and

G

rks
rijs “ Grijs ´ GriksG´1rkksGrkjs,
´

¯´1 ´ G´1riisGriksG´1rkksGrkis

rks
riis

¯´1

.

rks
riis

G
(iii) All of the above identities hold for GrJs instead of G for J Ă I.

G

G´1riis “

19

“ G

rks
rijs ` GriksG´1rkksGrkjs.
¯´1

¯´

rks
riis

G

´
Griis ´ GriksG´1rkksGrkis

¯´1

.

Grijs “ G
“ G

rks
rijs `
rks
rijs `

For (3.15), we use (3.14) to get that
rks
riis

G

´

rks
G´1riis “ G´1riisG
riis
rks
riis

“

G

¯

G

Grkks

rks
rl1js

rks
rilsHrlksGrkksHrkl1sG

´
¯
l,l1‰k
´G´1rkksGrkjs
´GriksG´1rkks
´
´
¯´1 “ G´1riis
¯´1 ´ G´1riisGriksG´1rkksGrkis
Y “ Nÿ
a
˜

λkξkζ

k“1

rks
riis

:
¯k

G

Proof. Identities (3.11) and (3.12) follow from Schur’s complement formula directly. Applying the
second identity in (3.12) to Gris in the ﬁrst identity of (3.12), we get that

ÿ

Grijs “ ´Griis

HriksG

ris
“ ´GriisHrijsG
rjjs ` Griis

ÿ

ris
ris
rkjs “ ´GriisHrijsG
rjjs ´ Griis
ri,js
rkls HrljsG

HriksG

kRti,ju
ris
rjjs.

k,lRti,ju

HriksG

ris
rkjs

For (3.14), we apply the Schur’s complement formula and use (3.12) to get

ÿ

k‰i

ÿ
´

Next we introduce the spectral decomposition of G. Let

be the singular decomposition of Y , where λ1 ě λ2 ě . . . ě λN ě 0 and tξkuN
¸
orthonormal bases of CI1 and CI2 respectively. Then by (3.1), we have

k“1 and tζ¯kuN

k“1 are

Gpwq “ Nÿ

1

λk ´ w

k“1

ξkξ

w´1{2?

:
k
λkζ¯kξ

:
k

w´1{2?

:
λkξkζ
¯k
:
ζ¯kζ
¯k

.

(3.16)

Deﬁnition 3.4 (Generalized entries). For v, w P CI, s P I and an I ˆI matrix A, we shall denote

Avw :“ xv, Awy, Avs :“ xv, Aesy, Asw :“ xes, Awy,

(3.17)

ˆ

ˆ

where es is the standard unit vector.

˙
Given vectors v P CI1 and w P CI2 , we always identify them with their natural embeddings
v
0

in CI. The exact meaning will be clear from the context.

˙

0
w

and

Lemma 3.5. Fix τ ą 0. The following estimates hold uniformly for any w P Dpτ, Nq. We have

Let v P CI1 and w P CI2, we have the bounds

}G} ď Cη´1, }BwG} ď Cη´2.

ÿ

µPI2

|Gwµ|2 “ Im Gww

η

,

20

(3.18)

(3.19)

ÿ
ÿ
ÿ

iPI1

iPI1

µPI2

|Gvi|2 “ Im Gvv

η

,

|Gwi|2 “ |w|´1 Gww ` ¯w |w|´1 Im Gww

η

,

|Gvµ|2 “ |w|´1 Gvv ` ¯w |w|´1 Im Gvv

η

.

(3.20)

(3.21)

(3.22)

All of the above estimates remain true for GpJq instead of G for J Ă I . Finally, suppose tviuN
i“1
and twµuN
µ“1 are orthonormal bases of CI1 and CI2, respectively, then the above estimates remain
true if we replace ei with vi and eµ with vµ.
Proof. The estimates in (3.18) follow from (3.16). For any unit vectors x, y P CI1, we have

|xx, Gyy| ď Nÿ

k“1

ˇˇˇxξ

ˇˇˇ

|xx, ξky|

:
k, yy
|λk ´ w|
Nÿ

?

ď 1
η

ˇˇˇxζ

Now consider any unit vectors x P CI1 and y P CI2, we have

k“1

|xx, Gyy| ď |w|´1{2

λk |xx, ξky|
|λk ´ w|
`
where we have used that for w “ E ` iη,
a
˜a
¸
λk ´ 2E ` pE2 ` η2q{λk
1{2

?
|λk ´ w| “

|w|´1{2

k“1

λk

1
2η

ˆ

ď

E2 ` η2
a
E2 ` η2 ` E
E2 ` η2

ď 1
η

.

|xx, ξky|2

«

k“1

Nÿ
ˇˇˇ
ď Nÿ
:
¯k, yy
˘´1{4

ﬀ1{2«

ˇˇˇxξ

Nÿ

k“1

:
k, yy

ﬀ1{2

ˇˇˇ2
ˇˇˇxζ
:
k, yy
˘´1{4

ˇˇˇ2

.

“ 1
η
˙

“ 1
η

,

|xx, ξ¯ky|2 `
`
b
a
E2 ` η2
E2 ` η2 ´ 2E

2

“ 1?
˙
2η

ˆ

y1
y2

ˆ

˙

x1
x2
}G} “

For the other two blocks of G, we can prove the estimates in a similar way. In sum, for any unit
vectors x “

in CI, we have that

and y “

sup

|x|“1,|y|“1

xx, Gyy ď 1
η

p|x1||y1| ` |x1||y2| ` |x2||y1| ` |x2||y2|q ď 2
η

.

It is trivial to generalize the proof to BwG, where η´2 comes from the pλk ´ wq´2 factor of BwG.

For (3.19), we observe

ImGww

η

andÿ

µPI2

|Gwµ|2 “

ÿ

µPI2

Nÿ

k“1

E

A
:
xw, ζky
ζ
k, w
λk ´ w
A
E

Im

“ 1
η
A

xw, GReµy

:
eµ, G
Rw

“

|xw, ζky|2

pλk ´ Eq2 ` η2

“ Nÿ
E

k“1

:
Rw

“ Nÿ

k“1

|xw, ζky|2

pλk ´ Eq2 ` η2

“ ImGww

η

.

(3.23)

w, GRG

21

Similarly, we can prove (3.20). To prove identity (3.21), we use (3.1) and (3.23) to get

|Gwi|2 “ |w|´1

GRY :Y G

:
R

Y :Y ´ ¯w

:
G
R

GR

` ¯w |w|´1

:
GRG
R

ww

”

`

˘

ı

´

ÿ

iPI1

¯

´

ww

¯
“ |w|´1

:
GRG
R

“ |w|´1 Gww ` ¯w |w|´1

Identity (3.22) is proved similarly. The above proof is unchanged if we replace ei and eµ with vi

´

¯

ww

.

(3.24)

(3.25)

and wµ, respectively, sinceÿ
:
i “

viv

iPI1

ÿ

iPI1

:
i “ II1ˆI1 ,
eie

ww

“ |w|´1 Gww ` ¯w |w|´1 Im Gww
ÿ

ÿ

η

wµw:

µ “

eµe:

µ “ II2ˆI2 .

µPI2

iPI2

Deﬁnition 3.6. We say that an event Ξ holds with high probability if 1 ´ 1pΞq ă 0.

The following large deviation bounds will be used in the proof. See Theorem B.1 and lemmas

pNq
ij q, pb

B.2, B.3 and B.4 in [13] for the proof. See also Theorem C.1 of [14].
Lemma 3.7. (Large deviation bounds) Let pX
ables and pa
satisﬁes (2.2) and (2.4). Then we have the following bounds:
ÿ

q be deterministic. Suppose all entries X
˙
ÿ

q, pY
¸

˜ř

ˆř

|aij|2

ÿ

pNq
i

pNq
i

pNq
i

1{2

1{2

q be independent families of random vari-
are independent and

pNq
and Y
i

pNq
i

¸

˜ř

i‰j

1{2

.

(3.26)

|aij|2

N

,

aijXiYj ă

i,j

N

,

i‰j

aijXiYj ă

|bi|2
?

N

biXi ă

i

i

If the coeﬃcients pa
uniform in u.

pNq
ij q and pb

i,j
pNq
i

q depend on some parameter u, then all of the above estimates are

We have stated some basic properties of ρ1,2c and m1,2c in Lemma 2.3 and Proposition 2.15. In
the following lemma, we collect more estimates for m1,2c that will be used in the following sections.
They are proved in Appendix A.2. In particular, we shall prove that

¸

ˇˇˇˇˇ ě c for all w P D and 1 ď i ď n

1 ` m1c

´wp1 ` m1cq2 ` |z|2

p1 ` m1cq ´ |z|2

ˇˇˇˇˇw

˜
1 ` si

under the regularity assumptions of Deﬁnition 2.4. This uniform bound guarantees that the gener-
alized matrix entries of Gpwq remain bounded. For w “ E ` iη P D, we deﬁne the distance to the
spectral edge through

κ ” κpEq :“

min

1ďkď2L,eką0

|E ´ ek|.

(3.27)

Notice in the |z| ă 1 case, we do not take into consideration the edge at e2L “ 0.
Lemma 3.8. Fix τ ą 0 and suppose τ ď ||z|2 ´ 1| ď τ´1. We denote w “ E ` iη P D.
Case 1 Fix τ1 ą 0. Suppose the bulk component re2k, e2k´1s is regular in the sense of Deﬁnition 2.4

(ii). Then for w P Db

kpτ, τ1, Nq, we have

|1 ` m1c| „ Im m1c „ 1, |m2c| „ Im m2c „ 1.

(3.28)

22

Case 2 Fix τ1 ą 0. Then for w P Dopτ, τ1, Nq, we have

Im m1,2c „ η, |1 ` m1c| „ 1, |m2c| „ 1.

(3.29)

Case 3 Suppose ek ‰ 0 is a regular edge. Then for w P De
if E P supp ρ1c
if E R supp ρ1c

Im m1,2c „

#?
κ ` η
η{?

κ ` η

kpτ, τ1, Nq, if τ1 ą 0 is small enough,

, |1 ` m1c| „ 1, |m2c| „ 1.

(3.30)

Case 4 Suppose |z|2 ď 1´ τ . We consider the edge e2L “ 0 and take τ1 ą 0 to be small enough. Then

for w P De

2Lpτ, τ1, Nq, if Im w ě τ1, we have

|1 ` m1c| „ Im m1c „ 1, |m2c| „ Im m2c „ 1.

If |w| ď 2τ1, we have

m1c “ i
for some constant t ą 0, and

In all the above cases, we have

ˇˇˇˇˇw

˜
1 ` si

?
t?
w

` Op1q, m2c “

?

?
i

t

wpt ` |z|2q ` Op1q,

Im m1,2c „ |w|´1{2.

¸

1 ` m1c

´wp1 ` m1cq2 ` |z|2

p1 ` m1cq ´ |z|2

ˇˇˇˇˇ ě c,

where c ą 0 is some constant that may depend on τ and τ1.

With the above lemma, we obtain the following basic estimates about πrisc and Π.

Lemma 3.9. In all the four cases of Lemma 3.8,

›››`

˘´1

››› ď C|w|1{2,

}πrisc} ď C|w|´1{2,

πrisc

Im Πvv ď CImpm1c ` m2cq,

(3.31)

(3.32)

(3.33)

(3.34)

(3.35)

(3.36)

and

where

uniformly in w and any deterministic unit vector v P CI.
F

B

ˆ

˙

ˆ

˙

Proof. The estimate (3.35) follows immediately from (3.34) and the deﬁnition (2.34). For (3.36), we
can write

D

v

uris, πriscuris

,

Πvv “

v,

U 0
0 U

u :“

Πd

ˆ

U:
0
0 U:

˙

U:
0
0 U:

@

“ pΠdquu “ Nÿ
˙
ˆ

i“1

v, uris :“

ui
u¯i

.

23

@

D
´1 ď C

uris, πriscuris

for each i. By the deﬁnition of πrisc in

To control Im Πvv, it is enough to bound
(2.34), we get

„
´wp1 ` |di|2m2cq ` |z|2
1 ` m1c

|w| Im
p1 ` |di|2Re m2cqIm w ` |di|2pRe wqIm m2c `

„

„
Im πii,c “ |ui|2Im

“ C
|w|

wp1 ` |di|2m2cq ´ |z|2
1 ` m1c
|z|2



|1 ` m1c|2 Im m1c

,



where in the second step we use (3.34) and |1`m1c| „ |w|´1{2. In the ﬁrst three cases of Lemma 3.8,
we have |w| „ 1 and Im w “ OpIm m1cq, which give that Im πii,c ď CImpm1c ` m2cq. On the other
hand, in case 4 of Lemma 3.8, we can use |Im w|`|Re w|`|1`m1c|´2 “ Op|w|q and Im m1,2c „ |w|´1{2
to get that Im πii,c ď CImpm1c ` m2cq. Similarly we have the bound Im π¯i¯i,c ď CImpm1c ` m2cq.
Finally we can estimate the following term using similar methods,

!

“

w´1{2

“ 2Rep¯uiu¯izq Im
@
ď CRep¯uiu¯izq Impm1c ` m2cq ď C

wp1 ` |di|2m2cqp1 ` m1cq ´ |z|2
D

|¯ui|2 ` |u¯i|2

uris, πriscuris

ď C|uris|2Impm1c ` m2cq, which implies

`

‰´1

)
Impm1c ` m2cq.

˘

`

¯u¯iuiπ¯ii,c ` ¯uiu¯iπi¯i,c

Im

˘

Combining the above estimates we get Im
(3.36).

The self-consistent equation (2.12) can be written as
Dpm1q “ 0,

«

˜

where

Dpm1q “ m1 ` 1
N

nÿ

i“1

lisip1 ` m1q

w

1 ` si

1 ` m1

´wp1 ` m1q2 ` |z|2

¸

p1 ` m1q ´ |z|2

(3.37)

ﬀ´1

.

(3.38)

In the rest of this section, we study the stability of (3.37). Roughly, it says that if Dpm1q is small
and m1pw1q´ m1cpw1q is small for w1 :“ w` iN´10, then m1pwq´ m1cpwq is small. For an arbitrary
w P D, we deﬁne the discrete set

Lpwq :“ twu Y tw1 P D : Re w1 “ Re w, Im w1 P rIm w, 1s X pN´10Nqu,

(3.39)
Thus, if Im w ě 1 then Lpwq “ twu, and if Im w ă 1 then Lpwq is a 1-dimensional lattice with
spacing N´10 plus the point w. Obviously, we have |Lpwq| ď N 10.
Deﬁnition 3.10 (Stability of (3.37)). We say that (3.37) is stable on D if the following holds.
Suppose that N´2|m1c| ď δpwq ď plog Nq´1|m1c| for w P D and that δ is Lipschitz continuous with
Lipschitz constant ď N 4. Suppose moreover that for each ﬁxed E, the function η ÞÑ δpE ` iηq is
non-increasing for η ą 0. Suppose that u1 : D Ñ C is the Stieltjes transform of a positive integrable
function. Let w P D and suppose that for all w1 P Lpwq we have

|Dpu1qpwq| ď δpwq.

Then

|u1pwq ´ m1cpwq| ď

?

for some constant C ą 0 independent of w and N .

Cδ

κ ` η ` δ

,

(3.40)

24

w, uq

ˇˇ ď |w|1{2δpwq.
ˇˇfp?
|upwq ´ mcpwq| ď C|w|1{2δ
κ ` η ` δ

?

Then

.

(3.41)

This stability condition has previously appeared in the works [4, 7, 24]. In [24], for example, the
stability condition was established under various assumptions on ρΣ. In the following lemma, we
establish the stability for each of the subdomains Db
k and Do separately, under the regularity
assumptions from Deﬁnition 2.4. The proof is presented in Section A.3 of the Appendix.
Lemma 3.11. Fix τ ą 0 and let τ1 ą 0 be a suﬃciently small constant that may depend on τ .
Suppose τ ď ||z|2 ´ 1| ď τ´1.
Case 1 Suppose the bulk component re2k, e2k´1s is regular in the sense of Deﬁnition 2.4. Then (3.37)

k, De

is stable on Db

kpτ, τ1, Nq in the sense of Deﬁnition 3.10.

Case 2 (3.37) is stable on w P Dopτ, τ1, Nq in the sense of Deﬁnition 3.10.
Case 3 Suppose ek ‰ 0 is a regular edge in the sense of Deﬁnition 2.4. Then (3.37) is stable on

kpτ, τ1, Nq in the sense of Deﬁnition 3.10.
De

Case 4 Suppose |z|2 ď 1 ´ τ and e2L “ 0. If |w|1{2 ` |z|2 ě  for some constant  ą 0, then (3.37) is

stable on De

2Lpτ, τ1, Nq in the sense of Deﬁnition 3.10.

Notice this lemma leaves the case |w|1{2 ` |z|2 “ op1q alone. We will deal with this case in a

diﬀerent way in Section 4.5.

4 Entrywise local law when T is diagonal

In this section we prove the entrywise local law and averaged local law in Theorem 2.19 when T is
diagonal. The proof is similar to the previous proofs of local entrywise laws in e.g. [4, 5, 7, 24]. We
basically follow the ideas in [7], but we present the full proof since there are a lot of technical details
associated with the pair of variables pm1, m2q in the self-consistent equations.

The main novel observation of this section is that the self-consistent equations (2.10) and (2.11)
can be “derived” from the random matrix model by an application of Schur’s complement formula.
It is helpful to give a heuristic argument here. For the diagonal Griis group, we ignore formally the
random ﬂuctuations in (3.11) to get that

Equivalently, the stability condition can be expressed in terms of m “ ?

w, mq as following. Let w P D and suppose that for all w1 P Lpwq we have

wp1` m1q, u “ ?

u1q and fp?

wp1`

´

ÿ

k,l‰i

ˆ
ˆ

G´1riis « EHriis ´
´w
´w1{2 ¯z
´w
´w1{2 ¯z

“

«

E

HriksG

ris
rklsHrlis

˙
˙

´w1{2z
´w
´w1{2z
´w

´ w
N
´ w
N

˙

ris
rkks

G

ˆ

`

ˆ

0
¯di

˙

0
0

˙

˙

0
¯dk

0
0

ˆ

˙

0 dk
0
0

ris
rkks

G

0

|dk|2Gkk

¯
„ˆ
ˆ

ÿ
ÿ

k‰i

0 di
0
0

|di|2G¯k¯k

0

k

25

ˆ

“

˙

ˆ

´ w

|di|2m2

0

˙

0
m1

,

´w
´w1{2 ¯z

´w1{2z
´w

(4.1)

where we use the deﬁnition of m1 and m2 in (2.36). The 11 entry of (4.1) gives the equation

from which we get that

Summing over i and using that N´1



´1 ´ m1
Gii «
w p1 ` |di|2m2qp1 ` m1q ´ |z|2 ,
„
˘
`
` |z|2
ř
ř
1 ` |di|2m2
´w
1 ` m1
µ Gµµ “ m2, the above equation becomes
i Gii “ N´1
´w pm2 ` m1m2q ` |z|2m2
1 ` m1

« 1.

« 1,

Gii

(4.2)

which gives (2.10). Multiplying (4.2) with |di|2 and summing over i, we get the self-consistent
equation (2.11). In this section we give a justiﬁcation of these approximations.

Before we start the proof, we make the following simpliﬁcation. The parameter z can be either
inside or outside of the unit circle. Recall Lemmas 3.8 and 3.11, the domain D of w can be divided
roughly into four cases: w near a nonzero regular edge, w Ñ 0, w in the bulk, or w outside the
spectrum. In this section we will only consider the case |z|2 ď 1 ´ τ since it covers all four diﬀerent
behaviors. We emphasize in this case |m1,2cpwq| „ |w|´1{2 for w in any compact set of C` by
Proposition 2.16. Also due to the remark below Lemma 3.11, throughout subsections 4.1-4.4, we
assume |w|1{2 ` |z|2 ě c for some small constant c ą 0. We will deal with the |w|1{2 ` |z|2 “ op1q
case in section 4.5.

4.1 The self-consistent equations

To begin with, we prove the following weak version of the entrywise local law in Theorem 2.19.
Proposition 4.1 (Weak entrywise law). Fix |z|2 ď 1 ´ τ and a small constant c ą 0. Suppose
Assumption 2.1 holds and N “ M . Suppose T ” D :“ diagpd1, ..., dNq is a diagonal matrix. Then
for any regular domain S Ă D,

›››pGpwq ´ Πpwqqrijs

››› ă 1

|w|1{2

max
i,jPI1

˙1{4

ˆ

|w|1{2
N η

(4.3)

for all w P S such that |w|1{2 ` |z|2 ě c.

›››pG ´ Πqrijs
ÿ
For J Ď I, deﬁne the averaged variables m

Λ :“ max
i,jPI1

pJq
m
1

:“ 1
N

iRJ

26

For the purpose of the proof, we deﬁne the following random control parameters.

Deﬁnition 4.2 (Control parameters). Suppose N “ M and T ” D :“ diagpd1, ..., dNq. We deﬁne
the random errors

››› , Λo :“ max

››› .

›››pG ´ Πqrijs
ÿ

i‰jPI1

(4.4)
rJs
1,2) by replacing G in (2.36) with GpJq (GrJs), i.e.
pJq
ii

GpJq
µµ .

pJq
2

(4.5)

, m

pJq
1,2 (m
|di|2G

:“ 1
N

µRJ

The averaged error is deﬁned as

We deﬁne the random control parameter

θ :“ |m1 ´ m1c| ` |m2 ´ m2c|.
d

Impm1c ` m2cq ` θ

Ψθ :“

N η

` 1
N η

,

and the deterministic control parameters

d

Ψ :“

Impm1c ` m2cq

N η

` 1
N η

, Ψ1 :“ 1
|w|1{2

ˆ

|w|1{2
N η

˙1{4

.

Remark: By (2.39) and (2.5), we immediately get that

|m1 ´ m1c| ď τ´1Λ,

|m2 ´ m2c| ď Λ,

which gives θ “ OpΛq, and

τ Im m

pJq
1 ď Im m

pJq
2 ď τ´1Im m

pJq
2 .

We introduce the conditional expectation

(4.6)

(4.7)

(4.8)

(4.9)

(4.10)

i.e. the partial expectation in the randomness of the i and ¯i-th rows and columns of H, and deﬁne
the Z variables

By the identity (3.11) we have

where

Zris “ w

Erisr¨s :“ Er¨ | Hriss,

´

¯´1

.

rJs
riis

G

rJs
ris

Z

´w ´ w |di|2 m

´w1{2 ¯z

`
ris
2 ´ |di|2
|di|2m
:
¯ii ´
w´1{2 ¯diX

:“ p1 ´ Erisq
˜
G´1riis “ ErisG´1riis ` Zris “
˜
`
XGrisX:˘
X:D:GrisX:D:˘
ˇˇˇma ´ mrJs
ˇˇˇ ď C |J|
ˇˇˇˇˇ “
ˇˇˇˇˇÿ

|dk|2 GkiGik
Gii

|m1 ´ m

piq
1 | “ 1

N

a

ii

¯ii

kPI1

27

¸

ris
2

´w1{2z
ris
´w ´ wm
1
`
`
w´1{2diXi¯i ´

ris
1 ´

m

DXGrisDX

X:D:GrisDX

¯i¯i

` Zris,
¸
˘
˘

i¯i

(4.11)

.

(4.12)

Lemma 4.3. For J Ď I1, the following crude bound on the diﬀerence between ma and m
holds:

rJs
a

(a “ 1, 2)

(4.13)

N η
where C “ Cpτq is a constant depending only on τ .
Proof. For i P I 1, we have

, a “ 1, 2,

ÿ

kPI1

1

N|Gii|

|dk|2|Gik|2

ÿ

kPI1

ď τ´1
N|Gii|

|Gik|2 “ τ´1

N η

Im Gii

|Gii| ď τ´1

N η

(4.14)

where in the ﬁrst step we use (3.9), in the third step the assumption (2.5), and in the fourth step
the equality (3.20). Similarly, using (3.9) and (3.21) we get

ˇˇˇˇˇÿ

ˇˇˇˇˇ “

ÿ

piq
|m
1 ´ m

pi¯iq
1

| “ 1
N
kPI1
ď τ´1
piq
¯i¯i |
N|G

piq
¯ik

piq
ˆ
|dk|2 G
k¯i G
1
ÿ
piq
piq
¯i¯i |
N|G
kPI1
G
¯i¯i
¯ik |2 “ τ´1
piq
|G
|w| ` ¯w
G¯i¯i
|w|
piq
¯i¯i |
N|G

kPI1

piq
|dk|2|G
¯ik |2

ImG¯i¯i

η

˙

ď 2τ´1

N η

.

ris
Combining the above two estimates, we conclude that |m1 ´ m
1 | ď C{pN ηq. By induction on the
indices in rJs, we can prove that |m1 ´ m
d
Lemma 4.4. Suppose |z|2 ď 1 ´ τ . For i P I1 and w P D, we have

rJs
1 | ď C|J|{pN ηq. The proof for m2 is similar.
`
d
|

ris
Im m
2
N η

ris
Im m
1
N η

˘

,

d
22 | ă |w|
Zris
ris
|m
1 |
N |w| `

ris
Im m
1
N η

,

˛‚ for s ‰ t P t1, 2u.

d
¨˝|w|´1{2?

N

`

˘
˘

`
`

|

|

Zris

Zris

11 | ă |w|

st | ă |w|

The above estimates also hold for Z

rJs
ris with J Ă I1. In particular, these imply that

(4.15)

(4.16)

(4.17)

Proof. Apply the large deviation Lemma 3.7 to Zris in (4.12), we get that

˘

Zris
w

11

`

ˇˇˇˇˇ
ˆ

ˇˇˇˇˇ ă |di|2
˙
»–˜ÿ
˜ÿ

µ

“ C
N

µ

E

`

ˇˇˇˇˇ

Zris
w

|Xiµ|2 ´ 1
N
˘

ˇˇˇˇˇ ă C

11

N

ˆ

»–ˇˇˇˇˇÿ

µ

Zris ă |w|Ψθ.
˙

Gris

µµ

|Xiµ|2 ´ 1
N
ˆ

ˇˇˇˇˇ ` 1
˙

N

“ 0 and E
¸
ˇˇˇ2
¸

ˇˇˇGris

µµ

1{2

1{2

ris
µµ

Im G
η

|Xiµ|2 ´ 1
N

˜ÿ
d

µ‰ν

`

ˇˇˇGris

µν

ˇˇˇ2

ris
Im m
2
N η

.

¸

1{2

ˇˇˇ2

ﬁﬂ .

ˇˇˇGris

µν

˜ÿ

µ‰ν

2 “ E|Xiµ|4 ´ 1
¸

ﬁﬂ ď C

N 2 ď C
N 2 .
˜ÿ
¸
ˇˇˇ2

ˇˇˇGris

1{2

µν

N

µ,ν

1{2

Notice for the ﬁrst term in the above equation, we have

Applying the large deviation estimates to the ﬁrst sum, we get that

“ C

28

where in the third step we use the equality (3.19). Similarly for

, the large deviation estimate

and (3.20) yieldˇˇˇˇˇ
`

Zris
w

˘

22

ˇˇˇˇˇ ă

$&%

¸

ˇˇˇ2

1{2

,.-

˘

Zris

`
˜ÿ

j‰k

22

ˇˇˇG
|dj|2 |dk|2
¸
ˇˇˇ2
¸
ˇˇˇ2

ˇˇˇG
ˇˇˇG

ris
jk

1{2

ris
jk

1{2

ris
jk

ﬁﬂ

|dj|2

|dj|2 |dk|2

|dj|2

j

ˇˇˇˇˇÿ
»–˜ÿ
˜ÿ
˜ÿ

j,k

j

G

˙

N
1{2

ˆˇˇXj¯i
ˇˇ2 ´ 1
¸
ˇˇˇ2
ˇˇˇG
`
¸
ˇˇˇ2
ˇˇˇG
¸1{2

ris
jk

ris
jj

|dj|4

|dj|2 |dk|2

ris
|dj|2 Im G
jj
η

1{2

ris
jj

N

ˇˇˇˇˇ ` 1
˜ÿ
˜ÿ

j‰k

d
ď C
N

j,k
ris
Im m
1
N η

ă 1
N

ď C
N

Now we consider
large deviation estimates and (3.22) to get that

12

Zris

`

j

“ C

“ C
˘
N
. First, we have Xi¯i ă N´1{2 using (2.4). For the second part, we use the
¯

¸

˜ÿ

1{2

.

ˇˇˇ´

DXGrisDX

ˇˇˇ “

i¯i

ris
µj

ˇˇˇ2
ˇˇˇG
¸ﬀ1{2
d

|dj|2

j,µ

ris
jj

Im G
η
ris
|m
1 |
N |w| `

ris
Im m
1
N η

(4.18)

(4.19)

˛‚.

N

ˇˇˇˇˇ ă 1
¨˝d
¯

ď C

N η

ris
|dj|XiµG
˜
µjXj¯i
ris
|w|´1 G
ﬀ
jj ` ¯w
|w|
1{2

|dj|2

j,µ

ď

ˇˇˇˇˇ|di|
ÿ
«ÿ
«
“ 1
N
j
ris
|m
1 |
N |w| ` Im m
˘
`
gffe Im m2c ` Im
d
d

Zris

21

.

ris
1
N η

´

N

N |w| „ |w|´1{2?
|m1c|
|m1c|
N |w| ď C

d

N η

Im m1c

Similarly we can prove the estimate for

ˇˇ`

˘

Zris

11

d
ˇˇ ă |w|
`

ris
Im m
2
N η

˘

“ |w|

Now we prove (4.17). By the deﬁnitions (4.6), (4.7) and using (4.13), we get that

ris
2 ´ m2

m

` Impm2 ´ m2cq

ď C|w|Ψθ.

(4.20)

We can estimate
|m1c| „ |w|´1{2 „ 1 for |w| „ 1, and Im m1c „ |w|´1{2 „ |m1c| for |w| Ñ 0. Thus we have

and the third term in (4.16) in a similar way. By Lemma 3.8, we have that

22

Zris

and

d

d

ris
1 |
|m
N |w| ď

Then for the second term in (4.16), we have that

ris
1 ´ m1| ` |m1 ´ m1c| ` |m1c|
|m

N |w|

ď C

29

for |w| „ 1,

for |w| Ñ 0.

¨˝d

d

˛‚

|m1c|
N |w|

ris
1 ´ m1| ` θ
|m

N η

`

˜

d

ď C

`

1
N η

` max

θ
N η

#

d

,

+¸

¸

˜
Ψθ ` |w|´1{2?

N

.

ď C

Im m1c

N η

¸

N

|w|´1{2?
˜d

Finally we can check that for all cases in Lemma 3.8,

`

˘

|w|´1{2?

N

“ O

Im m1c

N η

“ O pΨθq .
˘
`

Zris

st ă |w|Ψθ for s ‰ t. In sum, we have shown that

st ă |w|Ψθ for all s, t P t1, 2u,
Thus
which gives the estimate (4.17).
ﬀ
Lemma 4.5. Suppose |z|2 ď 1 ´ τ . Deﬁne the w-dependent event Ξ :“ tΛ ď |w|´1{2plog Nq´1u.
Then we have
` OăpΨθq

1pΞqm2 “ 1pΞq

1 ` m1

(4.21)

«

Zris

,

´w p1 ` m1q2 ` |z|2

and

where

Υpw, m1q “ m1 ` 1
N

Proof. Using (4.11), we get

where πris is deﬁned in (2.38) and

˜

ris “ w

Notice by (4.13),

1pΞqΥpw, m1q ă 1pΞqΨθ,
nÿ

´
1 ` si

lisi

w

i“1

1 ` m1
´wp1`m1q2`|z|2

1`m1

¯

p1 ` m1q ´ |z|2

.

G´1riis “ π´1ris ` ris,
´
ris
m2 ´ m
2
0

¯
ˇˇˇ ď C

ris
1,2 ´ m1,2

N η

|di|2

ˇˇˇm

0

m1 ´ m

ď CΨθ.

¸

` Zris.

ris
1

(4.22)

(4.23)

(4.24)

(4.25)

Thus combining with the estimate (4.17), we get that ris ă |w|Ψθ.
Bi “ π´1ris ´ π´1risc, where πrisc is deﬁned in (2.34). By (3.35) and (4.9), we have

We would like to estimate Griis by treating π´1ris as the main term and the rest as errors. Let

1pΞq}Bi} ď C|w|1{2plog Nq´1 and 1pΞq}Biπrisc} ď Cplog Nq´1.

Thus we can do the expansion

1pΞqπris “ 1pΞqpπ´1risc ` Biq´1 “ 1pΞqπrisc
where the error term a can be estimated as

››πriscpBiπriscql

8ÿ

l“1

1pΞq}a} ď 1pΞq

1 ´ Biπrisc ` pBiπriscq2 ` . . .

“ 1pΞqpπrisc ` aq, (4.26)

››Biπrisc

››l ď 1pΞqC|w|´1{2plog Nq´1.

˘

`
›› ď 1pΞq}πrisc}

8ÿ

l“1

30

›› ă 1pΞq|w|1{2Ψθ, where ac-

››risπris
˛‚ď 1pΞq CN´τ{2.
¸
`
˘

(4.27)

˘

,

(4.28)

This shows that 1pΞq}πris} “ 1pΞqOp|w|´1{2q and hence 1pΞq
cording to the deﬁnition of D in (2.41),

¨˝d
¯´1 “ 1pΞqπris

1pΞq|w|1{2Ψθ ď 1pΞq C
´
π´1ris ` ris

N η

` |w|1{2
8ÿ
`

|w|1{2
N η
˜
1 `

l“1

Again we can do the expansion for (4.24),

1pΞqGriis “ 1pΞq

´risπris

l

“ 1pΞq

πris ` b

where through a similar calculation as above we get 1pΞqb ă 1pΞqΨθ. Now we look at the 11 entry
of (4.28),

1pΞqGii “ 1pΞq
`

„

´w

w p1 ` |di|2m2qp1 ` m1q ´ |z|2 ` 1pΞqOă pΨθq ,
¯ı

´1 ´ m1


´

˘

”
1 ` Oă

|w|1{2Ψθ

(4.29)

.

(4.30)

which follows from Proposition 2.16 or Lemma 3.8. Summing (4.30) over i and using N´1
N´1

from which we get that
1pΞqGii

Here we use that

ř
µ Gµµ “ m2,
1pΞq

1 ` |di|2m2
„

`

` |z|2
1 ` m1
˘

1pΞq

´w

1 ` |di|2m2

„
´w pm2 ` m1m2q ` |z|2m2
1 ` m1

“ 1pΞq


` |z|2
1 ` m1


“ 1pΞq

which, by Proposition 2.16, gives

1pΞq 1
m2

“ 1pΞq

The reciprocal of the above equation further gives

„
´w p1 ` m1q ` |z|2
1 ` m1

1 ` m1

´w p1 ` m1q2 ` |z|2

ř

i Gii “

“ Op|w|1{2q,
”
´
1 ` Oă
|w|1{2Ψθ


` Oă p|w|Ψθq

¯ı

,

.

(4.31)

Now plug (4.32) into (4.29), multiply with |di|2 and sum over i, we get

»– 1
»– 1

N

1pΞqm2 “ 1pΞq
Nÿ
nÿ

´
1 ` |di|2
´
1 ` si

lisi

w

w

i“1

N

i“1

1pΞqm1 “ 1pΞq

“ 1pΞq

` 1pΞqOă pΨθq .

(4.32)

1`m1

¯
|di|2p´1 ´ m1q
p1 ` m1q ´ |z|2 ` Oăp|w|1{2Ψθq
´wp1`m1q2`|z|2
¯
´1 ´ m1
1`m1
´wp1`m1q2`|z|2

p1 ` m1q ´ |z|2

` Oă pΨθq

ﬁﬂ ,

ﬁﬂ

` Oă pΨθq

(4.33)

where we use (3.34) and 1pΞqp1 ` m1q “ 1pΞqOp|w|´1{2q. This concludes the proof.

31

4.2 The large η case

In order to prove the weak estimate of the Green function (Proposition 4.1), we would like to ﬁx E and
then apply a continuity argument in η by ﬁrst showing that the rough bound Λ ď |w|´1{2plog Nq´1
in Lemma 4.5 holds for large η. To start the argument, we ﬁrst need to establish estimates on G
when η „ 1. This is the main purpose of this section.
Lemma 4.6. For any w P D and η ě c for ﬁxed c ą 0, we have the bound

|Gst pwq| ď C

max
s,t

(4.34)
for some C ą 0. Notice this bound is deterministic and is independent of the randomness. This
estimate also holds if we replace G with GpJq for J Ă I.
Proof. This is a trivial consequence of (3.18).
Lemma 4.7. Fix c ą 0 and |z|2 ď 1 ´ τ . We have the following estimate

Proof. By the previous lemma, we have
η „ 1, Lemma 4.4 gives that

As in (4.24),

max

wPSpτq,ηěc

ris
1,2

(4.35)

ˇˇˇm

Λpwq ă N´1{2.

ˇˇˇ “ Op1q. Combining with the fact that |w| “ Op1q for
Zris ă N´1{2.
´
π´1ris ` ris
¯´1 “ Griis
`

˘´1 “ Oăp1q

¯´1

(4.37)

(4.36)

,

Griis “

´

πi “

G´1riis ´ ris

where }π´1ris } “ Op1q and }ris} ă N´1{2. Notice since Griis “ Op1q, we have the estimate

using the taylor expansion. Then we can expand (4.37) to get that

(4.38)

(4.39)

(4.40)

(4.41)

.

N´1{2

1 ´ risGriis
´
¯
ﬀ´1
ﬀ´1

` |z|2
1 ` m1

` Oă

Griis “ πi ` Oă

˘

`

1 ` |di|2m2

«
´w
«
´w p1 ` m1q `
«
´w

|di|2

`

|z|2

˘

32

1 ` |di|2m2

` |z|2
1 ` m1

¯

´

N´1{2

¯

´

N´1{2

,

.

` Oă

ﬀ´1

´

¯

.

N´1{2

` Oă

The 11 entries of (4.38) give that

Gii “

while the 22 entries give

1 ` |di|2m2
Multiplying (4.39) with |di|2 and summing over i, we get

G¯i¯i “
Nÿ

m1 “ 1
N

i“1

Summing (4.40) over i, we get

«
´w p1 ` m1q `

ﬀ´1

´

N´1{2

¯

.

` Oă

|z|2

1 ` |di|2m2

(4.42)

Nÿ

i“1

Using the spectral decomposition (3.16), we note that for l ą 1,

lη

pλk ´ Eq2 ` η2 ď lIm m2.

|λk´E|ďlη

Summing up these two inequalities and optimizing l, we get

Im m2

η

.

(4.43)

Assume that Im m2 ď Cplog Nq´1, then by (4.10) we also have Im m1 ď τ´1plog Nq´1. From (4.43),
we get |m2| ď Cplog Nq´1{2. Together with Im w “ η ě c and Imr|z|2{p1 ` m1qs ă 0, (4.41) now
gives

|m1| ď 1
N

(4.44)
for some constant C ą 0 with high probability. Using the above estimate and |m2| ď Cplog Nq´1{2
we get

i

` |z|2
1 ` m1

¸ˇˇˇˇˇ´1
˘
ˇˇˇˇˇ ď C with high probability.

` op1q ď C

|z|2

1 ` |di|2m2

ﬀ

Furthermore,

m2 “ 1
N
ÿ
ÿ

|λk´E|ělη

|λk´E|ďlη

1
N

1
N

|E ´ λk|

pλk ´ Eq2 ` η2 ď 1
pλk ´ Eq2 ` η2 ď 1

|E ´ λk|

,

N

lη

ÿ
d
|Re m2| ď 2
˜
´w

1 ` |di|2m2

`

|di|2

ˇˇˇˇˇIm
ÿ
ˇˇˇˇˇ´w p1 ` m1q `
«
´w p1 ` m1q `
Nÿ

«

Im

Impwm1q “ Im

1
N

k“1

where we use that Imr|z|2{p1 ` |di|2m2qs ă 0 and

|z|2

ď ´Im w “ ´η,

1 ` |di|2m2
ˆ
´1 ` λk

|di|2|ξkpiq|2

λk ´ w

˙ﬀ

(4.45)

ě 0.

Hence (4.42) implies that Im m2 ě Cη with high probability for some C ą 0. This contradicts
Im m2 ď Cplog Nq´1. Thus we can assume that Im m2 ě Cplog Nq´1 with high probability for some
C ą 0 , which also implies Im m1 ě Cplog Nq´1.
˘

Now the proof proceeds as in Lemma 4.5. From (4.39) we get that

¯



N´1{2

.

(4.46)

„
`
where, by Im m1 ě cplog Nq´1,ˇˇˇˇ´w

´w

Gii

“ 1 ` Oă

´
ˇˇˇˇ “ Oplog Nq.

1 ` |di|2m2
`

1 ` |di|2m2

` |z|2
1 ` m1
˘

` |z|2
1 ` m1

33

Summing (4.46) over i, we get

which further gives

´w pm2 ` m1m2q ` |z|2m2
1 ` m1
“ ´w p1 ` m1q ` |z|2
1 ` m1

1
m2

´
“ 1 ` Oă
´

` Oă
´

N´1{2

,

¯
¯

N´1{2

¯

N´1{2

,

(4.47)

(4.48)

¯

(4.49)

´

` Oă

N´1{2

by using Im m2 ě Cplog Nq´1. Taking reciprocal of the above equation and doing the expansion,
we get

where we use that

m2 “

1 ` m1

„
´w p1 ` m1q2 ` |z|2
´w p1 ` m1q ` |z|2
1 ` m1

` Oă


Im

ď ´η,

which follows from the same argument as in (4.45). Now plug (4.47) into (4.41), we get

Here to do the Taylor expansion in the second step we use that

Nÿ
nÿ

i“1

i“1

´
1 ` |di|2
´
1 ` si

¯
´1 ´ m1
1`m1
¯
´1 ´ m1
1`m1
´wp1`m1q2`|z|2

´wp1`m1q2`|z|2

|di|2

w

lisi

w

m1 “ 1
N

“ 1
N

.

p1 ` m1q ´ |z|2
¸

` Oă

N´1{2

p1 ` m1q ´ |z|2

´
¯
p1 ` m1q ´ |z|2 ` OăpN´1{2q
ˇˇˇˇˇ ě Cplog Nq´1,
ﬀ
ﬁﬂ ě Im w “ η

´ |z|2
1 ` m1

´ |z|2
1 ` m1

ˇˇˇˇˇw

1 ` m1

˜
1 ` si
´w p1 ` m1q2 ` |z|2
«
˜
»–w `
1 ` si

“Im

Im

si

w

1 ` m1

´wp1 ` m1q2 ` |z|2

´p1 ` m1q ` |z|2

wp1`m1q

¸
which holds because |1 ` m1| ě Im m1 ě Cplog Nq´1 and

since each term in the sum has positive imaginary part.
Now applying Lemma 3.11 for (4.49), we conclude that |m1 ´ m1c| ă N´1{2 uniformly in η ě c.
Then using (4.47) and (4.48) we conclude |m2 ´ m2c| ă N´1{2 uniformly in η ě c. Using (4.38), we
get }pG ´ Πqriis} ă N´1{2 uniformly in η ě c and i P I1. Finally using the diagonal estimate and
(3.13), we prove the oﬀ-diagonal estimate using large deviation bounds (see (4.62)).

4.3 Proof of the weak entrywise local law
In this subsection, we ﬁnish the proof of Proposition 4.1. We shall ﬁx the real part E of w “ E ` iη
and decrease the imaginary part η. Recall Lemma 4.5 is based on the condition Λ ď |w|´1{2plog Nq´1

34

(i.e. event Ξ). So far this is only established for large η in (4.35). We want to show this condition
for small η also by using a continuity argument.

It is convenient to introduce the random function

w1PLpwq θpw1q

vpwq “ max
´

where Lpwq is deﬁned in (3.39). Fix an  ă τ{4 and an arbitrary D ą 0, our goal is to prove that
˘
(4.50)
`
for all w P S and large enough N ě Np, Dq. In other words, with high probability there is a gap
ď
in the range of v. Notice for w with ηpwq „ 1, (4.50) is true since P
P

vpwq ď N , vpwq ą 2N 3{4

vpwq ď N , vpwq ą 2N 3{4

˘

P

ˆ

N Im w1
|w1|1{2

1{4 |w1|1{2,

˙
¯

ď N´D`21
`
˙1{4 ď C|w1|´1{2plog Nq´1
d
˛‚ď N´D.
¸

vpwq ą 2N 3{4
If vpwq ď N , then we have

and vpwq ă 1 by (4.35).
ˆ

θpw1q ď N 
|w1|1{2

|w1|1{2
N Im w1

(4.51)
for all w1 P Lpwq. Hence tvpwq ď N u Ă Ξpw1q for all w1 P S X Lpwq. Then by (4.22), we have that
for all w1 P SXLpwq, there exists an N0 ” N0p, Dq such that
|w1|´1{2
N Im w1

(4.52)

P

¨˝vpwq ď N , Υpw1q ą N 
d

˜
for all N ą N0. Taking the union bound we get
w1PLpwq Υpw1q
vpwq ď N , max
`
w1˘

P

δ

Now we plan to apply the Lemma 3.11, where we use

N Im w1
|w1|´1{2 ą N 
d
“ N 
d

|w1|´1{2
N Im w1 .

Suppose

ď N´D`10.

(4.53)

(4.54)
Then Υpw1q ď δ pw1q for all w1 P Lpwq. By Lemma 3.11, if κ ! 1, then |w| „ 1 (recall the deﬁnition
of κ in (3.27)) and we have

|m1pw1q ´ m1cpw1q| ď C

for all w1 P Lpwq; if κ ě c ą 0 for some constant c ą 0, then

max

N Im w1
|w1|´1{2 ď N .
ˆ

w1PLpwq Υpw1q
a
δpw1q ď C1N {2
˜

˙

1{4

¸

1{2

1

N Im w1

|w1|´1{2
N Im w1

|m1pw1q ´ m1cpw1q| ď Cδpw1q ď C2N 

35

for all w1 P Lpwq. Combining these two cases we get that
N {2
|w1|1{2

|m1pw1q ´ m1cpw1q| ď A

¸

1{4

˜

|w1|1{2
N Im w1

(4.55)

for all w1 P Lpwq and some constant A ą 0 under the assumption vpwq ď N  and (4.54). By (4.21),
if (4.51) and (4.55) hold then we have

˜

¸

1{4

|m2pw1q ´ m2cpw1q| ă |m1pw1q ´ m1cpw1q| ` Ψθ ă N {2
|w1|1{2

|w1|1{2
N Im w1

i.e for all w1 P SXLpwq, there exists an N1 ” N1p, Dq such that
N Im w1
˜
¸
|w1|´1{2 ď N ,

˜
vpwq ď N , max

d
w1PLpwq Υpw1q

P

¸

1{4

|w1|1{2
N Imw1

|m2pw1q ´ m2cpw1q| ą N 3{4
|w1|1{2
for all N ě N1. Combining with (4.55) we get that

d

¨˝vpwq ď N , max
w1PLpwq Υpw1q
˜
vpwq ď N , θpw1q ą 2

P

˜
for N ě maxtN0, N1u. Adding (4.53) and (4.56), we get

N Im w1
|w1|´1{2 ď N , θpw1q ą 2

P

N 3{4
|w1|1{2

|w1|1{2
N Imw1

N 3{4
|w1|1{2
¸
¸

1{4

˜

ď N´D
¸

|w1|1{2
N Imw1

1{4

ď N´D`11

,

˛‚ď N´D

(4.56)

Taking the union bound over Lpwq we get (4.50) for all N ě maxtN0, N1u.
Now we conclude the proof of Proposition 4.1 by combining (4.50) and the large η estimate
(4.35). We choose a lattice ∆ Ă S such that |∆| ď N 20 and for any w P S there is a w1 P ∆ with
|w1 ´ w| ď N´9. Taking the union bound we get

(4.57)

(4.58)

P

´
´

¯
Dw P ∆ : vpwq P p2N 3{4, N s
ď N´D`41.
¯
Dw P S : vpwq P p3N 3{4, N {2s

ď N´D`41.

P

Notice v has Lipshcitz constant bounded by, say, N 6. Then we have

Since v is continuous, we have

!
Dw P S : vpwq P

´
ı)
c “
3N 3{4, N {2

!
@w P S : vpwq ď 3N 3{4

)

Y t@w P S : vpwq ą N {2u.

From (4.35), we have seen that for some w0 P S with Im w0 „ 1, there exists N2 ” N2p, Dq such
that

Ppvpw0q ą N {2q ď N´D`41

36

´
for all N ą N2. Therefore we have for N ą maxtN0, N1, N2u
@w P S : vpwq ď 3N 3{4
P

¯

´!
´!
Dw P S : vpwq P
“ P
ě P
Dw P S : vpwq P
ě1 ´ 2N´D`41.

´
´

ı)
ı)
3N 3{4, N {2
3N 3{4, N {2

¯
¯

´ Pp@w P S : vpwq ą N {2q
´ Ppvpw0q ą N {2q

c

c

Since ε can be arbitrarily small and D arbitrarily large, the above inequality shows that vpwq ă 1
uniformly in w P S, or equivalently

ˆ

˙1{4

θpwq ă 1
|w|1{2

|w|1{2
N η

.

(4.59)

In particular we see for all w P S, the event Ξ holds with high-probability.
ˆ

Now using (4.28) and (4.59), we get

›› ď

››Griis ´ πrisc
d

Im mc ` θ

N η

››πris ´ πrisc
›› `
››Griis ´ πris
¨˝d

ď C

|w|´1{2
N η

` 1
N η

` 1
N η

Ψθ “

where we use that

|w|1{2
N η

˙1{4
ˆ

ď C
|w|1{2

|w|1{2
N η

,

(4.60)

˙1{4

.

To conclude Proposition 4.1, it remains to prove the estimate for the oﬀ-diagonal entries. Using
(4.25) and (3.14), it is not hard to prove by induction that

|w|1{2

d

›› ă Ψθ ` θ ď C
˛‚ď C
ˆ

˙1{4

|w|´1{2
N η

|w|1{2
N η

`
for any J Ă I such that |J| ď l with l P N ﬁxed. Thus we have the diagonal estimates G
O
diagonal estimates, we get that

rJs
riis “
with high probability. Let i ‰ j P I1, using (3.13) and the
ÿ

|w|´1{2

|w|1{2

rJs
riis

G

(4.61)

››Grijs

››› ă 1

|w|1{2

rJs
riis ´ πrisc
˘

and

˘
›› ď
ă |w|´1|w|1{2?
“ |w|´1{2?

›››G
´
¯´1 “ O
`
››››››Griis
›››GriisHrijsG
››› `
››››››
ÿ
›››››
˜ř
` |w|´1
ř
›››››
˜ř
ř

k,lRti,ju Xi¯kG
:
k,lRti,ju X
¯ikG

ris
rjjs

`

N

N

››››››

››››››

HriksG

ri,js
rkls HrljsG

ris
rjjs

k,lRti,ju

HriksG

ri,js
rkls Hrljs

ř
ř

k,lRti,ju

k,lRti,ju Xi¯kG
:
k,lRti,ju X
¯ikG

ri,js
:
¯k¯l X
¯lj
ri,js
:
k¯l X
¯lj

k,lRti,ju Xi¯kG
:
k,lRti,ju X
¯ikG

ri,js
¯kl Xl¯j
ri,js
kl Xl¯j

ri,js
:
¯k¯l X
¯lj
ri,js
:
k¯l X
¯lj

k,lRti,ju Xi¯kG
:
k,lRti,ju X
¯ikG

ri,js
¯kl Xl¯j
ri,js
kl Xl¯j

where, as in the proof of Lemma 4.4, we use Lemmas 3.5 and 3.7 to get that

¸››››› ă Ψθ ď CΨ1,
¸››››› ă Ψθ.

(4.62)

(4.63)

ř
ř

37

4.4 Proof of the strong enterywise local law

In this section, we ﬁnish the proof of the (strong) entrywise local law in Theorem 2.19 under the
condition |w|1{2 ` |z|2 ě c. In Lemma 4.5, we have proved an error estimate of the self-consistent
equations of m1,2 linearly in Ψθ. The core part of the proof is to improve this estimate to quadratic
in Ψθ. For the sequence of random variables Zris, we deﬁne the averaged quantities

Nÿ

i“1

Nÿ

i“1

rZs “ 1
N

πrisZrisπris,

xZy “ 1
N

|di|2πrisZrisπris.

The following Lemma is an improvement of Lemma 4.5.
Lemma 4.8. Fix |z|2 ď 1 ´ τ . Then we have

m2 “

1 ` m1

´w p1 ` m1q2 ` |z|2

` Oăp|w|1{2Ψ2

θ ` }rZs} ` }xZy}q,

Proof. The proof is almost the same as the proof of Lemma 4.5, we only lay out the diﬀerence.
Suppose the event Ξ holds. Using (3.14), we have

ˆ

θ ` }rZs} ` }xZy}.
˙
ÿ
››› ă |w|1{2Ψ2

Griis
N

` 1
N

θ.

kPI1,k‰i

˙

|dk|2
0

0
1

and

Υpw, m1q ă |w|1{2Ψ2

ˆ

ˆ

˙´

¯

0
1

0
1

“

1
N

kPI1

|di|2
0

|dk|2
0

ris
Grkks ´ G
rkks

ÿ
By Proposition 4.1 and (3.35), we have›››GrkisG´1riisGriks
›››› ď C

By Lemma 3.8, if |w| „ 1, then Impm1c ` m2cq ě cη and

if |w| Ñ 0, then Impm1c ` m2cq „ |w|´1{2 and

ď |w|1{2Ψ2
θ;

N

|w|´1{2

N

ď C
N η

ď |w|1{2Ψ2
θ.

Plug the above estimates into (4.66), we get

ˇˇˇ ă |w|1{2Ψ2
ris
1,2 ´ m1,2
”
ı
Using (4.17) and (4.67), the error b in p4.28q is bounded by
1 ` Oăp|w|1{2Ψθq

b “ Oăp|w|1{2Ψ2

θq ´ πrisZrisπris

θ.

“ Oăp|w|1{2Ψ2

θq ´ πrisZrisπris.

(4.64)

(4.65)

GrkisG´1riisGriks.

(4.66)

(4.67)

N

›››› Griis
›››› ď C
ˇˇˇm

›››› Griis

N

Then following the arguments in Lemma 4.5, we can obtain the desired result on the event Ξ. Finally
we use that Ξ holds with high probability by Proposition 4.1.

38

In the following lemma we prove a strong bounds on rZs and xZy by keeping track of the

cancellation eﬀects due to the average over the indices i.

Lemma 4.9. (Fluctuation averaging) Suppose Φ and Φo are positive, N -dependent deterministic
functions on S satisfying N´1{2 ď Φ, Φo ď N´c for some constant c ą 0. Suppose moreover that
Λ ă |w|´1{2Φ and Λo ă |w|´1{2Φo. Then

}rZs} ` }xZy} ă |w|´1{2 Φ2
o.

(4.68)

Proof. The estimate (4.68) is an extension of [4, Lemma 4.9], [7, Lemma 7.3] and [14, Theorem 4.7].
Here we only prove the bound for }rZs}. The proof for }xZy} is exactly the same except that we
change πris to |di|πris in all the expressions.
For i P I1, we deﬁne Pi “ Eris and Qi “ 1´ Pi. Recall that Zris “ QiG´1riis, we need to prove that
Nÿ

¯

rZs “ 1
N

πris

i“1

πris ă |w|´1{2 Φ2
o.

´
QiG´1riis

rJs
for J Ă I by replacing m1,2 in (2.38) with m
ris
We deﬁne π
ris
1,2 ´ m1,2| ă |w|´1{2 Φ2
prove that |m
´
Nÿ
Nÿ
QiG´1riis

´
ris
|w|´1{2 Φ2
ris ` Oă
´

rZs “ 1
N

¯
¯

“ 1
N

¯

ris
ris

o, which further gives that

i“1

π

Qi

i“1

π

o

´

rJs
1,2 deﬁned in (4.5). As in (4.67), we can

¯

´

ris
risG´1riisπ

ris
ris

π

` Oă

¯

.

|w|´1{2 Φ2
ř

o

Thus if we abbreviate Bi :“ |w|1{2Qi
We estimate B by bounding the p-th moment of the norm by Φ2p
E}B}p ă Φ2p
o
for any square matrix K, we get that for p “ 2n,

i Bi ă Φ2
o.
for p “ 2n with n P N, i.e.
o . The lemma then follows from the Chebyshev’s inequality. Using that }KK:} “ }K}2

, it suﬃces to prove that B :“ N´1

ris
risG´1riisπ

ris
ris

π

››BB:››n “ }B}2n .

TrpBB:qn ě

Thus it suﬃces to prove that

ETrpBB:qp{2 ă Φ2p
o ,

for p “ 2n.

(4.69)

This estimate can be proved with the same method in [14, Appendix B], with the only complication
being that πris is random and depends on i. In principle, this can be dealt with by using (3.14) to
put any indices j, k, ... P I1 (that we wish to include) into the superscripts of πris. This leads to a
minor modiﬁcation of the proof in [14, Appendix B]. Here we lay out the basic ideas of the proof,
without writing down all the details.
The proof is based on a decomposition of the space of random variables using the operations Ps
and Qs. It is evident that Ps and Qs are projections, Ps`Qs “ 1 and all of these projections commute
with each other. For a set J Ă I, we use the notations PJ :“
sPJ Qs. Let
:
p “ 2n for an n P N and introduce the shorthand notation ˜Bks :“ Bks for s ď p odd and ˜Bks :“ B
ÿ
for s ď p even. Then we get
ks

ś
sPJ Ps and QJ :“
˜
pź
pź

pź

ÿ

ś

¸

ETrpBB:qp{2 “ 1
N p

ETr

s“1

˜Bks “ 1
N p

k1,k2,...,kp

ETr

pPkr ` Qkrq ˜Bks

.

(4.70)

k1,k2,...,kp

s“1

r“1

39

(4.71)

Introducing the notations k “ pk1, k2, . . . , kpq and tku “ tk1, k2, . . . , kpu, we thus get

ETrpBB:qp{2 “ 1
N p

ÿ

ÿ

I1,...,IpĂtku

k

pź

´

¯

ETr

PI c

QIs

˜Bks

.

s

s“1
˜Bks “ 0. Hence PI c

s

E

s“1

˜Bks

QIs

s

PI c

s

QIs

´

¯ﬀ

¯ﬀ

˜Bks “ ˜Bks and Pks

˜Bks “ 0 if ks R Is.
By the deﬁnition of ˜Bks, we have Qks
Thus we can restrict the summation to Is satisfying ks P Is for all s. Moreover, we claim that the
q for some s. WLOG, assume s “ 1. Then for
right hand side of (4.71) vanishes if ks P Xq‰sI c
each s “ 2, . . . , p, the factor PI c
˜Bks is independent of k1-th row and column. Thus we get, for
«
«
i, j P t1, 2u,
´
´
¯
pź
pź
$&%´
,.- “
¯ﬀ
ÿ
ÿ
,.- “ 0,
¯ﬀ
ÿ
ď

where in the last step we use that EQk1 “ 0. We conclude that the summation on the right hand
side of (4.71) is restricted to indices satisfying

$&%ÿ
“ E
«
´
¯
pź
«
´
¯
pź

$&%Qk1

$&%´

,.-

pź

EQk1

QI1Qk1

«

´

¯

´

˜Bk1

˜Bk1

˜Bk1

PI c

s

QIs

PI c

s

QIs

PI c

s

QIs

PI c

s

QIs

PI c

1

QI1

PI c

1

QI1

PI c

1

QI1

˜Bk1

˜Bks

˜Bks

˜Bks

˜Bks

il

s“2

il

s“2

il

s“2

il

s“2

“

“

lj

lj

PI c

1

l

l

E

E

l

ij

lj

l

¯ﬀ

,.-

lj

Under these two restrictions we have

Iq.

q‰s

ks P Is, ks P
pÿ

|Is| ě 2|tku| .

s“1

(4.72)

(4.73)

(4.74)

˙

p ď CpΦ2p
o .

since every index ks belong to at least two diﬀerent sets: Is and some Iq with q ‰ s.

We claim that it suﬃces to prove

for k P I. Using (4.74), under the condition (4.72), we have that

ETrpBB:qp{2 “ 1
N p

ETr

I1,...,IpĂtku

ÿ
pÿ

k

r“1

ÿ

ÿ

k

o

}QI Bk} ă Φ|I|
´
pź

PI c

s

QIs

s“1

˜Bks

¯
pÿ

r“1

ÿ

k

Φ

ř
ˆ

s
o

|Is|

ă Cp
N p

“ Cp

Φ2r
o

1
N p

1p|tku| “ rq ď Cp

Φ2r
o

N p´r ď Cp

o ` 1
Φ2

N

The lemma then follows from the Chebyshev’s inequality.

Now we prove the estimate (4.74). For the case |I| “ 1 (i.e. I “ tku),
ris
ris} ď |w|´1{2}Zrks} ă Φo,

}Bk} “ |w|1{2}π

ris
risZrksπ

40

where we can prove }Zrks} ă |w|1{2Φo by modifying the proof in Lemma 4.4. For the case |I| ě 2,
WLOG, we may assume k “ 1 and I “ t1, . . . , tu with t ě 2. It is enough to prove that

›››Qt . . . Q2π

|w|1{2

››› ă Φt

r1s
r1sG´1r11sπ

r1s
r1s

(4.75)
We take the case t “ 3 as an example to describe the ideas for the proof of (4.75), Using (3.14), we
get

o.

π

where

r1s
r1s “ π

r12s
r1s ` |w|1{2

r1s
11 π

r12s
r1s A1π

r1s
r1s :“ |w|1{2



r1s
r22s
N

` 1
N

kRt1,2u

¨˝ G

r1s
¯1¯1 π

r12s
r1s A2π

r12s
r1s ` error1,2,

r12s
r1s ` |w|1{2
´
ÿ

¯´1

r1s
rk2s

G

r1s
r22s

G

r1s
r2ks

G

˛‚ă Φ2

o,

A1,2 are deterministic matrices with operator norm Op1q, and the error term has operator norm
ď |w|´1{2Φ4
o. Then we get
r12s
r1s
r1s
r12s
r1s “ π
r1s ` |w|1{2
r1s G´1r11sπ
r1sG´1r11sπ
r12s
r12s
r1s G´1r11s
r1s A1π

r12s
r12s
r12s
r1s
r1s ` |w|1{2
r1s G´1r11sπ
r1s A1π
¯1¯1 π
r12s
r12s
r12s
r1s ` |w|1{2π
r1s G´1r11s
r1s A2π

` |w|1{2π

r1s
¯1¯1 π

r1s
11 π

r1s
11 π

π

(4.76)

We ﬁrst handle the π

´

(3.15) we get that

Q2G´1r11s “ Q2

r2s
r11s

G

r12s
r1s G´1r11sπ

¯´1´Q2G´1r11sGr12sG´1r22sGr21s

where the ﬁrst term vanishes since G

term, we ﬁrst expand π

π

G

´

´

(4.77)

r2s
r11s

r12s
r1s

Q2G´1r11s

r12s
r1s “ π

r12s
r1s G´1r11sπ

r12s
r1s . Using

r12s
r1s term. Notice Q2π

r12s
r12s
r1s A2π
r1s
r12s
r1s ` Oăp|w|´1{2Φ4
oq.
´

¯´1
r2s
r11s is independent of r2s columns and rows. For the remaining

r12s
r1s G´1r11sπ
¯
¯´1 “ ´Q2G´1r11sGr12sG´1r22sGr21s
´
oq and have
Q2Q3G´1r11sGr12sG´1r22sGr21s
´
´

r123s
r1s ` Oăp|w|´1{2Φ4
oq.
¯
¯

r2s
r11s

r2s
r11s

G

G

G

,

„
r123s
r12s
r1s ` Oăp|w|´1{2Φ2
r1s “ π
r123s
r12s
r1s “ ´π
r1s
„´
¯´1 ´ G´1r11sGr13sG´1r33sGr31s
„´
¯´1 ´ G´1r22sGr23sG´1r33sGr32s
„´
¯´1 ´
´
¯´1
¯´1

r3s
r11s
r3s
r22s
r2,3s
r11s

¯´1

r2s
r11s

r2s
r13s

´

´

´

G

G

G

G

G

G

r3s
r11s
r3s
r22s

¯´1

G

r2s
r33s

´

Q3

r3s
r11s

G

r3s
r12s

G

r3s
r22s

G

r3s
r21s

G

r2,3s
r11s

G

π

G


¯´1
´
¯´1
´
¯´1
´
¯´1 “ 0

r2s
r31s

G

G

G

r3s
r12s ` Gr13sG´1r33sGr32s
¯´1
r3s
r21s ` Gr23sG´1r33sGr31s
r2,3s
r11s



.

Apply (3.14) and (3.15) with k “ 3 to each resolvent entry in the ﬁrst term, we get

Q3Q2π

r12s
r1s G´1r11sπ

Q3Q2G´1r11s “ ´Q2Q3
ˆ
ˆ

The leading term vanishes as

by the independence, while the remaining terms have at least three oﬀ-diagonal resolvent entries in
the numerator, giving that

Q3Q2π

r12s
r1s G´1r11sπ

r12s
r1s ă |w|´1{2Φ3
o.

41

Then we deal with the other terms in (4.77), e.g. the second term. We ﬁrst expand

where

r3s
r1s :“ |w|1{2

e

¨˝ G

r1s
r1s “ e



r13s
r22s
N

` 1
N

r3s
r1s ` OăpΦ3
oq,
ÿ

´

kRt1,2,3u

Using the similar arguments as above we have

Q3|w|1{2e

r3s
11 π

r12s
r1s A1π

r12s
r1s G´1r11sπ

r12s
r1s “ |w|1{2e

r3s
11 π
ă |w|´1{2Φ4
o.

r123s
r1s A1π

r123s
r1s

¯´1

¯

´
Q3G´1r11s

r13s
rk2s

G

r13s
r22s

G

r13s
r2ks

G

˛‚.

r123s
r1s ` Oăp|w|´1{2Φ4
oq

π

Thus we have

Q2Q3|w|1{2

r1s
11 π

r12s
r1s A1π

r12s
r1s G´1r11sπ

r12s
r1s ă |w|´1{2Φ3
o.

Obviously this estimate also works for the rest of the terms in (4.77). In sum we have proved (4.75)
in the case t “ 3.

We can continue in this manner for a general t; at the l-th step, we expand the leading order terms
using (3.14) and (3.15), and they will vanish after applying Ql . . . Q3Q2 on them by independence.
Thus the number of oﬀ-diagonal resolvent entries in the numerator increases by at least one at each
step, and by induction we prove the (4.75). In fact the expansions can be performed in a systematic
way using the method in [14, Appendix B], and we leave the details to the reader. Also we remark
that similar techniques are also used in the proof in Section 5, and we choose to present the details
there (in fact the proof here is much easier than the one in Section 5).

d

Now we can ﬁnish the proof of the entrywise local law and averaged local law. By Proposition

4.1, we have that Λ ă |w|´3{8pN ηq´1{4. Therefore θ ă |w|´3{8pN ηq´1{4 and

where we use |w|´1{2pN ηq´3{8 ě pN ηq´1 by the deﬁnition (2.41) of D. Now we apply Lemma 4.9
with

Λo ă Ψθ ă

d

Impm1c ` m2cq ` |w|´3{8pN ηq´1{4
ˆ

N η

,

Φo “ |w|1{2

Impm1c ` m2cq ` |w|´3{8pN ηq´1{4

N η

, Φ “

|w|1{2
N η

˙1{4

to get }rZs} ` }xZy} ă |w|´1{2Φ2

o. Plugging into (4.65) we have

Υpw, m1q ă

|w|1{2Impm1c ` m2cq ` |w|1{4pN ηq´1{4

N η

.

Then using the stability Lemma 3.11,

By Lemma 3.8, if

?

|w|1{2Impm1c ` m2cq

?
|m1 ´ m1c| ă
κ ` η ě plog Nq´1, we have

N η

κ ` η

` |w|1{8pN ηq´1{2´1{8,

|w|1{2Impm1c ` m2cq

?

κ ` η

N η

ď C log N
N η

ă 1
N η

,

42

?

while if

κ ` η ď plog Nq´1, we have Impm1c ` m2cq “ Op?
ď C
N η

|w|1{2Impm1c ` m2cq

κ ` η

.

?

N η

κ ` ηq, which gives that

Therefore we have

|m1 ´ m1c| ă pN ηq´1 ` |w|1{8pN ηq´1{2´1{8 ă |w|´1{2

Then we use (4.64) to get that

|m2 ´ m2c| ă |m1 ´ m1c| ` |w|1{2Impm1c ` m2cq ` |w|1{4pN ηq´1{4

ă |w|´1{2

The above two estimates give that

ˆ

|w|1{2
N η

˙1{2`1{8
ˆ

.

|w|1{2
N η

˙1{2`1{8

.

(4.78)

Then repeating the previous steps with the new estimate (4.78) and doing inductions, we can improve
(4.78) to

N η

ˆ

|w|1{2
N η
˙ř

θ ă |w|´1{2
ˆ

θ ă |w|´1{2

|w|1{2
N η

˙1{2`1{8

.

l

k“1 1{2k`1{2l`2

after l steps. This implies the averaged local law θ ă pN ηq´1 since we can take l to be arbitrarily
large.

Finally as in (4.60) and (4.62), we have that

d

›› ă Ψθ ` θ ă
d

››Griis ´ πrisc
›› ă Ψθ ă
››Grijs

Impm1c ` m2cq

` 1
N η

,

Impm1c ` m2cq

N η

N η
` 1
N η

, i ‰ j.

This concludes the proof of entrywise local law and averaged local law in Theorem 2.19 when
|w|1{2 ` |z|2 „ 1.
4.5 Proof of Theorem 2.19 when |z| and |w| are small
In the previous proof, we have left the case |w|1{2 ` |z|2 ď  (for some small constant  ą 0) alone.
The only reason for doing this is that Lemma 3.11 does not apply in this case. In this section, we
deal with this problem in a diﬀerent way.
equations. Multiplying (4.29) with |di|2 and summing over i, we get

We will follow the arguments in previous subsections, but use a diﬀerent set of self-consistent

ﬀ

.

(4.79)

«

nÿ

lisi

i“1

1pΞqm1 “ 1pΞq

1
N

´1 ´ m1

w p1 ` sim2qp1 ` m1q ´ |z|2 ` Oă pΨθq

43

Recall that Σ :“ DD: “ D:D. We now introduce a new matrix

ˆ

˜Hpwq :“

´wΣ´1

w1{2pX ´ D´1zq:

w1{2pX ´ D´1zq

´wI

“
pX ´ D´1zqpX ´ D´1zq: ´ wΣ´1

‰´1

,

˜GL “

˙

,

and deﬁne ˜G :“ ˜H´1. By Schur’s complement formula, the upper left block of ˜G is

and the lower right block is equal to

“
pX ´ D´1zq:ΣpX ´ D´1zq ´ w

˜GR “

Now we write m1,2 in another way as

“

‰´1 “
ı

˘´1

pDX ´ zq:pDX ´ zq ´ w

‰´1 “ GR.

”
D:`
Tr
“
Tr ˜GR “ 1
N

Tr

m1 “ 1
N
m2 “ 1
N
“ 1
N

“
Y Y : ´ w
pX ´ D´1zq:ΣpX ´ D´1zq ´ w

“ 1
N

Tr ˜GL,

Tr

D

‰´1
´

Σ´1 ˜GL

¯

.

Tr

pX ´ D´1zqpX ´ D´1zq:Σ ´ w
ˆ

‰´1 “ 1
˙

N

˜G´1riis “

i

i

´w1{2 ¯z ¯d´1

` Oăp|w|Ψθq,

´w|di|´2 ´ wm2 ´w1{2zd´1
´w ´ wm1
„


wp|di|´2 ` m2qp1 ` m1q ´ |z|2|di|´2 ` OăpΨθq
nÿ

´1 ´ m1

´1 ´ m1

ﬀ
` OăpΨθq

wps´1

i ` m2qp1 ` m1q ´ |z|2s´1

i

.

1pΞq ˜Gii “ 1pΞq
«

1pΞqm2 “ 1pΞq

1
N

li
si

i“1

from which we get that

Plugging this into (4.82), we get

We apply the arguments in the proof of Lemma 4.5 to ˜H, and get that

(4.80)

(4.81)

(4.82)

(4.83)

We take the equations in (4.79) and (4.84) as our self-consistent equations, namely,

f1pm1, m2q “ OpΨθq, f2pm1, m2q “ OpΨθq,

where

f1pm1, m2q :“ m1 ` 1
N
f2pm1, m2q :“ m2 ` 1
N

1 ` m1

lisi

w p1 ` sim2qp1 ` m1q ´ |z|2 ,
wp1 ` sim2qp1 ` m1q ´ |z|2 .

1 ` m1

li

i

ÿ
ÿ

i

According to the following lemma, this system of self-consistent equations are stable when |w| and
|z|2 are small enough .

44

.

(4.84)

(4.85)

(4.86)

(4.87)

Lemma 4.10. Suppose that N´2|w|´1{2 ď δpwq ď plog Nq´1|w|´1{2 for w P D. Suppose u1,2 : D Ñ
C are Stieltjes transforms of positive integrable functions such that

maxt|f1pu1, u2qpwq| ,|f2pu1, u2qpwq|u ď δpwq.

Then there exists an  ą 0 such that if |w|1{2 ` |z|2 ď , we have

|u1pwq ´ m1cpwq| ` |u2pwq ´ m2cpwq| ď Cδ,

(4.88)

for some constant C ą 0 independent of w, z and N .
Proof. The proof depends crucially on the estimate of the Jacobian at pm1c, m2cq. By (3.32) and
(A.40), we see that

?

t0 ` Op|w|1{2 ` |z|2q

?
ř
m1c “ i
, m2c “ it
˙
where t0 “ pN´1
i“1 li{siq´1. Then we can calculate that
B2f1
B2f2

1 ` Op|z|2q
Op|z|2q

B1f1
B1f2

“ det

w

ˆ

ˆ

det

n

˙
t0 ` Op|w|1{2 ` |z|2q
2 ` Op|w|1{2 ` |z|2q

u1,2“m1,2c

´1{2
0

` Op|w|1{2 ` |z|2q

,

?

w

“ 2 ` Op|w|1{2 ` |z|2q.

We can conclude the stability by expanding f1,2pu1, u2q around pm1c, m2cq and using a ﬁxed point
argument as in the proof of Lemma 3.11 in Section A.3.

With this stability lemma, we can repeat all the arguments in the previous subsections to prove

the entrywise local law and averaged local law for DX when |w|1{2 ` |z|2 ď .

5 Anisotropic local law when T is diagonal

In this section we prove the anisotropic local law in Theorem 2.19 when T is diagonal. The ideas
of the proof follow from [4, section 5], but there are many technical points diﬀerent from the proof
there. So we present the full proof in this section. By the Deﬁnition 2.18 (ii) and the deﬁnition of
matrix norm, it suﬃces to prove the following proposition for generalized entries of G.
Proposition 5.1. Fix |z|2 ď 1 ´ τ and suppose that the assumptions of Theorem 2.19 hold. Then
for any regular domain S Ă D,

(5.1)

(5.2)

|xu,pGpwq ´ Πpwqq vy| ă Ψ
uniformly in w P S and any deterministic unit vectors u, v P CI.
It is equivalent to show that ÿ

`

˘

i,jPI1

:
ris
u

ˆ

Grijs ´ Πrijs
˙

ˆ

vrjs ă Ψ,

˙

where

uris :“

, vrjs :“

ui
u¯i

vj
v¯j

.

45

(cid:13)(cid:13)Griis ´ Πriis(cid:13)(cid:13)ˇˇuris

Using the the entrywise local law, we get that

˘

`

:
ris
u

ÿ

ˇˇˇˇˇ ď

Grijs ´ Πrijs

ˇˇˇˇˇÿ
Thus to show (5.2), it suﬃces to proveˇˇˇˇˇÿ

vrjs

i,j

i

ˇˇˇˇˇÿ
ˇˇ `
ˇˇˇˇvris
ˇˇˇˇˇ ă Ψ.

i‰j

:
risGrijsvrjs
u

ˇˇˇˇˇ ă Ψ `

ˇˇˇˇˇÿ

i‰j

ˇˇˇˇˇ .

:
risGrijsvrjs
u

:
risGrijsvrjs
u

(5.3)

Notice by the entrywise law, we can only get

i‰j

:
risGrijsvrjs

u

ˇˇˇˇˇ ă Ψ}u}1}v}1 ď N Ψ,

ˇˇˇˇˇÿ

i‰j

using }u}1 ď N 1{2}u}2 and the similar estimate for }v}1. In particular, this estimate of the (cid:96)1 norm
is sharp when u, v are delocalized, i.e. their entries have size of order N´1{2.

The estimate (5.3) follows from the Chebyshev’s inequality if we can prove the following lemma.

Lemma 5.2. Suppose the assumptions in Proposition 5.1 hold. For any positive integer p, there
exists a constant Cp which is independent of N such that

ˇˇˇˇˇÿ

i‰j

E

ˇˇˇˇˇp

:
risGrijsvrjs
u

ď CpΨp.

The rest of this section is devoted to prove Lemma 5.2. The proof is based on the polynomial-

ization method developed in [4, section 5].

5.1 Rescaling and partition of indices

For our purpose, it is convenient to deﬁne the rescaled matrix

RpJq :“ w1{2GpJq,

for any J Ă I. Consequently we deﬁne the control parameter Φ

Φ “ |w|1{2 Ψ.

¯´1 “ Oăp1q, R
ˇˇˇˇˇp

:
risRrijsvrjs
u

ď CpΦp.

´

pTq
rsss

E

R

ˇˇˇˇˇÿ
ÿ

i‰j

By the entrywise law,
pTq
rsss “ Oăp1q,

R

pTq
rsts “ OăpΦq for s ‰ t

under the above scaling. Now to prove Lemma 5.2, it is equivalent to prove

ˇˇˇˇˇÿ

i‰j

ˇˇˇˇˇp

We expand the product in (5.7) as

:
risRrijsvrjs
u

“

ik‰jkPI1

k“1

p{2ź

pź

k“p{2`1

:
riksRrikjksvrjks.
u

:
riksRrikjksvrjks ¨
u

46

(5.4)

(5.5)

(5.6)

(5.7)

Formally, we regard ti1, ..., ip, j1, ..., jpu as the set of 2p (index) variables that take values in I1. Let
Bp be the collection of all partitions of ti1, ..., ip, j1, ..., jpu such that ik, jk are not in the same block
for all k “ 1, ..., p. For any Γ P Bp, let npΓq be the number of its blocks and deﬁne a set of I1-valued
variables as

LpΓq :“ tb1, ..., bnpΓqu.

Now it is convenient to regard Γ as a symbol-to-symbol function
Γ : ti1, ..., ip, j1, ..., jpu Ñ LpΓq,
p{2ź

such that each Γ´1 pbkq is a block of the partition. Then we can rewrite the sum as

pź

˚ÿ

ÿ

ˇˇˇˇˇp

:
risRrijsvrjs
u

“

ˇˇˇˇˇÿ

i‰j

:
rΓpikqsRrΓpikqΓpjkqsvrΓpjkqs ¨
u

:
rΓpikqsRrΓpikqΓpjkqsvrΓpjkqs,
u

k“p{2`1

ΓPBp

blPI1,

l“1,...,npΓq

k“1

(5.10)
where Σ˚ denote the summation subject to the condition that the values of b1, ...bn are ordered as
b1 ă b2 ă . . . ă bn. We pick one term from the above summation and denote

(5.8)

(5.9)

∆pΓq :“ p{2ź

k“1

pź

k“p{2`1

:
rΓpikqsRrΓpikqΓpjkqsvrΓpjkqs ¨
u

:
rΓpikqsRrΓpikqΓpjkqsvrΓpjkqs.
u

(5.11)

Notations: For any bk P L, we can deﬁne a corresponding I2-valued variable ¯bk in the obvious way,
and we denote

rLs :“ tb1, ..., bn, b1, ..., bnu.

(5.12)

For notational convenience, we will also use letters i, j, k, l to denote the symbols in L.

5.2 String and string operators

During the proof we will frequently use the following resolvent identities for rescaled matrix R. They
follows immediately from Lemma 3.3.
Lemma 5.3 (Resolvent identities for Rrijs groups). For k R J and i ‰ j P I1zJ Y tku, we have the
following identities:

¯´1
´

rJs
rkks

R

´
¯´1 “
¯´1 ´
¯´1 “ w´1{2H

´
rJks
rJs
rijs ` R
riks
rJks
riis

´
rJs
rijs “ R
´
rJs
riis
rJs
riis

R

R

R

R

R

rJs
riis
rJs
riis ´ w´1

R

rJs
rkjs,

¯´1
ÿ

´

¯´1

´

¯´1

rJs
riks

R

rJs
rkks
rJis
rll1sH

R
rJs
rilsR

R
rJs
rl1is.

H

rJs
rkis

rJks
riis

R

l,l1RJYtiu
Furthermore, for i ‰ j and L deﬁned in (5.8), we have
rLztijus
ÿ
riis

rLztijus
rijs

“ R

where

R

SrijsR

rLztjus
rjjs

,

Srijs “ ´w´1{2Hrijs ` w´1

HriksR

rLs
rklsHrljs.

k,lRL

,

(5.13)

(5.14)

(5.15)

(5.16)

(5.17)

47

´

¯´1

In this section, we expand the R variables in ∆pΓq using the indentities in Lemma 5.3. During
the expansion, we need to distinguish carefully between an algebraic expression and its values as a
random variable.

R

rJs
rijs

rJs
rijs,

Deﬁnition 5.4 (Strings). Let A be an alphabet containing all symbols that may appear during the

:
ris and vrjs for all possible i, j, J Ă LpΓq. We deﬁne a
string s to be a formal expression consisting of the symbols from A. We denote by (cid:74)s(cid:75) the random
, Srijs, u
expansion, such as R
variable corresponding to s. We denote an empty string by H. Let M be the collection of all possible
strings with alphabet A.
deﬁne the same random variable, i.e. (cid:74)s(cid:75) “(cid:74)s1(cid:75). During the proof, we will identify more elements
To perform the expansions in a systematical way, we deﬁne the following operators acting on
to be maximally expanded if J Y ti, ju “ L. We call a

Given a string s, after an expansion of R’s in it, we will get a diﬀerent string s1. However they

strings. We call the symbols R
string s to be maximally expanded if all R symbols in s is maximally expanded.

of A (see the symbols in (5.33)).

¯´1

rJs
rijs,

rJs
rijs

´

R

pkq
Deﬁnition 5.5 (String operators). (i) Deﬁne an operator τ
0

rJs
rijs in Ω such that k R J Yti, ju, or the ﬁrst

Find the ﬁrst R

´

¯´1
´

for all Ω P M, in the following sense.
rJs
rJs
riis
rijs
R

such that k R J Ytiu. If R
rJks
riis

; if neither is found,

¯´1

R

rJs
riis

is trivial for Ω.

is found, replace it with

is found, replace it with R
pkq
τ
0

rJks
rijs ; if
pkq
pΩq “ Ω and we say that τ
´
0
pkq
for any Ω P M, in the following sense. Find the ﬁrst R
(ii) Deﬁne an operator τ
´
¯´1
1
that k R J Y ti, ju, or the ﬁrst
rJs
rJs
rJs
riis
riis
riks
is null for Ω.
(iii) Deﬁne an operator ρ for all Ω P M, in the following sense. Find each maximally expanded
rLztijus
. If nothing is found, ρpΩq “ Ω and we say
rijs

´
pΩq “ H and we say that τ

rJs
rijs in Ω such
rJs
rijs is found, replace it with
rJs
riksR

rJs
riis
R
is found, replace it with ´

rJs
rkjs; if
pkq
if neither is found, τ
1

such that k R T Y tiu. If R

in Ω and replace it with R

¯´1

¯´1

¯´1

rLztijus
riis

rLztjus
rjjs

SrijsR

rJks
riis

rJs
rkks

rJs
rkks

rJs
rkis

R

pkq
1

R

R

R

R

R

;

R

R

´

´

´1

R
that ρ is trivial for Ω.

According to Lemma 5.3, it is easy to see that

(cid:74)ρpΩq(cid:75) “(cid:74)Ω(cid:75)

(5.18)

for any Ω P M.
Deﬁnition 5.6. Deﬁne the function Fd´max : M Ñ N (where the subscript “d-max” stands for
“distance to being maximally expanded”) by

where ˚ could be 1 or ´1, and

Fd´max

Fd´maxpΩq “

“ |LzpJ Y ti, juq| ,
ÿ

Fd´maxpRq.

Deﬁne another function Foﬀ : M Ñ N with FoﬀpΩq giving the total number of oﬀ-diagonal symbols
in Ω.

R variables in Ω

48

´

R

¯´1
¯´1

(cid:114)´

pkq
0 ` τ

pkq
1

τ

¯

pΩq(cid:122) “(cid:74)Ω(cid:75) ,
˚¯

´

rJs
rijs

R

By oﬀ-diagonal symbols, we mean the terms of the form Ast with s R tt, ¯tu or Arijs with i ‰ j, e.g.
rJs
rijs and Srijs with i ‰ j are oﬀ-diagonal symbols. Later we will deﬁne other types of oﬀ-diagonal
R
symbols (see the symbols in (5.33)). Note that a R symbol is maximally expanded if and only if
Fd´maxpRq “ 0 and a string Ω is maximally expanded if and only if Fd´maxpΩq “ 0. The next two
pkq
lemmas are almost trivial by the deﬁnitions of τ
¯
0
pΩq “ H,
“ Fd´maxpΩq, Fd´max
pΩq
¯

pΩq “ Ω and τ
pkq
Fd´max
τ
0

pkq
Lemma 5.7. If τ
0

¯
pΩq

pkq
, τ
1

otherwise,

“ 0;

and ρ.

(5.19)

pkq
1

pkq
1

´

´

¯

´

´

τ

Fd´max

pkq
τ
0

pΩq

“ Fd´maxpΩq ´ 1, Fd´max

pkq
τ
1

pΩq

ď Fd´maxpΩq ` 4npΓq.

For ρ, we have

Fd´max pρpΩqq “ Fd´maxpΩq ` a,
where a is the number of maximally expanded oﬀ-diagonal R in Ω.
¯
Lemma 5.8. For any Ω P M, we have
pΩq
´

pkq
τ
0

Foﬀ

´

and

“ FoﬀpΩq, Foﬀ pρpΩqq “ FoﬀpΩq,

FoﬀpΩq ` 1 ď Foﬀ

pkq
τ
1

ď FoﬀpΩq ` 2 if τ

pkq
1

pΩq ‰ H.

¯
pΩq

5.3 Expansion of the strings

(5.20)

(5.21)

(5.22)

(5.23)

For simplicity of notations, in the rest of this section we omit the complex conjugates on the right
hand side of (5.11) (if we keep the complex conjugates, the proof is the same but with slightly heavier
notations). Suppose the right hand side of (5.11) is represented by a string Ω∆. Given a binary
word w “ a1a2...am with ai P t0, 1u, we deﬁne the operation
ρτpb1q

(5.24)
where bqn`r :“ br for any 1 ď r ď n and any q P N. By (5.24), a binary words w uniquely determines
an operator composition. By (5.18),

pΩ∆qw “ ρτpbmq

am ¨¨¨ ρτpb2q

pΩ∆q

a2

a1

from which we get an expansion

(cid:74)pΩ∆qw0(cid:75) `(cid:74)pΩ∆qw1(cid:75) “(cid:74)pΩ∆qw(cid:75) ,

ÿ
|w|“m(cid:74)pΩ∆qw(cid:75) “(cid:74)Ω∆(cid:75)

for any m ě 1, where |w| is the length of w.
Lemma 5.9. Given any w such that |w| “ pn2` 1qpp` 6l0q and pΩ∆qw ‰ H, either FoﬀppΩ∆qwq ě
l0, where

ˆ

or pΩ∆qw is maximally expanded.

˙
` 2

p,

l0 :“

8
τ

49

Proof. We use m0 to denote the number of 0’s in w, and m1 to denote the number of 1’s. Further-
p0q
to denote
more, we use m
0
the number of 0’s corresponding to the non-trivial τ0’s.
Assume FoﬀppΩ∆qwq ă l0 and pΩ∆qw is not maximally expanded. By (5.22)-(5.23), m1 ď l0´p ď

p1q
to denote the number of 0’s corresponding to the trivial τ0’s, and m
0

l0. By (5.19)-(5.21),

Fd´maxppΩ∆qwq ď Fd´maxpΩ∆q ` l0 ` 4nm1 ´ m

p1q
0 .

p1q
Using Fd´maxpΩ∆q “ np, we get a rough estimate m
0 ` m1 ď npp` 6l0q. By pigeonhole principle,
there are at least n 0’s in a row in w that correspond to trivial τ0. This indicates that pΩ∆qw is
maximally expanded, which gives a contradiction.

Next Lemma shows that all the strings with suﬃciently many oﬀ-diagonal symbols contributes

at most Φp.
Lemma 5.10. There exists constants Cp,l0, Cp,τ ą 0 such that
(cid:113)pΩ∆pΓqqw(cid:121)

ÿ

ÿ

˚ÿ

ΓPBp

blPI1,

l“1,...,npΓq

|w|“pn2`1qpp`6l0q,
FoffppΩ∆pΓqqwqěl0

ˇˇˇˇˇˇˇˇˇE

ˇˇˇˇˇˇˇˇˇ ď Cp,l0N 2pΦl0 ď Cp,τ Φp.

(5.25)

Proof. The ﬁrst bound is due to the fact that each summand is bounded by CΦl0 and there are at
most N 2p of them. For the second bound, we used Φ ď CN´τ{2.

Now it is suﬃcient to deal with the maximally expanded strings. Deﬁne a diagonal symbol in A

diXi¯i

` w´1

HriksR

rLs
rklsHrlis,

Notice all the R symbols in a maximally expanded string is diagonal. We taylor expand R

as

such that

ˆ

0
:
¯diX
¯ii

Sriis :“ ´
´

rLztius
riis

R

0

ˆ

¯´1 “
ˆ

rLztius
riis

R

„
“ l0´1ÿ

“

˙

ÿ

˙

k,lRL
´z
´w1{2

´w1{2
´¯z
ˆ
w1{2|di|2m2c
˘
`
‰
0
k ` Oă
˙

Φl0

˜πic

˜πic

Sriis ´
˘

w´1{2π´1risc `
“`
Sriis ´ Bi
ˆ
w1{2|di|2m2c
˜
w1{2m1c
|di|2pm2c ´ m

0

0

k“0
where ˜πrisc “ w1{2πrisc and Bi “
and

rLztius
Sriis ´ Bi “ w´1{2Z
ris

` w1{2

by (4.17) and the averaged law.

(5.26)

(5.27)

rLztius
riis

as

(5.28)

´ Sriis.

0

w1{2m1c

,

˙˙´1

¸

Here for the error term, we use ˜πic ă 1

rLs
2 q

0

m1c ´ m

rLs
1

ă Φ,

0

50

Now for all maximally expanded pΩ∆qw with |w| “ pn2 ` 1qpp ` 6l0q, denote by σ(cid:74)pΩ∆qw(cid:75) the

expression after plugging in (5.27) and (5.28) without the tail terms. Similar to Lemma 5.10,

ˇˇˇˇˇˇˇˇE

ÿ

˚ÿ

ÿ

ΓPBp

blPI1,

l“1,...,npΓq

|w|“pn2`1qpp`6l0q,

pΩ∆qw maximally expanded

ÿ

˚ÿ

ÿ

ˇˇˇˇˇˇˇˇE

`(cid:113)pΩ∆pΓqqw(cid:121) ´ σ(cid:113)pΩ∆pΓqqw(cid:121)˘ˇˇˇˇˇˇˇˇ ď Cp,τ Φp.
ˇˇˇˇˇˇˇˇ ď Cp,τ Φp.

σ(cid:113)pΩ∆pΓqqw(cid:121)

From the above bound and Lemmas 5.9-5.10, we see that to prove (5.7), it suﬃces to show

ΓPBp

blPI1,

l“1,...,npΓq

|w|“pn2`1qpp`6l0q,

pΩ∆qw maximally expanded

Next write σ(cid:74)pΩ∆qw(cid:75) as a sum of monomials in terms of Srijs,
Mpw, ∆pΓq, iq,

ÿ

σ(cid:74)pΩ∆qw(cid:75) “

i

(5.29)

(5.30)

where i is an index to label these monomials. Notice that after plug (5.30) in (5.29), the number
of summands Mpw, ∆pΓq, iq inside the expectation only depends on p, τ . Thus to show (5.29), it
suﬃces to prove the following lemma.
is maximally expanded. Let Mpw, ∆pΓqq be an monomial in σ(cid:113)pΩ∆pΓqqw(cid:121) in terms of Srijs for all
Lemma 5.11. Fix any Γ P Bp and binary word w with |w| “ pn2 ` 1qpp ` 6l0q. Suppose pΩ∆qw
i, j P L. We have

|EMpw, ∆pΓqq| ď Cp,τ Φp

(5.31)

˚ÿ

blPI1,

l“1,...,npΓq

for some constant Cp,τ that depends only on p, τ .

|w| “ pn2 ` 1qpp ` 6l0q. Then we ﬁx a monomial Mpw, ∆pΓqq in σ(cid:113)pΩ∆pΓqqw(cid:121). Let ΩM be the
For the rest of this section, we pick ﬁx a Γ P Bp and a maximally expanded pΩ∆pΓqqw with
string form of Mpw, ∆pΓqq in terms of Srijs. It is not hard to see that

Now we decompose Srijs as

Foﬀ pΩMq “ Foﬀ ppΩ∆qwq .

where we deﬁne the following symbols in A:

˜

SXrijs :“

0
diX

:
¯ij

Srijs “ SX
¸
ˆ

diXi¯j

ÿ

0

i¯j ` SX

¯ij ` SR

¯i¯j ` SR
¯ij,

i¯j ` SR
ˆ

ij ` SR
˙
˜

1
0

0
0

, SX

:
¯ij

¯ij :“ diX
¸

SRrijs :“

0
diX

:
¯ik

k,lRL

rLs
rkls

R

0
dlX

:
¯lj

dlXl¯j

0

,

, SX

i¯j :“ diXi¯j
˙

diXi¯k

0

51

ˆ

˙

0
1

0
0

,

(5.32)

(5.33)

(5.34)

(5.35)

ÿ
ÿ

k,lRL

i¯j :“
SR

¯i¯j :“
SR

didlXi¯kXl¯j

, SR

ij :“

:
didlXi¯kX
¯lj

,

(5.36)

˙
˙

ˆ
ˆ

rLs
0 R
¯kl
0
0

0
0
rLs
0 R
kl

ÿ
ÿ

k,lRL

ˆ
ˆ

rLs
R
¯k¯l
0

0
rLs
R
k¯l

˙
˙

0
0

0
0

:
¯ikXl¯j

, SR

¯ij :“

:
:
¯ikX
¯lj

didlX

(5.37)
We expand each Srijs of Mpw, ∆pΓqq as in (5.33) and write it as a sum of monomials in terms of
st and SR
SX
st,

ÿ

didlX

k,lRL

k,lRL

.

Mpw, ∆pΓqq “

Qpw, ∆pΓq, iq,

where i is an index to label these monomials. It is not hard to see that

i

Foﬀ pΩQq “ Foﬀ pΩMq “ Foﬀ ppΩ∆qwq

(5.38)

(5.39)

by (5.32). Since the number of summands in (5.38) is independent of N , to show (5.31), it suﬃces
to prove

|EQpw, ∆pΓqq| ď Cp,τ Φp

(5.40)

˚ÿ

blPI1,

l“1,...,npΓq

for any monomial Qpw, ∆pΓqq in (5.38). Throughout the following, we ﬁx a Qpw, ∆pΓqq with nonzero
expectation, and denote by ΩQ the string form of Qpw, ∆pΓqq in terms of SX
st. Notice the
st variables are independent of SR
R variables in SR
variables in Qpw, ∆pΓqq. Therefore we make the following observation: if SX
st
st appears as a symbol
in ΩQ, then ΩQ contains at least two of them.

st are maximally expanded. As a result, the SX

st and SR

Deﬁnition 5.12. Recall the deﬁnition of Γ in (5.9). Deﬁne h to be the number of blocks of Γ whose
size is 1, i.e.

h :“ npΓqÿ
ˇˇti1, . . . , ipu X Γ´1pblq

l“1

1

.

`ˇˇΓ´1pblq
ˇˇ “ 1
˘
ˇˇtj1, . . . , jpu X Γ´1pblq
ˇˇ , Jl :“
nź
ˇˇJl
ˇˇurbls

ˇˇvrbls

ˇˇIl

l“1

ˇˇ .

(5.41)

(5.42)

| E Qpw, ∆pΓqq| ď CN´h{2Φp

Lemma 5.13. Suppose for any b1, ..., bl taking distinct values in I 1,

For l “ 1, ..., n, deﬁne

Il :“

holds for some constant C independent of N . Then the estimate (5.40) holds.

Proof. By Cauchy-Schwarz inequality,

Then using

k“1

Nÿ

ˇˇurks

ˇˇa
ˇˇvrks
ˇˇb ď
h “ nÿ

l“1

#

52

N 1{2
1

if a ` b “ 1
if a ` b ě 2

.

1pIl ` Jl “ 1q ,

we get

˚ÿ

blPI1,

l“1,...,npΓq

|EQpw, ∆pΓqq| ď Cp,τ ΦpN´h{2

nź

ÿ

l“1

blPI1

ˇˇurbls

ˇˇIl

ˇˇvrbls

ˇˇJl ď Cp,τ Φp.

Hence it suﬃces to prove (5.42). The key of the proof is to extract the N´h{2 factor from

E Qpw, ∆pΓqq. For this purpose, we need to keep track of the indices in L during the expansion.
Deﬁnition 5.14. Deﬁne a function Fin : L ˆ M Ñ N with Finpl, Ωq giving the number of times l
or ¯l appears as an index of oﬀ-diagonal R or S in Ω.

The following lemma follows immediately from Deﬁnition 5.5 and the expansions we have done

Lemma 5.15. (1) For any string Ω, if τ

is trivial for Ω, then

to obtain ΩQ from pΩ∆qw.
´

Fin

pkq
0

l, τ

¯
pΩq

(2) For any string Ω,

(3) For any maximally expanded pΩ∆qw,

pkq
0

´

¯
pΩq

“ Finpl, Ωq, Fin

pkq
1

l, τ

Fin pl, ρpΩqq “ Finpl, Ωq.

Finpl, ΩQq “ Finpl,pΩ∆qwq.

“ Finpl, Ωq ` 2δkl.

(5.43)

(5.44)

(5.45)

Let ΩX

Q be the substring of ΩQ containing only SX symbols, and ΩR

Q be the substring of ΩQ

containing only SR symbols. Deﬁne

V :“ tl P L| Finpl, Ω∆q “ 1u,

and

V0 :“ tl P L| Finpl, Ω∆q “ 1 and Finpl, ΩX
V1 :“ tl P L| Finpl, Ω∆q “ 1 and Finpl, ΩX

Qq “ 0u,
Qq ě 2u.

Recall the observation above Deﬁnition 5.12, V “ V0 Y V1 and
h “ |V| “ |V0| ` |V1|.

(5.46)

(5.47)

(5.48)

Q. Notice that no :“ nX ` nR is the total number of oﬀ-diagonal symbols in ΩQ.

Q and nR be the number of oﬀ-diagonal SR
Let nX be the number of oﬀ-diagonal SX symbols in ΩX
symbols ΩR
Here we describe the rough idea for the proof of (5.42). For N´h{2 “ N´|V0|{2 ¨ N´|V1|{2, the
N´|V1|{2 factor comes from the SX variables in Qpw, ∆pΓqq. Each SX variable contributes N´1{2,
and we will show that there are at least |V1| of them. Hence in total they contribute N´|V1|{2. On
the other hand, the N´|V0|{2 factor comes from the SR variables. We expand SR edges as in (5.36)
and (5.37). After expansion, the SR variables produce 2nR summation indices, 2nR X variables and
nR R variables. Roughly speaking, the 2nR summation indices contribute a factor N nR , the 2nR
X variables contribute a factor N´nR , and the nR R variables contribute a factor of ΦnR . Hence
in total they contribute ΦnR . In addition, we will show that each oﬀ-diagonal SR variable with an
index in V0 produces an extra N´1{2 factor, which gives the factor N´|V0|{2. In the above argument
we have assumed the R variables are oﬀ-diagonal. We will also take care of the diagonal R variables
during the proof in next section.

53

5.4 Introduction of graphs and conclusion of the proof

We introduce the graphs to conclude the proof of (5.42). We use a connected graph to represent
the string ΩQ, call it by GQ0. The symbols in rLs are represented by black nodes in GQ0. The SX
st
or SR
st symbols in ΩQ are represented by edges connecting the nodes s and t. We also deﬁne colors
for the nodes and edges, where the color set for nodes is tblack, whiteu and the color set for edges
is tSX , SR, X, Ru. For GQ0, all the nodes are black, all SX edges are assigned SX color and all SR
edges are assigned SR color. We show a possible graph in Fig. 3. In this subsection, we identify an
index with its node representation, and a symbol with its edge representation.
Deﬁnition 5.16. Deﬁne function deg on the nodes set rLs, where degplq is the number of SR edges
connecting to the node l.

Finpl, ΩQq ” degplq ` degp¯lq ” 1

Hence

By Lemma 5.15, we see that for any l P V0,
ÿ
“`
ÿ
“

rFin pl, ΩQq mod 2s “
ď

|V0| “

ÿ

lPV0

lPV0

lPV0

pmod 2q.
˘
degplq ` degp¯lq
pdegplq mod 2q `

`

‰

mod 2
degp¯lq mod 2

(5.49)

(5.50)

˘‰

.

ˆ

Now we expand the SR edges. Take the SR

i¯j edge as an example (recall its deﬁnition in (5.36)).
We replace the SR
i¯j edge with an R-group, deﬁned as following. We add two white colored nodes to
represent the summation indices ¯k, l R rLs, two X-colored edges to represent Xi¯k and Xl¯j, and an
R-colored edge connecting ¯k and l to represent
. We call the subgraph consisting of the
three new edges and their nodes an R-group. If i “ j, we call it a diagonal R-group; otherwise, call
it an oﬀ-diagonal R-group. We expand all SR edges in GQ0 into an R-group and call the resulting
graph GQ1. For example, after expanding the SR edges in Fig. 3, we get the graph in Fig. 4. In the
graph GQ1, the R edges, X edges and SX edges are mutually independent (since the R symbols are
maximally expanded, and the white nodes are diﬀerent from the black nodes).

rLs
0 R
¯kl
0
0

˙

b1

b1

b2

b2

b3

b3

Figure 3: An example of the graph GQ0.

54

SR

b1

b2

b3

R

b1

b2

b3

Figure 4: The resulting graph GQ1 after expanding each SR in Fig. 3 into R-groups.

Notice that each white node represents a summation index. As we have done for the black nodes
(see (5.9)), we ﬁrst partition the white nodes into blocks and then assign values to the blocks when
doing the summation. Let W be the set of all white nodes in GQ1, and let W be the collection of
all partitions of W .
Fix a partition ζ P W and denote its blocks by W1, ..., Wmpζq. If two white nodes of some oﬀ-
diagonal R-group happen to lie in the same block, then we merge the two nodes into one diamond
pdq
R be the
white node (see Fig. 5a). All the other white nodes are called normal (see Fig. 5b). Let n
number of diamond nodes (which is less than or equal to the number of diagonal R-edges in GQ1).
Then we trivially have

# of white nodes “ ´n

.

(5.51)

R ` nÿ

pdq

k“1

“

‰
degpbkq ` degp¯bkq

(a) Diamond white node.

(b) Normal white nodes.

Figure 5: Two types of white nodes

By (5.50), there are |V0| black nodes with odd deg in rV0s (where rV0s are deﬁned in the obvious
way). WLOG, we assume these nodes are b1, ..., b|V0|. Notice to have nonzero expectation, each
white block must contain at least two white nodes. Therefore for each k “ 1, ...,|V0|, there exists a

55

block connecting to bk which contains at least 3 white nodes. Call such a block Wpbkq, and denote
by Apbkq the set of the adjacent white nodes to bk in Wpbkq. (Note that the Wpbkq’s or Apbkq’s are
not necessarily distinct.) WLOG, let W1, ..., Wd be the distinct blocks among all Wpbkq’s. Deﬁne

V00 :“ tbk| Apbkq has no normal white nodes, 1 ď k ď |V0|u,

and

V01 :“ tbk| Apbkq has at least one normal white node, 1 ď k ď |V0|u.

The following lemma gives the key estimates we need.
Lemma 5.17. For any partition ζ P W,

m ď ´|V00| ´ |V01|{2 ´ n

pdq
R `

n
k“1

ř

2

“

degpbkq ` degp¯bkq

‰

,

and

nX ` nR ě p ` |V1| ` |V00|, nX ě |V1|, n
Proof. The second inequality of (5.53) can be proved easily through

pdq
R ě |V00|.

ˇˇtk P L|Finpk, ΩX

Qq ě 2u

ˇˇ ď nX .

|V1| ď

(5.52)

(5.53)

Notice for bk P V0, Apbkq contains at least three diamond white nodes, while each of the white node
is share by another bl. Thus we trivially have |V00| ď n
Now we prove (5.52). A diamond white node is connected to two black nodes and a normal white
node is connected to one black node. Hence a diamond white node belongs to two sets Apbk1q, Apbk2q,
and a normal white node belongs to exactly one set Apbkq. Therefore for each i “ 1, ..., d, if Wi
contains exactly one Apbkq then

pdq
R .

|Wi| ě 3 ě 2 ` 1V01pbkq ` 1V00pbkq
ÿ

2

.

ˆ

˙
Otherwise if Wi contains more than one Apbkq, then
¨ 1V00pbkq

|Wi| ě

2 ¨ 1V01pbkq ` 3
2

ÿ

ˆ

bk:ApbkqĎWi

ě 2 `

bk:ApbkqĎWi

˙

.

1V01pbkq ` 1V00pbkq

2

Here the ﬁrst inequality can be understood as following. For each black node bk with Apbkq Ď Wi,
we count the number of white nodes in Apbkq and add them together. During the counting, we
assign weight-1 to a normal white node and weight-1{2 to a diamond white node (since it is shared
by two diﬀerent black nodes). If bk P V00, there are at least three diamond white nodes in Apbkq
with total weight ě 3{2; if bk P V01, there are at least one normal white node and two other white
nodes in Apbkq with total weight ě 2. Thus
is smaller than
the number of white nodes in Wi.

2 ¨ 1V01pbkq ` 3

2 ¨ 1V00pbkq

bk:ApbkqĎWi

ř

˘

`

Summing |Wi| over i we get

dÿ

i“1

|Wi| ě 2d ` |V01| ` |V00|

2

.

56

For the other m ´ d blocks, each of them contains at least two white nodes. Therefore
‰

“

|Wi| ` 2pm ´ dq ď ´n

degpbkq ` degp¯bkq

,

2m ` |V01| ` |V00|

2

R ` nÿ

pdq

k“1

ď dÿ

i“1

where we use (5.51) in the last step. This proves (5.52).

For bk P V00, Apbkq contains at least three white nodes from oﬀ-diagonal R-groups,

V00 Ďtbk P L| Finpbk, Ω∆q “ 1 and Finpbk, ΩR
pkq
1 may increase Fin. Thus w contains τ

Qq ě 3u “: V2.

for each bk P V1 Y V2 (recall
Recall (5.43)-(5.44), only τ
the deﬁnition of V1 in (5.48)). Therefore by (5.23), (5.39) and the fact that V00 and V1 are disjoint,

pbkq
1

nX ` nR “ FoﬀppΩ∆qwq ěFoﬀpΩ∆q ` |V1 Y V2| ě p ` |V1| ` |V00|.

This proves the ﬁrst inequality of (5.53).

By (2.4) and (5.6), a diagonal R edge contributes 1, an oﬀ-diagonal R edge contributes Φ, and

SX or X edge contributes N´1{2. Denote

Then using Lemma 2.22, we have

|EQpw, ∆pΓqq| ď CU

N´1{2

´

¯
degpbkq`degp¯bkq

nź

´

k“1

N´1{2

ΦnR´n

pdq
R

ˇˇvrbls

ˇˇJl .

ˇˇIl
ˇˇurbls
˚ÿ

l“1

U “ nź
¯
nX ÿ
ÿ
ÿ

N m´

ζPW

ζPW

nř
k“1

ď CUN´nX{2

ζpW1q,...,ζpWmqPI zL
degpbkq`degp¯bkq

2

ΦnR´n

pdq
R

´|V01|´|V00|{2´n

N

ζPW

ď CUN´nX{2
ÿ

ď CUN´h{2N´pnX´|V1|q{2
ď CUN´h{2

2

ÿ

ζPW

ζPW

pdq
R

ΦnR´n

pdq
R

N´pn

pdq
R ´|V00|q{2ΦnR´n

pdq
R

ΦnX`nR´|V1|´|V00| ď CpN´h{2Φp,

where in the third step we used (5.52), in the fourth step h “ |V| “ |V1| ` |V00| ` |V01|, in the ﬁfth
step N´1{2 ď Φ and (5.53), and in the last step (5.53). Thus we have proved (5.42), which concludes
the proof of Proposition 5.1.

6 Anisotropic local law for general T

In this section we establish Theorem 2.20 under the vanishing third moment assumption (2.23).
Without loss of generality, we again only consider the case |z|2 ď 1 ´ τ in this section, and so
|m1,2cpwq| „ |w|´1{2 for w in any compact set of C`.

57

˙

ˆ
ˆ
˜

Following the notations in the arguments between Theorems 2.19 and 2.20,

HpT X ´ z, wq “

“

Now we deﬁne

Gpwq :“ |w|1{2

where

˙ˆ
´wI

w1{2pU DV1X ´ zq:
U D 0
I

0

w1{2pU DV1X ´ zq

´wI
´wpD:Dq´1

w1{2pV1X ´ pU Dq´1zq:
`

w1{2

`

w1{2

´wpD:Dq´1
V1X ´ pU Dq´1z

˘:

ˆ

V1X ´ pU Dq´1z
´wI
˙

w1{2pV1X ´ pU Dq´1zq

´wI
˘

¸´1

˙ˆ

˙

.

pU Dq:

0

0
I

“ |w|1{2T

:

GT ,

(6.1)

rΠpwq :“ |w|1{2T

(6.2)
Since T is invertible and maxt}T},}T ´1}u ď τ´1 by (2.5), to prove the anisotropic law in Theorem
2.20, it suﬃces to show that

0

.

T :“

U D 0
I

Gpwq ´rΠpwq ă Φpwq

(6.3)

where

Notice we have }rΠ} “ Op1q by (3.35). By the remark around (2.51), if X “ X Gauss is Gaussian,

ΠpwqT , Φpwq :“ |w|1{2Ψpwq.

(6.4)

:

then (6.3) holds. Hence for a general X, it suﬃces to prove that
GpX, wq ´ GpX Gauss, wq ă Φpwq.

(6.5)

Similar to Lemma 3.5, we have the following estimates for G.
Lemma 6.1. For i P IM
1 , we deﬁne vi “ V1ei P RI1, i.e. vi is the i-th column vector of V1. Let
ÿ
u P RI1 and w P RI2 , then we have the bounds
ÿ
ÿ
ÿ

|Gwµ|2 “ |w|1{2 ImGww
|Guvi|2 ď C|w|1{2 ImGuu

ˆ
ˆ
|Gwvi|2 ď C

|w|´1{2 Gww ` ¯w |w|´1{2 ImGww
˙
|w|´1{2 Guu ` ¯w |w|´1{2 ImGuu

|Guµ|2 ď C

˙

iPIM

iPIM

µPI2

(6.9)

(6.6)

(6.7)

(6.8)

,

η

,

η

1

1

,

,

η

µPI2
for some constant C ą 0.
Proof. The proof is almost the same as the proof of Lemma 3.5, except that we use

η

ÿ

iPIM

1

:
i “ V1V

:
1 “ INˆN .

viv

58

6.1 Self-consistent comparison

Our proof basically follows the arguments in [24, Section 7] with some minor modiﬁcations. By
polarization, it suﬃces to show that for any for any deterministic unit vector v P RI we have

(6.10)
for w P S. The proof consists of a bootstrap argument from larger scales to smaller scales in
multiplicative increments of N´δ, where

v,

v

A

´
¯
Gpwq ´rΠpwq

E

ă Φpwq

ˆ

˙

δ P

τ

(6.11)
with C0 ą 0 being a universal constant that will be chosen large enough in the proof. For any
η ě |m1c|´1 N´1`τ , we deﬁne

2C0

0,

,

(6.12)

ηl :“ ηN δl for l “ 0, ..., L ´ 1,

!
l P N| ηN δpl´1q ă 1

ηL :“ 1.
)

.

where

L ” Lpηq :“ max

(6.13)

Note that L ď 2δ´1.
bounded by CN 3. Thus to prove (6.10) for all w P S, it suﬃces to show (6.10) holds for all w in

By (3.18), the function w ÞÑ Gpwq ´rΠpwq is Lipschitz continuous in S with Lipschitz constant
some discrete (but suﬃciently dense) subset pS Ă S. In this section, we always use the following
discretized domain pS.
Deﬁnition 6.2. Let pS be an N´10-net of S such that |pS| ď N 20 and
E ` iη PpS ñ E ` iηl PpS for l “ 1, ..., Lpηq.
)

The bootstrapping is formulated in terms of two scale-dependent properties (Am) and (Cm)

pAmq For all w PpSm, all deterministic unit vector v, and all X satisfying (2.2)-(2.4), we have
pCmq For all w PpSm, all deterministic unit vector v, and all X satisfying (2.2)-(2.4), we have

ImGvvpwq ă |w|1{2Imrm1cpwq ` m2cpwqs ` N C0δΦpwq.

!
w PpS | Im w ě N´δm

pSm :“

deﬁned on the subsets

(6.14)

.

Lemma 6.3. Property pA0q holds.

ˇˇˇGvvpwq ´rΠvvpwq

ˇˇˇ ă N C0δΦpwq.
Proof. By Lemma 3.5 and the assumption (2.5), we have for w PpS0,
where we use (3.28) for w PpS0.

ImGvvpwq ď C|w|1{2 }Gpwq} ď C
η

ď C|w|1{2Imrm1cpwq ` m2cpwqs ,

(6.15)

59

Lemma 6.4. For any m, property pCmq implies property pAmq.
E
Proof. Suppose property pCmq holds. By (6.4), we have

A

rΠvv “ |w|1{2

:

v, T

ΠT v

“ |w|1{2 pΠdquu ,

where u “ ¯T v. Now (3.36) implies

ImrΠvv ď CImpm1c ` m2cq,

and further

ImGvvpwq ď ImrΠvv `

ˇˇˇGvvpwq ´rΠvvpwq

ˇˇˇ ă |w|1{2Imrm1cpwq ` m2cpwqs ` N C0δΦpwq.

(6.16)

Thus the property pAmq follows.

The key step is the following result.

Lemma 6.5. For any 1 ď m ď 2δ´1, property pAm´1q implies property pCmq.

Combining Lemmas 6.3-6.5, we conclude that (6.15) holds for all w PpS. Since δ can be chosen
arbitrarily small under the condition (6.11), we conclude that (6.10) holds for all w P pS, and the

FvpX, wq “

anisotropic local law in Theorem 2.20 follows.

What remains now is the proof of Lemma 6.5. Denote

ˇˇˇGvvpX, wq ´rΠvvpwq
ˇˇˇ .
˘p
`
for all w PpSm and all deterministic unit vector v.

vpX, wq ď

N C0δΦpwq

EF p

By Markov’s inequality, it suﬃces to show the following lemma.
Lemma 6.6. Fix p P 2N and m ď 2δ´1. Suppose that the assumptions of Theorem 2.20 and property
pAm´1q hold. Then we have

(6.18)

(6.17)

The rest of Section 6.1 is devoted to the proof of Lemma 6.6 using the self-consistent comparison

method presented in [24, Section 7]. We separate the proof into several small subsections.

6.1.1 Rough bound

In order to make use of the assumption pAm´1q, which has spectral parameters in pSm´1, to get
some estimates for spectral parameters inpSm, we shall use the following rough bound for Gxy.

Lemma 6.7. For any w “ E ` iη P S and x, y P CI, we have

ˇˇˇGxypwq ´rΠxypwq
ˆ
˙

x1
x2

and y “

y1
y2

rImGx1x1pE ` iηlq ` ImGx2x2pE ` iηlq

ˇˇˇ ăN 2δ
Lpηqÿ
˙
`ImGy1y1pE ` iηlq ` ImGy2y2pE ` iηlqs ` |x||y|,
for x1, y1 P CI1 and x2, y2 P CI2 .

l“1

ˆ

where x “

60

Proof. By (3.35) and the deﬁnition of rΠ in (6.4), we get that

estimate |Gxy|. By the deﬁnition of G in (6.1), we see that Gxy “ R¯x¯y for R :“ |w|1{2G and ¯u :“ T u
for u P tx, yu. Using the singular value decomposition (3.16) we get that
|x¯x1, ξky|2
2|λk ´ w| ` |w|1{2

|x¯y1, ξky|2
2|λk ´ w| ,

|R¯x1 ¯y1| “

¯y1y

(6.19)

ˇˇˇˇˇx¯x1,|w|1{2
Nÿ
ˇˇˇˇˇx¯x1, w´1{2|w|1{2

k“1

ˇˇˇˇˇ ď |w|1{2
Nÿ
ˇˇˇˇˇ ď Nÿ

¯y2y

k“1

k“1

:
ξkξ
k
λk ´ w
Nÿ

k“1

?
:
λkξkζ
¯k
λk ´ w

?

ˇˇˇrΠxy

ˇˇˇ ď |x||y|. Thus it suﬃces to
Nÿ
2|λk ´ w| ` Nÿ

λk|x¯y2, ζ¯ky|2
2|λk ´ w|

λk|x¯x1, ξky|2

(6.20)

k“1

?

.

k“1

and

|R¯x1 ¯y2| “

Recall the notation ηl in (6.12), deﬁne the subsets of indices

l “ 0, ..., L ` 1,

where we set η´1 “ 0 and ηL`1 “ 8. Now we split the summation in (6.20) according to Ul. For
l “ 1, ..., L we have

ÿ

?

λk|x¯x1, ξky|2
2|λk ´ w| ď

Ul “ tk | ηl´1 ď |λk ´ E| ă ηlu,
ÿ
ÿ

?

E ` ηl|x¯x1, ξky|2ηl
ÿ
2pλk ´ Eq2

ď

kPUl

|E ` iηl|1{2|x¯x1, ξky|2ηl

pλk ´ Eq2 ` η2

l

kPUl

kPUl

ď21{4N 2δ
ÿ

λk|x¯x1, ξky|2
2|λk ´ w| ď

?
E ` η|x¯x1, ξky|2
2rpλk ´ Eq2 ` η2s ď 2´1{4N 2δ

2η

?

kPU0
“2´1{4N δIm R¯x1 ¯x1 pE ` iη1q ;

kPUl

for l “ 0,ÿ

?

kPU0

p2E2 ` 2η2

l q1{4|x¯x1, ξky|2ηl

pλk ´ Eq2 ` η2
l´1
“ 21{4N 2δIm R¯x1 ¯x1 pE ` iηlq ;
ÿ

|E ` iη1|1{2|x¯x1, ξky|2η1

pλk ´ Eq2 ` η2

1

kPU0

where in the second step we used that λk ă 1, which follows from (2.62). Combining the above
estimates we get that

and for l “ L ` 1,ÿ
?
λkx¯x1, ξky2
2|λk ´ w| ď

kPUL`1

ÿ

?

?

λk|x¯x1, ξky|2
2rpλk ´ Eq2 ` η2
Ls

2|λk ´ E|

ÿ

kPUL`1

ă

|x¯x1, ξky|2ηL
pλk ´ Eq2 ` η2

L

kPUL`1

ď CIm R¯x1 ¯x1 pE ` iηLq ,
Lÿ

λk|x¯x1, ξky|2
2|λk ´ w| ă N 2δ

?

Nÿ

?

λk|x¯y2, ζky|2
|λk ´ w|

ă N 2δ

61

Im R¯x1 ¯x1pE ` iηlq.

Im R¯y2 ¯y2pE ` iηlq.

l“1

Lÿ

l“1

k“1

Similarly, we can prove that

Nÿ

k“1

Since |E ` iη| ď |E ` iηl|, we immediately get
x¯u1, ξky2
|λk ´ w| ă N 2δ

Nÿ

k“1

Lÿ

l“1

for u P tx, yu. Plugging into (6.19) and (6.20), we get that

Im R¯u1 ¯u1pE ` iηlq

|w|1{2
Lpηqÿ

l“1

|R¯x1 ¯y1| ` |R¯x1 ¯y2| ă N 2δ

rIm R¯x1 ¯x1pE ` iηlq ` Im R¯y1 ¯y1pE ` iηlq ` Im R¯y2 ¯y2pE ` iηlqs .

We can bound |R¯x2 ¯y1| and |R¯x2 ¯y2| in a similar way. This concludes the proof.
Lemma 6.8. Suppose pAm´1q holds, then

Gpwq ´rΠpwq “ OăpN 2δq

(6.21)

ImGvv ď N 2δ

”
ı
|w|1{2Impm1cpwq ` m2cpwqq ` N C0δΦpwq

for all w PpSm. Moreover for any unit vector v we have
for all w PpSm.
Proof. Let w “ E ` iη P pSm. Then E ` iηl P pSm´1 for l “ 1, . . . , Lpηq, and (6.14) gives that
E ` iη1 PpSm´1,
”
ı
ImGvvpwq ď N δ |w|1{2
|w1|1{2
|w|1{2Impm1cpwq ` m2cpwqq ` N C0δΦpwq

ImGvvpwq ă 1. The estimate (6.21) now follows immediately from Lemma 6.7.
To prove (6.22), we remark that if spwq is the Stieltjes transform of any positive integrable
function on R, the map η ÞÑ ηIm spE ` iηq is nondecreasing and the map η ÞÑ η´1Im spE ` iηq
is nonincreasing. We apply them to |w|´1{2Im GvvpE ` iηq and Im m1,2cpE ` iηq to get for w1 “


„
|w|1{2Impm1cpw1q ` m2cpw1qq ` N C0δ |w|1{2
|w1|1{2

ImGvvpw1q ă N δ

ď N 2δ

Φpw1q

(6.22)

,

where we use Φpwq :“ |w|1{2Ψpwq and the fact that η ÞÑ ΨpE ` iηq is nonincreasing, which is clear
from the deﬁnition (4.8).

6.1.2

Interpolation and expansion

The self-consistent comparison is performed with the following interpolation.
Deﬁnition 6.9 (Interpolating matrices). Introduce the notation X 0 :“ X Gauss and X 1 :“ X. Let
1 and µ P I2 (recall the Deﬁnition
ρ0
iµ and ρ1
2.10). For θ P r0, 1s, we deﬁne the interpolated law

iµ, respectively, for i P IM

iµ be the laws of X 0

iµ and X 1

iµ :“ p1 ´ θqρ0
ρθ
We shall work on the probability space consisting of triples pX 0, X θ, X 1q of independent IM
ź
ź
iµq has law
random matrices, where the matrix X θ “ pX θ
iµpdX θ
ρθ

iµ ` θρ1
iµ.

iµq.

1 ˆ I2

(6.23)

iPIM

1

µPI2

62

For λ P R, i P IM

#

´

1 and µ P I2, we deﬁne the matrix X θ,λpiµq through
if pj, νq ‰ pi, µq
if pj, νq “ pi, µq .
´

X θ
iµ
λ

X θ,λpiµq

:“

jν

¯
`

˘

Gθpwq :“ G

X θ, w

,

Gθ,λpiµqpwq :“ G

X θ,λpiµq, w

,

¯

We also introduce the matrices

(recall (6.1) and the Deﬁnition 2.12.)

We shall prove Lemma 6.6 by interpolation matrices X θ between X 0 and X 1. The bound for

X 0 holds by the following Lemma.
Lemma 6.10. Lemma 6.6 holds if X “ X 0.
Proof. As remarked above (6.5), the anisotropic law (6.3) holds for X 0, i.e. F p
to apply (iii) of Lemma 2.22, we need an upper bound E
Cp. This follows easily from (3.18) and (3.35).

vpX 0, wq
F p

˘

`

vpX 0, wq ă Φp. Now
2 ď N Cp for some constant

Using (6.23) and fundamental calculus, we get the following basic interpolation formula.

Lemma 6.11. For F : RIM

„
ÿ
1 ˆI2 Ñ C we have

ÿ

ˆ

EFpX θq “

d
dθ

iPIM

1

µPI2

˙

ˆ

˙

EF

X

θ,X 1
iµ
piµq

´ EF

X

θ,X 0
iµ
piµq

(6.24)

provided all the expectations exists.

„

We shall apply Lemma 6.11 with FpXq “ F p

work is devoted to prove the following self-consistent estimate for the right-hand side of (6.24).
Lemma 6.12. Fix p P 2N and m ď 2δ´1. Suppose (2.23) and pAm´1q holds, then we have

vpX, wq for FvpX, wq deﬁned in (6.17). The main
˙

ÿ

ÿ

`
for all θ P r0, 1s, all w PpSm, and all deterministic unit vector v.

´ EF p

θ,X 1
iµ
piµq

θ,X 0
iµ
piµq

“ O

EF p

iPIM

µPI2

X

X

v

v

1

˘
vpX θ, wq

pN C0δΦqp ` EF p

ˆ

˙

ˆ

(6.25)

Combining Lemmas 6.10, 6.11 and 6.12 with a Gr¨onwall argument, we can conclude the proof of

Lemma 6.6 and hence the anisotropic law in Theorem 2.20.

In order to prove Lemma 6.12, we compare X

the assumptions of Lemma 6.12,

ÿ

ÿ

”

´

¯

´

θ,X 0
iµ
piµq

¯ı

`

EF p

X

θ,X u
iµ
piµq

´ EF p

X θ,0piµq

“ O

pN C0δΦqp ` EF p

iPIM

for all u P t0, 1u, all θ P r0, 1s, all w PpSm, and all deterministic unit vector v.

µPI2

v

v

1

and X

θ,X 1
iµ
piµq

via a common X θ,0piµq, i.e. under

˘
vpX θ, wq

(6.26)

63

Underlying the proof of (6.26) is an expansion approach which we will describe below. Through-
out the rest of Section 6.1, we suppose that pAm´1q holds. Also the rest of the proof is performed

at a single w PpSm. Deﬁne the I ˆ I matrix ∆λpiµq through

´

¯

´

αk Gθ,λpiµq

V ∆λ´λ1
piµq V

∆λpiµq
Then we have for any λ, λ1 P R and K P N,

st

piµq “ Gθ,λpiµq ` Kÿ

Gθ,λ1

k“1

where

V :“

:“ λδisδµt ` λδitδµs.
¯
: Gθ,λpiµq
˙

k ` αK`1 Gθ,λ1
piµq

, α :“ w1{2
|w|1{2

.

V1
0

0
I

ˆ

(6.27)

´

V ∆λ´λ1
piµq V

: Gθ,λpiµq

¯

K`1

,

(6.28)

The following result provides a priori bounds for the entries of Gθ,λpiµq.
Lemma 6.13. Suppose that y is a random variable satisfying |y| ă N´1{2. Then

Gθ,ypiµq ´rΠ “ OăpN 2δq

(6.29)

for all i P IM

1 and µ P I 2.

Proof. It suﬃces to show that Gθ,ypiµq “ OăpN 2δq since rΠ “ Oăp1q. By assumption (Am´1), the

piµq “
Lemma 6.8 holds for the matrix ensemble X θ (since it satisﬁes (2.2)-(2.4)). In particular G
iµ, λ1 :“ y and large enough K such that
OăpN 2δq. Now we apply the expansion (6.28) with λ :“ X θ
Kp2δ ´ 1{2q ď ´2. Using |λ ´ λ1| ă N´1{2, it is easy to estimate all the terms in (6.28) except
the rest term. To handle the rest term, we use the rough bound Gθ,λ1
piµq ă N 2 coming from a simple
modiﬁcation of (3.18).

θ,X u
iµ

In the following, for simplicity of notations we introduce fpiµqpλq :“ F p

pnq
piµq to
denote the n-th derivative of fpiµq. By Lemma 6.13 and expansion (6.28) we get the following result.
Lemma 6.14. Suppose that y is a random variable satisfying |y| ă N´1{2. Then for any ﬁxed n P N
we have

vpX θ,λpiµqq. We use f

ˇˇˇ ă N 2δpn`pq.

By this lemma, the Taylor expansion of fpiµq gives

pnq
piµqp0q ` OăpΦpq,

f

´

¯

¯

´

provided C0 is chosen large enough (see (6.11)). Therefore we have for u P t0, 1u,
`
EF p

´ EF p

θ,X u
iµ
piµq

X θ,0piµq

X

´ fpiµqp0q
p2q

v

v

X u
iµ

fpiµq

“ E
“ E fpiµqp0q ` 1
2N

‰
piµqp0q ` 4pÿ

E f

pnq
piµqp0q E

1
n!

E f

n“4

(6.30)

(6.31)

˘

X u
iµ

n ` OăpΦpq

pnq
piµqpyq

ˇˇˇf
fpiµqpyq “ 4pÿ
“
`

n“0

yn
n!

˘

64

iµ has vanishing ﬁrst and third moments and its variance is 1{N . Thus to

pnq
piµqp0q

pN C0δΦqp ` EF p

vpX θ, wq

(6.32)

where we used that X u
show (6.26), we only need to prove

ÿ

ÿ

ˇˇˇE f

N´n{2

ˇˇˇ “ O

`

µPI2
for n “ 4, 5, ..., 4p. Here we use X u
iµ ă N´1{2 due to (2.4).

iPIM

1

˘

In order to get a self-consistent estimate in terms of the matrix X θ on the right-hand side of

(6.32), we want to replace X θ,0piµq in fpiµqp0q :“ F p

vpX θ,0piµqq with X θ “ X

θ,X θpiµq
piµq

. We have the following

lemma.

Lemma 6.15. Suppose that

ÿ

ÿ

iPIM

1

µPI2

N´n{2

ˇˇˇE f

ˇˇˇ “ O

pnq
piµqpX θ
iµq

`

˘

pN C0δΦqp ` EF p

vpX θ, wq

holds for n “ 4, ..., 4p, Then (6.32) holds for n “ 4, ..., 4p.

Proof. From (6.31) we can get

piµqpyq ´ 4p´lÿ

plq

n“1

plq
piµqp0q “ f

f

pl`nq
piµq p0q ` OăpN l{2Φpq.

f

yn
n!

(6.33)

(6.34)

The result follows by repeatedly applying (6.34). The details can be found in [24, Lemma 7.16].

6.1.3 Conclusion of the proof with words

What remains now is to prove (6.33). In order to exploit the detailed structure of the derivatives
on the left-hand side of (6.33), we introduce the following algebraic objects.
Deﬁnition 6.16 (Words). Given i P IM
1 and µ P I2. Let W be the set of words of even length in
two letters ti, µu. We denote the length of a word w P W by 2npwq with npwq P N. We use bold
symbols to denote the letters of words, e.g. w “ t1s2t2s3 ¨¨¨ tnsn`1 denotes a word of length 2n.
Deﬁne W n :“ tw P W : npwq “ nu to be the subset of words of length 2n. We require that each word
w P W n satisﬁes that tlsl`1 P tiµ, µiu for all 1 ď l ď n.

Next we assign each letter ˚ its value r˚s through

ris :“ vi,

rµs :“ µ,

where vi P CI1 is deﬁned in Lemma 6.1 and is regarded as a summation index. Here it is important
to distinguish the abstract letter from its value, which is a summation index. Finally, to each word
w we assign a random variable Av,i,µpwq as follows. If npwq “ 0 we deﬁne

Av,i,µpWq :“ Gvv ´rΠvv.

If npwq ě 1, say w “ t1s2t2s3 ¨¨¨ tnsn`1, we deﬁne

Av,i,µpWq :“ Gvrt1s Grs2srt2s ¨¨¨Grsnsrtns Grsn`1s v .

(6.35)

65

ÿ

wPWn

Av,i,µpwq

ˆ

˙

Notice the words are constructed such that, by (6.28),
“ p´αqnn!
p{2ź

for n “ 0, 1, 2, . . ., which gives that

B
BXiµ

ˆ

˙

n

B
BXiµ

vpXq “ p´αqnn!
F p

¯

n´
Gvv ´rΠvv
ÿ
¨˝ ÿ

n1`¨¨¨`np“n
ˆ

wrPW nr

wr`p{2PW nr`p{2

Then to prove (6.33), it suﬃces to show that

nr!nr`p{2!

1

r“1

ÿ
ˇˇˇˇˇˇ “ O
ˇˇˇˇˇ “ O
`
ˇˇˇˇˇ “ O

`

Av,i,µpwrqAv,i,µpwr`p{2q

ÿ

ÿ

iPIM

1

µPI2

N´n{2

p{2ź

r“1

ˇˇˇˇˇˇE
ÿ

r“1

pź

ˇˇˇˇˇE
ÿ
ˇˇˇˇˇAp´q
v,i,µpw0q qź

r“1

E

N´n{2

iPIM

1

µPI2

ÿ

ÿ

iPIM

1

µPI2

N´n{2

˛‚.

(6.36)

Av,i,µpwrqAv,i,µpwr`p{2q
˘
vpX θ, wq

pN C0δΦqp ` EF p

`

for 4 ď n ď 4p and all words w1, ..., wp P W satisfying npw1q ` ¨¨¨ ` npwpq “ n. To avoid the
unimportant notational complications coming from the complex conjugates, we in fact prove that

Av,i,µpwrq

pN C0δΦqp ` EF p

,

(6.37)

˘
vpX θ, wq

and the proof of p6.36q is essentially the same but with slightly heavier notations. Treating empty
words separately, we ﬁnd it suﬃces to prove

˘
vpX θ, wq

(6.38)

Av,i,µpwrq

pN C0δΦqp ` EF p
ř

for 4 ď n ď 4p, 1 ď q ď p, and words wr such that npw0q “ 0,
r ě 1.

r npwrq “ n and npwrq ě 1 for

To estimate (6.38) we introduce the quantity

Rs :“ |Gvvs| ` |Gvsv|.
for s P I, where as a convention we let vµ “ eµ for µ P I 2.
Lemma 6.17. For w P W we have the rough bound

|Av,i,µpwq| ă N 2δpnpwq`1q.

Furthermore, for npwq ě 1 we have

|Av,i,µpwq| ă pR2

i ` R2

µqN 2δpnpwq´1q.

For npwq “ 1 we have better bound

|Av,i,µpwq| ă RiRµ.

66

(6.39)

(6.40)

(6.41)

(6.42)

Proof. (6.40) follows immediately from the rough bound (6.21) and deﬁnition (6.35). For (6.41) we
break Av,i,µpwq into Gvrt1spGrs2srt2s ¨¨¨Grsnsrtnsq1{2 times pGrs2srt2s ¨¨¨Grsnsrtnsq1{2 Grsn`1s v and use
Cauchy-Schwarz. (6.42) follows from the constraint t1 ‰ s2 in the deﬁnition (6.35).

Therefore by Lemma 6.17 we have

By pigeonhole principle, if n ď 2q ´ 2 there exists at least two words wr with npwrq “ 1.
˘

`

pXq

1pn ě 2q ´ 1qpR2

i ` R2

µq ` 1pn ď 2q ´ 2qR2

ˇˇˇˇˇAp´q
v,i,µpw0q qź
ÿ

r“1

ˇˇˇˇˇ ă N 2δpn`qqF p´q
Av,i,µpwrq
ÿ

v

(6.44)
where in the second step we used the two bounds in Lemma 6.8 and |w|´1{2η “ Op|w|Im m1cq by
Lemma 3.8, and in the last step the deﬁnition of Φ. Using the same method we can get

N η

1

N

N η

1
N

µPI2

iPIM

i ` 1
R2

|w|1{2ImGvv `η|w|´1{2 Gvv

Then using Lemma 6.1 we get
R2
µ ă
ă N 2δ |w|Impm1c ` m2cq ` |w|1{2N C0δΦ
¯
ÿ
ˆ

NpC0`2qδΦ2

R2
iR2

ÿ

1
N 2

iPIM

µPI2

µ ă

´

2

.

1

Plugging (6.44) and (6.45) into (6.43), we get that the left hand side of (6.38) is bounded by

ă NpC0`2qδΦ2,

˙

(6.45)

.

µ

iR2
(6.43)

´

´

¯

v

N C0δ{2Φ

pXq
ˆ

N´n{2`2N 2δpn`q`2q E F p´q

2 ` 1pn ď 2q ´ 2q
¯
´
Using that Φ ě cN´1{2, we ﬁnd that the left hand side of (6.38) is bounded by
¯
´
n´2 ` 1pn ď 2q ´ 2q
n´2 ` 1pn ď 2q ´ 2q

1pn ě 2q ´ 1q
´

N 2δpn`q`2q E F p´q

1pn ě 2q ´ 1q

1pn ě 2q ´ 1q

N C0δ{2`12δΦ

ď E F p´q

N C0δ{2Φ

pXq

pXq

ˆ

´

v

v

N C0δ{2Φ

N C0δ{2`12δΦ

N C0δ{2Φ

4

.

¯
˙
˙

¯
¯

n

n

where we used that q ď n and n ě 4. Choose C0 ě 25, then by assumption (6.11) on δ we have
N C0δ{2`12δ ď N τ{2 and hence N C0δ{2`12δΦ ď 1. Moreover, if n ě 4 and n ě 2q ´ 1, then n ě q ` 2.
Therefore we conclude that the left hand side of p6.38q is bounded by

`

˘q

E F p´q

v

pXq

N C0δΦ

.

(6.46)

Now (6.38) follows from Holder’s inequality. This concludes the proof of (6.33), and hence of (6.26),
and then of Lemma 6.5. The ﬁnishes the proof of the anisotropic local law in Theorem 2.20.

6.2 Averaged local law for T X

In this section we prove the averaged local law in Theorem 2.20. Again we only consider the case
|z|2 ď 1´ τ for simplicity. The anisotropic local law proved above gives a good priori bound for the
proof. In analogy to (6.17) we deﬁne

rFpX, wq : “ |w|1{2|m2pwq ´ m2cpwq| “

Gννpwq ´ |w|1{2m2cpwq

ˇˇˇˇˇ 1

N

ÿ

νPI2

67

ˇˇˇˇˇ .

Since Φ2 “ Op|w|1{2{pN ηqq, it suﬃces to show that rF ă Φ2. Following the argument in Section 6.1,

analogous to (6.33), it is enough to prove that

ÿ

ÿ

iPIM

1

µPI2

N´n{2

˙

n rF ppXq

B
BXiµ

ˆ

ˇˇˇˇE
˜
pź

¯

ˇˇˇˇ “ O
´
pN δΦ2qp ` ErF ppXq
¸ˇˇˇˇˇ “ O

´

ÿ

for all n “ 4, ..., 4p. Here δ ą 0 is an arbitrary positive constant. Analogously to (6.37), it suﬃces
ÿ
to prove that for n “ 4, ..., 4p,
ř
r npwrq “ n. The only diﬀerence in the deﬁnition of Av,i,µpwq is that when npwq “ 0, we deﬁne

¯
pN δΦ2qp ` ErF ppXq

Aeν ,i,µpwrq

ˇˇˇˇˇE

N´n{2

ÿ

(6.48)

iPIM

νPI2

µPI2

1
N

r“1

for

1

(6.47)

Av,i,µpwq :“ Gvv ´|w|1{2m2c.

Similar to (6.39) we deﬁne

By the anisotropic local law, we have G ´rΠ “ OăpΦq. Hence combining with Lemma 6.1 and (6.16),

Rν,s :“ |Gνvs| ` |Gvsν|.

(6.49)

we get

1
N

R2
ν,s ă

|w|1{2ImGvsvs

ă

N η

Using the anisotropic local law again, we have G “ Oăp1q. Then we have that

“ OpΦ2q.

(6.50)

Aeν ,i,µpwq

ă Φ2 for npwq ě 1.

ν,µ

(6.51)

νPI2

ÿ
ˇˇˇˇˇ 1

N

ÿ

νPI2

N η

|w|Impm1c ` m2cq ` |w|1{2Φ
`

˘

ÿ

ˇˇˇˇˇ ă 1
ErF p´qpXqpΦ2qq.

ν,i ` R2
R2

νPI2

N

Following (6.51), for n ě 4, the left-hand side of (6.48) is bounded by

Applying Holder’s inequality, we conclude the proof.

A Properties of ρ1,2c and Stability of (2.12)

This appendix is devoted to the proofs of Lemma 2.3 and Proposition 2.15, which give the basic
properties of ρ1,2c. Then we will prove Lemmas 3.8 and 3.11, which establish the properties of
m1,2c, and the stability of (2.12) on each of the subdomains Db
k under the regularity
assumptions from Deﬁnition 2.4. We get the ideas of our proofs from [24].

k, Do and De

A.1 Proof of Lemma 2.3 and Proposition 2.15

We now prove Lemma 2.3. First is a technical lemma for f deﬁned in (2.18).

68

ˆ
Lemma A.1. For w ą 0 and |z| ą 0, f can be written as

fp?

w, mq “ ´?

w ` m ` w´1{2 ` 1
N

nÿ

lisi

i“1

˙

,

(A.1)

Ai
m ´ ai

` Bi
m ´ bi

` Ci
m ` ci

where we have the following estimates for the roots and the coeﬃcients

ˆ

˙

w

max

|z|,

ˆ

˙
ă ai ă si ` |z|2?
si ` |z|2?
` |z|, an ă an´1 ă . . . ă a1,
w
a
|z|2?
|z|,
w
psi ` |z|2q2 ` 4w|z|2
?
ă ci ă |z|,
2

0 ă b1 ă b2 ă . . . ă bn ă min
´psi ` |z|2q `

c1 ă c2 ă . . . ă cn,

w

,

(A.2)

(A.3)

(A.4)

and

0 ă Ai ď 2

si ` |z|2 ` ?

w|z|

w

, 0 ă Bi ď 2

si ` |z|2 ` ?

w|z|

w

, 0 ă Ci ď si ` |z|2 ` ?

w

w|z|

.

(A.5)

Proof. The proof is based on basic algebraic arguments. Let

pi “ ?

wm3 ´ psi ` |z|2qm2 ´ ?

w|z|2m ` |z|4.

Then the ∆ of pi is

∆ “ 18psi ` |z|2qw|z|6 ` 4psi ` |z|2q3|z|4 ` psi ` |z|2q2w|z|4 ` 4w2|z|6 ´ 27w|z|8,

where the si-independent part is

18w|z|8 ` 4|z|10 ` w|z|8 ` 4w2|z|6 ´ 27w|z|8 “ 4|z|6p|z|2 ´ wq2 ě 0.

All other parts of ∆ are positive, so ∆ ą 0. Thus pi has three distince real roots, and by its form
we claim that there are two positive roots and one negative root. We call them ai ą bi ą 0 ą ´ci,
which satisfy

ai ` bi ´ ci “ si ` |z|2?

, aibi ` aici ` bici “ |z|2, aibici “ |z|4?

It is easy to see that pi and pj cannot have common roots for i ‰ j. Otherwise, we should have
psi´sjqm2 “ 0, but 0 is not a root for either pi or pj. Now we perform the partial fraction expansion
for the rational functions in (2.18),

.

w

w

?

m2 ´ |z|2
wm3 ´ psi ` |z|2qm2 ´ ?

w|z|2m ` |z|4 “ A1
m ´ ai

i

` B1
m ´ bi

i

´ C1
m ` ci

i

,

where

i “
A1

?

i ´ |z|2
a2

i “
wpai ´ biqpai ` ciq , B1

?

We take si “ 0 in pi and call the resulting polynomial as
w|z|2m ` |z|4 “ ?

wm3 ´ |z|2m2 ´ ?

p0 “ ?

w

i ´ |z|2
b2

i “
wpbi ´ aiqpbi ` ciq , C1
ˆ

?

´c2

i ` |z|2

wpci ` aiqpci ` biq .
˘

˙`

m2 ´ |z|2

,

m ´ |z|2?

w

(A.6)

(A.7)

69

which has roots m “ ˘|z|,|z|2{?
ˆ
Comparing the graphs of pi’s (as cubic functions of m) for 0 ď i ď n, we get that
|z|,

ă an ă an´1 ă . . . ă a1, 0 ă b1 ă b2 ă . . . ă bn ă min

|z|2?

˙

ˆ

|z|,

w. By (2.8), we have p1 ă p2 ă . . . ă pn ă p0 for all m ‰ 0.

,

(A.8)

˙

max

|z|2?

w

w

(A.9)

i ` |z|2 ą 0,

and

0 ă c1 ă c2 ă . . . ă cn ă |z|.

i ´ |z|2 ă 0 and ´c2

i ´ |z|2 ą 0, b2

ˆ
˙
i ą 0. Plugging (A.6) into f we get
˙
ˆ

` C,

lisi

i“1

fp?

nÿ
i ą 0 and C1
nÿ

Thus we get (A.3). Using these bounds, we get that a2
i ą 0, B1
which, by (A.7), give that A1
w, mq “ ´?
w ` m ` 1
N
“ ´?
w ` m ` 1
ř
N
ibi, Ci “ C1
ici and C “ N´1
i“1 lisiw´1{2 “ w´1{2.
i :“ ?
p1
a
psi ` |z|2q2 ` 4w|z|2
?
2

A1
` B1
im
im
m ´ ai
m ´ bi
` Bi
Ai
ř
m ´ ai
m ´ bi
i“1 lisipA1

where Ai “ A1
(2.18), we see that C “ N´1

To get more precise bounds for ai and ci, we compare pi with

iai, Bi “ B1

which has roots

w|z|2m,

m “ 0,

i“1

lisi

w

n

n

i ` B1

.

´ C1
im
m ` ci
` Ci
m ` ci
i ` C1

Since p1

i ă pi for all m, we get

wm3 ´ psi ` |z|2qm2 ´ ?
psi ` |z|2q ˘
a
psi ` |z|2q2 ` 4w|z|2
?
2

w

ai ă psi ` |z|2q `

ă si ` |z|2?

` |z|,

(A.10)

iq. By the form of f in

w

a
psi ` |z|2q2 ` 4w|z|2
?
2

w

and

ci ą ´psi ` |z|2q `

.

(A.11)

Combining (A.9) and (A.11), we get (A.4). Finally, we compare pi with

i ą pi for m ą |z|2{?

where p2
and ai ą |z|2{?

w, we get

wm3 ´ psi ` |z|2qm2,

i :“ ?
p2
i ă pi for m ă |z|2{?
w and p2
ai ą si ` |z|2?

.

w

w. Since p2

i has roots w “ 0, psi`|z|2q{?

w

(A.12)

Combining (A.8), (A.10) and (A.12), we get (A.2).

Now we estimate the coeﬃcients Ai, Bi and Ci. Using (A.7) and (A.2)-(A.4), we ﬁrst can

estimate that

i “ pai ´ |z|qpai ` |z|q
A1

wpai ´ biqpai ` ciq ď ai ` |z|

wpai ` ciq ď 2?

?

?

w

,

70

from which we get that

w|z|

,

?

?

?

wpai ´ biqpbi ` ciq ď |z| ` bi
?
wpci ` aiqpci ` biq ď |z| ´ ci
˙

i “ p|z| ` biqp|z| ´ biq
B1
i “ p|z| ´ ciqpci ` |z|q
C1
ˆ
si ` |z|2?
iai ď 2?
si ` |z|2 ` ?
ibi ď 2
w|z|
ici ď si ` |z|2 ` ?
w|z|
w|z|

si ` |z|2 ` ?
wpbi ` ciq ď 2
w|z|
wpci ` biq ď si ` |z|2 ` ?
w|z|
w|z|
si ` |z|2 ` ?
“ 2
` |z|
si ` |z|2 ` ?
w|z|
|z| “ 2
|z| “ si ` |z|2 ` ?
w|z|

Ai “ A1
Bi “ B1
Ci “ C1

w|z|

w|z|

w

w

w

w

w

,

,

.

,

(A.13)

(A.14)

(A.15)

In (A.1), it is sometimes convenient to reorder the terms and rename the constants to write f as

(A.16)

(A.17)

2nÿ

k“1

nÿ

l“1

fpmq “ ´?

w ` m ` w´1{2 ` 1
N

C`
m ´ xk

k

` 1
N

C´
m ` yl

l

.

where all the constants C`

k and C´
0 ă x1 ă x2 ă . . . ă x2n, 0 ă y1 ă y2 ă . . . ă yn.

l are positive, and we choose the order such that

Clearly, f is smooth on the n ` 1 open intervals of R deﬁned by

I´n :“ p´8,´ynq, I´k :“ p´yk`1,´ykq pk “ 1, . . . , n ´ 1q, I0 :“ p´y1, x1q,

and

Ik :“ pxk, xk`1q pk “ 1, . . . , 2n ´ 1q, I2n :“ px2n,`8q.

f1pmq “ 1 ´ 1
N

Next, we introduce the multiset C of critical points of f , using the conventions that a nondegenerate
critical point is counted once and a degenerated critical point twice. First we will prove the following
elementary lemma about the structure of C (see Fig. 6 and 7).
Lemma A.2. (Critical points) We have |C X I´n| “ |C X I2n| “ 1 and |C X Ik| P t0, 2u for k “
´n ` 1, . . . , 2n ´ 1.
2nÿ
Proof. We omit the dependence of f on w for now. By (A.16) we have

2nÿ

nÿ

nÿ

N

N

l“1

k“1

k“1

pm ` ylq3 .
We see that f2 is decreasing on all the intervals Ik for k “ ´n ` 1, . . . , 2n ´ 1. Thus there is
at most one point m P Ik such that f2pmq “ 0. We conclude that Ik has at most two critical
points of f . Using the boundary conditions of f1 on BIk, we conclude that |C X Ik| P t0, 2u for
k “ ´n ` 1, . . . , 2n ´ 1. For x ă ´yn, we have f2pmq ă 0, while for m ą x2n, we have f2pmq ą 0.
By the boundary conditions of f1 on BI´n and BI2n, we see that f1 decreases from 1 to ´8 when
m increases from ´8 to ´yn, while f1 increases from ´8 to 1 when m increases from x2n to `8.
Hence we conclude that each of the intervals p´8,´ynq and px2n,`8q contains a unique critical
point in it, i.e. |C X I´n| “ |C X I2n| “ 1.

l“1

N

C´
pm ` ylq2 , f2pmq “ 1

l

2C`
pm ´ xkq3 ` 1

k

C`

pm ´ xkq2 ´ 1

k

2C´

l

71

w, C0pτ´1|w|´1{2 ` |z|q ´ ?

From this lemma, we deduce that |C| “ 2p is even. We denote by z2p the critical point in I´n, z1
the critical point in I2n, and z2 ě . . . ě z2p´1 the 2p ´ 2 critical points in I´n`1 Y . . . Y I2n´1. For
k “ 1, . . . , 2p, we deﬁne the critical values hk :“ fpzkq. The next lemma is crucial in establishing
the basic properties of ρ1c (see e.g. Fig. 6).
Lemma A.3. (Orderings of the critical values) The critical values are ordered as h1 ě h2 ě
. . . ě h2p. Furthermore, there is an absolute constant C0 ą 0 independent of τ such that hk P
r´C0pτ´1|w|´1{2 ` |z|q ´ ?
Proof. Notice for the equation (2.17), if we multiply both sides with the product of all denominators
in f , we get a polynomial equation Pwpmq “ 0 with Pw being a polynomial of degree 3n ` 1. An
immediate consequence is that for any ﬁxed w ą 0 and E P R, fp?
w, mq “ E can have at most
3n ` 1 roots in m. This fact is useful in the proof of this lemma and Lemma 2.3.
For i “ ´n, . . . , 2n, deﬁne the subset Jipwq :“ tm P Ii : Bmfp?
w, mq ą 0u. From Lemma A.2,
we deduce that if i “ ´n ` 1, . . . , 2n ´ 1, then Ji ‰ H iﬀ Ii contains two distinct critical points of
f , in which case Ji is an interval. Moreover, we always have J´n, J2n ‰ H. Next, we observe that
for any ´n ď i ă j ď 2n, we have fpJiq X fpJjq “ H. Otherwise if there were E P fpJiq X fpJjq,
then we will have |tx : fpxq “ Eu| ą 3n ` 1. We hence conclude that the sets fpJiq, ´n ď i ď 2n
can be strictly ordered. The claim h1 ě h2 ě . . . ě h2p is now reformulated as

ws for k “ 1, . . . , 2p.

fpJiq ă fpJjq whenever i ă j and Ji, Jj ‰ H.
nÿ

To prove (A.18), we use a continuity argument. Let t P p0, 1s and introduce
C´
m ` yl

w ` m ` w´1{2 ` t
N

f tpmq “ ´?

C`
m ´ xk

` t
N

2nÿ

k

l

k“1

l“1

(A.18)

.

It is easy to check (A.18) holds for small enough t ą 0. First we claim that

Ji ‰ H ñ J t

i ‰ H for all t P p0, 1s.

(A.19)

i ‰ H is equivalent
This is trivial for i “ ´n, 2n. We remark that if ´n ` 1 ď i ď 2n ´ 1, then J t
to Ii containing two distinct critical points. Moreover, BtBmf tpmq ă 0 in I´n`1 Y . . . Y I2n´1, from
which we deduce that the number of distinct critical points in each Ii, i “ ´n ` 1, . . . , 2n ´ 1, does
not decreases as t decreases. This proves (A.19).
Next, suppose that there exist i ă j such that Ji, Jj ‰ H and fpJiq ą fpJjq. From (A.19), we
iq ą
jq for all t P p0, 1s. However, this is impossible for small enough t as explained before (A.19).
To prove the second statement of Lemma A.3, we only need to show that h1 ď C0pτ´1|w|´1{2 `
w for some absolute constant C0. We only give the

deduce that J t
f tpJ t
This concludes the proof of (A.18).
|z|q ´ ?

j ‰ H for all t P p0, 1s. By a simple continuity argument, we get that f tpJ t

i , J t

proof for h1; the proof for h2p is similar. At z1, we have

w and h2p ě ´C0pτ´1|w|´1{2 ` |z|q ´ ?
2nÿ
C`
2nÿ
z1 ´ xk
C`
pz1 ´ xkq2 ` 1

«
w “ z1 ` w´1{2 ` 1
N
1 ` 1
N

fpz1q ` ?

` 1
N

ď pz1 ` ynq
k“1
“ 2pz1 ` ynq ` w´1{2,

nÿ

k“1

l“1

k

k

l

C´
nÿ
z1 ` yl

C´

l

pz1 ` ylq2

N

l“1

ﬀ

` w´1{2

(A.20)

72

2nÿ

k“1

nÿ

l“1

where we use

0 “ f1pz1q “ 1 ´ 1
N

C`

pz1 ´ xkq2 ´ 1

N

k

C´

l

pz1 ` ylq2 .

Now we would like to estimate z1 ` yn. Again using (A.21), we have that
pz1 ´ x2nq2 ě 1.

nÿ

2nÿ

C´

C`

1
N

k

l

k“1

(A.21)

w|z|

.

Then using (A.5) we get

gffe 1

2nÿ

k ` 1
C`

N

N

k“1

z1 ´ x2n ď

Finally we get

fpz1q ď 2pz1 ` ynq ` w´1{2 ´ ?

si ` |z|2 ` ?

N

l“1

pz1 ´ x2nq2 ` 1
nÿ
nÿ

gffe5
l ď
C´
˜c
w|z|
w ď 2
ď C0pτ´1|w|´1{2 ` |z|q ´ ?

1 ` |z|2 ` ?

1
N

i“1

w

li

5

w

l“1

w

c

5

w|z|

“

1 ` |z|2 ` ?
¸

w

` s1 ` |z|2?

w

` 2|z|

` w´1{2 ´ ?

w

Ť

2n

w, mq “ 0u has 3n` 1 points. Since fp?

for some absolute constant C0.
i“´n fpJipwqq “ fpJpwqq, then the set tm P R :
Proof of Lemma 2.3. Given w ą 0 such that 0 P
fp?
w, mq “ 0 has at most 3n` 1 solutions in m, we deduce
that mcpwq is real and hence m1cpwq is also real. Since m1c is the Stieltjes transform of ρ1c, we
conclude that w R supp ρ1c. On the other hand, suppose w ą 0 and 0 R f pJpwqq. Then the set
of preimages tm P R : fp?
w, mq “ 0u “ tm P R : Pwpmq “ 0u has 3n ´ 1 points. Since Pwpmq
is a degree 3n ` 1 polynomial with real coeﬃcients, we conclude that Pw has a unique root with
positive imaginary part. By the uniqueness of the solution of Pw`iη in C` (see Lemma 2.2) and the
continuity of the roots of Pw`iη in η, we conclude that Im mcpwq ą 0 (and hence Im m1cpwq ą 0)
by taking η Œ 0, i.e. w P supp ρ1c. In sum, we get that

supp ρ1c “ tw ą 0 : 0 R f pJpwqqu.

(A.22)
From Lemma A.3, we see that there exists an absolute constant C1 ą 0 such that if w ě C1τ´1,
then h1pωq ď C0pτ´1|w|´1{2 ` |z|q ´ ?
w ă 0. Hence ﬁx w ě C1τ´1, we have 0 P fpJ2npwqq and
w R supp ρ1c (see the upper graphs in Fig. 6 and 7). This shows that ρ1c is compactly supported in
r0, C1τ´1s. Now we decrease w so that w ă s1 ` |z|2 ` 1, then using (A.2),
ą 0.

?

By continuity, there must be some 0 ă w ă Cτ´1 such that 0 R f pJpwqq. Thus supp ρ1c ‰ H. From
(A.22), it is easy to see that supp ρ1c is a disjoint union of (countably many) closed intervals,

h1pwq ą z1 ` w´1{2 ´ ?
w ą s1 ` |z|2 ` 1 ´ w
ď
re2k, e2k´1s,

supp ρ1c “

w

(A.23)

k

73

Figure 6: The graphs of fp?
2{17.
We take |z| “ 1.5, and w “ 10 and 0.01 in the upper and lower graphs, respectively. In the lower
graph, we only plot the ﬁve branches near m “ 0, while the remaining two branches are pushed far
away.

w, mq for the example from Figure 1, i.e. ρΣ “ 0.5δ?

?
2{17 ` 0.5δ4

fp?

ei, mcpeiqq “ 0, Bmfp?

ei, mcpeiqq “ 0.

where C1τ´1 ě e1 ě e2 ě . . .. Furthermore, for ei to be a boundary point, we must have that 0 is
a critical value of fp?

ei, mq, i.e. there is a unique critical point m “ mcpeiq such that

(A.24)
Notice the two equations in (A.24) are equivalent to two polynomial equations in p?
w, mq with
order 3n ` 1 and 6n, respectively. By B´ezout’s theorem, there are at most ﬁnitely many solutions
to (A.24). Hence there are ﬁnitely many ei’s, call them e1 ě e2 ě . . . ě e2L, where L may depend
on n.

To prove the statement about e2L, we use Lemma A.4 below. This concludes the proof of Lemma

2.3.
Lemma A.4. If 1 ` τ ď |z|2 ď 1 ` τ´1, there is a constant pτq ą 0 so that e2L ě pτq.
|z|2 ď 1 ´ τ , e2L “ 0 and ρ1cpxq „ x´1{2 when x Œ 0.

If

74

m-50510f (w1/2, m)-10-50510m-4-202468f (w1/2, m)-505Figure 7: The graphs of fp?
2{17.
We take |z| “ 0.5, and w “ 6 and 0.01 in the upper and lower graphs, respectively. In the lower
graph, we only plot the ﬁve branches near m “ 0, while the remaining two branches are pushed far
away.

w, mq for the example from Figure 1, i.e. ρΣ “ 0.5δ?

?
2{17 ` 0.5δ4

ř

Proof. By this lemma, the behavior of the leftmost edge e2L changes essentially when z crosses the
unit circle. From the following proof, we see that the singularity happens at |z|2 “ N´1
n
i“1 lisi.
Thus the fact that the singular circle has radius 1 comes from our normalization (2.6) for T .
We ﬁrst study equation (2.17) when w Œ 0 in the case 1 ` τ ď |z|2 ď 1 ` τ´1. We calculate the

derivative of f as

?

lisi

Bmfp?

m2 ´ |z|2
˘
wm3 ´ psi ` |z|2qm2 ´ ?
r?
wm3 ´ psi ` |z|2qm2 ´ ?
It is easy to see that J0 ‰ H, since Bmfp?
w, 0q “ 1 ´ |z|´2 ą 0. Call the end points of J0 as

w, mq “ 1 ` 1
N
´ m
N

w|z|2m ` |z|4s2 .

w|z|2m ` |z|4

m2 ´ |z|2

2 ` 2si|z|2m

(A.25)

lisi

nÿ
nÿ

i“1

i“1

`

?

w

75

m-3-2-10123456f (w1/2, m)-505m-5-4-3-2-10123456f (w1/2, m)-505zkpwq ą 0 and zk`1pwq ă 0. By the deﬁnition of I0, we have zk ă b1 ă |z|. Suppose zk “ op|z|q as
w Ñ 0, then (A.25) gives that

which is a contradiction. Thus we must have zk „ |z| as w Ñ 0. Now we can estimate fp?

0 “ 1 ´ |z|´2 ` op1q,
nÿ

fp?

w, zkq “ ´?
“ ´?
ě ´?

w ` zk ` zk
nÿ
N
nÿ

w ` z2
k
N
w ` 1
N

i“1

i“1

?

k ´ |z|2
`
˘
k ´ ?
z2
k ´ psi ` |z|2qz2
wz3
?
2 ` 2si|z|2zk
k ´ |z|2
z2
k ´ ?
k ´ psi ` |z|2qz2
ě C ´ ?

w

w

lisi

i“1
r?
wz3
2si|z|2z3
|z|8

k

lisi

lisi

w|z|2zk ` |z|4

w|z|2zk ` |z|4s2

for some C ą 0 independent of w. Here in the second step we use Bmfp?

step we use that

?

k ´ psi ` |z|2qz2

wz3

k ´ ?

w|z|2zk ` |z|4 ą 0, and

?

k ´ psi ` |z|2qz2

wz3

w, zkq “ 0, and in the third
k ´ ?

w|z|2zk ă 0

which come from that 0 ă zk ă bi for all 1 ď i ď n. Thus we can ﬁnd  small enough such that
fp?
w, zkq ą 0 for all 0 ă w ď . In this case 0 P fpJ0pwqq and hence w R supp ρ1c. In fact, it is not
hard to see that there is a solution m0 “ ?
w, m0q “ 0
and Bmfp?
Now we study equation (2.17) when |z|2 ď 1 ´ τ and w Ñ 0. For later purpose, we allow w to
be complex and prove a more general result than what we need for this lemma. Let w “ 0 in the
equation (2.17), we get

wq P I0 such that fp?

w|z|2{p|z|2 ´ 1q ` op?

w, m0q ą 0.

(A.26)
w, zkq as

(A.27)

(A.28)

(A.29)

ˆ

˙
|z|4
sn ` |z|2 ,8

.

0 “ m ` m
N

0 “ 1 ` 1
N

m2 ´ |z|2

´psi ` |z|2qm2 ` |z|4 ,

lisi

m2 ´ |z|2

´psi ` |z|2qm2 ` |z|4 .

lisi

gpxq “ 1 ` 1
N

x ´ |z|2

´psi ` |z|2qx ` |z|4 .

lisi

nÿ

i“1

nÿ
nÿ

i“1

i“1

nÿ

which gives that m “ 0 or

We deﬁne

It is easy to calculate that

ˆ

g1pxq “ ´ 1
N
ˆ

˙

lisi

i“1

rpsi ` |z|2qx ´ |z|4s2 .

si|z|2
˙

Thus g is decreasing on the intervals deﬁned through
|z|4
K1 :“

, Ki :“

´8,

|z|4

|z|4

si´1 ` |z|2 ,

si ` |z|2

s1 ` |z|2

pi “ 2, . . . , nq, Kn`1 :“

76

By the boundary values of g on these intervals, we see that gpxq has exactly one zero on intervals
Ki for i “ 1, . . . , n, and has no zero on Kn`1. Since gpxq “ 0 is equivalent to a polynomial equation
of order n, it has at most n solutions. We conclude that all of its solutions are real. Obviously the
zeros on the intervals Ki are positive for i “ 2, . . . , n. Now we study the zero on K1. Observe that
gp0q “ 1 ´ |z|´2 ă 0 (as |z|2 ď 1 ´ τ ), the zero on K1 is negative, call it ´t. If |z|2 ě τ{2, then by
the concavity of g on the K1, we get

t ě gp0q

g1p0q ě |z|4p1 ´ |z|2q

s1

ě τ 4
4

.

(A.30)

In the case |z|2 ď τ{2, we shall have |z|2 ´ sn ď ´τ{2. We have that

gp|z|2 ´ snq “ |z|2

N

gp|z|2 ´ s1q “ |z|2

N

nÿ
nÿ

i“1

li

i“1

sn ´ si

´psi ` |z|2qx ` |z|4 ď 0,

li

´psi ` |z|2qx ` |z|4 ě 0,


s1 ´ si
nÿ

1 ` si

x ´ |z|2

´psi ` |z|2qx ` |z|4

“ |z|2

N

li

i“1

´x ` |z|2 ´ si

´psi ` |z|2qx ` |z|4 .

and

where we use that
gpxq “ 1
N

„

nÿ

i“1

li

Hence we must have that

´ τ´1 ď |z|2 ´ s1 ď ´t ď |z|2 ´ sn ď ´τ{2.

Combining (A.30) and (A.32), we get that C´1τ 4 ď t ď Cτ´1 for some constant C ą 0.

Now we return to the self-consistent equation (2.17). The previous discussions show that

?
fp0, i

tq “ 0, t ě C´1τ 4.
`
It is easy to see that there exists constants c, τ1 ą 0 such that

˘ˇˇ ě c for |m ´ i
First we consider the case |z| ě c ą 0 for some c ą 0. Expanding fp?

ˇˇ´psi ` |z|2qm2 ` |z|4 ` ?

m3 ´ |z|2m

w

?

t| ď τ1.

(A.33)
?
tq and
w, mq around p0, i

using (A.33), we get
0 “ B?

?
wfp0, i

tq?

Using (A.25) and

wfp?

B?

w, mq “ ´1 ´ m2
N

tq.

?
w ` Bmfp0, i
nÿ

?
tqpm ´ i
`

?
tq ` op?
wq ` opm ´ i
˘
m2 ´ |z|2
r´pλi ` |z|2qm2 ` |z|4 ` ?
w pm3 ´ |z|2mqs2 ,
˘
`

liλi

i“1

2

we get

?
wfp0, i

B?

tq “ ´1 ` t
N

t ` |z|2

2

rpsi ` |z|2qt ` |z|4s2 ,

nÿ

lisi

i“1

77

(A.31)

(A.32)

(A.34)

(A.35)

(A.36)

and

?
Bmfp0, i

tq “ t
N

2si|z|2

rpsi ` |z|2qt ` |z|4s2 ě c1

for some c1 ą 0. Using (A.36) and (A.37), we get from (A.34) that
if |z| ě c.

wq,

nÿ

lisi

i“1
t “ Op?

?
m ´ i
nÿ

li

i“1

1
N

nÿ

i“1

In particular, this shows that |m| « Im m „ 1 as w Ñ 0.

Then assume that |z|2 ă c, where c is suﬃciently small. From gp´tq “ 0 and (A.31), we get that

The leading order term gives that
´ 1
N

1
N

nÿ
˘´1

li
si

i“1
i li{si

ř

`

t ` |z|2 ´ si
psi ` |z|2qt ` |z|4 “ 0.
ÿ

li
t

˜

“ Op|z|2q ñ t´1 “ 1
N
ÿ

¸
´ 2

t2
0
N

li
s2
i

i

t “ t0 `

|z|2 ` Op|z|4q.

` Op|z|2q.

li
si

i

Deﬁne t0 :“

N´1

. Expanding (A.39) to the ﬁrst order term of |z|2, we also get

(A.37)

(A.38)

(A.39)

(A.40)

(A.41)

Now we can calculate that (the partial derivatives of F can be easily obtained from (A.25) and
(A.35))

tq and using (A.33), we get

tq?
w
?
t|2,|m ´ i

t|?

wq.

(A.42)

Now we write equation (2.17) as

Fp?
?
tqpm ´ i

w, mq “ 0,

tq ` BmB?

?
w, mq :“ fp?
where Fp?
w, mq{m. Expanding F around p0, i
?
?
tq?
w ` BmFp0, i
wFp0, i
0 “B?
?
?
tqw ` 1
mFp0, i
B2
wFp0, i
` 1
B2?
2
2
nÿ
nÿ

tqpm ´ i
`

BmFp?

?
w, i

j“1

?

?

w

t

‰2 ´

?
wFp0, i

2 ` 2isj|z|2?

?
tqpm ´ i
?
tq2 ` opw,|m ´ i
˘
“
t ` |z|2
?
wtpt ` |z|2q
psj ` |z|2qt ` |z|4 ´ i
0 ` 2isj|z|2?
?
?
` op|z|2,
´
wt2
w
psjt0q2
t0
?
˘
` op|z|2,
“
t ` |z|2
ˆ
˙
?
wtpt ` |z|2q
psj ` |z|2qt ` |z|4 ´ i
?
´ 2|z|2 ´ 2i
1 ` 2|z|2
wt

`
wq,

wq

ljsj

wt0

?

t0

2

t

sj

?

w
t

ljsj

j“1

tq “ ´ 1
N
“ ´ 1
ljsj
N
?
“ ´ 2i|z|2 ` 2
nÿ
t3{2
t
nÿ
N

?
tq “ 1
i
?
“ 1
i

#
«

t
N

j“1

t

t

0

j“1

lj
sj

wFp?

?
w, i

B?

78

+
‰2 ´ 1

(A.43)

ﬀ
wq

?

´ 1 ` op|z|2,

From the above results, we get that

?
BmFp0, i

tq “ ´ 2i|z|2
t3{2
?
tq “ ´ 2
wFp0, i
t0

0

` op|z|2q, B?

` Op|z|2q, B2?

t0

?
wFp0, i

tq “ i|z|2?
nÿ
N
?
tq “ 2t0
wFp0, i
N

j“1

BmB?

and

«

0 “

Plugging (A.45)-(A.47) into (A.42), we get that
?

tq “ Op|z|2q.

«
´2

?
mFp0, i
B2
ﬀ
nÿ
` op|z|2q
lj
s2
?
a
ˇˇ „ |z|2 `
t|?
j
wq.
t|2,|m ´ i
«
nÿ
|w|, we get

w `

j“1

N

i|z|2?
t0 ` ?
wt0
?
ˇˇi|z|2?
` opw,|m ´ i
t0 ` ?
wt0
?
mp?
w,|z|2q ´ i

Observing that

nÿ

j“1
lj
s2
j

` op|z|2q,

lj
s2
j
` Op|z|2q,

ﬀ
` op|z|2q

pm ´ i

?

tq

i|z|2 ` ?
t3{2

0

wt0

(A.44)

(A.45)

(A.46)

(A.47)

(A.48)

(A.49)

«

?
“ 1
t
`
t0
i
t0
i|z|2 ` 2

“

´ 1 ` 2|z|2
˘ ?
?

t0

wt0

t0
N

nÿ

´ 2|z|2t0
nÿ

?
` 2i
lj
s2
N
j“1
j
?
` op|z|2,

wq.

lj
s2
j

j“1

wt3{2
N

0

nÿ

j“1

` op|z|2,

lj
s2
j

ﬀ
wq

?

ﬀ

w,

?
t ` Op?

t “

t2
0
2N

lj
s2
j

j“1

` Op|w|1{2 ` |z|2q
?

if |z| ă c.

Combing (A.38) and (A.49), we get that if |z|2 ă 1 ´ τ , m “ i

wq „ 1 when w Ñ 0. In
particular, this shows that |m| « Im m „ 1 when w Ñ 0. Now we conclude the proof by recalling
that m1cpwq “ mcpwqw´1{2 ´ 1.

To prove Proposition 2.15, we need the following lemma, which is a consequence of the edge

regularity conditions (2.21) and (2.22).
Lemma A.5. Suppose ek ‰ 0 is a regular edge. Then |m1cpwq ´ m1cpekq| „ |w ´ ek|1{2 as w Ñ ek
and minl‰k |el ´ ek| ě  for some constant  ą 0.
Proof. Let mk :“ mcpekq and w Ñ ek. Notice by Lemma 2.3, if ek ‰ 0, we have

Then we expand f around p?

“
ek, mkqp?
wfp?
0 “B?
|?
w ´ ?
` O

where by (A.35),

wfp?

B?

 ď ek ď Cτ´1.

w ´ ?

mfp?
B2
ek|2 ` |mcpwq ´ mk|3 ` |?

ek, mkq to get that
ekq ` 1
2
nÿ

‰

,

ek, mkqpmcpwq ´ mkq2
w ´ ?
`

ek||mcpwq ´ mk|

˘

k ´ |z|2
m2

2

ek, mkq “ ´1 ´ m2
k
N

lisi

i“1

ekpmk ´ aiq2pmk ´ biq2pmk ` ciq2 ,

(A.50)

(A.51)

(A.52)

79

and by (A.1),

mfp?
B2

nÿ

i“1

lisi

ek, mkq “ 2
N

ˇˇB?

1 ď

wfp?
ˇˇˇB2?
ˇˇ ,

w, mcpwqq

wfp?

Ai

Bi

„
pmk ´ aiq3 `
pmk ´ biq3 `
ˇˇB2
ˇˇ ď C1,  ď
mfp?
ˇˇˇ ,
ˇˇBmB?

w, mcpwqq

ek, mkq

wfp?

Ci

pmk ` ciq3

ˇˇ ď C2

ˇˇ)

w, mcpwqq

ď C3.

Applying the estimates (A.2)-(A.5), (A.50) and the conditions (2.21)-(2.22) to (A.52) and (A.53),
we get that

(A.54)
for some C1, C2 ą 0. Similarly, if |w ´ ek| ď τ1 and |mcpwq´ mk| ď τ1 for some suﬃciently small τ1,
using the condition (2.21) we can get that

ek, mkq



,

(A.53)

!ˇˇB3
mfp?

max

(A.55)
Plug them into equation (A.51), for |w ´ ek| ď τ1 and |mcpwq ´ mk| ď τ1, we get |mcpwq ´ mk| „
|?
w ´ ?
´ B?

w ´ ?
(A.56)
ek| „ |w´ ek| and |mcpwq´ mk| „ |m1cpwq´ m1cpekq|,

By (A.50), we immediately get that |?

ekq ` Op|?
w´?

ek|1{2 and
wfp?

ek, mkqpmcpwq ´ mkq2.

ek|3{2q “ 1
2

ek, mkqp?

mfp?
B2

w ´ ?

˘

1{2

which proves the ﬁrst part of the lemma.

By (A.56), if w is real and |w ´ ek| ď τ1, we have that
wfp?
´2B?
ek, mkqpm ´ mkq2 ` Op|?
mfp?
B2

mcpwq ´ mk “

ek, mkq

w ´ ?

„

1{2`?

ek|1{2q

w ´ ?

.

ek

(A.57)
Thus on a suﬃciently small interval U “ rek ´ , ek ` s, mcpwq has positive imaginary part for w
on one side of ek and mcpwq is real for w on the other side. Hence U does not contain another edge.
This shows that minl‰k |el ´ ek| ě .
Proof of Proposition 2.15. The properties of ρ1c have been proved in Lemmas 2.3, A.4 and A.5, and
included in the Deﬁnition 2.4.
During the discussions after lemma 2.2, we have seen that supppρ2cq “ supppρ1cq, which proves
property (i) for ρ2c. The conclusion ρ2c being a probability measure is due to the deﬁnition of m2
in (2.36) and the fact that m2c is the almost sure limit of m2.
The properties (ii) and (iv) for ρ2c can be easily obtained by plugging m1c into (2.10). To prove
the property (iii) for ρ2c, we need to know the behavior of Im m2cpwq when w Ñ ej along the real
line. Using (2.10), we see that it suﬃces to prove that if |x ´ ej| ď τ1 for some small enough τ1 ą 0,
then

ˇˇ “

ˇˇ ě 

ˇˇm2
ˇˇ “ op1q. Plugging mc into Bmfp?

c ´ |z|2

w, mcq in

for some constant  ą 0. Suppose that
(A.25), and using condition (2.21) and Lemma A.5, we get that

cpwq ´ |z|2

ˇˇ´wp1 ` m1cq2 ` |z|2
Bmfp?

ˇˇm2

w, mcpwqq “ ´1 ` Op|m2

c ´ |z|2|q.

Again using condition (2.21) and Lemma A.5, we can get upper bounds for B?
mfp?
and B2
0 “ Bmfp?

w, mcpwqq for w near ej. Thus we shall have that
ej, mcpejqq “ Bmfp?

w, mcpwqq`Op|w´ej|1{2q “ ´1`Op|m2

c´|z|2|`|w´ej|1{2q. (A.59)

wBmfp?

(A.58)
w, mcpwqq

ˇˇm2

c ´ |z|2

ˇˇ.

This gives a contradiction. Thus we must have a lower bound for

80

Remark: Here we add a small remark on Example 2.9. Given the assumptions in Example 2.9, it
is easy to see that f can only take critical values on intervals I´n, I0, In and I2n, since maxt|ai ´
ai´1|,|bi ´ bi´1|,|ci ´ ci´1|u Ñ 0 in this case. Thus the number of connected components of supp ρ1c
is independent of n, and all the edges and the bulk components are regular as in Example 2.8.

A.2 Proof of Lemma 3.8

In this subsection, we prove Lemma 3.8, which gives some useful estimates of m1c and m2c on each
of the subdomains Db
k, De
k and Do, under the regularity assumptions from Deﬁnition 2.4. We prove
the four cases separately.
Case 1: For w “ E ` iη P Db
m1cpwq “

ż
kpτ, τ1, Nq,

ρ1cpxqrpx ´ Eq ` iηs

ρ1cpxq

(A.60)

ż

dx.

px ´ Eq2 ` η2

R

Hence

(A.61)
By the regularity condition of Deﬁnition 2.4 (ii), ρ1cpxq ě c for x P re2k ` τ1, e2k´1 ´ τ1s. Thus
we get immediately that Im m1c „ 1. Since Im m1c ď |1 ` m1c| ď C by Proposition 2.16, we get
|1 ` m1c| „ 1.

R

For m2c, we have

R

x ´ pE ` iηq dx “
ż

Im m1cpwq “

ρ1cpx, zqη
px ´ Eq2 ` η2 dx.

„

m2c “

´wp1 ` m1cq ` |z|2
1 ` m1c

´1
ż

by (2.10). Notice wm1c can be expressed as
wρ1cpx, zq
x ´ w

wm1cpwq “

R

ρ1cpx, zqdx `

xρcpx, zq
x ´ w

dx.

R

By the same argument as above and using the fact that x ě τ1 for x P re2k ` τ1, e2k´1 ´ τ1s, we get
that

ż

R

dx “ ´
ż

Since the imaginary parts of ´w and |z|2{p1 ` m1cq are also negative, we get

ż

„

Im

Using the bounds for m1c and Im m1c proved above, it is easy to see

Impwm1cq “ Im

dx „ 1.

R

xρ1cpx, zq
x ´ w


ď ´Impwm1cq.

´wp1 ` m1cq ` |z|2
1 ` m1c

ˇˇˇˇ´wp1 ` m1cq ` |z|2
«
´p1 ` m1cq `

1 ` m1c

ˇˇˇˇ “ Op1q.
ﬀ´1

|z|2

wp1 ` m1cq

P C`

wm2c “

81

(A.62)

(A.63)

Equations (A.62) and (A.63) together give that Im m2c „ 1 and |m2c| „ 1. Similarly, we can also
prove that

«
and Impwm2cq „ 1. Now (3.34) follows by noticing that

˜

¸

ﬀ

1 ` m1c

´wp1 ` m1cq2 ` |z|2

´ |z|2
1 ` m1c

“ Im

Im

w

1 ` si

˜
w ` siwm2c ´ |z|2
1 ` m1c

¸

ě siImpwm2cq.

Case 2: For w “ E ` iη P Dopτ, τ1, Nq, using (A.61) and distpE, supp ρ1,2cq ě τ1, we immediately
get Im m1,2c „ η. Now we prove the other estimates.
We ﬁrst prove (3.34). If η „ 1, the proof is exactly the same as in Case 1. Hence we assume
η ď c1, where c1 ” c1pτ, τ1q ą 0 is suﬃciently small. We separate it into two cases.
(i) Suppose E „ 1. We shall prove that

min

i

t|mcpwq ´ aipwq|,|mcpwq ´ bipwq|,|mcpwq ` cipwq|u ě 1,

¸

ˇˇˇˇˇw

for some constant 1. This leads immediately to (3.34) since

˜
1 ` si
For pi “ ?
and ´cipEq decrease as E increase. Since E R supp ρ1c, we have m1cpEq P R and

´wp1 ` m1cq2 ` |z|2
Em3 ´pλi `|z|2qm2 ´?

p1 ` m1cq ´ |z|2

ˇˇˇˇˇ “

ˇˇˇˇˇ?

1 ` m1c

´m2

c ` |z|2

wpmc ´ aiqpmc ´ biqpmc ` ciq

E|z|2m`|z|4, it is not hard to prove that its roots aipEq, bipEq

(A.64)

(A.65)

ˇˇˇˇˇ .

ż

dm1cpEq

dE

“

ρ1cpx, zq
px ´ Eq2 dx ě 0.

R

So m1cpEq (and hence mcpEq) increases as E increases. If ek is the smallest edge that is bigger than
E, then for aipEq bigger than mcpEq, we have that

aipEq ´ mcpEq ě aipekq ´ mcpekq ě 

(A.66)

by the regularity condition (2.21). On the other hand, If ek´1 is the largest edge value that is smaller
than E, then for aipEq smaller than mcpEq, we have that

mcpEq ´ aipEq ě mcpek´1q ´ aipek´1q ě .

(A.67)

Applying the same arguments to bipEq and ´cipEq, we get

i

min

t|mcpEq ´ aipEq|,|mcpEq ´ bipEq|,|mcpEq ` cipEq|u ě 

(A.68)
for E P pe2k´1, e2kq for some k. Now we are only left with the case E ă e2L, the rightmost edge,
when |z|2 ě 1` τ . In this case, we have seen that 0 ă mcpEq ă bipEq for all i in the proof of Lemma
A.4. Thus we can use (A.66) to get lower bounds for |mcpEq ´ aipEq| and |mcpEq ´ bipEq|. Since
cipEq „ 1 in this case (e.g. by (A.4) and using E,|z| „ 1), |mcpEq ` cipEq| ě  is trivial. Again we
get the estimate (A.68).
Then we consider w “ E ` iη with η ď c1. First it is easy to check that aipE ` iηq, bipE ` iηq
and cipE ` iηq are continuous in η. On the other hand for mcpE ` iηq, we have

Bwm1cpwq “

ρ1cpx, zq
px ´ wq2 dx ď C

(A.69)

R

ż

82

by the condition distpE, supp ρ1cq ě τ1. Thus we immediately get |mcpE ` iηq ´ mcpEq| “ Opηq.
Hence as long as c1 is small enough, (A.64) is true, which further gives (3.34).
(ii) Suppose w “ E ` iη Ñ 0, where we must have |z|2 ě 1 ` τ and E ă e2L. In this case,
|m1cpwq| „ 1 by Proposition 2.16. Then we can calculate directly that

¸

ˇˇˇˇˇ “

ˇˇ|z|2 ` Opwq

ˇˇ ě c.

1 ` |di|2

1 ` m1c

´wp1 ` m1cq2 ` |z|2

p1 ` m1cq ´ |z|2

˜

ˇˇˇˇˇw

Then we show that |1` m1c| „ 1 for w P Do and η ď c1. We again divide it into two cases. First

This concludes the proof of (3.34).
suppose |w| „ 1. If |mc| can be arbitrarily small, then using (3.34) we get that

which gives a contradiction. Then suppose w “ E ` iη Ñ 0 when |z|2 ě 1 ` τ and E ă e2L. We
have seen in the proof of Lemma A.4 that

fp?

w, mcq “ ´?

w ` Opmcq ‰ 0,
¯

´?

E

.

mcpEq “ ?

E

|z|2
|z|2 ´ 1

` o

This gives that 1 ` m1cpEq “ |z|2{p|z|2 ´ 1q ` op1q. Again using the bound (A.69), we get

ˇˇˇˇ „ 1.
ˇˇ ě  for some  ą 0. If |z|2 ě 1 ` τ and w Ñ 0, then we have

|1 ` m1cpE ` iηq| “

` op1q ` Opηq

|z|2
|z|2 ´ 1

ˇˇˇˇ

ˇˇ|z|2 ´ m2

ˇˇm2

ˇˇ ě .
ˇˇ Ñ 0. Then plugging mc into (A.25) and

ˇˇ|z|2 ´ wp1 ` m1cq2

Finally we want to prove that |m2c| „ 1 for w P Do and η ď c1. By (2.10), it suﬃces to show that

c ´ |z|2

Bmfp?

c

Now we are left with the case E „ 1. Suppose
using (3.34), we get

However, since E R supp ρ1c we must have

w, mcq “ ´1 ` Op|m2
Bmfp?
Using (3.34), we can get upper bounds for B?
|mcpE ` iηq ´ mcpEq| „ η, we get
Bmfp?

E, mcpEqq ě 0.
wBmfp?
w, mcpwqq “ Bmfp?

c ´ |z|2|q.

(A.70)

(A.72)

(A.71)
w, mcq for w near E. Since

w, mcq and B2

mfp?
ˇˇm2
E, mcpEqq ` Opηq.

ˇˇ.

c ´ |z|2

Now we have a contradiction. Thus there must be a lower bound for
Case 3: For regular edge ek ‰ 0, we always have ek ě  for some  ą 0 by Lemma A.4. Thus we
kpτ, τ1, Nq as long as τ1 is suﬃciently small. If η „ 1, then
always have |w| „ 1 for w “ E ` iη P De
κ ` η „ η{?
?
Now pick τ1 small and consider the case η ď τ1. Then we have κ ` η ď 2τ1. By the regularity

κ ` η „ 1 and the proof is exactly the same as in Case 1.

assumption (2.21) and Lemma A.5, we have

min
1ďiďn
uniformly in w P tw P De
bound implies the estimate (3.34). If mcpwq Ñ 0, then using (3.34) we get from fp?

t|mcpwq ´ aipwq|,|mcpwq ´ bipwq|,|mcpwq ` cipwq|u ě {2
kpτ, τ1, Nq : κpwq` ηpwq ď 2τ1u, provided τ1 is suﬃciently small. The above
w, mcq that

(A.73)

83

ˇˇ|z|2 ´ m2

w ` Opmcq “ 0, which gives a contradiction. Thus we must have |1 ` m1c| „ |mc| „ 1. To show

´?
|m2c| „ 1, it suﬃces to show that
method as in Case 2.
Now we still need to prove the estimates for Im m1,2c when η ď τ1. Recall the expansion (A.56)
ek, mkq are real (as ek and
around ek and equation (A.57). Notice both B?
mk are real). Suppose k is odd, then Im mcpEq “ 0 for E Œ ek (i.e. E R suppρc) and Im mcpEq ą 0
for E Õ ek (i.e. E P suppρc). Thus (A.57) gives

ˇˇ ě  for some  ą 0. This can be proved using the same
wfp?

ek, mkq and B2

mfp?

c

mcpwq ´ mk “ Ckpwqpw ´ ekq1{2 ` Op|w ´ ek|q,

with Ck ą 0 and Ck „ 1. Then for E ě ek, we have
a
κ2 ` η2

´
´κ `

Im mcpE ` iηq „

giving that

„

¯

mcpE ` iηq ´ mk “ Ckpwqpκ ` iηq1{2 ` Op|w ´ ek|q,

1{2 „

η?
κ ` η

.

For E ď ek, we have

which gives

mcpE ` iηq ´ mk “ Ckpwqp´κ ` iηq1{2 ` Op|w ´ ek|q,

´
κ `

a
κ2 ` η2

¯

1{2 „ ?

κ ` η.

Im mcpE ` iηq „

1
2

„

1
2

If k is even, the proof is the same except that in this case

mcpwq ´ mk “ Ckpwqpek ´ wq1{2 ` Op|w ´ ek|q.

For m1cpwq and m2cpwq, we get the conclusion by noticing w « ek and

´

¯

Im m1c “ Im

w´1{2mc

„ Im mcpwq,

Im m2c “ Im

?

mc
wp´m2
c ` |z|2q

„



„ Im mcpwq.

Case 4: Again if η „ 1, the proof is the same as in Case 1. If |w| ď 2τ1 for small enough τ1, in the
proof of Lemma A.4, we have seen that mc “ i
wq, which gives the ﬁrst equation in (3.32).
Plugging it into (2.10), we get the second equation in (3.32). Taking the imaginary part, we obtain
(3.33). Finally using (3.32), we get (3.34) easily.

t` Op?

?

A.3 Proof of Lemma 3.11

In this subsection, we prove lemma 3.11, which establishes the stability of the self-consistent equation
(2.12) on each of the subdomains Db
k under the regularity assumptions from Deﬁnition
2.4.

a
|w|δ. Then we write the equation fp?

Case 1: We take over the notations in Deﬁnition 3.10 and abbreviate R :“ fp?
|R| ď

w, uq ´ fp?

w, mcq “ R as

k, Do and De

w, uq, so that

(A.74)

αpu ´ mcq2 ` βpu ´ mcq “ R,

84

where using (A.1), α and β can be expressed as

nÿ

i“1

lisi

„

nÿ

i“1

α :“ 1
N

and

β :“ 1 ´ 1
N

We shall prove that

Bi

pu ´ biqpmc ´ biq2 `

Ai

pu ´ aiqpmc ´ aiq2 `
„

lisi

Ai

pmc ´ aiq2 `

Bi

pmc ´ biq2 `

Ci

pmc ` ciq2

Ci

pu ` ciqpmc ` ciq2


“ Bmfp?

w, mcq.



,

(A.75)

(A.76)

(A.77)

(A.78)

(A.80)

for w P Db
giving that Im u „ 1. By (3.34), we have

k and u satisfying |u´ mc| ď plog Nq´1{3. If |u´ mc| ď plog Nq´1{3, then Im u ě Im mc{2,

|α| ` |Buα| ď C, |β| „ 1,

t|mc ´ ai|,|mc ´ bi|,|mc ` ci|u ě 
for some  ą 0. Replacing the mc in (3.34) with u, we also get that
t|u ´ ai|,|u ´ bi|,|u ` ci|u ě 1

min

min

i

i

(A.79)
for some 1 ą 0. Using (A.78) and (A.79), we get immediately that |α| ` |Buα| ` |β| ď C. What
remains is the proof of the lower bound |β| ě c. If Im w ě  for some constant  ą 0, the lower
bound comes from Lemma A.6 below. If Im w ď  with  small enough, then the lower bound follows
from Lemma A.7 below.
Now we prove the inequality (3.41) through a ﬁxed point argument. Let x “ u ´ mc. Then

equation (A.74) can be written as

Gpxq “ x,

where Gpxq “ aαpxqx2`paβ ` 1qx´ aR. Here we pick the constant a ‰ 0 such that aβ “ ´ 3
4 . Since
|β| „ 1, we also have |a| „ 1. For all |x1|,|x2| ď plog Nq´1{3, we can ﬁnd a uniform constant C1 ą 0
such that

´
|aβ ` 1| ` C1plog Nq´1{3

¯

(A.81)
where we use the bound (A.77). Now we do the iteration xn “ Gpxn´1q. We start with x0 “ 0.
Then x1 “ Gpx0q “ ´aR, with |x1| “ |aR| ď Cplog Nq´1. Using the Lipschitz condition (A.81), it
is easy to see that |xn| ď plog Nq´1{3 for all n and xn converges to a ﬁxed point x8 of G with

|x1 ´ x2| ď 1
2

|x1 ´ x2|,

|Gpx1q ´ Gpx2q| ď

ˆ

8ÿ

˙

1
2

n“0

|x8| ď |aR|

n “ 2|aR|.

This proves (3.41) and hence the stability of (3.37).

Lemma A.6. Suppose that Im w „ 1 and |mc| „ Im mc „ 1. Then |Bmfp?
constant c ą 0.
Proof. Using (2.15), mc “ ?

wp1 ` m1cq and the conditions Im w „ 1, Im mc „ 1, we can get that

w, mcq| ě c for some

ˇˇˇˇB?

wfp?
Bmfp?

w, mcq
w, mcq

ˇˇˇˇ “

ˇˇˇˇ Bmc

B?

w

ˇˇˇˇ ď C

85

ˇˇ .

w, mcq

w, mcq

ˇˇB?

ˇˇB?
for some constant C ą 0. Thus we have
wfp?
Now we assume that |Bmfp?
w, mcq| can be arbitrarily small. Then
arbitrarily small. Denote a :“ Bmfp?
w, mcq and b :“ B?
equation fp?
nÿ
w, mcq “ 0, we get that
nÿ
nÿ

ˇˇBmfp?
ˇˇ ď C
wfp?
`
˘
c ´ |z|2
2 ` 2si|z|2mc
m2
c ` |z|4 ` ?
r´psi ` |z|2qm2
`
˘
c ´ |z|2
m2
c ` |z|4 ` ?
w pm3

From (A.83) and (A.84), we get that

b “ ´1 ´ m2
c
N

r´psi ` |z|2qm2

´ mc
N

w pm3

w
mc

a “

i“1

i“1

and

?

?

lisi

lisi

w

2

c ´ |z|2mcqs2

c ´ |z|2mqs2 .

mca ´ ?

?
wb “ 2

w ´ m2
c
N

lisi

i“1

r´psi ` |z|2qm2

w pm3

c ´ |z|2mcqs2 .

2si|z|2mc
c ` |z|4 ` ?

ˇˇ can also be

(A.82)

wfp?

w, mcq

w, mcq. Using (A.25), (A.35) and the

(A.83)

(A.84)

(A.85)

Combing (A.84) and (A.85) we get that

N

mc

mc

lisi

i“1

(A.86a)

wmc ´ |z|2q|z|2

wmc ´ |z|2q|z|2

mc
wmc ´ |z|2q|z|2

nÿ
´psi ` |z|2qm2
w ´ mcq “ p|z|2 ´ ?

c ´ |z|2qpmca ´ ?
pm2
b ´ 1
2
´ ?
wpm2
´ ?

p?
wbq
“ ´ p?
c ´ |z|2q ´ mc|z|2
“ ´ p?
c ´ |z|2q ´ |z|2p?
wpm2
(A.86b)
where we use again the equation fp?
w, mcq “ 0 in the derivation. By our assumption, the expression
in (A.86a) can be arbitrarily small. For the last expression in (A.86b), we have |mc| „ 1 and
|?
wmc ´ |z|2| „ 1 (because Imp?
wmcq “ Impw ` wm1cq „ 1). Thus if |mc ´ i|z|| ě c1 for some
constant c1 ą 0, we have |m2 ` |z|2| „ 1, and
b ´ 1
2

c ´ |z|2qpmca ´ ?
wbq
pm2
which gives a contradiction. Thus we must have a lower bound |Bmfp?

w, mcq| ě c if |m´ i|z|| ě c1.
We still need to deal with the case where |mc ´ i|z|| Ñ 0. Notice |z| „ 1 in this case. Then we

c ´ |z|2
c ` |z|4 ` ?
m2
wmcqpm2

w pm3
c ` |z|2q

ˇˇˇˇ „ 1,

wmc ´ |z|2q|z|2

ˇˇˇˇp?

mc

mc

,

c ´ |z|2mcq

have

p?

w, i|z|q “ ´1 ` |z|2

Bf
B?
w

?
Let Li :“ psi`|z|2q|z|2`|z|4´2i
we see that Re Li ą 0, Im Li ă 0 and |Re Li|,|Im Li| „ 1. Furthermore, Im L2
Thus each fraction 4|z|4{L2
order 1, which gives that

?
lisi
rpsi ` |z|2q|z|2 ` |z|4 ´ 2i
i“1
?
w|z|3. Since i
w “ ipα`iβq “ iα´β with α, β ą 0 and α, β „ 1,
i| „ 1.
„
i in (A.87) has positive imaginary part and all the imaginary parts are of

i ă 0 and |Im L2

w|z|3s2 .

(A.87)

N

4|z|4

ˇˇˇˇ Bf

B?

w

ˇˇˇˇ ě Im

p?

w, i|z|q


w, i|z|q

„ 1.

p?

Bf
B?
w

nÿ

86

Then by (A.82), we get that |Bmfp?

w, i|z|q| ě c for some c ą 0. Using (3.34), it is easy to see that

Bmfp?

w, mcq “ Bmfp?

w, i|z|q ` Op|mc ´ i|z||q.
Thus in the case |mc ´ i|z|| Ñ 0, we still can ﬁnd c ą 0 such that |Bmfp?
Lemma A.7. Suppose that w P Db
|Bmfp?
˙
Proof. By (3.28) and (3.34),
mfpw, mq “ Op1q. Denote w “ E ` iη. Taking the imaginary part of the following equation
B2
0 “ fp?

if |w| „ 1 and Im m „ 1, we have B?

k and Im w ď . Then for suﬃciently small  ą 0, we have

wBmfpw, mq “ Op1q and

E, mcpEqq “ ´?

w, mcq| „ 1.

w, mcq| ě c.

` Bi

` Ci

nÿ

(A.88)

ˆ

lisi

Ai

,

i“1

mc ´ ai

mc ` ci

and noticing that Ai, Bi, Ci and ai, bi, ci are all positive real numbers for real E, we get

ˆ

E ` mc ` E´1{2 ` 1
N
nÿ
ˆ

i“1

lisi

Ai

|mc ´ ai|2 `
„

nÿ

lisi

i“1
pmc ´ aiq2 `

Ai

from which we get that

Im mc ´ 1
N
nÿ

1
N

lisi

i“1

AiIm mc

|mc ´ ai|2 ` BiIm mc

|mc ´ bi|2 ` CiIm mc
|mc ` ci|2
˙

Bi

|mc ´ bi|2 `

Ci

|mc ` ci|2

Using the above equation, we get

Bmfp?
E, mcpEqq “ 1 ´ 1
nÿ
N
“ 1
N

|mc ´ ai|2 ´

„

lisi

Ai

i“1

Ai

pmc ´ aiq2 `

Bi

pmc ´ biq2 `

Bi

|mc ´ bi|2 ´

Bi

pmc ´ biq2 `

|mc ` ci|2 ´

mc ´ bi
˙

“ 1.


Ci

pmc ` ciq2
Ci

“ 0,

(A.89)

(A.90)



.

Ci

pmc ` ciq2

(A.91)

where mc ´ ai :“ |mc ´ ai|eiθi. Using Im mc „ 1, it is easy to see that Rep1 ´ e´2iθiq ě c1 for some
constant c1 ą 0. Applying the same estimates for the B, C terms in (A.91), we get

We look at, for example, the term

Ai

Ai

|mc ´ ai|2 ´
ˇˇˇBmfp?

pmc ´ aiq2 “
ˇˇˇ ě Re
”
Bmfp?
Now for w “ E ` iη with η ď , we can expand Bmfp?

E, mcpEqq

for some constant c ą 0.

Ai

|mc ´ ai|2p1 ´ e´2iθiq,

ı

ě c

E, mcpEqq
w, mcpwqq around Bmfp?

(A.92)

E, mcpEqq,

Bmfp?

w, mcpwqq “ BmfpE, mcpEqq ` Opηq,

where we again use (3.34). Combing with (A.92), we conclude that |Bmfpw, mcpwqq| „ 1 for small
enough .

87

wq ě c.

Bmfp?

w, mcq “ 1 ´ 1

Finally, we are left with the case E “ Re w „ 1 and η “ Im w Ñ 0. Using (2.15), mc “ ?
|w| „ 1 and distpE, supp ρ1cq ě τ1, we can get that
w, mcq
w, mcq

|z|2 ` Op?
ˇˇˇˇ Bmc
ˇˇˇˇ ď C
ˇˇˇˇ “
ˇˇB?
B?
for some constant C ą 0. Thus it is enough to prove that
w, mcq
˘
`
(A.35) and noticing that mcpEq P R, we get
c ´ |z|2
m2
c ` |z|4 ` ?
w pm3
ˇˇ ě 1 ` Opηq ě c.

ˇˇˇˇB?
wfp?
Bmfp?
nÿ
ˇˇB?

i“1
w, mcpwqq around B?
wfp?

wfp?
Expanding B?
we get for η small

r´psi ` |z|2qm2
wfp?
w, mq

E, mcpEqq “ ´1 ´ m2
c
N
wfp?

wfp?

B?

lisi

w

2

c ´ |z|2mcqs2 ď ´1.

wp1`m1cq,

ˇˇ has a lower bound. Using

Case 2: We mimic the argument in the proof of Case 1. We see that it suﬃces to prove |α|`|Buα| ď C
and |β| „ 1 for α, β deﬁned in (A.75) and (A.76) and |u ´ mc| ď plog Nq´1{3. Using (3.34), it is
not hard to prove that |α| ` |Buα| ` |β| ď C. What remains is the proof of the lower bound |β| ě c.
For the case Im w „ 1, it follows from Lemma A.6.
If w Ñ 0 in the case |z|2 ě 1 ` τ , then
mcpwq “ Op?

wq Ñ 0 by (3.29). Thus we can use (A.25) to get directly that

E, mcpEqq, using (3.34) and |mcpE`iηq´mcpEq| „ η,

Case 3: The case Im w ě τ1 can be proved using the same method as in the proof of case 1. Hence
we only need to consider the case |w ´ ek| ď 2τ1 in the following. Notice we have |w| „ 1 in this
case. Suppose that

Then we claim that

|α| „ 1,

|w ´ ek| ď 2τ1,

|u ´ mc| ď plog Nq´1{3.
|β| „ ?

κ ` η

(A.93)

(A.94)

for small enough τ1. Using (A.93), (3.34), (2.22) and Lemma A.5, we can get that
ek, mcpekqq ` Op|mcpwq ´ mcpekq| ` plog Nq´1{3q
ek, mcpekqq ` Op|w ´ ek|1{2 ` plog Nq´1{3q „ 1.

mfp?
B2
mfp?
B2

α “ 1
2
“ 1
2

w

d

ek
w

To prove the estimate for β, we use (2.20), (3.34) and Lemma A.5, to get
β “

ż
dw1Bmfp?
ż
w1, mcpw1qqdw1
w1Bmfp?
mfp?
w1, mcpw1qq dmcpw1q
B
ż
ż
”
”
ı
?
B?
B2
1
dw1
w1
mfp?
wBmfp?
?
B2
B?
dw1 `
ek, mkq ` Op|w ´ ek|1{2q
1
w1
mfp?
ek, mkqpmcpwq ´ mcpekqq ` Op|w ´ ek|q.

w1, mcpw1qqdw1 `

mcpwq
mcpekq

“
“ B2

ż

“

ek
w

ek

ek

2

2

w

dw1

ı
ek, mcpekqq ` Op|w ´ ek|1{2q

dm

(A.95)

88

Thus we conclude for small enough τ1 that

|β| „ |w ´ ek|1{2 „ ?

κ ` η.

Now we prove the inequality (3.41) using a continuity argument. As in the proof of Case 1, we
shall study equation (A.74), but with α1 :“ |w|´1{2α, β1 :“ |w|´1{2β and R1 :“ |w|´1{2R. Since
|w| „ 1, we still have

(A.96)
Also by assumptions we have |R1pwq| ď δpwq and C´1N´2 ď δpwq ď Cplog Nq´1 for some constant
C ą 0. For notational convenience, we still write α, β and R in the following.

|α1| „ 1,

|β1| „ ?

κ ` η.

If Im w ě τ1, we can prove that

|upwq ´ mcpwq| ď

a
Cδpwq
κ ` η ` δpwq

(A.97)

using the same method as in the proof of Case 1. For an arbitrary w P De
consider the discrete set Lpwq deﬁned in (3.39). We denote

k with Im w ă τ1, we

Lpwq “ tw0, w1, . . . , wLu,

(A.98)
where w1, . . . , wL are elements of Lpwq with Im w1 “ 1, wL “ w and Im wl`1 ă Im wl. Note that
|wl`1 ´ wl| ď N´10.

Deﬁne xpwq :“ upwq ´ mcpwq. For each wl P Lpwq, we solve (A.74) to get that

a
β2 ` 4αpxqRpxq
2αpxq

x1,2 “ ´β ˘

a

|xipwlq| ď

A1|Rpwlq|
κ ` ηl ` |Rpwlq| .

(A.99)
where the square root in the above expression takes the principal branch. In particular, for R “ 0
we have x1 “ 0, giving that u “ mc. If |xpwlq| ď plog Nq´1{3, then by the estimate (A.94) we have
that for i “ 1 or i “ 2,

,

Note ﬁrst that (A.99) and (A.94) imply

(A.100)
for some constant A1 ą 0 that is uniform in wl P Lpwq. If ηl „ 1, then κ ` ηl " |R| and (A.100)
holds for i “ 1. What remains is to show that (A.100) holds for x1 for all wl P Lpwq.

b
pκ ` ηl ´ |Rpwlq|q` ď |x1pwlq ´ x2pwlq| ď A2

a
κ ` ηl ` |Rpwlq|,

a2

(A.101)
for some constants a2, A2 ą 0 that are uniform in wl P Lpwq. We can choose A1 or a2 such that
A1 ą a2{4. We then consider two cases, depending on whether

holds or not. If (A.102) does not hold, then δ ě a2pκ ` ηq{p4A1q, so that (A.100), |R| ď δ and the
upper bound (A.101) imply
|upwq ´ mcpwq| ď

δ ď

?

?

Cδ

κ ` η ` δ

.

(A.102)

b
pκ ` η ´ δpwqq`

a
A1δpwq
κ ` η ` δpwq ď a2
a
κ ` ηl ` |R| ď C

a
A1|R|
κ ` η ` |R| ` A2

4

89

What remains is the case where (A.102) holds. By the monotonicity assumption on δ, we ﬁnd

b
that (A.102) holds for all wl P Lpwq. Since |R| ď δ, then (A.100) and (A.101) yield
pκ ` ηl ´ δpwlqq` ď |x1 ´ x2|.

a
A1δpwlq
κ ` ηl ` δpwlq , a2

Di “ 1, 2 : |xipwlq| ď

(A.103)

We now prove that

|upwlq ´ mcpwlq| “ |x1pwlq| ď

?

A1δ
κ ` η ` δ

(A.104)
for all l “ 1, . . . , L by induction on l. For l “ 1, the bound (A.104) is simply (A.97) proved above.
Suppose that (A.104) holds for some l. Since u and mc are Lipschitz continuous with Lipschitz
constant ď N 2, we get

w“wl

|upwl`1q ´ mcpwl`1q| ď 2N 2N´10 ` |upwlq ´ mcpwlq| ď 2N´8 `
?

ď 2N´6 ` CN 6N´10 `

?

ď

A1δ
κ ` η ` δ

w“wl`1

ˇˇˇˇ

?

A1δ
κ ` η ` δ
2A1δ
κ ` η ` δ

ˇˇˇˇ

w“wl

w“wl`1

,

(A.105)

ˇˇˇˇ

where in the second step we use the induction assumption, in the third step the Lipschitz continuity
of δ and η ě cN´1`τ , in the fourth step the bounds δ ě C´1N´2 and κ` η ` δ ď C. Using (A.105)
and recalling δ ď Cplog Nq´1 and (A.102), we get that

|upwl`1q ´ mcpwl`1q| ď plog Nq´1{3,

|upwl`1q ´ mcpwl`1q| ď a2
2

pκ ` η ´ δpwl`1qq`.

b

Using these two estimates, we deduce from (A.100) and (A.101) that (A.104) is valid with l replaced
by l ` 1. By induction we conclude that (A.104) is true for wL “ w. This concludes the proof.
Case 4: The case when Im w ě τ1 can be proved using the same method as in the proof of Case 1.
Now we are left with the case |w| ď 2τ1 for some suﬃciently small τ1.
First we assume |z| ě c ą 0 for some small c ą 0. Then mimicking the argument in the proof
of Case 1, we see that it suﬃces to prove |α| ` |Buα| ď C and |β| „ 1 when |u ´ mc| ď plog Nq´1{3.
Using (3.34), it is not hard to prove that |α| ` |β| ď C. The lower bound |β| ě c can be obtained
easily from (A.37).
|w|1{2 ` |z|2, we can verify that

Then suppose |z|2 ă c, but |w|1{2`|z|2 ě . According to (A.43) and using that

ˇˇi|z|2 ` ?

ˇˇ „

wt0

β “ Bmfp?

w, mcpwqq „ |w|1{2 ` |z|2 „ 1.

ˇˇˇˇ

¯

It is also easy to verify that

mfp?
B2

w, mcpwqq “ Op|w|1{2 ` |z|2q, B3

w, mcpwqq “ Op|w|1{2 ` |z|2q.

mfp?

´
Hence if |u ´ mc| ď plog Nq´1{3, we have
mfp?
B3

w, mcpwqq ` O

mfp?
B2

α “ 1
2

w, mcpwqqplog Nq´1{3

“ Op|w|1{2 ` |z|2q.

Mimicking the ﬁxed point argument in the proof of Case 1, we get the stability of (3.37).

90

Remark: The estimate (3.34) has been used repeatedly during the proof of Lemma 3.11. Here we re-
mark that it also gives the stability of the regularity conditions in Deﬁnition 2.4 under perturbations
of |z| and ρΣ. For example, we deﬁne the shifted empirical spectral density

ρΣ,t :“
and the associated mcpw, tq and function fp?

1

δσi`t,

N ^ M
w, m, tq. Given a regular edge ek, it satisﬁes that

i“1

(A.106)

N^Mÿ

ek, mk, t “ 0q “ 0, Bmfp?
˙
where we denote mk :“ mcpekq. We have the Jacobian

fp?

ˆ

ek, mk, t “ 0q “ 0.

J :“ det

Bmf
B?
wf
ˇˇB?
wBmf
B2
B?
p?
mf
ek,mk,0q
wfp?
ek, mk, 0q
ek, mk, 0q “ Op1q and BtBmfp?
we can verify that Btfp?
By (A.35), we have
ek, mk, 0q “ Op1q. Thus if we regard ek and
mk as functions of t, then Btmkpt “ 0q “ Op1q and Btekpt “ 0q “ Op1q. Then it is easy to verify
that

ˇˇ ě 1. Combining with (2.22), we get |J| ě . Using (3.34),

ek, mk, 0qB2

ek, mk, 0q.

w,m,tq“p?

mfp?

“ B?

wfp?

|mcpek, tq ´ aipek, tq| ` |mcpek, tq ´ bipek, tq| ` |mcpek, tq ` cipek, tq|
“|mcpekq ´ aipekq| ` |mcpekq ´ bipekq| ` |mcpekq ` cipekq| ` Optq

a
mfp
B2

and, by (3.34),

ekptq, mcpek, tqq “ B2

mfp?

ek, mcpekqq ` Optq.

Thus if Deﬁnition 2.4 (i) holds for some ρΣ, then it holds for all ρΣ,t provided that t is small enough.
Now given a regular bulk component re2k, e2k´1s and E P re2k ` τ1, e2k´1 ´ τ1s. Diﬀerentiating

the equation fp?

E, mcpE, tq, tq “ 0 in t yields

BtmcpE, tq “ ´ Btfp?
Bmfp?
E, mcpEq, 0q “ Op1q, while by (A.77), |Bmfp?

E, mcpE, tq, tq
E, mcpE, tq, tq .

By (3.34), we ﬁnd that Btfp?
E, mcpEq, 0q| “ β „ 1.
Thus BtmcpE, 0q “ Op1q. A simple extension of this argument shows that mcpE, tq “ mcpEq ` Optq
and hence Im mcpE, tq is bounded from below by some c1 “ c1pτ, τ1q. Thus we conclude that if
Deﬁnition 2.4 (ii) holds for some ρΣ, then it holds for all ρΣ,t with t in some ﬁxed small interval
around zero. Obviously, the above arguments also work for the perturbation of |z|.

References

[1] Z. D. Bai. Circular law. Ann. Probab., 25(1):494–529, 1997.

[2] Z. D. Bai and J. W. Silverstein. Spectral Analysis of Large Dimensional Random Matrices,

volume 2 of Mathematics Monograph Series. Science Press, Beijing, 2006.

[3] Z. Bao, G. Pan, and W. Zhou. Universality for the largest eigenvalue of sample covariance

matrices with general population. Ann. Statist., 43(1):382–421, 2015.

91

[4] A. Bloemendal, L. Erd˝os, A. Knowles, H.-T. Yau, and J. Yin. Isotropic local laws for sample

covariance and generalized Wigner matrices. Electron. J. Probab., 19(33):1–53, 2014.

[5] A. Bloemendal, A. Knowles, H.-T. Yau, and J. Yin. On the principal components of sample

covariance matrices. Prob. Theor. Rel. Fields, 164(1):459–552, 2016.

[6] A. Borodin and C. D. Sinclair. The Ginibre ensemble of real random matrices and its scaling

limits. Commun. Math. Phys., 291(1):177–224, 2009.

[7] P. Bourgade, H.-T. Yau, and J. Yin. Local circular law for random matrices. Probab. Theory

Relat. Fields, 159:545–595, 2014.

[8] P. Bourgade, H.-T. Yau, and J. Yin. The local circular law II: the edge case. Probab. Theory

Relat. Fields, 159(3):619–660, 2014.

[9] K. R. Davidson and S. J. Szarek. Local operator theory, random matrices and banach spaces.
volume 1 of Handbook of the Geometry of Banach Spaces, pages 317 – 366. North-Holland,
Amsterdam, 2001.

[10] A. Edelman. The probability that a random real gaussian matrix has k real eigenvalues, related

distributions, and the circular law. J. Multivar. Anal., 60(2):203 – 232, 1997.

[11] N. El Karoui. Tracy-widom limit for the largest eigenvalue of a large class of complex sample

covariance matrices. Ann. Probab., 35(2):663–714, 2007.

[12] L. Erd˝os, A. Knowles, and H.-T. Yau. Averaging ﬂuctuations in resolvents of random band

matrices. Ann. Henri Poincar´e, 14:1837–1926, 2013.

[13] L. Erd˝os, A. Knowles, H.-T. Yau, and J. Yin. Delocalization and diﬀusion proﬁle for random

band matrices. Commun. Math. Phys., 323:367–416, 2013.

[14] L. Erd˝os, A. Knowles, H.-T. Yau, and J. Yin. The local semicircle law for a general class of

random matrices. Electron. J. Probab., 18:1–58, 2013.

[15] L. Erd˝os, A. Knowles, H.-T. Yau, and J. Yin. Spectral statistics of Erd˝os-R´enyi graphs I: Local

semicircle law. Ann. Probab., 41(3B):2279–2375, 2013.

[16] L. Erd˝os, B. Schlein, and H.-T. Yau. Local semicircle law and complete delocalization for

Wigner random matrices. Commun. Math. Phys., 287(2):641–655, 2008.

[17] L. Erd˝os, H.-T. Yau, and J. Yin. Bulk universality for generalized Wigner matrices. Probab.

Theory Relat. Fields, 154(1):341–407, 2012.

[18] P. J. Forrester and T. Nagao. Eigenvalue statistics of the real Ginibre ensemble. Phys. Rev.

Lett., 99:050603, 2007.

[19] J. Ginibre. Statistical ensembles of complex, quaternion, and real matrices. J. Math. Phys.,

6(3):440–449, 1965.

[20] V. Girko. The circular law. Russ. Teor. Veroyatnost. i Primenen., 29(4):669–679, 1984.

[21] F. G˝otze and A. Tikhomirov. The circular law for random matrices. Ann. Probab., 38(4):1444–

1491, 2010.

92

[22] A. Guionnet, M. Krishnapur, and O. Zeitouni. The single ring theorem. Ann. Math.,

174(2):1189–1217, 2011.

[23] W. Hachem, A. Hardy, and J. Najim. Large complex correlated wishart matrices: Fluctuations

and asymptotic independence at the edges. arXiv:1409.7548.

[24] A. Knowles and J. Yin. Anisotropic local laws for random matrices. arXiv:1410.3516.

[25] A. Knowles and J. Yin. The isotropic semicircle law and deformation of wigner matrices. Comm.

Pure Appl. Math., 66(11):1663–1749, 2013.

[26] J. O. Lee and K. Schnelli. Tracy-widom distribution for the largest eigenvalue of real sample

covariance matrices with general population. arXiv:1409.4979.

[27] A. Litvak, A. Pajor, M. Rudelson, and N. Tomczak-Jaegermann. Smallest singular value of

random matrices and geometry of random polytopes. Adv. Math., 195(2):491 – 523, 2005.

[28] M. L. Mehta. Random matrices, volume 142 of Pure and Applied Mathematics. Elsevier,

Amsterdam, 3 edition, 2004.

[29] A. Onatski. The tracy-widom limit for the largest eigenvalues of singular complex wishart

matrices. Ann. Appl. Probab., 18(2):470–490, 2008.

[30] G. Pan and W. Zhou. Circular law, extreme singular values and potential theory. J. Multivar.

Anal., 101(3):645–656, 2010.

[31] M. Rudelson. Invertibility of random matrices: norm of the inverse. Ann. Math., 168(2):575–

600, 2008.

[32] M. Rudelson and R. Vershynin. The littlewood-oﬀord problem and invertibility of random

matrices. Adv. Math., 218:600–633, 2008.

[33] M. Rudelson and R. Vershynin. The smallest singular value of a random rectangular matrix.

Comm. Pure Appl. Math., 62:1707–1739, 2009.

[34] M. Rudelson and R. Vershynin. Small ball probabilities for linear images of high-dimensional

distributions. Int. Math. Res. Notices, 2015(19):9594–9617, 2015.

[35] C. D. Sinclair. Averages over Ginibre’s ensemble of random real matrices. Int. Math. Res. Not.,

2007:1–15.

[36] T. Tao and V. Vu. Random matrices: the circular law. Commun. Contemp. Math., 10(2):261–

307, 2008.

[37] T. Tao and V. Vu. Random matrices: Universality of local spectral statistics of non-Hermitian

matrices. Ann. Probab., 43(2):782–874, 2015.

[38] T. Tao, V. Vu, and M. Krishnapur. Random matrices: Universality of ESDs and the circular

law. Ann. Probab., 38(5):2023–2065, 2010.

[39] J. Yin. The local circular law III: general case. Probab. Theory Relat. Fields, 160(3):679–732,

2014.

93

