6
1
0
2

 
r
a

 

M
5
1

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
3
1
6
4
0

.

3
0
6
1
:
v
i
X
r
a

AMinorization–MaximizationAlgorithmforHeteroscedasticRegressionHienD.Nguyen12,LukeR.Lloyd-Jones3,andGeoﬀreyJ.McLachlan1∗March15,2016AbstractThecomputationofthemaximumlikelihood(ML)estimatorforhet-eroscedasticregressionmodelsisconsidered.ThetraditionalNewtonal-gorithmsfortheproblemrequiresmatrixmultiplicationsandinversions,whicharebottlenecksinmodernBigDatacontexts.AnewBigData-appropriateminorization–maximization(MM)algorithmisconsideredforthecomputationoftheMLestimator.TheMMalgorithmisprovedtogeneratemonotonicallyincreasingsequencesoflikelihoodvaluesandtobeconvergenttoaglobalmaximumofthelog-likelihoodfunction.Adis-tributedandparallelimplementationoftheMMalgorithmispresentedandtheMMalgorithmisshowntohavediﬀeringtimecomplexitytotheNewtonalgorithm.SimulationstudiesdemonstratesthattheMMalgo-rithmimprovesuponthecomputationtimeoftheNewtonalgorithminsomepracticalscenarioswherethenumberofobservationsislarge.Keywords:Minorization–maximizationalgorithm;heteroscedasticregression;maximumlikelihoodestimation;parallelalgorithm1IntroductionOneofthemajorchallengesoftheanalysisofBigDataistherequirementtomakefundamentalandstandardstatisticalprocessesapplicableinthepresenceofthevariouscomputationalchallenges;see[1,2]fordetails.OnemajorthemeofBigDataresearchistoconstructalgorithmsforstandardstatisticalprocessesthatallowforparallelizationanddistributedcomputing.LetY1,...,YnbeanindependentandidenticallydistributedsamplesuchthatYiisnormalwithmeanµi(xi)andvarianceσ2i(xi)fori=1,...,n,wherexTi=(xi1,...,xid)∈Rdarecovariatevectors.ThesuperscriptTindicatesmatrixtransposition.Supposethatweobservetherealizationsy1,...,ynand∗1SchoolofMathematicsandPhysics,UniversityofQueensland(Email:h.nguyen7@uq.edu.au;g.mclachlan@uq.edu.au).2CentreforAdvancedImaging,Uni-versityofQueensland.3QueenslandBrainInstitute,UniversityofQueensland(Email:l.lloydjones@uq.edu.au).1wishtoestimatetheparametricmeanandvariancefunctionsµi(xi)=βTxiandσ2i(xi)=σ2,respectively,whereβT=(β1,...,βd)∈Rdandσ2>0.Thecharacterizationdescribesthestandardlinearregressionmodel,andtheparametervectorofthemodel,θT=(cid:0)β,σ2(cid:1),canbeestimatedviathemaximumlikelihood(ML)estimatorˆθ=argmax(β,σ2)nYi=1φ(cid:0)yi;βTxi,σ2(cid:1),(1)whereφ(cid:0)y,µ,σ2(cid:1)=(cid:0)2πσ2(cid:1)−1/2exp(cid:16)−[y−µ]2/(cid:2)2σ2(cid:3)(cid:17)isthenormaldensityfunctioniny,withmeanµandvarianceσ2.Basedontheminorization–maximizationframework(MM;see[3]fordetails),[4]con-structedasimplealgorithmforcomputing(1)thatallowsforparallelizationanddistributedcomputing.Inthisletter,weconsiderthecasewheretheparametricmeanandvari-ancefunctionsarecharacterizedbyµi(xi)=βTxiandσ2i(xi)=exp(cid:0)αTxi(cid:1),whereαT=(α1,...,αd)∈Rd.Thecharacterizationdescribesthemultiplica-tiveheteroscedasticityregressionmodelof[5];seealso[6]and[7,Sec.11.7].Theparametervectorofthemodel,ψT=(α,β),canbeestimatedviatheMLestimatorˆψ=argmax(α,β)nYi=1φ(cid:0)yi;βTxi,exp(cid:2)αTxi(cid:3)(cid:1).(2)TheMLestimator(2)canbecomputedviaaNewtonalgorithm[7,Sec.11.7],whichisnotappropriateforparralelizationanddistributedcomputing.Thisisduetotheneedfortherepeatedinversionofd×dmatrices,whichcanbothbelargeornumericallysingular.InarecentreviewofBigDataalgo-rithms,[8]presentednospecializedalgorithmsorsoftwareforregressionunderheteroscedasticity.Weextendupontheworkof[4]toproduceaBigData-appropriateMMalgorithmforthecomputationof(2),usingtherecentdevel-opmentsingeometricandsignomialprogrammingof[9].AlthoughitispossibletoparallelizetheNewtonalgorithmviamatrixparallelizationtechniquessuchasthosethatarediscussedin[10,11,12],ouralgorithmpermitsamoreintuitiveandsimpleimplementation.Weuserecentresultsfromoptimizationinsignalsprocessing[13,14]toestablishglobalconvergenceresultsforthederivedalgorithm.WealsobrieﬂystudythetimecomplexityoftheMMalgorithm,andoutlineaframeworkforaparallelanddistributedimplementationofthealgorithm.Simulationstudiesisconductedtodemonstratethecomputationalperformanceofthealgorithminbothserialandparallelimplementations.ComparisonsbetweentheMMalgorithmandtheNewtonalgorithmaremade.Theletterproceedsasfollows.TheMMalgorithmanditsresultsregardingitsconvergencearepresentedinSectionII.WediscussaparallelimplementationinSectionIII,andwestudythetimecomplexityofthealgorithminSectionIV.2SimulationstudiesarepresentedinSectionV.ConclusionsaredrawninSectionVI.2TheMMAlgorithmTheMLestimator(2)canberewrittenasˆψ=argmax(α,β)‘(α,β),(3)where‘(α,β)=nXi=1logφ(cid:0)yi;βTxi,exp(cid:2)αTxi(cid:3)(cid:1)(4)isthelog-likelihoodfunction.Deﬁnetheblockwiseminorizerof‘(α,β)atthepoint˜ψT=(cid:16)˜α,˜β(cid:17),intheαblock,asafunctionQα(cid:16)α;˜ψ(cid:17)forα∈Rd,withthepropertiesthat(i)‘(cid:16)˜α,˜β(cid:17)=Qα(cid:16)˜α;˜ψ(cid:17)and(ii)‘(cid:16)α,˜β(cid:17)≥Qα(cid:16)α;˜ψ(cid:17);theblockwiseminorizerintheβblockissimilarlydeﬁnedasQβ(cid:16)β;˜ψ(cid:17)forβ∈Rd,withproperties(i)and(ii)replacedby‘(cid:16)˜α,˜β(cid:17)=Qβ(cid:16)˜β;˜ψ(cid:17)and‘(˜α,β)≥Qβ(cid:16)β;˜ψ(cid:17)forβ∈Rd,respectively.Letψ(0)beaninitialvalue;ablockwiseMMalgorithmforcomputing(3)isdeﬁnedviatheupdateschemeψ(r+1)T=(cid:16)argmaxαQα(cid:0)α;ψ(r)(cid:1),β(r)(cid:17)ifrisodd,(cid:18)α(r),argmaxβQβ(cid:0)β;ψ(r)(cid:1)(cid:19)ifriseven,whereψ(r)T=(cid:0)α(r),β(r)(cid:1)istherthiterateofthealgorithm.Thefollowingpropositionsprovidesα-andβ-blockwiseminorizersfor‘.Proposition1.Givenψ(r),thelog-likelihood(4)canbeα-blockwiseminorizedbytheminorizerQα(cid:16)α;ψ(r)(cid:17)=−n2log(2π)−12nXi=1dXj=1αjxij−12dnXi=1dXj=1(cid:0)yi−xTiβ(r)(cid:1)2exp(cid:16)dxij(cid:16)αj−α(r)j(cid:17)+xTiα(r)(cid:17).(5)3Proposition2.Givenψ(r),thelog-likelihood(4)canbeβ-blockwiseminorizedbytheminorizerQβ(cid:16)β;ψ(r)(cid:17)=−n2log(2π)−12nXi=1dXj=1α(r)jxij−12dnXi=1dXj=1(cid:16)yi−dxij(cid:16)βi−β(r)i(cid:17)−xTiβ(r)(cid:17)2exp(cid:0)xTiα(r)(cid:1).(6)Propositions1and2areadaptedfrom[9,Eqn.7]and[4,Eqn.4.5].Weobservethat(6)islinearlyseparableinβjforj=1,...,d,andthateachβjoccurswithinaconcavequadraticexpression.Thus,(6)isconcaveinβandwesolvetheﬁrst-orderconditionequation∇Qβ(cid:0)β;ψ(r)(cid:1)=0toobtainβ∗=argmaxβQβ(cid:16)β;ψ(r)(cid:17),where∇isthegradientoperator,0isavectorofzeros,β∗T=(β∗1,...,β∗d),andβ∗j=β(r)j+Pni=1xij(cid:0)yi−xTiβ(r)(cid:1)exp(cid:0)−xTiα(r)(cid:1)dPni=1x2ijexp(cid:0)−xTiα(r)(cid:1).Next,weobservethat(5)islinearlyseparableinαjforj=1,...,d.Fur-ther,eachαjoccurswithinalinearcompositioninsideofanegativeexponentialfunction,whichimpliesthat(5)isconcaveinα;see[15]regardingconvexalge-bra.Unfortunatelythereisnoclosed-formsolutionfortheﬁrst-orderconditionequation∇Qα(cid:0)α;ψ(r)(cid:1)=0.However,wecancomputeα∗=argmaxαQα(cid:16)α;ψ(r)(cid:17)byconsideringthepartialderivativeequations(∂/∂αj)Qα(cid:0)α;ψ(r)(cid:1)=0foreachj=1,...,dinstead,whereα∗T=(α∗1,...,α∗d).Thesolutionto(∂/∂αj)Qα(cid:0)α;ψ(r)(cid:1)=0canbeobtainedviaaNewtonalgorithmusingtheﬁrstandsecondpartialderivatives∂Qα∂αj=−12nXi=1xij+12nXi=1xij(cid:0)yi−xTiβ(r)(cid:1)2exp(cid:16)dxij(cid:16)αj−α(r)j(cid:17)+xTiα(r)(cid:17)and∂2Qα∂α2j=−12nXi=1x2ij(cid:0)yi−xTiβ(r)(cid:1)2exp(cid:16)dxij(cid:16)αj−α(r)j(cid:17)+xTiα(r)(cid:17).4Alternatively,abisectionalgorithmcanbeusedtoobtaintherootofeachpartialderivativeequation;seeforexample[16,Sec.9.1.1].UsingPropositions1and2,theMMalgorithmforcomputing(3)canbedeﬁnedviatheupdateschemeψ(r+1)T=((cid:0)α∗,β(r)(cid:1)ifrisodd,(cid:0)α(r),β∗(cid:1)ifriseven,(7)whereα∗andβ∗areobtainedviathedescriptionsabove.2.1ConvergenceAnalysisStartingfromsomeinitialvalueψ(0),updatescheme(7)isrepeateduntilsomenumericalconvergencecriterionisreached;forexample,thealgorithmcanbeterminatedonce‘(cid:0)α(r+1),β(r+1)(cid:1)−‘(cid:0)α(r),β(r)(cid:1)<forsomesmall>0.Upontermination,theﬁnaliterateofthealgorithmisdeclaredtheMLestimatorˆψ.See[17,Sec.11.5]regardingtherelativemeritsofvariousconvergencecriteria.Letψ(∞)=limr→∞ψ(r)(oralternatively,ˆψ→ψ(∞)as<0)bealimitpointoftheblockwiseMMalgorithm.Wehave,fromPropositions1and2,that(5)and(6)areα-andβ-blockwiseminorizersof(4),respectively.Further,both(5)and(6)areconcaveandsmoothintherespectiveparametercomponents.Thus,theMMalgorithmdeﬁnedby(7)satisﬁestheassumptionsof[13,Thm.2],whichyieldsthefollowingresult.Proposition3.Letψ(r)beasequenceofblockwiseMMalgorithmiterates(asdeﬁnedby(7))withlimitψ(∞),forsomeinitialvalueψ(0).Thefollowingstatementsaretrue.(a)Thesequenceoflog-likelihoodvalues‘(cid:0)α(r),β(r)(cid:1)ismonotonicallyincreas-inginr.(b)Thelimitpointψ(∞)isastationarypointofthelog-likelihoodfunc-tion‘(α,β).Astrongerresultcanbeobtainedbynotingthat(4)isnonlinear(inψ)onlyintermsthatarenegativesofproductsofpositiveconvexfunctioncompositionsoflinearcombinations.Assuch,‘(α,β)isaconcavefunctioninψ;again,see[15]regardingconvexalgebra.Wethereforehavethefollowingresult.Proposition4.Everylimitpointψ(∞)oftheblockwiseMMalgorithm(asdeﬁnedby(7))isaglobalmaximizerofthelog-likelihoodfunction‘(α,β).ThemonotonicityresultfromProposition3guaranteesthattheblockwiseMMalgorithmisstableandwillnottakeastepthatdetrimentstheobjectivelog-likelihoodvalue.Proposition4thenguaranteesthattheMLestimatesob-tainedbythealgorithmconvergestoaglobalmaximizerof(4),givensuﬃcientiterations.53AParallelandDistributedImplementationSupposethatwehaveamasterprocessingelement(PE)ManduptodslavePEsS1–Sd.Storey1,...,ynineachslavePE,andpartitionstorethevectorx1j,...,xnjonSjforj=1,...,d.Storeaninstanceoftheparametervectorψ(0)onthemasterPEandeachoftheslavePEs.Toinitializethealgorithm,haveeachSjsendMthequantitiesxijα(0)jandxijβ(0)jforeachi.ThemasterPEMthencomputesxTiα(0)andxTiβ(0)foreachi,andsendsthequantitiestoeachoftheslavesSj.Ateachodditerationr+1,Msendsα(r)j,xTiα(r),andxTiβ(r)foreachi,toeachoftherespectiveslavePEsSj.EachSjthencomputesα∗jandsendsα∗jandxijα∗jforeachitoM.ThemasterPEMthencombinesthequantitiesα∗jandxijα∗jtoproduceα(r+1)andxTiα(r+1)foreachi,respectively.Ateacheveniterationr+1,Msendsβ(r)j,xTiα(r),andxTiβ(r)foreachi,toeachoftherespectiveslavePEsSj.EachSjthencomputesβ∗jandsendsβ∗jandxijβ∗jforeachitoM.Mthencombinesthequantitiesβ∗jandxijβ∗jtoproduceβ(r+1)andxTiβ(r+1)foreachi,respectively.Notethatafterinitialization,theimplementationrequiresthestorageofonly2d+2nreal-valuedquantitiesonM,andthestorageofonly2+4nquantitiesonSjforeachj,atanyiterationr>0.Furthermore,ateachiteration1+2nquantitiesaresentfromMtoeachoftheslavesSjandeachslavesends1+nquantitiesbacktoM.ThealgorithmthatisdescribedrequiresnomatrixcomputationsandallowsforthedatatobedistributedbetweenuptodslavePEs.TheroleofeachofthedPEscanbepartitionedoverasmallernumberofPEsiflessthandPEsareavailable.In[9],itisnotedthatsuchdistributedalgorithmsforsigno-mialprogramming-typeproblemsarebestimplementedinparallelviagraphicsprocessingunits.Descriptionsofsuchimplementationscanbefoundin[18].4TimeComplexityLetNNewtonn,betheaveragenumberofiterationsrequiredfortheNewtonalgo-rithmforthecomputationof(2)[7,Sec.11.7]toconvergefornobservationsandsomecriterionthreshold.Startingfromsomeinitialvalueψ(0),the(r+1)thiterationoftheNewtonalgorithm,ψ(r+1),requiresthecomputationofthetwostepsβ(r+1)="nXi=1xixTiexp(cid:0)xTiα(r)(cid:1)#−1nXi=1xiyiexp(cid:0)xTiα(r)(cid:1)6andα(r+1)=α(r)−"nXi=1xixTi#−1nXi=1xi+"nXi=1xixTi#−1Pni=1xi(cid:0)yi−xTiβ(r+1)(cid:1)2exp(cid:0)xTiα(r)(cid:1).Weobservethatbothstepsaredominatedbythesumsofnouterproducesofddimensionalvectors,andd×dmatrixinversions.ThesumofproductshasorderO(cid:0)nd2(cid:1)andtheinversionhasorderO(cid:0)d3(cid:1);see[16,Secs.2.1–2.3].TheoverallorderisthusO(cid:16)NNewtonn,(cid:2)nd2+d3(cid:3)(cid:17).LetNMMn,betheaveragenumberofcycles(anoddandanevenstep)re-quiredfortheblockwiseMMalgorithmtoconverge,fornobservationsandsomecriticalthreshold.From(7),weobservethatwhenriseven,thecomputationofβ∗requiresthecomputationofxTiβ(r)andxTiα(r)once,foreachi=1,...,n,whichrequiresO(nd)operations.GivenxTiβ(r)andxTiα(r),thecomputationofeachβ∗jrequiresO(n)operations,foreachj=1,...,d,thustheoverallcom-plexityisO(nd)whenriseven.Ineachoddstep,eitheraNewtonalgorithmorbisectionalgorithmisrequiredtoevaluatetherootsofeachofthedequations(∂/∂αj)Qα(cid:0)α;ψ(r)(cid:1)=0.Tosolvetheseequations,xTiβ(r)andxTiα(r)arerequiredtobecomputedonce,foreachi.LetNRootn,bethenumberofiter-ationsrequiredbytheroot-ﬁndingalgorithm.GivenxTiβ(r)andxTiα(r),thedominanttermineachroot-ﬁndingalgorithmiterationisdominatedbyntimesaconstantnumberofoperations,foreachofthedcomponentsofα∗.Therefore,eachoddstephascomplexityorderO(cid:16)nd+NRootn,nd(cid:17).TheoverallorderisthusO(cid:16)NMMn,hnd+NRootn,ndi(cid:17).5SimulationStudiesWenowreportonasetofsimulationstudies.Inoursimulationstudies,wegen-erateasampleofn=100,1000,10000observationsfromthemodelφ(cid:0)y;βTx,exp(cid:2)αTx(cid:3)(cid:1),whered∈{5,10,20,50}inallcasesofn.Here10αj,βj,andxijareeachran-domlygeneratedfromastandardnormaldistribution.Usingeachsample,wecomputeˆψviatheMMalgorithmandtheNewtonalgorithmof[7,Sec.11.7].Theprocessisrepeated100time;thecomputationtimeandconvergencestatusofthealgorithmisrecordedfromeachrepetition.Theaverageandstandarddeviationofcomputationtimes,arereportedinTableI,respectively.Alsore-portedinTableIisthetheoreticalcomputationtimeoftheMMalgorithmunderparallelization.Thisiscomputedasthecomputationtimedividedbyd,wheredisthemaximumpossiblenumberofslavePEsthatcanbeused,asdescribedinSectionIV.ThetheoreticalcomputationtimeunderparallelizationassumesnegligiblecommunicationtimesbetweenPEs.7Table1:Averagecomputationtimes(over100replications)foreachalgorithmandscenarioarepresentedininboldface.Standarddeviationsarepresentedinitalics.∗NoreplicationoftheNewtonalgorithmconvergedinthisscenario.d=5102050n=100Newton0.00070.00250.0804—∗0.00050.00090.3682—∗MM(Serial)0.00280.02270.61554.76690.00090.00680.35281.4667MM(Parallel)0.00030.00210.05460.42460.00020.00180.04870.00031000Newton0.02800.05500.12220.48460.00220.00610.00900.0326MM(Serial)0.01530.09080.50527.04120.00230.00990.04470.4897MM(Parallel)0.00140.00840.04680.65270.00110.00640.03480.481210000Newton3.92435.525110.743830.04190.31810.19011.05193.3048MM(Serial)0.16650.86544.797864.52580.01720.05740.20674.9660MM(Parallel)0.01540.08060.44605.93940.01160.06070.33564.3808ThealgorithmswereappliedviaimplementationsintheRprogrammingen-vironment([19];version3.2.2)onanIntelCorei7CPUrunningat2.40GHzwith16GBinternalRAM,andthetimingwasconductedusingtheproc.timefunctionfromsaidenvironment.Furthermore,allcomputationsoflog-likelihoodvalues,minorizers,andderivativesareperformedusingfunctionsthatareprogrammedinCandintegratedviaRcppandRcpparmadillo[20].TheMMalgorithmisthresholdedusingtheconvergencecriteriondescribedinSectionII.Aandtheconstant=10−3.AnabsoluteconvergencecriterionisusedforallNewtonalgorithmswithathresholdconstantof=10−3;see[17,Sec.11.5].5.1ResultsThereareanumberofnotablefeaturesfromTableI.Firstly,wenotethattheNewtonalgorithmcouldnotbeimplementedontherelativelysmalldatacaseofd=50andn=100.Uponinspection,theNewtonalgorithmsuﬀersfromproblemsofrankdeﬁcienciesintheinversionofrankdeﬁcientmatrices,causingthesequenceoflog-likelihoodvaluestodiverge.Secondly,foralldinthen=100case(wherecomparable),theparallelMMalgorithmisfasterthantheNewtonalgorithm,whichisfasterthantheserialMMalgorithm.Thistrendchangesinthen=1000casewheretheserial81020304050−8−6−4−2024Log−Ratios of Computation TimesdLog−Ratio (Base 2)Figure1:Averagelog-ratios(base2)areplottedforcasesn=100,1000,10000.RatiosbetweentheserialMMandNewtonalgorithmsarepresentedassolidlines.RatiosbetweentheparallelMMandNewtonalgorithmsarepresentedasdashedlines.Circles,triangles,andplusesindicaten=100,1000,10000,respectively.Thedottedlineindicatesunitratio.MMalgorithmisfasterthantheNewtonalgorithmwhend=5.Inthen=10000case,weobservethattheserialMMalgorithmisfasterthantheNewtonalgorithmwhend=5,10,20.WeconcludethattheserialimplementationoftheMMalgorithmisfasterthantheNewtonalgorithmwhendissmallandnislarge.Furthermore,inoursimulations,theMMalgorithmismorestablethantheNewtonalgorithmandcanbeappliedwheretheNewtonalgorithmmayfail.InFigure1,weplottheaveragelog-ratiosofcomputationtimesbetweentheserialMMandNewtonalgorithms,andthetheoreticalparallelMMandNewtonalgorithms.Fromtheassessedscenarios,weobservethattheserialimplementationoftheMMalgorithmcanbemorethan23timesfasterthantheNewtonalgorithm(d=5,n=10000)orasslowas15timesthecomputationtimeoftheNewtonalgorithm(d=50,n=1000),onaverage.Theparallelimplementationcanbemorethan254timesfasterthantheNewtonalgorithm(d=5,n=10000)orasslowas1.4timesthecomputationtimeoftheNewtonalgorithm(d=50,n=1000)onaverage.WefurtherobservethatthecomputationtimeratiosarebetweentheMMalgorithm,inbothserialandparallel,aredecreasinginnandincreasingind.Thus,itcanberecommendedthattheMMalgorithmispreferabletotheNewtonalgorithmincaseswherenislargeanddisrelativelysmall.6ConclusionsTheubiquityofBigDatahasintroducednumerousproblemsfordataanalysts.TraditionalmethodologiessuchasthecomputationoftheMLestimator,as9characterizedbyequation(2),canbediﬃcultintheBigDatacontext.Thisisduetomatrixoperationbottlenecks,anddiﬃcultyindistributionandparal-lelizationoftheNewtonalgorithmforthecomputationof(2).Inthisletter,weintroduceanMMalgorithmforthecomputationof(2)thatrequiresnomatrixoperations.Thealgorithmisshowntobegloballyconvergentandtogeneratemonotonicsequencesoflog-likelihoodvalues.Furthermore,adistributedandparallelimplementationoftheMMalgorithmisdescribedanditisshownthattheMMalgorithmhasadiﬀerentorderofcomputationalcomplexitytotheNewtonalgorithm.Viasimulationstudies,serialimplementationisdemonstratedtobeupto23timesfasterandtheparallelimplementationishypothesizedtobeupto254timesfasterthantheNewtonalgorithm,whennislargeanddisrelativelysmall.WethusrecommendtheuseoftheMMalgorithminsuchscenarios.References[1]J.Fan,F.Han,andH.Liu,“Challengesofbigdataanalysis,”NationalScienceReview,vol.1,pp.293–314,2014.[2]X.Wu,X.Zhu,G.-Q.Wu,andW.Ding,“Dataminingwithbigdata,”IEEETransactionsonKnowledgeandDataEngineering,vol.26,pp.97–107,2014.[3]D.R.HunterandK.Lange,“AtutorialonMMalgorithms,”TheAmericanStatistician,vol.58,pp.30–37,2004.[4]M.P.Becker,I.Yang,andK.Lange,“EMalgorithmswithoutmissingdata,”StatisticalMethodsinMedicalResearch,vol.6,pp.38–54,1997.[5]G.E.P.BoxandR.D.Meyer,“Dispersioneﬀectsfromfractionaldesigns,”Technometrics,vol.28,pp.19–27,1986.[6]M.DavidianandR.J.Carroll,“Variancefunctionestimation,”JournaloftheAmericanStatisticalAssociation,vol.82,pp.1079–1091,1987.[7]W.H.Greene,EconometricAnalysis.NewJersey:PrenticeHall,2003.[8]C.Wang,M.-H.Chen,E.Schifano,J.Wu,andJ.Yan,“Statisticalmethodsandcomputingforbigdata,”ArXiv,vol.1502.07989v2,2015.[9]K.LangeandH.Zhou,“MMalgorithmsforgeometricandsignomialpro-gramming,”MathematicalProgrammingSeriesA,vol.143,pp.339–356,2014.[10]L.Csanky,“Fastparallelmatrixinversionalgorithms,”SIAMJournalofComputing,vol.5,pp.618–623,1976.[11]E.Dekel,D.Nassimi,andS.Sahni,“Parallelmatrixandgraphalgorithms,”SIAMJournalofComputing,vol.10,pp.657–675,1981.10[12]E.S.Quintana,G.Quintana,X.Sun,andR.VandeGeijn,“Anoteonparallelmatrixinversion,”SIAMJournalofScientiﬁcComputing,vol.22,pp.1762–1771,2001.[13]M.Razaviyayn,M.Hong,andZ.-Q.Luo,“Auniﬁedconvergenceanalysisofblocksuccessiveminimizationmethodsfornonsmoothoptimization,”SIAMJournalofOptimization,vol.23,pp.1126–1153,2013.[14]M.Hong,M.Razaviyayn,Z.-Q.Luo,andJ.-S.Pang,“Auniﬁedalgorithmicframeworkforblock-structuredoptimizationinvolvingbigdata:withap-plicationsinmachinelearningandsignalprocess,”IEEESignalProcessingMagazine,vol.33,pp.57–77,2016.[15]S.BoydandL.Vandenberghe,ConvexOptimization.Cambridge:Cam-bridgeUniversityPress,2004.[16]W.H.Press,S.A.Teukolsky,W.T.Vetterling,andB.P.Flannery,Nu-mericalRecipes:TheArtofScientiﬁcComputing.Cambridge:CambridgeUniversityPress,2007.[17]K.Lange,Optimization.NewYork:Springer,2013.[18]H.Zhou,K.Lange,andM.A.Suchard,“Graphicalprocessunitsandhigh-dimensionaloptimization,”StatisticalScience,vol.25,pp.311–324,2010.[19]RCoreTeam,R:alanguageandenvironmentforstatisticalcomputing.RFoundationforStatisticalComputing,2013.[20]D.Eddelbuettel,SeamlessRandC++IntegrationwithRcpp.NewYork:Springer,2013.11