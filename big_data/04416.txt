6
1
0
2

 
r
a

 

M
4
1

 
 
]

G
L
.
s
c
[
 
 

1
v
6
1
4
4
0

.

3
0
6
1
:
v
i
X
r
a

Criteria of eﬃciency for conformal prediction

Vladimir Vovk, Valentina Fedorova,

Ilia Nouretdinov, and Alex Gammerman

{v.vovk,valentina,ilia,alex}@cs.rhul.ac.uk

March 15, 2016

Abstract

We study optimal conformity measures for various criteria of eﬃciency
in an idealized setting. This leads to an important class of criteria of
eﬃciency that we call probabilistic; it turns out that the most standard
criteria of eﬃciency used in literature on conformal prediction are not
probabilistic.

1

Introduction

Conformal prediction is a method of generating prediction sets that are guaran-
teed to have a prespeciﬁed coverage probability; in this sense conformal predic-
tors have guaranteed validity. Diﬀerent conformal predictors, however, widely
diﬀer in their eﬃciency, by which we mean the narrowness, in some sense, of
their prediction sets. Empirical investigation of the eﬃciency of various con-
formal predictors is becoming a popular area of research: see, e.g., [1, 11] (and
the COPA Proceedings, 2012–2015). This paper points out that the standard
criteria of eﬃciency used in literature have a serious disadvantage, and we de-
ﬁne a class of criteria of eﬃciency, called “probabilistic”, that do not share this
disadvantage. In two recent papers [3, 5] two probabilistic criteria have been
introduced, and in this paper we introduce two more and argue that probabilis-
tic criteria should be used in place of more standard ones. We concentrate on
the case of classiﬁcation only (the label space is ﬁnite).

Surprisingly few criteria of eﬃciency have been used in literature, and even
fewer have been studied theoretically. We can speak of the eﬃciency of individ-
ual predictions or of the overall eﬃciency of predictions on a test sequence; the
latter is usually (in particular, in this paper) deﬁned by averaging the eﬃciency
over the individual test examples, and so in this introductory section we only
discuss the former. This section assumes that the reader knows the basic deﬁni-
tions of the theory of conformal prediction, but they will be given in Section 2,
which can be consulted now.

The two criteria for eﬃciency of a prediction that have been used most often

in literature (in, e.g., the references given above) are:

1

• The conﬁdence and credibility of the prediction (see, e.g., [13], p. 96;
introduced in [12]). This criterion does not depend on the choice of a
signiﬁcance level .

• Whether the prediction is a singleton (the ideal case), multiple (an ineﬃ-
cient prediction), or empty (a supereﬃcient prediction) at a given signiﬁ-
cance level . This criterion was introduced in [10], Section 7.2, and used
extensively in [13].

The other two criteria that have been used are the sum of the p-values for all
potential labels (this does not depend on the signiﬁcance level) and the size of
the prediction set at a given signiﬁcance level: see the papers [3] and [5].

In this paper we introduce six other criteria of eﬃciency (Section 2). We
then discuss (in Sections 3–5) the conformity measures that optimize each of the
ten criteria when the data-generating distribution is known; this sheds light on
the kind of behaviour implicitly encouraged by the criteria even in the realistic
case where the data-generating distribution is unknown. As we point out in
Section 5, probabilistic criteria of eﬃciency are conceptually similar to “proper
scoring rules” in probability forecasting [2, 4], and this is our main motivation
for their detailed study in this paper. After that we brieﬂy illustrate the em-
pirical behaviour of two of the criteria for standard conformal predictors and a
benchmark data set (Section 6).

We only consider the case of randomized (“smoothed”) conformal predictors:
the case of deterministic predictors may lead to packing problems without an
explicit solution (this is the case, e.g., for the N criterion deﬁned below). The
situation here is analogous to the Neyman–Pearson lemma: cf. [7], Section 3.2.

2 Criteria of Eﬃciency for Conformal Predic-

tors and Transducers

Let X be a measurable space (the object space) and Y be a ﬁnite set equipped
with the discrete σ-algebra (the label space); the example space is deﬁned to be
Z := X × Y. A conformity measure is a measurable function A that assigns
to every ﬁnite sequence (z1, . . . , zn) ∈ Z∗ of examples a same-length sequence
(α1, . . . , αn) of real numbers and that is equivariant with respect to permuta-
tions: for any n and any permutation π of {1, . . . , n},

(α1, . . . , αn) = A(z1, . . . , zn) =⇒(cid:0)απ(1), . . . , απ(n)

(cid:1) = A(cid:0)zπ(1), . . . , zπ(n)

(cid:1) .

The conformal predictor determined by A is deﬁned by
Γ(z1, . . . , zl, x) := {y | py > } ,

(1)
where (z1, . . . , zl) ∈ Z∗ is a training sequence, x is a test object,  ∈ (0, 1) is a
given signiﬁcance level, for each y ∈ Y the corresponding p-value py is deﬁned
by

2

py :=

1

l + 1

(cid:12)(cid:12)(cid:8)i = 1, . . . , l + 1 | αy

(cid:9)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:8)i = 1, . . . , l + 1 | αy

l+1

i < αy
τ

+

l + 1

(cid:9)(cid:12)(cid:12) ,

(2)

i = αy

l+1

τ is a random number distributed uniformly on the interval [0, 1] (even con-
ditionally on all the examples), and the corresponding sequence of conformity
scores is deﬁned by

(αy

1, . . . , αy

l , αy

l+1) := A(z1, . . . , zl, (x, y)).

Notice that the system of prediction sets (1) output by a conformal predictor is
decreasing in , or nested.
The conformal transducer determined by A outputs the system of p-values
(py | y ∈ Y) deﬁned by (2) for each training sequence (z1, . . . , zl) of examples
and each test object x. (This is just a diﬀerent representation of the conformal
predictor.)

The standard property of validity for conformal predictors and transducers
is that the p-values py are distributed uniformly on [0, 1] when the examples
z1, . . . , zl, (x, y) are generated independently from the same probability distri-
bution Q on Z (see, e.g., [13], Proposition 2.8). This implies that the probability
of error, y /∈ Γ(z1, . . . , zl, x), is  at any signiﬁcance level .

Suppose we are given a test sequence (zl+1, . . . , zl+k) and would like to use it
to measure the eﬃciency of the predictions derived from the training sequence
(z1, . . . , zl). (The eﬃciency of conformal predictors means that the prediction
sets they output tend to be small, and the eﬃciency of conformal transducers
means that the p-values that they output tend to be small.) For each test
i |  ∈ (0, 1))
example zi = (xi, yi), i = l +1, . . . , l +k, we have a nested family (Γ
| y ∈ Y). In this paper we will
of subsets of Y and a system of p-values (py
i
discuss ten criteria of eﬃciency for such a family or a system, but some of them
will depend, additionally, on the observed labels yi of the test examples. We
start from the prior criteria, which do not depend on the observed test labels.

2.1 Basic criteria

We will discuss two kinds of criteria: those applicable to the prediction sets Γ
i
and so depending on the signiﬁcance level  and those applicable to systems of
i | y ∈ Y) and so independent of . The simplest criteria of eﬃciency
p-values (py
are:

• The S criterion (with “S” standing for “sum”) measures eﬃciency by the

average sum

1
k

l+k(cid:88)

(cid:88)

i=l+1

y

py
i

of the p-values; small values are preferable for this criterion. It is -free.

3

• The N criterion uses the average size

l+k(cid:88)

i=l+1

1
k

|Γ
i|

of the prediction sets (“N” stands for “number”: the size of a prediction
set is the number of labels in it). Small values are preferable. Under this
criterion the eﬃciency is a function of the signiﬁcance level .

Both these criteria are prior. The S criterion was introduced in [3] and the N
criterion was introduced independently in [5] and [3], although the analogue of
the N criterion for regression (where the size of a prediction set is deﬁned to
be its Lebesgue measure) had been used earlier in [9] (whose arXiv version was
published in 2012).

2.2 Other prior criteria

A disadvantage of the basic criteria is that they look too stringent. Even for
a very eﬃcient conformal transducer, we cannot expect all p-values py to be
small: the p-value corresponding to the true label will not be small with high
probability; and even for a very eﬃcient conformal predictor we cannot expect
the size of its prediction set to be zero: with high probability it will contain
the true label. The other prior criteria are less stringent. The ones that do not
depend on the signiﬁcance level are:

• The U criterion (with “U” standing for “unconﬁdence”) uses the average

unconﬁdence

1
k

l+k(cid:88)

i=l+1

min

y

max
y(cid:48)(cid:54)=y

py(cid:48)

i

(3)

over the test sequence, where the unconﬁdence for a test object xi is the
second largest p-value miny maxy(cid:48)(cid:54)=y py(cid:48)
i ; small values of (3) are preferable.
The U criterion in this form was introduced in [3], but it is equivalent to
using the average conﬁdence (one minus unconﬁdence), which is very com-
mon. If two conformal transducers have the same average unconﬁdence
(which is presumably a rare event), the criterion compares the average
credibilities

max

y

py
i ,

(4)

l+k(cid:88)

i=l+1

1
k

where the credibility for a test object xi is the largest p-value maxy py
i ;
smaller values of (4) are preferable. (Intuitively, a small credibility is a
warning that the test object is unusual, and since such a warning presents
useful information and the probability of a warning is guaranteed to be
small, we want to be warned as often as possible.)

4

• The F criterion uses the average fuzziness

l+k(cid:88)

1
k

(cid:32)(cid:88)

y

i − max
py

y

(cid:33)

py
i

,

(5)

i=l+1

values apart from a largest one, i.e., as(cid:80)

where the fuzziness for a test object xi is deﬁned as the sum of all p-
i ; smaller values of
(5) are preferable. If two conformal transducers lead to the same average
fuzziness, the criterion compares the average credibilities (4), with smaller
values preferable.

i − maxy py

y py

Their counterparts depending on the signiﬁcance level are:

• The M criterion uses the percentage of objects xi in the test sequence for
which the prediction set Γ
i at signiﬁcance level  is multiple, i.e., contains
more than one label. Smaller values are preferable. As a formula, the
criterion prefers smaller

1{|Γ

i|>1},

(6)

where 1E denotes the indicator function of the event E (taking value 1 if
E happens and 0 if not). When the percentage (6) of multiple predictions
is the same for two conformal predictors (which is a common situation:
the percentage can well be zero), the M criterion compares the percentages

l+k(cid:88)

i=l+1

1
k

l+k(cid:88)

i=l+1

1
k

1{Γ

i =∅}

(7)

of empty predictions (larger values are preferable). This is a widely used
criterion. (In particular, it was used in [13] and papers preceding it.)

• The E criterion (where “E” stands for “excess”) uses the average (over
the test sequence, as usual) amount the size of the prediction set exceeds
1. In other words, the criterion gives the average number of excess labels
in the prediction sets as compared with the ideal situation of one-element
prediction sets. Smaller values are preferable for this criterion. As a
formula, the criterion prefers smaller

l+k(cid:88)

i=l+1

1
k

(|Γ

i| − 1)+ ,

where t+ := max(t, 0). When these averages coincide for two conformal
predictors, we compare the percentages (7) of empty predictions; larger
values are preferable.

5

2.3 Observed criteria

The prior criteria discussed in the previous subsection treat the largest p-value,
or prediction sets of size 1, in a special way. The corresponding criteria of this
subsection attempt to achieve the same goal by using the observed label.

These are the observed counterparts of the non-basic prior -free criteria:
• The OU (“observed unconﬁdence”) criterion uses the average observed

unconﬁdence

1
k

l+k(cid:88)

i=l+1

max
y(cid:54)=yi

py
i

over the test sequence, where the observed unconﬁdence for a test example
i for the false labels y (cid:54)= yi. Smaller values
(xi, yi) is the largest p-value py
are preferable for this test.

• The OF (“observed fuzziness”) criterion uses the average sum of the p-

values for the false labels, i.e.,

1
k

smaller values are preferable.

l+k(cid:88)

(cid:88)

i=l+1

y(cid:54)=yi

py
i ;

(8)

The counterparts of the last group depending on the signiﬁcance level  are:
• The OM criterion uses the percentage of observed multiple predictions

l+k(cid:88)

i=l+1

1
k

1{Γ

i\{yi}(cid:54)=∅}

in the test sequence, where an observed multiple prediction is deﬁned to
be a prediction set including a false label. Smaller values are preferable.
• The OE criterion (OE standing for “observed excess”) uses the average

number

1
k

l+k(cid:88)

i=l+1

|Γ
i \ {yi}|

of false labels included in the prediction sets at signiﬁcance level ; smaller
values are preferable.

The ten criteria used in this paper are given in Table 1. Half of the criteria
depend on the signiﬁcance level , and the other half are the respective -free
versions.
In the case of binary classiﬁcation problems, |Y| = 2, the number of diﬀerent
criteria of eﬃciency in Table 1 reduces to six: the criteria not separated by a
vertical or horizontal line (namely, U and F, OU and OF, M and E, and OM
and OE) coincide.

6

Table 1: The ten criteria studied in this paper: the two basic ones in the upper
section; the four other prior ones in the middle section; and the four observed
ones in the lower section

-free

S (sum of p-values)

U (unconﬁdence)

F (fuzziness)

-dependent

N (number of labels)

M (multiple)

E (excess)

OU (observed unconﬁdence) OM (observed multiple)

OF (observed fuzziness)

OE (observed excess)

3 Optimal Idealized Conformity Measures for a

Known Probability Distribution

Starting from this section we consider the limiting case of inﬁnitely long training
and test sequences (and we will return to the realistic ﬁnitary case only in Sec-
tion 6, where we describe our empirical studies). To formalize the intuition of an
inﬁnitely long training sequence, we assume that the prediction algorithm is di-
rectly given the data-generating probability distribution Q on Z instead of being
given a training sequence. Instead of conformity measures we will use idealized
conformity measures: functions A(Q, z) of Q ∈ P(Z) (where P(Z) is the set of
all probability measures on Z) and z ∈ Z. We will ﬁx the data-generating dis-
tribution Q for the rest of the paper, and so write the corresponding conformity
scores as A(z). The idealized conformal predictor corresponding to A outputs
the following prediction set Γ(x) for each object x ∈ X and each signiﬁcance
level  ∈ (0, 1). For each potential label y ∈ Y for x deﬁne the corresponding
p-value as
py = p(x, y) := Q{z ∈ Z | A(z) < A(x, y)} + τ Q{z ∈ Z | A(z) = A(x, y)} (9)
(it would be more correct to write A((x, y)) and Q({. . .}), but we often omit
pairs of parentheses when there is no danger of ambiguity), where τ is a random
number distributed uniformly on [0, 1]. (The same random number τ is used
in (9) for all (x, y).) The prediction set is

Γ(x) := {y ∈ Y | p(x, y) > } .

(10)

The idealized conformal transducer corresponding to A outputs for each object
x ∈ X the system of p-values (py | y ∈ Y) deﬁned by (9); in the idealized case
we will usually use the alternative notation p(x, y) for py.

The standard properties of validity for conformal transducers and predictors

mentioned in the previous section simplify in this idealized case as follows:
• If (x, y) is generated from Q, p(x, y) is distributed uniformly on [0, 1].

7

• Therefore, at each signiﬁcance level  the idealized conformal predictor

makes an error with probability .

The test sequence being inﬁnitely long is formalized by replacing the use of a
test sequence in the criteria of eﬃciency by averaging with respect to the data-
generating probability distribution Q. In the case of the top two and bottom two
criteria in Table 1 (the ones set in italics) this is done as follows. Let us write
Γ
A(x) for the Γ(x) in (10) and pA(x, y) for the p(x, y) in (9) to indicate the
dependence on the choice of the idealized conformity measure A. An idealized
conformity measure A is:

• S-optimal if, for any idealized conformity measure B,

Ex,τ

pA(x, y) ≤ Ex,τ

pB(x, y),

(cid:88)

y∈Y

(cid:88)

y(cid:48)(cid:54)=y

(cid:88)

y∈Y

B(x)| ;
(cid:88)

y(cid:48)(cid:54)=y

where the notation Ex,τ refers to the expected value when x and τ are
independent, x ∼ QX, and τ ∼ U ; QX is the marginal distribution of Q
on X, and U is the uniform distribution on [0, 1];

• N-optimal if, for any idealized conformity measure B and any signiﬁcance

level ,

Ex,τ |Γ

A(x)| ≤ Ex,τ |Γ

• OF-optimal if, for any idealized conformity measure B,

E(x,y),τ

pA(x, y(cid:48)) ≤ E(x,y),τ

pB(x, y(cid:48)),

where the lower index (x, y) in E(x,y),τ refers to averaging over (x, y) ∼ Q
(with (x, y) and τ independent);

• OE-optimal if, for any idealized conformity measure B and any signiﬁcance

level ,

E(x,y),τ |Γ

A(x) \ {y}| ≤ E(x,y),τ |Γ

B(x) \ {y}| .

We will deﬁne the idealized versions of the other six criteria listed in Table 1 in
Section 5.

4 Probabilistic Criteria of Eﬃciency

Our goal in this section is to characterize the optimal idealized conformity mea-
sures for the four criteria of eﬃciency that are set in italics in Table 1. We will
assume in the rest of the paper that the set X is ﬁnite (from the practical point
of view, this is not a restriction); since we consider the case of classiﬁcation,
|Y| < ∞, this implies that the whole example space Z is ﬁnite. For simplic-
ity, we also assume that the data-generating probability distribution Q satisﬁes

8

QX(x) > 0 for all x ∈ X (we often omit curly braces in expressions such as
QX({x})).

The conditional probability (CP) idealized conformity measure is

A(x, y) := Q(y | x) :=

Q(x, y)
QX(x)

.

(11)

This idealized conformity measure was introduced by an anonymous referee
of the conference version of [3], but its non-idealized analogue in the case of
regression had been used in [9] (following [8] and literature on minimum volume
prediction). We say that an idealized conformity measure A is a reﬁnement of
an idealized conformity measure B if

B(z1) < B(z2) =⇒ A(z1) < A(z2)

(12)
for all z1, z2 ∈ Z. Let R(CP) be the set of all reﬁnements of the CP idealized
conformity measure. If C is a criterion of eﬃciency (one of the ten criteria in
Table 1), we let O(C) stand for the set of all C-optimal idealized conformity
measures.
Theorem 1. O(S) = O(OF) = O(N) = O(Œ) = R(CP).

We omit the proof of Theorem 1 (and also the proofs Theorems 2–4 below)

in this version of the paper.

Let us say that an eﬃciency criterion is probabilistic if the CP idealized
conformity measure is optimal for it. Theorem 1 shows that four of our ten
criteria are probabilistic, namely S, N, OF, and OE (they are set in italics in
Table 1). In the next section we will see that in general the other six criteria
are not probabilistic. The intuition behind probabilistic criteria will be brieﬂy
discussed also in the next section.

5 Criteria of Eﬃciency that are not Probabilis-

tic

Now we deﬁne the idealized analogues of the six criteria that are not set in
italics in Table 1. An idealized conformity measure A is:

• U-optimal if, for any idealized conformity measure B, we have either

Ex,τ min

y

max
y(cid:48)(cid:54)=y

pA(x, y(cid:48)) < Ex,τ min

y

pB(x, y(cid:48))

max
y(cid:48)(cid:54)=y

or both

and

Ex,τ min

y

max
y(cid:48)(cid:54)=y

pA(x, y(cid:48)) = Ex,τ min

y

pB(x, y(cid:48))

max
y(cid:48)(cid:54)=y

Ex,τ max

y

pA(x, y) ≤ Ex,τ max

y

pB(x, y);

9

pA(x, y) − max

y

pA(x, y)

pA(x, y) − max

y

pA(x, y)

< Ex,τ

= Ex,τ

pB(x, y) − max

y

pB(x, y)

pB(x, y) − max

y

pB(x, y)

(cid:17)
(cid:17)

(cid:16)(cid:88)
(cid:16)(cid:88)

y

y

(cid:16)(cid:88)
(cid:16)(cid:88)

y

Ex,τ

or both
Ex,τ

y

and

• M-optimal if, for any idealized conformity measure B and any signiﬁcance

level , we have either

Px,τ (|Γ

A(x)| > 1) < Px,τ (|Γ

B(x)| > 1)

or both

and

Px,τ (|Γ

A(x)| > 1) = Px,τ (|Γ

B(x)| > 1)

Px,τ (|Γ

A(x)| = 0) ≥ Px,τ (|Γ

B(x)| = 0);

• F-optimal if, for any idealized conformity measure B, we have either

(cid:17)
(cid:17)

Ex,τ max

y

pA(x, y) ≤ Ex,τ max

y

pB(x, y);

• E-optimal if, for any idealized conformity measure B and any signiﬁcance

level , we have either

or both

Ex,τ

Ex,τ

(cid:0)(|Γ
A(x)| − 1)+(cid:1) < Ex,τ
(cid:0)(|Γ
A(x)| − 1)+(cid:1) = Ex,τ

(cid:0)(|Γ
B(x)| − 1)+(cid:1)
(cid:0)(|Γ
B(x)| − 1)+(cid:1)

and

B(x)| = 0);
• OU-optimal if, for any idealized conformity measure B,

A(x)| = 0) ≥ Px,τ (|Γ

Px,τ (|Γ

E(x,y),τ max
y(cid:48)(cid:54)=y

pA(x, y(cid:48)) ≤ E(x,y),τ max
y(cid:48)(cid:54)=y

pB(x, y(cid:48));

• OM-optimal if, for any idealized conformity measure B and any signiﬁ-

cance level ,

P(x,y),τ (Γ

A(x) \ {y} (cid:54)= ∅) ≤ P(x,y),τ (Γ

B(x) \ {y} (cid:54)= ∅).

In the following three deﬁnitions we follow [13], Chapter 3. The predictability
of x ∈ X is

Q(y | x).

f (x) := max
y∈Y

10

A choice function ˆy : X → Y is deﬁned by the condition

∀x ∈ X : f (x) = Q(ˆy(x) | x).

Deﬁne the signed predictability idealized conformity measure corresponding to ˆy
by

(cid:40)

A(x, y) :=

f (x)
−f (x)

if y = ˆy(x)
if not;

a signed predictability (SP) idealized conformity measure is the signed pre-
dictability idealized conformity measure corresponding to some choice function.
For the following two theorems we will need to modify the notion of reﬁne-
ment. Let R(cid:48)(SP) be the set of all idealized conformity measures A such that
there exists an SP idealized conformity measure B that satisﬁes both (12) and

B(x, y1) = B(x, y2) =⇒ A(x, y1) = A(x, y2)

for all x ∈ X and y1, y2 ∈ Y.
Theorem 2. O(U) = O(M) = R(cid:48)(SP).

Deﬁne the MCP (modiﬁed conditional probability) idealized conformity mea-

sure corresponding to a choice function ˆy by

(cid:40)

A(x, y) :=

Q(y | x)
Q(y | x) − 1

if y = ˆy(x)
if not;

an MCP idealized conformity measure is an idealized conformity measure cor-
responding to some choice function; R(cid:48)(MCP) is deﬁned analogously to R(cid:48)(SP)
but using MCP rather than SP idealized conformity measures.
Theorem 3. O(F) = O(E) = R(cid:48)(MCP).

The modiﬁed signed predictability idealized conformity measure is deﬁned by

f (x)

0
−f (x)

A(x, y) :=

if f (x) > 1/2 and y = ˆy(x)
if f (x) ≤ 1/2
if f (x) > 1/2 and y (cid:54)= ˆy(x),

where f is the predictability function; notice that this deﬁnition is unaﬀected by
the choice of the choice function. Somewhat informally and assuming |Y| > 2
(we are in the situation of Theorem 1 when |Y| = 2), we deﬁne a set R(cid:48)(cid:48)(MSP)
in the same way as R(cid:48)(MSP) (analogously to R(cid:48)(SP)) except that for A ∈
R(cid:48)(cid:48)(MSP), f (x) = 1/2, and y (cid:54)= ˆy(x) we allow A(x, y) < A(x, ˆy(x)).
Theorem 4. If |Y| > 2, O(OU) = O(OM) = R(cid:48)(cid:48)(MSP).

11

Theorems 2–4 show that the six criteria that are not set in italics in Table 1
are not probabilistic (except for OU and OM when |Y| = 2, of course). Criteria
of eﬃciency that are not probabilistic are somewhat analogous to “improper
scoring rules” in probability forecasting (see, e.g., [2] and [4]). The optimal
idealized conformity measures for the criteria of eﬃciency given in this paper
that are not probabilistic have clear disadvantages, such as:

• They depend on the arbitrary choice of a choice function. In many cases
there is a unique choice function, but the possibility of non-uniqueness is
still awkward.

• They encourage “strategic behaviour” (such as ignoring the diﬀerences,
which may be very substantial, between potential labels other than ˆy(x)
for a test object x when using the M criterion).

However, we do not use the terminology “proper/improper” in the case of cri-
teria of eﬃciency for conformal prediction since it is conceivable that some
non-probabilistic criteria of eﬃciency may turn out to be useful.

6 Empirical Study

In this section we demonstrate diﬀerences between two of our -free criteria, OF
(probabilistic) and U (standard but not probabilistic) on the USPS data set of
hand-written digits [6]. We use the original split of the data set into the training
and test sets. Our programs are written in R, and the results presented in the
ﬁgures below are for the seed 0 of the R random number generator; however,
we observe similar results in experiments with other seeds.
The problem is to classify handwritten digits, the labels are elements of
{0, . . . , 9}, and the objects are elements of R256, where the 256 numbers repre-
sent the brightness of pixels in 16 × 16 pictures. We normalize each object by
applying the same aﬃne transformation (depending on the object) to each of
its pixels making the mean brightness of the pixels in the picture equal to 0 and
making its standard deviation equal to 1. The sizes of the training and test sets
are 7291 and 2007, respectively.

We evaluate six conformal predictors using the two criteria of eﬃciency. Fix
a metric on the object space R256; in our experiments we use tangent distance
(as implemented by Daniel Keysers) and Euclidean distance. Given a sequence
of examples (z1, . . . , zn), zi = (xi, yi), we consider the following three ways of
computing conformity scores: for i = 1, . . . , n,

j=1 d

j , where d

(cid:54)=
j=1 d=
j are the distances, sorted in the in-
creasing order, from xi to the objects in (z1, . . . , zn) with labels diﬀerent
(cid:54)=
1 is the smallest distance from xi to an object xj with
from yi (so that d
yj (cid:54)= yi), and d=
j are the distances, sorted in the increasing order, from
xi to the objects in (z1, . . . , zi−1, zi+1, . . . , zn) labelled as yi (so that d=
1 is
the smallest distance from xi to an object xj with j (cid:54)= i and yj = yi). We
refer to this conformity measure as the KNN-ratio conformity measure; it

• αi := (cid:80)K

j /(cid:80)K

(cid:54)=

12

has one parameter, K, whose range is {1, . . . , 50} in our experiments (so
that we always have K (cid:28) n).

• αi := Ni/K, where Ni is the number of objects labelled as yi among
the K nearest neighbours of xi (when dK = dK+1 in the ordered list
d1, . . . , dn−1 of the distances from xi to the other objects, we choose the
nearest neighbours randomly among zj with yj = yi and with xj at a
distance of dK from xi). This conformity measure is a KNN counterpart
of the CP idealized conformity measure (cf. (11)), and we will refer to
it as the KNN-CP conformity measure; its parameter K is in the range
{2, . . . , 50} in our experiments.
• ﬁnally, we deﬁne fi := maxy(N y

i /K), where N y

i

labelled as y among the K nearest neighbours of xi, ˆyi ∈ arg maxy(N y
(chosen randomly from arg maxy(N y

i /K) if |arg maxy(N y

is the number of objects
i /K)
i /K)| > 1), and

(cid:40)

αi :=

fi
−fi

if yi = ˆyi
otherwise;

this is the KNN-SP conformity measure.

The three kinds of conformity measures combined with the two metrics (tangent
and Euclidean) give six conformal predictors.

Figure 1 gives the average unconﬁdence (3) (top panel) and the average
observed fuzziness (8) (bottom panel) over the test sequence (so that k = 2007)
for a range of the values of the parameter K. Each of the six lines corresponds
to one of the conformal predictors, as shown in the legends; in black-and-white
the lines of the same type (dotted, solid, or dashed) corresponding to Euclidean
and tangent distances can always be distinguished by their position: the former
is above the latter.

The best results are for the KNN-ratio conformity measure combined with
tangent distance for small values of the parameter K. For the two other types
of conformity measures their relative evaluation changes depending on the kind
of a criterion used to measure eﬃciency: as expected, the KNN-CP conformal
predictors are better under the OF criterion, whereas the KNN-SP conformal
predictors are better under the U criterion (cf. Theorems 1 and 2), if we ignore
small values of K (when the probability estimates N y
i /K are very unreliable).

Acknowledgments

This work was partially supported by EPSRC (grant EP/K033344/1), the Air
Force Oﬃce of Scientiﬁc Research (grant “Semantic Completions”), and the EU
Horizon 2020 Research and Innovation programme (grant 671555).

13

References

[1] Vineeth N. Balasubramanian, Shen-Shyang Ho, and Vladimir Vovk, edi-
tors. Conformal Prediction for Reliable Machine Learning: Theory, Adap-
tations, and Applications. Elsevier, Amsterdam, 2014.

[2] A. Philip Dawid. Probability forecasting. In Samuel Kotz, N. Balakrishnan,
Campbell B. Read, Brani Vidakovic, and Norman L. Johnson, editors,
Encyclopedia of Statistical Sciences, volume 10, pages 6445–6452. Wiley,
Hoboken, NJ, second edition, 2006.

[3] Valentina Fedorova, Alex Gammerman, Ilia Nouretdinov, and Vladimir
Vovk. Conformal prediction under hypergraphical models. In Harris Pa-
padopoulos, Andreas S. Andreou, Lazaros Iliadis, and Ilias Maglogiannis,
editors, Artiﬁcial Intelligence Applications and Innovations. Second Work-
shop on Conformal Prediction and Its Applications (COPA 2013), pages
371–383, Heidelberg, 2013. Springer.

[4] Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules,
prediction, and estimation. Journal of the American Statistical Association,
102:359–378, 2007.

[5] Ulf Johansson, Rikard K¨onig, Tuve L¨ofstr¨om, and Henrik Bostr¨om. Evolved
decision trees as conformal predictors. In Luis Gerardo de la Fraga, editor,
Proceedings of the 2013 IEEE Conference on Evolutionary Computation,
volume 1, pages 1794–1801, Cancun, Mexico, 2013.

[6] Yann Le Cun, Bernhard E. Boser, John S. Denker, Donnie Henderson,
R. E. Howard, Wayne E. Hubbard, and Lawrence D. Jackel. Handwritten
digit recognition with a back-propagation network. In David S. Touretzky,
editor, Advances in Neural Information Processing Systems 2, pages 396–
404. Morgan Kaufmann, San Francisco, CA, 1990.

[7] Erich L. Lehmann. Testing Statistical Hypotheses. Springer, New York,

second edition, 1986.

[8] Jing Lei, James Robins, and Larry Wasserman. Distribution free prediction

sets. Journal of the American Statistical Association, 108:278–287, 2013.

[9] Jing Lei and Larry Wasserman. Distribution free prediction bands for
nonparametric regression. Journal of the Royal Statistical Society B, 76:71–
96, 2014.

[10] Thomas Melluish, Craig Saunders, Ilia Nouretdinov, and Vladimir Vovk.
Comparing the Bayes and typicalness frameworks. In Luc De Raedt and
Peter A. Flach, editors, Proceedings of the Twelfth European Conference
on Machine Learning, volume 2167 of Lecture Notes in Computer Science,
pages 360–371, Heidelberg, 2001. Springer.

14

[11] Harris Papadopoulos, Alex Gammerman, and Vladimir Vovk, editors. Spe-
cial Issue of the Annals of Mathematics and Artiﬁcial Intelligence on Con-
formal Prediction and its Applications, volume 74(1–2). Springer, 2015.

[12] Craig Saunders, Alex Gammerman, and Vladimir Vovk. Transduction with
conﬁdence and credibility. In Thomas Dean, editor, Proceedings of the Six-
teenth International Joint Conference on Artiﬁcial Intelligence, volume 2,
pages 722–726, San Francisco, CA, 1999. Morgan Kaufmann.

[13] Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic Learning

in a Random World. Springer, New York, 2005.

[14] Vladimir Vovk, Ivan Petej, and Valentina Fedorova. From conformal to
probabilistic prediction. In Lazaros Iliadis, Ilias Maglogiannis, Harris Pa-
padopoulos, Spyros Sioutas, and Christos Makris, editors, AIAI Work-
shops, COPA 2014, volume 437 of IFIP Advances in Information and Com-
munication Technology, pages 221–230, 2014.

15

Figure 1: Top plot: average unconﬁdence for the USPS data set (for diﬀerent
values of parameters). Bottom plot: average observed fuzziness for the USPS
data set. In black-and-white the lines of the same type (dotted, solid, or dashed)
corresponding to Euclidean and tangent distances can always be distinguished
by their position: the former is above the latter.

16

Kunconfidence510152025303540455000.0020.0040.0060.0080.01Euclidean KNN−ratiotangent KNN−ratioEuclidean KNN−CPtangent KNN−CPEuclidean KNN−SPtangent KNN−SPKobserved fuzziness510152025303540455000.020.040.060.080.1Euclidean KNN−ratiotangent KNN−ratioEuclidean KNN−CPtangent KNN−CPEuclidean KNN−SPtangent KNN−SP