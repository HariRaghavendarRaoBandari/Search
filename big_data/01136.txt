Multilevel Sequential Monte Carlo Samplers for

Pierre Del Moral∗

Normalizing Constants
Kody Law‡

Ajay Jasra†

Yan Zhou§

March 4, 2016

Abstract

This article considers the sequential Monte Carlo (SMC) approxima-
tion of ratios of normalizing constants associated to posterior distributions
which in principle rely on continuum models. Therefore, the Monte Carlo
estimation error and the discrete approximation error must be balanced.
A multilevel strategy is utilized to substantially reduce the cost to obtain
a given error level in the approximation as compared to standard esti-
mators. Two estimators are considered and relative variance bounds are
given. The theoretical results are numerically illustrated for the example
of identifying a parametrized permeability in an elliptic equation given
point-wise observations of the pressure.

Key words: Multi-Level Monte Carlo, Sequential Monte Carlo, Bayesian
Inverse Problems.
AMS subject classiﬁcation: 82C80, 60K35.

1

Introduction

Over the past decades there has been an explosion of interest in accounting for
uncertainty in the simulation of systems in science and engineering applications
which are governed by continuum limiting systems such as partial diﬀerential
equations (PDEs) [18, 23, 24]. The setting bears similarities to the case of
continuous stochastic processes, which have enjoyed attention for much longer
(e.g. [19]).

6
1
0
2

 
r
a

M
3

 

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
6
3
1
1
0

.

3
0
6
1
:
v
i
X
r
a

de Bordeaux I, 33405, FR

∗Center INRIA Bordeaux Sud-Ouest & Institut de Mathematiques de Bordeaux, Universite
†Department of Statistics & Applied Probability, National University of Singapore, Singa-
‡Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge,
§Department of Statistics & Applied Probability, National University of Singapore, Singa-

pore, 117546, SG

37934 TN, USA

pore, 117546, SG

1

Consider a sequence of probability measures {ηl}l≥0 on a common mea-
surable space (E,E); assume that the probabilities have common dominating
ﬁnite-measure du. In particular, for some known κl : E → R+, let

where the normalizing constant Zl = (cid:82)

ηl(du) =

κl(u)

Zl

du

(1)

E κl(u)du may be unknown. The con-
text of interest is when the sequence of densities is associated to an ‘accuracy’
parameter hl, with hl → 0 as l → ∞ with ∞ > h0 > h1 > ··· > h∞ = 0.
When estimating statistics Eη∞ [g(U )], for g : E → R, in general one must
approximate the limiting measure by ηL and perform statistical estimation with
respect to this. For larger L, the approximation of the limit is better, and yet the
simulations are more expensive and indeed the measure may also be supported
on a subspace of the underlying space E whose dimension is larger.
Monte Carlo methods for statistical estimation are robust and scalable, al-
though they are plagued by a “slow” convergence rate of O(N−1/2) for ap-
proximations using N degrees of freedom. Attempts to circumvent this issue,
for example using sophisticated deterministic high-dimensional approximation
methods typically result in some manifestation of the “curse of dimensionality”
[2], although recent work has indicated potential for the mitigation of such eﬀect
for suitably regular problems [6, 21].

The multilevel Monte Carlo framework [10, 11, 12] allows one to leverage in
an optimal way the nested problems arising in this context, hence minimizing
the necessary cost to obtain a given level of mean square error. In particular, the
multilevel Monte Carlo method seeks to sample from η0 as well as a sequence of
coupled pairs (η0, η1), . . . , (ηL−1, ηL) and using a collapsing sum representation
of EηL [g(U )]. Then using a suitable trade oﬀ of computational eﬀort, one can
reduce the amount of work, relative to i.i.d. sampling from ηL and using Monte
Carlo integration, for a given amount of error. However, we are concerned
with the scenario where such independent sampling is not possible, that is,
either ηL or from the sequence of couples. As it is well-known, the use of
importance sampling to then use the collapsing sum representation, is often
not reasonable, in the sense that for importance proposals that can be sampled
independently, the associated variance typically explodes exponentially in the
dimension of the problem (e.g. [5]). As a result, there has been an extension of
multilevel Monte Carlo methods in which the approximate target distribution
can be sampled from directly, to more sophisticated Monte Carlo techniques
for inference; however, this is still in its infancy. Important examples include
the preliminary exploration of multilevel Markov chain Monte Carlo (MCMC)
[13, 17], multilevel sequential Monte Carlo samplers [4], multilevel ensemble
Kalman ﬁlter [14] and multilevel particle ﬁlters [16]. It should be noted that
MCMC and SMC can perform at a polynomial cost in the dimension; see e.g. [3]
and the references therein.
A signiﬁcant challenge for inference problems is estimation of the normalizing
constant ZL or ratios thereof Zl/Zk, L ≥ l > k ≥ 0. Such quantities are
central to Bayesian model comparison and choice [15, 25]. In addition, obtaining

2

unbiased estimates (in the sense that the expectation is equal to the value,
that is, potentially including discretization bias) are often central in pseudo-
marginal algorithms (e.g. [1]). In general the calculation of these quantities are
notoriously challenging (see for instance [26]) from a computational perspective.
In this article we extend the framework of [4] to consider the estimation of the
ratio of normalizing constants. This is a framework which uses SMC. We con-
sider both the ‘standard’ unbiased estimator ([8]) used in SMC, adapted to the
multilevel setting and an estimator which follows the collapsing sum approach
for multilevel methods. For the latter, we introduce a novel decomposition of the
normalizing constant of a Feynman-Kac formula, which corresponds to ZL/Z0,
which facilitates unbiased estimation. We consider new variance bounds for the
estimator [8] and our new estimate and show that, in general, both approaches
perform in a similar manner. For a given level of error, the cost is less than a
Monte Carlo estimate that uses i.i.d. sampling from η0, to estimate ZL/Z0; we
assume that the former is possible.

The paper is structured as follows. In Section 2 the setup will be given, along
with a description of the multilevel algorithm and the new novel estimator for
the normalizing constant. Section 3 contains the theoretical results, including
the main theorems of the paper which allow the multilevel theory to carry
through. Finally, section 4 presents the results of numerical experiments on an
example Bayesian inverse problem. The proofs are housed in the appendix.

2 Estimation

2.1 Notations
Let (E,E) be a measurable space. The notation Bb(E) denotes the class of
bounded and measurable real-valued functions. The supremum norm is written
as (cid:107)f(cid:107)∞ = supu∈E |f (u)| and P(E) is the set of probability measures on (E,E).
We will consider non-negative operators K : E × E → R+ such that for each
u ∈ E the mapping A (cid:55)→ K(u, A) is a ﬁnite non-negative measure on E and for
each A ∈ E the function u (cid:55)→ K(u, A) is measurable; the kernel K is Markovian
if K(u, dv) is a probability measure for every u ∈ E. For a ﬁnite measure µ on
(E,E), and a real-valued, measurable f : E → R, we deﬁne the operations:

µK : A (cid:55)→

We also write µ(f ) =(cid:82) f (u)µ(du).

K(u, A) µ(du) ; Kf : u (cid:55)→

f (v) K(u, dv).

(cid:90)

(cid:90)

2.2 Algorithm

As described in the Introduction, the context of interest is when a sequence of
densities {ηl}l≥0, as in (1), are associated to an ‘accuracy’ parameter hl, with
hl → 0 as l → ∞, such that ∞ > h0 > h1 ··· > h∞ = 0. In practice one cannot
treat h∞ = 0 and so must consider these distributions with hl > 0. The laws

3

with large hl are easy to sample from with low computational cost, but are very
diﬀerent from η∞, whereas, those distributions with small hl are hard to sample
with relatively high computational cost, but are closer to η∞. Thus, we choose
a maximum level L ≥ 1 and we will estimate

By the standard telescoping identity used in MLMC, one has

EηL[g(U )] :=

E

g(u)ηL(du) .

(cid:90)
(cid:110)Eηl [g(U )] − Eηl−1[g(U )]
(cid:111)
(cid:17)

(cid:104)(cid:16) κl(U )Zl−1

− 1

Eηl−1

κl−1(U )Zl

L(cid:88)
L(cid:88)

l=1

l=1

(cid:105)

EηL [g(U )] = Eη0[g(U )] +

= Eη0[g(U )] +

g(U )

.

(2)

Suppose now that one applies an SMC sampler [9] to obtain a collection of
samples (particles) that sequentially approximate η0, η1, . . . , ηL. We consider
the case when one initializes the population of particles by sampling i.i.d. from
η0, then at every step resamples and applies a MCMC Markov kernel to mutate
), with +∞ > N0 ≥ N1 ≥
the particles. We denote by (U 1:N0
··· NL−1 ≥ 1, the samples after mutation; one resamples U 1:Nl
according to the
l ), for indices l ∈ {0, . . . , L − 1}. We will denote
weights Gl(U i
by {Ml}1≤l≤L−1 the sequence of MCMC kernels used at stages 1, . . . , L − 1,
such that ηlMl = ηl. For ϕ : E → R, l ∈ {1, . . . , L}, we have the following
estimator of Eηl−1[ϕ(U )]:

l ) = (κl+1/κl)(U i

, . . . , U 1:NL−1

L−1

0

l

ηNl−1
l−1 (ϕ) =

1

Nl−1

ϕ(U i

l−1) .

Nl−1(cid:88)

i=1

We deﬁne

ηNl−1
l−1 (Gl−1Ml(dul)) =

Nl−1(cid:88)

i=1

1

Nl−1

Gl−1(U i

l−1)Ml(U i

l−1, dul) .

The joint probability distribution for the SMC algorithm is

N0(cid:89)

L−1(cid:89)

Nl(cid:89)

η0(dui
0)

i=1

l=1

i=1

ηNl−1
l−1 (Gl−1Ml(dui

l))

ηNl−1
l−1 (Gl−1)

.

The algorithm is summarized in Figure 1. If one considers one more step in
the above procedure, that would deliver samples {U i
L}NL
i=1, a standard SMC
sampler estimate of the quantity of interest in (2) is ηN
L (g); the earlier samples
are discarded. Within a multi-level context, a consistent SMC estimate of (2)
is

(cid:98)Y = ηN0

0 (g) +

L(cid:88)

(cid:110) ηNl−1

l−1 (gGl−1)
ηNl−1
l−1 (Gl−1)

− ηNl−1

l−1 (g)

l=1

,

(3)

(cid:111)

4

The motivation for such a procedure is that, as shown in [4], the amount of
work, for a given level of error, relative to i.i.d. sampling from ηL is reduced.
Thus the idea of using the approach is clear. However, as is well known in
the literature (e.g. [9]) SMC samplers can also estimate ratios of normalizing
constants as a by-product of the algorithm. We now consider this and also the
amount of work to obtain a given level of error in this context.

0. Sample U 1

0 , . . . U N0
0

i ∈ {1, . . . , N0}: Set l = 0.

i.i.d. from η0 and compute G0(ui

0) for each sample

1. Sample ˇU 1

l , . . . , ˇU Nl+1

l

probabilities {Gl(u1

with replacement from u1:Nl
j=1 Gl(uj

l ), . . . , (Gl(uNl

l )/(cid:80)Nl

2. Sample U i

sample i ∈ {1, . . . , Nl+1}.

l from Ml+1(ˇui

l+1|ˇui

l,·) and compute Gl+1(ui

l )/(cid:80)Nl

l

with selection
l )}.
j=1 Gl(uj
l+1) for each

3. Set l = l + 1. If l = L stop, otherwise return to the start of Step 1.

Figure 1: The SMC algorithm.

2.3 Normalizing Constant
Deﬁne, for l ≥ 0

(cid:90)

(cid:16) l−1(cid:89)

(cid:17)

l(cid:89)

p=1

η0(dul)

Mp(up−1, dup).

γl(dul) =

Gp(up)

El

p=0

In our context, it is well known that:

l−1(cid:89)

p=0

ηp(Gp).

γl(1) =

Zl
Z0

=

This suggests the estimator:

γN0:l−1
l

(1) =

l−1(cid:89)

p=0

ηNp
p (Gp)

(4)

which is known to be unbiased ([8]). We consider the relative variance of this
estimator in Section 3. However, at least on appearance it may not take ad-
vantage of the nature of the ML method. In addition, we show that the new
estimator below, can potentially be leveraged to remove the discretization bias.
We propose the following procedure. It should be remarked that it holds in
the particular context under study, but not for general Feynman-Kac models as

5

will be explained below. We have that for any l ≥ 2

(cid:17)

γl(1) = η0(G0) +

= η0(G0) +

= η0(G0) +

γp(1) − γp−1(1)

(cid:0)Gp−2(Mp−1(Gp−1) − 1)(cid:1)(cid:17)
(cid:0)Gp−2(Gp−1 − 1)(cid:1)(cid:17)

.

γp−2

γp−2

(cid:16)
(cid:16)
(cid:16)

p=2

l(cid:88)
l(cid:88)
l(cid:88)

p=2

p=2

It is the ﬁnal line that we will approximate with our MLSMC sampler. It is
noted that the ﬁnal line holds in the speciﬁc case of interest, but is not generally
true for a given Feynman-Kac formula. The proposed approximation is

˜γN0:l−2
l

(1) = ηN0

0 (G0) +

γN0:p−2
p−2

l(cid:88)

(cid:16)

(cid:0)Gp−2(Gp−1 − 1)(cid:1)(cid:17)

where for any g ∈ Bb(E), p ≥ 2

γN0:p−2
p−2

(g) =

p=2

(cid:16) p−3(cid:89)

Note that for l ≥ 2, one has, almost surely,

k=0

p−2 (g).

ηNk

k (Gk)(cid:1)ηNp−2
l−1(cid:89)

ηNp
p (Gp).

˜γN0:l−1
l

(1) (cid:54)=

Using [8] it clearly follows that

p=0

γl(1) = E[˜γN0:l−2

l

(1)]

where E is the expectation w.r.t. the law of the SMC algorithm; the estimator
is unbiased.

2.4 Biased Estimator

Noting the estimator (3) another alternative estimator of γl(1) is

(cid:32)

l−1(cid:89)

p(cid:88)

(cid:110) ηNl−1

ηN0
0 (G) +

p=0

l=1

l−1 (GpGl−1)
ηNl−1
l−1 (Gl−1)

− ηNl−1

l−1 (Gp)

One can easily prove that this estimate is consistent, but biased, in the sense
that

(cid:34) l−1(cid:89)

(cid:32)

E

p(cid:88)

(cid:110) ηNl−1

ηN0
0 (G) +

p=0

l=1

l−1 (GpGl−1)
ηNl−1
l−1 (Gl−1)

− ηNl−1

l−1 (Gp)

.

(cid:111)(cid:33)
(cid:111)(cid:33)(cid:105) (cid:54)= γl(1).

6

to the cost of computing this estimate. If (cid:80)l−1
(cid:80)l−1

However, the main reason why one may not want to consider its use is due
p=0 NpCp is the ordinary cost of
computing (4) (Cp is the cost per sample), then the cost of this estimator is
q=p Cq. Such a procedure is undesirable in general and this is not

(cid:80)l−1

p=0 Np

investigated further.

2.5 Estimator with no Discretization Bias
Let M ∈ {1, 2, . . .} be a random variable that is independent of the MLSMC
algorithm with PM (M ≥ m) > 0 ∀m > 0. Suppose further that one can prove
for N0, N1, . . . ﬁxed that

lim
p→∞

γN0:p−2
p−2

E(cid:104)(cid:16)
E(cid:104)(cid:16)
E(cid:104)(cid:16)

lim
p→∞

(cid:17)2(cid:105)1/2
(cid:0)Gp−2Gp−1) − γ∞(1)
(cid:17)2(cid:105)1/2
(cid:0)Gp−2) − γ∞(1)
(cid:17)2(cid:105)
(cid:0)Gp−2(Gp−1) − γ∞(1)

γN0:p−2
p−2

γN0:p−2
p−2

∞(cid:88)

p=2

1

PM (M ≥ p)

= 0

= 0

< +∞

(5)

(6)

(7)

then one can use the estimator from [20] to obtain an unbiased estimator for
γ∞(1):

1

PM (M ≥ 1)

ηN0
0 (G0) +

1

PM (M ≥ p)

γN0:p−2
p−2

M(cid:88)

p=2

(cid:16)

(cid:0)Gp−2(Gp−1 − 1)(cid:1)(cid:17)

.

Note that, even if one can prove (5)-(7), one must be prepared to spend an
arbitrary amount of computational cost, which is not reasonable in the current
context. Hence we do not consider this further here. We further remark that
this particular approach is unlikely to work when estimating Eη∞[g(U )] (as in
(2)) as there is no unbiased property of the estimators (unbiased in the sense of
expectations and not associated to the discretization).

3 Theory

3.1 Relative Variance Bounds

Throughout E is compact. We make the following assumptions:
(A1) There exist 0 < C < C < +∞ such that

sup
sup
u∈E
0≤l<∞
0≤l<∞ inf
inf
u∈E

κl(u) ≤ C ;
κl(u) ≥ C .

7

(A2) There exists a ρ ∈ (0, 1) such that for any l ≥ 1, (u, v) ∈ E2, A ∈ E:

(cid:90)

(cid:90)

Ml(u, du(cid:48)) ≥ ρ

Ml(v, dv(cid:48)) .

A

A

These assumptions are almost identical to those in [4].

(A1) is diﬀerent
but equivalent to (A1) in [4]. The proofs of the following Theorems are in
Appendices B and C respectively. It is remarked that there are other results in
the spirit of Theorem 1 below, (see [7, 22]) but the bounds are not sharp enough
for the purposes of this work.
Theorem 1. Assume (A1-2). Then there exists a c, C < +∞ such that for any
L ≥ 2, N0 ≥ N1 ≥ ··· ≥ NL−1 > cL,

L−1(cid:88)

p=0

C

(cid:16)(cid:16) L−1(cid:88)

q=p

1
Np

(1)

− 1

(cid:17)2

(cid:13)(cid:13)(cid:13)∞

− 1

(cid:17)

.

(p + 1)

Np

+

ηp(Gp)

Theorem 2. Assume (A1-2). Then there exists a c, C < +∞ such that for any
L ≥ 2, N0 ≥ N1 ≥ ··· ≥ NL−2 > c(L − 1),

(cid:17)2(cid:105) ≤
(cid:13)(cid:13)(cid:13) Gp
(cid:17)2(cid:105) ≤

− 1

ηq(Gq)

L
γL(1)

E(cid:104)(cid:16) γN0:L−1
(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13) Gq
E(cid:104)(cid:16) ˜γN0:L−2
L(cid:88)

L
γL(1)

(1)

− 1

p−1(cid:88)

p=2

q=2

(cid:16) 1

N0

C

L(cid:88)

p=2

+

(p − 1)
Np−2

3.2 Cost Analysis

(cid:107)Gp−1 − 1(cid:107)2∞ +

(q − 1)
Nq−2

(cid:107)Gp−1 − 1(cid:107)∞(cid:107)Gq−1 − 1(cid:107)∞

In order to investigate the cost for a given level of error, we introduce the
following assumption.

(A3) (i) There exist α, ζ > 0, and a C > 0 such that for all p > 0

(cid:40)| γp(1)

γ∞(1) − 1| ≤ Chα
p ;
≤ Ch−ζ
C(Gp−1)
p ,

(cid:17)

.

(8)

where C(Gp−1) denotes the cost to evaluate Gp−1.
(ii) There exist a β > 0 and a C > 0 such that for all p > 0

(cid:13)(cid:13)(cid:13) Gp−1

ηp−1(Gp−1)

(cid:13)(cid:13)(cid:13)2

∞

− 1

≤ Chβ
p .

(iii) There exist a β > 0 and a C > 0 such that for all p > 0

(cid:107)Gp−1 − 1(cid:107)2∞ ≤ Chβ
p .

8

Corollary 3.1. Assume (A1,2,3(i)(ii)) and 2α ≥ max{β, ζ}. Then for any
ε > 0, there exist L,{Nl}L

l=0 and C > 0 such that
(1) − γ∞(1)

(cid:17)2(cid:105) ≤ Cε2,

1

γ∞(1)2

for the following cost

COST ≤ C

if β > ζ,
if β = ζ,

if β < ζ.

α )| log(ε)|,

Corollary 3.2. Assume (A1,2,3(i)(iii)) and 2α ≥ max{β, ζ}. Then for any
ε > 0, there exist L,{Nl}L

l=0 and C > 0 such that
(1) − γ∞(1)

˜γN0:L−2
L

(cid:17)2(cid:105) ≤ Cε2,

1

γ∞(1)2

for the following cost

(9)

(10)

(11)

(12)

ε−2| log(ε)|3,
ε−(2+ ζ−β

γN0:L−2
L

E(cid:104)(cid:16)
ε−2| log(ε)|,
E(cid:104)(cid:16)
ε−2| log(ε)|,
(cid:13)(cid:13)(cid:13) Gp
L−1(cid:88)

ε−2| log(ε)|3,
ε−(2+ ζ−β

− 1

COST ≤ C

if β > ζ,
if β = ζ,

if β < ζ.

α )| log(ε)|,

We give the proof for Corollary 3.2 only. The proof of Corollary 3.1 is almost

identical. The only diﬀerence is treating the term in the relative variance of

(p + 1)

(cid:13)(cid:13)(cid:13)∞

(cid:17)2(cid:105)

N 2
p
which is smaller than O(2), under our assumptions.

ηp(Gp)

p=0

Proof of Corollary 3.2. The MSE can be bounded by
(1) − γ∞(1)

˜γN0:L−2
L

1

E(cid:104)(cid:16)
E(cid:104)(cid:16) ˜γN0:L−2

γ∞(1)2

(cid:17)2(cid:105) ≤
(cid:12)(cid:12)(cid:12)(cid:12)(cid:18) γL(1)

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)2

γ∞(1)

(cid:18) γL(1)
(cid:19)2
(cid:17)2
(cid:16) γL(1)
E(cid:104)(cid:16) ˜γN0:L−2

γ∞(1)

L
γL(1)

(1)

− 1

L
γL(1)

− 1
γ∞(1)
(cid:104) ε, and assuming
Following from (A3(i)), the second term requires that hα
hL = M−L for some M ≥ 2, this translates to L (cid:104) log ε. Notice that it also
L
= O(1). Now, deﬁning Vp = (cid:107)Gp−1 − 1(cid:107)2∞, Theorem 2
follows that
provides the following bound for the ﬁrst term

+

.

(cid:17)2(cid:105) ≤ V := C

(cid:32)

(1)

− 1

L−1(cid:88)

p=1

1
N0

+ L

Vp

Np−1

(cid:33)

.

9

To see this observe that

L−1(cid:88)

p(cid:88)

q

Nq−1

V 1/2
p V 1/2

q =

L−1(cid:88)

p=1

L−1(cid:88)

q=p

p

Np−1

V 1/2
p

q ≤ CL
V 1/2

L−1(cid:88)

p=1

Vp

Np−1

.

p=1

q=1

(cid:112)LVl/Cl (cid:104) L1/2h(β+ζ)/2
where KL =(cid:80)L−1

l

Optimizing the cost, given that the variance is O(ε2), dictates that Nl ∝

. The constraint then requires that Nl ∝ Lε−2KLh(β+ζ)/2

,

l

l=1 h(β−ζ)/2

. By assumption max{β, ζ} ≤ 2α, so (β + ζ)/2α ≤ 2
and the requirement for all the Nl in Theorem 2 is guaranteed (as long as the
proportionality constant is greater than 1). Therefore, the MSE is controlled
by O(ε2) with a cost given by

l

L(cid:88)

NlCl (cid:104) Lε−2K 2
L ,

and the result follows.

l=0

Remark 1. If one were able to perform i.i.d. sampling from η0 (denote the
samples u1, . . . , uN ), with estimator

N(cid:88)

i=1

1
N

γL(ui)
γ0(ui)

−ζ
for ZL/Z0 a computational eﬀort proportional to N h
L is used, with N the num-
ber of simulated samples. To make the overall error (bias squared plus variance)
of using i.i.d. sampling O(2) then one must take N ∝ O(−2), as the variance
of the MC estimate is O(N−1), independently of L. This is a computational
−ζ
cost of O(−2h
L ) is used which is far worse than MLSMC samplers in most
cases of practical interest.

4 Numerical Example

4.1 Setup

The performance of the proposed estimator will be demonstrated by a Bayesian
inverse problem example. The same example was also used in [4], which intro-
duced the MLSMC algorithm.
Introduce the Gelfand triple V := H 1(D) ⊂ L2(D) ⊂ H−1(D) =: V ∗, where
the domain D will be understood. Let D ⊂ Rd with ∂D ∈ C 1 convex. For
f ∈ V ∗, consider the following PDE on D:
−∇ · (ˆu∇p) = f,
p = 0,

on ∂D,

on D,

(14)

(13)

10

where

ˆu(x) = ¯u(x) +

K(cid:88)

k=1

ukσkφk(x).

(15)

Deﬁne u = {uk}K
Assume that ¯u, φk ∈ C∞ for all k and (cid:107)φk(cid:107) = 1. In particular {σk}K
with k. In addition, the following property shall hold:

i.i.d.∼ U[−1, 1] (the uniform distribution on [-1,1]).
k=1 decay

k=1, with uk

ˆu(x) ≥ inf

x

inf
x

σk ≥ u∗ > 0

(16)

¯u(x) − K(cid:88)

k=1

so that the operator on the left-hand side of Equation (13) is uniformly elliptic.
Let p(·; u) denote the weak solution of Equation (13) for parameter u. Deﬁne
the following vector-valued function

G(p) = [g1(p), . . . , gM (p)]T,

where gm are elements of the dual space V ∗ for m = 1, . . . , M . It is assumed
that the data take the form

y = G(p) + ξ,

ξ ∼ N (0, Ξ),

(17)
where N (0, Ξ) denotes the Normal distribution with zero mean and covariance
Ξ.
The speciﬁc setting of the simulations are as the following. Let D = [0, 1]
and f (x) = 100x. Set K = 50, ¯u(x) = 0.15 = const., σk = (2/5)4−k φk(x) =
sin(kπx) if k is odd and φk(x) = cos(kπx) if k is even. The forward problem at
resolution level l is solved using a ﬁnite element method with piecewise linear
shape functions on a uniform mesh of with hl = 2−(l+k), for some starting
k ≥ 1 (so that there are at least two grid-blocks in the coarsest, l = 0, case).
Thus, on level l the ﬁnite element basis functions are {ψl
deﬁned as (for
xi = i · 2−(l+k)):

i}2l+k−1

i=1

(cid:40)

ψl

i(x) =

(1/hl)[x − (xi − hl)]
(1/hl)[(xi + hl) − x]

if x ∈ [xi − hl, xi],
if x ∈ [xi, xi + hl].

The function of interest g is taken as the solution of the forward problem at the
midpoint of the domain, that is g(u) = p(0.5; u). The observation operator is
G(u) = [p(0.25; u), p(0.75; u)]T, and the observational noise covariance is taken
to be Ξ = 0.252I.

Detailed error rates analysis of this example can be found in [4]. In partic-
ular, when the purpose of the study was to estimate ηL(g), the variance rate
was β = 4 empirically. Later we will show that for estimating the normalizing
constant, the variance rate is very similar.

11

4.2 Veriﬁcation of Assumptions
Assumptions (A1) and (A3(i)(iii)) (for | γp(1)
Proposition 4.1 of [4]. For (A3(ii)) this follows directly from proving (A3(iii)).
It is natural to model the cost at level p by a power of the number degrees of
freedom, which is in turn related to h−1
p , verifying (A3(i)) (for C(Gp−1)). The
stiﬀness matrix of the ﬁnite element method is tridiagonal and thus the system
can be solved with cost O(2l+k), corresponding to a computational cost rate of
ζ = 1. Assumption (A2) is veriﬁed for Gibbs sampler in section 4.2 of [4].

γ∞(1)−1|), with β = 2α = 2, follow from

4.3 Experiments

We begin by using the theoretical rates β = 2α = 2 to estimate the MSE and
hence the cost ratio. Three cases are considered:

• A standard SMC algorithm, with the estimator γN0:l−1
• MLSMC sampler for γN0:l−1
• MLSMC sampler for ˜γN0:l−2

(1).

(1).

l

l

l

(1).

The cost vs. MSE is plotted in Figure 2. The cost rates are −1.271, −0.967, and
−1.038 for the SMC, MLSMC with the standard estimator, and MLSMC with
the new estimator, respectively.
It is clear that the MLSMC algorithm with
both estimators provides superior performance when compared to the standard
SMC algorithm. It is interesting that for the given MLSMC ensemble, the per-
formance of the new estimator is comparable to that of the standard estimator,
as proven in Corollaries 3.1 and 3.2. It shall be noted that in practice, given the
same samples (U 1:N0
), the new estimator is capable of estimating
γL+1(1) while the standard one can only estimate the γL(1), which has a higher
bias.

, . . . , U 1:NL−1

L−1

0

The variance rate β can also be estimated empirically by consider the vari-
ance of ηl(Gl). The quantity, multiplied by the sample size, as a proxy of Vl
is plotted in Figure 3. The estimated empirical rate is β = 4.148. This is
consistent with the rate estimates in [4].

Acknowledgements

KJHL was supported in part by DARPA FORMULATE and in part by ORNL
LDRD Strategic Hire. AJ & YZ were supported by Ministry of Education AcRF
tier 2 grant, R-155-000-161-112.

12

Figure 2: Computational cost against mean squared error

Figure 3: Variance rate estimate

13

2152202252302−302−252−202−15MSE(𝜀2)Runtimecost∝∑𝐿𝑙=0𝑁𝑙ℎ−1𝑙AlgorithmMLSMC(̃𝛾𝑁0∶𝑙−2𝑙(1))MLSMC(𝛾𝑁0∶𝑙−1𝑙(1))SMC(𝛾𝑁0∶𝑙−1𝑙(1))2−402−302−202−10202−122−102−82−62−4ℎ𝑙𝑉𝑙≈𝑁𝑙var[𝜂𝑁𝑙𝑙(𝐺𝑙)]A Notations
We give a collection of deﬁntions which are used in the appendices. Let n ≥ 0,
F ∈ Bb(E × E) and deﬁne

(γN0:n

n

)⊗2(F ) =

ηNp
p (Gp)

n )⊗2(F )
(ηNn

(cid:16) n−1(cid:89)

p=0

(cid:17)2

where for a ﬁnite (possibily signed) measure on E, µ, µ⊗2(d(u1, u2)) = µ(du1)µ(du2).
We recall the semi-group for p ≤ n (for p = n it is the identity operator):

(cid:90)

En−p−1

Qp,n(xp, dxn) =

Qp+1(xp, dxp+1) . . . Qn(xn−1, dxn)

where for n ≥ 1, Qn(x, dy) = Gn−1(x)Mn(x, dy). We also deﬁne the coalescent
operator for F ∈ Bb(E × E), (x, y) ∈ E × E:

C(F )(x, y) = F (x, x).

Then for 0 ≤ s ≤ (n + 1), 0 ≤ i1 < ··· < in ≤ n, F ∈ Bb(E × E)

Γi1:is

n

(F ) = γ⊗2

i1

CQ⊗2

i1,i2

CQ⊗2

i2,i3

. . . CQ⊗2

is,n(F )

and

i1:is
Γ
n

(F ) =

1

γn(1)2 Γi1:is

n

(F ).

∅
The conventions, for s = 0, Γ∅
n(F ) = η⊗2
n (F ) and Γ
Recall the selection-mutation operator for any µ ∈ P(E), n ≥ 1

n(F ) = γ⊗2

n (F ) are adopted.

Φn(µ)(dx) =

µ(Gn−1Mn(·, dx))

µ(Gn−1)

.

n

F N0:n
denotes the natural ﬁltration generated by the particle system up-to
time n. For f1, f2 ∈ Bb(E) we write the tensor product of functions for every
(x, y) ∈ E × E:

f1 ⊗ f2(x, y) = f1(x)f2(y).

B Proofs for Theorem 1
Lemma 1. Assume (A1-2). Then there exist a C < +∞ such that for any

0 ≤ p ≤ n, x ∈ E:(cid:12)(cid:12)(cid:12) Qp,n(1)(x)

(cid:81)n−1

q=p ηq(Gq)

(cid:12)(cid:12)(cid:12) ≤ C

n−1(cid:88)

q=p

(cid:13)(cid:13)(cid:13) Gq

ηq(Gq)

(cid:13)(cid:13)(cid:13)∞

− 1

− 1

14

Proof. We ﬁx n, p and note that the case p = n is trivial, so we suppose p < n.
We prove the result by induction. We consider p = n − 1 and thus

(cid:81)n−1

Qp,n(1)(x)
q=p ηq(Gq)

− 1 =

Gn−1(x)

ηn−1(Gn−1)

− 1

so the initialization is proved. Suppose the result holds at rank p and consider
the case p − 1. We have

(cid:16) Gp−1(x)

ηp−1(Gp−1)

(cid:17)

− 1

Mp

By [7, Lemma 4.1]

− 1 =

(x) + Mp

(cid:16) Qp,n(1)(x)
(cid:81)n−1

q=p ηq(Gq)

(cid:17)

− 1

(x).

≤ C

(18)

(cid:17)

Qp−1,n(1)(x)
q=p−1 ηq(Gq)

q=p ηq(Gq)

(cid:81)n−1
(cid:16) Qp,n(1)(x)
(cid:81)n−1
(cid:81)n−1
(cid:12)(cid:12)(cid:12) ≤ C

− 1

Qp,n(1)(x)
q=p ηq(Gq)

where C does not depend upon p, n. Thus, by applying the induction hypothesis
and the above result it follows that:

(cid:12)(cid:12)(cid:12) Qp−1,n(1)(x)
(cid:81)n−1

q=p−1 ηq(Gq)

n−1(cid:88)

q=p−1

(cid:13)(cid:13)(cid:13) Gq

ηq(Gq)

(cid:13)(cid:13)(cid:13)∞

− 1

and hence the proof is completed.

The result below follows one in [7].

Proposition 1. Assume (A1-2). Then there exists a C < +∞ such that for
any n ≥ 0, F ∈ Bb(E × E) and N0 ≥ ··· ≥ Nn > c(n + 1)

(cid:12)(cid:12)(cid:12)E(cid:104) (γN0:n

n

)⊗2(F )

γn(1)2

(cid:105) − η⊗2

n (F )

(cid:12)(cid:12)(cid:12) ≤ 8c(cid:107)F(cid:107)∞

n(cid:88)

p=0

1
Np

.

Proof. The case with F constant essentially follows from the proofs of [7]. The
only diﬀerence is the fact that we have a decreasing number of samples; this
does not change the calculations of that paper, so the case of F constant is in [7].
If F is a non-constant function, one has, from the equation above Proposition
3.4 (page 638) of [7]:

(cid:12)(cid:12)(cid:12)E(cid:104) (γN0:n
(cid:16) s(cid:89)

n

)⊗2(F )

γn(1)2

(cid:17)(cid:16) (cid:89)

1
Nik

(cid:105) − η⊗2
(cid:0)1 − 1

n (F )

(cid:12)(cid:12)(cid:12) =
(cid:1)(cid:17)

Nk

(cid:12)(cid:12)(cid:12).

(F − η⊗2

n (F ))

Γ

i1:is
n

0≤i1<···<is≤n

k=1

k /∈{i1,...,is}

(cid:88)

(cid:12)(cid:12)(cid:12) n+1(cid:88)

s=1

15

Following the proof of Theorem 5.1 of [7] and noting that one can allow the
function in that paper to be negative, it follows that

|Γ

i1:is
n

(F − η⊗2

)⊗2(F )

Thus one has

n

(cid:12)(cid:12)(cid:12)E(cid:104) (γN0:n
n+1(cid:88)

Note that

γn(1)2

s=1

0≤i1<···<is≤n

s=1

n (F )

n (F ))| ≤ (cid:107)F − η⊗2

n (F )(cid:107)∞
(cid:12)(cid:12)(cid:12) ≤ 2(cid:107)F(cid:107)∞
(cid:105) − η⊗2
n+1(cid:88)
(cid:16) s(cid:89)
(cid:17)s
(cid:17)(cid:16)
(cid:88)
(cid:17) − 1 ≤ 2ρ
(cid:105) − η⊗2

n(cid:89)
(cid:12)(cid:12)(cid:12)E(cid:104) (γN0:n

)⊗2(F )

1
Nik

n (F )

1
Ns

1 + ρ

(cid:16)

C
C

C
C

k=1

s=0

=

ρ

n

γn(1)2

and for N0 > C(n + 1), . . . , Nn > C(n + 1)

(cid:16)

ρ

C
C

.

ρ

(cid:17)s ≤ 2(cid:107)F(cid:107)∞
(cid:16) s(cid:89)
(cid:88)
(cid:16)

k=1

ρ

C
C

(cid:16)

(cid:17)s
(cid:17)(cid:16)
(cid:17) − 1 ,

1
Nik

1 + ρ

C
C

1
Ns

n(cid:89)

s=0

0≤i1<···<is≤n

(cid:17)s

.

C
C

1
Np

,

C
C

n(cid:88)
(cid:12)(cid:12)(cid:12) ≤ 8C(cid:107)F(cid:107)∞

p=0

n(cid:88)

p=0

1
Np

,

(see for instance the proofs of Theorem 5.1 and Corollary 5.2 of [7]). It follows
that

C ; the proof is concluded.

with C = ρ C
Proof of Theorem 1. Throughout the proof C < +∞ is a constant whose value
may change from line-to-line. It will not depend on the level index. By [22,
Proposition 2.3]

(cid:17)2(cid:105)

(1)

− 1

=

L−1(cid:88)

p=0

1
Np

E[T N0:p
p,L ]

L
γL(1)

E(cid:104)(cid:16) γN0:L−1
(cid:17)2(cid:16)

p (hp,L)2 + ηNp

p (hp,L)ηNp
p

(19)

− 1(cid:1)(cid:17)

(cid:0) Gp

ηp(Gp)

where

(cid:16) γN0:p−1

p

(1)

γp(1)

ηNp
p (h2

T N0:p
p,L =
and we use the short-hand for 0 ≤ p ≤ n, x ∈ E:
Qp,n(1)(x)
q=p ηq(Gq)

hp,n(x) =

.

p,L) − ηNp
(cid:81)n−1
(cid:16) γN0:p−1

p

T N0:p
p,L =

Now, one has almost surely that

(cid:17)2×

(1)

γp(1)

16

(cid:16)

As

p ([hp,L − 1]2) − ηNp
ηNp

p (hp,L − 1)2 + ηNp

p (hp,L)ηNp
p

(cid:34)(cid:16) γN0:p−1
(cid:34)(cid:16) γN0:p−1

p

p

γp(1)

(1)

(1)

(cid:17)2(cid:16)
(cid:17)2

E

γp(1)

p ([hp,L − 1]2) − ηNp
ηNp
(cid:0) Gp

ηNp
p (hp,L)ηNp
p

ηp(Gp)

E[T N0:p

p,L ] = E

.

ηp(Gp)

− 1(cid:1)(cid:17)
(cid:0) Gp
p (hp,L − 1)2(cid:17)(cid:35)
− 1(cid:1)(cid:35)

+

(20)

we will consider controlling the two terms on the R.H.S. of (20) separately.
First term on the R.H.S. of (20).
We have, almost surely that
p ([hp,L − 1]2) − ηNp
ηNp

p (hp,L − 1)2 ≤ C(cid:107)hp,L − 1(cid:107)2∞

(cid:16) L−1(cid:88)

q=p

(cid:13)(cid:13)(cid:13) Gq

ηp(Gq)

(cid:17)2

(cid:13)(cid:13)(cid:13)∞

− 1

≤ C

where we have applied Lemma 1 to go to the second line. Then by Proposition
1 as N0 > cL, . . . , NL−1 > cL

≤

(21)

(cid:17)2(cid:105) ≤ C.

(1)

γp(1)

p

E(cid:104)(cid:16) γN0:p−1
(cid:17)2(cid:16)
(cid:16) L−1(cid:88)

C

.

q=p

− 1

ηp(Gq)

(cid:13)(cid:13)(cid:13) Gq

p (hp,L − 1)2(cid:17)(cid:35)
p ([hp,L − 1]2) − ηNp
ηNp
(cid:13)(cid:13)(cid:13)∞
(cid:17)2
(cid:0) Gp
hp,L ⊗(cid:16) Gp
(cid:17)(cid:17)

)⊗2(cid:16)
hp,L ⊗(cid:16) Gp

− 1(cid:1) =
(cid:17)(cid:17)

ηNp
p (hp,L)ηNp
p

(cid:17)2

ηp(Gp)

ηp(Gp)

− 1

− 1

= 0.

ηp(Gp)

(cid:16) γN0:p−1

p

(1)

γp(1)

1

p

γp(1)2 (γN0:p
(cid:16)

η⊗2

p

17

So we have shown that

(cid:34)(cid:16) γN0:p−1

p

E

(1)

γp(1)

Second term on the R.H.S. of (20).
We have almost surely that

and note that,

E(cid:104)

(cid:105)

So by Proposition 1 as N0 > cL, . . . , NL−1 > cL and (18)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E
(cid:34)(cid:16) γN0:p−1

γp(1)

p

(cid:17)2

(1)

ηNp
p (hp,L)ηNp
p

(cid:0) Gp

ηp(Gp)

− 1(cid:1)(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ C

(cid:13)(cid:13)(cid:13) Gp

ηp(Gp)

(cid:13)(cid:13)(cid:13)∞

− 1

(p + 1)

Np

.

(22)
Combining (19) with (20) and after applying the triangular inequality, the
bounds (21) and (22) complete the proof.

C Proofs for Theorem 2

Some of the proofs in this Section will use Proposition 1 in Appendix B.
Lemma 2. Let n ≥ 1 and f1, f2 ∈ Bb(E) then
= E[(γN0:n

(cid:105)
n − γn](f2)

)⊗2(f1⊗ f2)]− γn(1)2η⊗2

n − γn](f1)[γN0:n
[γN0:n

E(cid:104)

n

n (f1⊗ f2).

Proof. We have

n − γn](f1)[γN0:n
[γN0:n

n − γn](f2)

=
(f1)]−γn(f1)E[γN0:n

n

E[γN0:n

n

(f1)γN0:n
E[γN0:n

n
(f1)γN0:n

n

n

(f2)]−γn(f2)E[γN0:n

n

(f2)] − γn(f2)γn(f1) − γn(f1)γn(f2) + γn(f1)γn(f2)

(f2)]+γn(f1)γn(f2) =

where the unbiased property of the normalizing constant has been used to go
to the last line. Then it follows that

E[γN0:n

n

(f1)γN0:n

n

(f2)] − γn(f2)γn(f1) − γn(f1)γn(f2) + γn(f1)γn(f2) =

E[(γN0:n

n

)⊗2(f1 ⊗ f2)] − γn(1)2η⊗2

n (f1 ⊗ f2)

which concludes the proof.
Lemma 3. Assume (A1-2). Then there exists a C < +∞ such that for any
2 ≤ q < p, N0 ≥ N1 ≥ ··· ≥ Nq−2 > C(q − 1):
p−2 − γp−2](Gp−2(Gp−1 − 1))[γN0:q−2
c(q − 1)γq−2(1)2

(cid:13)(cid:13)(cid:13)Gq−2(Gq−1 − 1)Qq−2,p−2(Gp−2(Gp−1 − 1))
(cid:13)(cid:13)(cid:13)∞.

q−2 − γq−2](Gq−2(Gq−1 − 1))]| ≤

|E[[γN0:p−2

Nq−2

Proof. From [8, Proposition 7.4.1] we have

E[[γN0:p−2

p−2 − γp−2](Gp−2(Gp−1 − 1))[γN0:q−2
p−2(cid:88)

q−2(cid:88)

E(cid:104)

N0:s1−1
s1

γ

(1)[η

Ns1

s1 − Φs1(η

s1−1 )](Qs1,p−2(Gp))×
Ns1−1

q−2 − γq−2](Gq−2(Gq−1 − 1))] =

s1=0

s2=0

18

N0:s2−1
s2

γ

(1)[η

Ns2

s2 − Φs2 (η

(cid:105)

Ns2−1
s2−1 )](Qs2,q−2(Gq))

where we have used the shorthand Gs = Gs−2(Gs−1 − 1) for any s ≥ 2. For any
s ≥ 0, f ∈ Bb(E)

E[γN0:s−1

s

thus, it follows that

(1)[ηNs

s − Φs(ηNs−1

s−1 )](f )|F N0:s−1

s−1

] = 0

E[[γN0:p−2

p−2 − γp−2](Gp−2(Gp−1 − 1))[γN0:q−2
q−2(cid:88)

E[γN0:s−1

s

(1)2[ηNs

s − Φs(ηNs−1

s−1 )]⊗2(Qs,p−2(Gp) ⊗ Qs,q−2(Gq))].

q−2 − γq−2](Gq−2(Gq−1 − 1))] =

Now for any n ≥ 1, f1, f2 ∈ Bb(E), one can show, using almost the same
calculations as above, that the following holds

E[γN0:s−1

s

(1)2[ηNs

s − Φs(ηNs−1

s−1 )]⊗2(Qs,n(f1) ⊗ Qs,n(f2))] =

E(cid:104)

(cid:105)
n − γn](f2)

.

n − γn](f1)[γN0:n
[γN0:n

s=0

n(cid:88)

s=0

E(cid:104)

Using this equality with n = q − 2, and the fact that Qs,p−2 = Qs,q−2Qq−2,p−2,
ﬁnally

E[[γN0:p−2

p−2 − γp−2](Gp−2(Gp−1 − 1))[γN0:q−2

q−2 − γq−2](Gq−2(Gq−1 − 1))] =

q−2 − γq−2](Qq−2,p−2(Gp))[γN0:n
[γN0:n

(cid:105)
q−2 − γq−2](Gq)

.

Then, by Lemma 2:

E[[γN0:p−2

p−2 − γp−2](Gp−2(Gp−1 − 1))[γN0:q−2

E[(γN0:q−2

q−2

)⊗2(Qq−2,p−2(Gp) ⊗ Gq)] − γq−2(1)2η⊗2

q−2 − γq−2](Gq−2(Gq−1 − 1))] =
q−2(Qq−2,p−2(Gp) ⊗ Gq).

Then, one can apply Proposition 1 to obtain that

|E[[γN0:p−2

p−2 − γp−2](Gp−2(Gp−1 − 1))[γN0:q−2
C(q − 1)γq−2(1)2

(cid:13)(cid:13)(cid:13)Gq−2(Gq−1 − 1)Qq−2,p−2(Gp−2(Gp−1 − 1))
(cid:13)(cid:13)(cid:13)∞.

q−2 − γq−2](Gq−2(Gq−1 − 1))]| ≤

Nq−2

Proof of Theorem 2. Throughout the proof C < +∞ is a constant whose value
may change from line-to-line. It will not depend on the level index. We have

E(cid:104)(cid:16) ˜γN0:L−2

L
γL(1)

(cid:17)2(cid:105) ≤

(1)

− 1

19

1

γL(1)2

E[[ηN0

0 − η0](G0)2] +

1

γL(1)2

E[(

p−2 − γp−2](Gp))2].
[γN0:p−2

L(cid:88)

p=2

As γL(1) = ZL/Z0 ≥ C/C it follows by standard results for i.i.d. random
variables that one has

1

γL(1)2

E[[ηN0

.

0 − η0](G0)2] ≤ C
N0
L(cid:88)

γp−2(1)2E(cid:104) γN0:p−2

p−2

p=2

γp−2(1)2 − ηp−2(Gp)2(cid:105)

(Gp)2

E[[γN0:p−2

p−2 − γp−2](Gp)[γN0:q−2

q−2 − γq−2](Gq)].

Now

L(cid:88)

E[(

p−2 − γp−2](Gp))2] =
[γN0:p−2

p=2

+2

L(cid:88)

p−1(cid:88)

p=2

q=2

Applying Propositon 1 to the terms in the single sum and Lemma 3 to the terms
in the double sum, we have that

E[(

L(cid:88)
p−1(cid:88)

p=2

L(cid:88)

+

p=2

q=2

(cid:16) L(cid:88)

p−2 − γp−2](Gp))2] ≤ C
[γN0:p−2

γp−2(1)2 (p − 1)

(cid:107)Gp(cid:107)2∞

p=2

(cid:13)(cid:13)(cid:13)Gq−2(Gq−1 − 1)Qq−2,p−2(Gp−2(Gp−1 − 1))
(cid:13)(cid:13)(cid:13)∞

(cid:17)

.

Np

(q − 1)γq−2(1)2

Nq−2

As γp−2(1) ≤ C/C, γL(1) ≥ C/C one has

L(cid:88)

1

γL(1)2

p=2

γp−2(1)2 (p − 1)

Np

(cid:107)Gp(cid:107)2∞ ≤ C

L(cid:88)

p=2

(p − 1)
Np−2

(cid:107)Gp−1 − 1(cid:107)2∞.

We have

1

γL(1)2

L(cid:88)

p−1(cid:88)

(q − 1)γq−2(1)2

p=2

q=2

Nq−2

(cid:13)(cid:13)(cid:13)Gq−2(Gq−1−1)Qq−2,p−2(Gp−2(Gp−1−1))
(cid:13)(cid:13)(cid:13)∞ =

c(q − 1)γq−2(1)

L(cid:88)
p−1(cid:88)
(cid:13)(cid:13)(cid:13)Gq−2(Gq−1 − 1)Qq−2,p−2(Gp−2(Gp−1 − 1))
(cid:13)(cid:13)(cid:13)∞.

ηq−2(Qq−2,p−2(1))

γL(1)Nq−2

Zp−1
ZL

p=2

q=2

1

×

Then as γq−2(1) ≤ C/C, γL(1) ≥ C/C, Zp−1 ≤ C, ZL ≥ C and by [7, Lemma
4.1]

Qq−2,p−2(Gp−2(Gp−1 − 1))

≤ C(cid:107)Gp−1 − 1(cid:107)∞

ηq−2(Qq−2,p−2(1))

20

we have

1

γL(1)2

(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13)Gq−2(Gq−1−1)Qq−2,p−2(Gp−2(Gp−1−1))

≤

L(cid:88)

p−1(cid:88)

(q − 1)γq−2(1)2

p=2

q=2

C

Nq−2

L(cid:88)

p−1(cid:88)

p=2

q=2

(q − 1)
Nq−2

(cid:107)Gp−1 − 1(cid:107)∞(cid:107)Gq−1 − 1(cid:107)∞.

From here one can easily conclude.

References

[1] Andrieu, C., & Roberts, G. O. (2009). The pseudo-marginal approach for

eﬃcient Monte Carlo computations. Ann. Statist., 37, 697-725.

[2] Bellman, R. E. (2015) Adaptive Control Processes: A Guided Tour. Prince-

ton university press.

[3] Beskos, A., Crisan, D. & Jasra, A. (2014). On the stability of sequential
Monte Carlo methods in high dimensions. Ann. Appl. Probab., 24, 1396–
1445.

[4] Beskos, A., Jasra, A., Law, K. J. H, Tempone, R. & Zhou,
Y. (2015). Multilevel sequential Monte Carlo samplers. arXiv preprint
arXiv:1503.07259.

[5] Bickel, P., Li, B. & Bengtsson, T. (2008). Sharp failure rates for the
bootstrap particle ﬁlter in high dimensions. In Pushing the Limits of Con-
temporary Statistics, B. Clarke & S. Ghosal, Eds, 318–329, IMS.

[6] Bungartz, H-J., & Griebel, M. (2004). Sparse grids. Acta numerica 13.1,

147-269.

[7] Cerou, F., Del Moral, P., & Guyader, A. (2011). A non-asymptotic
theorem for unnormalized Feynman-Kac particle models. Ann. Inst. Henri
Poincaire, 47, 629–649.

[8] Del Moral, P. (2004). Feynman-Kac Formulae: Genealogical and Inter-

acting Particle Systems with Applications. Springer: New York.

[9] Del Moral, P., Doucet, A. & Jasra, A. (2006). Sequential Monte Carlo

samplers. J. R. Statist. Soc. B, 68, 411–436.

[10] Giles, M. B. (2008). Multilevel Monte Carlo path simulation. Op. Res. 56,

607-617.

[11] Giles, M. B (2015). Multilevel Monte Carlo methods. Acta Numerica, 24,

259-328.

21

[12] Heinrich, S. (2001). Multilevel Monte Carlo methods. Large-scale scien-

tiﬁc computing. Springer Berlin Heidelberg, 2001. 58-67.

[13] Hoang, V., Schwab, C. & Stuart, A. (2013). Complexity analysis of ac-
celerated MCMC methods for Bayesian inversion. Inverse Prob., 29, 085010.

[14] Hoel, H., Law, K. J., & Tempone, R. (2015). Multilevel ensemble Kalman

ﬁltering. arXiv preprint arXiv:1502.06069.

[15] Hoeting, J. A., Madigan, D., Raftery, A. E., & Volinsky, C. T.

(1999). Bayesian model averaging: a tutorial. Statist. Sci., 14, 382-401.

[16] Jasra, A., Kamatani, K., Law, K. J., & Zhou, Y. (2015). Multilevel

particle ﬁlter. arXiv preprint arXiv:1510.04977.

[17] Ketelsen, C., Scheichl, R. & Teckentrup, A. L. (2013). A hierarchical
multilevel Markov chain Monte Carlo algorithm with applications to uncer-
tainty quantiﬁcation in subsurface ﬂow. arXiv preprint arXiv:1303.7343.

[18] Le Matre, O. P., & Knio, O. M. (2010). Introduction: Uncertainty Quan-

tiﬁcation and Propagation. Springer Netherlands.

[19] Øksendal, B. (2003). Stochastic Diﬀerential Equations. Springer Berlin

Heidelberg.

[20] Rhee, C. H., & Glynn, P. W. (2015). Unbiased estimation with square

root convergence for SDE models. Op. Res., 63, 1026–1043.

[21] Schwab, C., & Gittelson, C. J. (2011). Sparse tensor discretizations of
high-dimensional parametric and stochastic PDEs. Acta Numerica, 20, 291.

[22] Schweizer, N. (2012). Non-asymptotic error bounds for sequential MCMC
and stability of Feynman-Kac operators. arXiv preprint arXiv:1204.2382v1.

[23] Walsh, J. B. (1986) An Introduction to Stochastic Partial Diﬀerential

Equations. Springer Berlin Heidelberg.

[24] Walstrom, J. E., Mueller, T. D. & McFarlane, R. C. (1967). Evalu-

ating uncertainty in engineering calculations. J. Pet. Tech. 19.12, 1-595.

[25] Wasserman, L. (2000). Bayesian model selection and model averaging. J.

Math. Psych., 44(1), 92-107.

[26] Zhou, Y., Johansen, A. M. & Aston, J. A. D. (2016). Towards automatic
model comparison: An adaptive sequential Monte Carlo approach. J. Comp.
Graph. Statist., (to appear).

22

