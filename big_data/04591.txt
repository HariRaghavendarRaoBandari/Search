Threshold Saturation of Spatially Coupled Sparse
Superposition Codes for All Memoryless Channels

Jean Barbier, Member IEEE, Mohamad Dia and Nicolas Macris, Member IEEE.

Laboratoire de Théorie des Communications, Ecole Polytechnique Fédérale de Lausanne.

{jean.barbier, mohamad.dia, nicolas.macris}@epﬂ.ch

6
1
0
2

 
r
a

 

M
5
1

 
 
]
T
I
.
s
c
[
 
 

1
v
1
9
5
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—We recently proved threshold saturation for spa-
tially coupled sparse superposition codes on the additive white
Gaussian noise channel [1]. Here we generalize our analysis to a
much broader setting. We show for any memoryless channel that
spatial coupling allows generalized approximate message-passing
(GAMP) decoding to reach the potential (or Bayes optimal)
threshold of the code ensemble. Moreover in the large input
alphabet size limit: i) the GAMP algorithmic threshold of the
underlying (or uncoupled) code ensemble is simply expressed as a
Fisher information; ii) the potential threshold tends to Shannon’s
capacity. Although we focus on coding for sake of coherence
with our previous results, the framework and methods are very
general and hold for a wide class of generalized estimation
problems with random linear mixing.

I. INTRODUCTION

Sparse superposition (SS) codes were developed for reli-
able communication over the additive white Gaussian noise
(AWGN) channel [2] and were proven to be capacity-achieving
for this channel when power allocation and iterative decoding
are employed [3, 4]. Later on,
the approximate message-
passing (AMP) decoder was introduced in [5] and spatial
coupling (SC) constructions (also combined with efﬁcient
Hadamard-based operators) were presented in [6, 7]. These SC
constructions have many similarities with those introduced in
the context of compressed sensing [8, 9], the ﬁrst successful
application of SC to dense systems. An independent
line
of work also studying the AMP decoder, but using power
allocation instead of SC, is presented in [10].

It appears that SC-SS codes have much better performances
than power allocated ones [7]. This motivated the initiation of
their rigorous study [1] using the potential method, originally
developed for low density parity check codes [11–13]. In [1]
we showed that i) threshold saturation occurs, i.e. minimum
mean square error (MMSE) performance is reached using SC
and AMP decoding, and ii) the potential threshold (above
which AMP decoding is not possible without using SC or
power allocation) tends to capacity in the large alphabet size
limit, and this even without power allocation.

B-dimensional (B-d) sections p0(s) =(cid:81)L

These encouraging results (obtained for the AWGN) natu-
rally led us to study a general setting that includes all memo-
ryless channels and any input signal model that factorizes over
l=1 p0(sl), sl ∈ RB.
The present analysis is also based on the potential method.
The correct potential and associated state evolution (SE) for
the present setting can be “guessed” using the replica method.

Alternatively, one can “integrate” the SE associated with
the GAMP algorithm in the vectorial setting. The GAMP
equations were originally derived for scalar estimation [14],
but their extension to the present vectorial setting is immediate.

II. CODE ENSEMBLES

In the sequel, the shorthands [a1 : an] and {a1 : an} refer
to [a1, . . . , an] and {a1, . . . , an} respectively. The probability
distribution of a Gaussian random variable x with mean m
and variance σ2 is denoted N (x|m, σ2).
Let us start deﬁning the underlying ensemble of SS
codes for transmission over a generic memoryless channel.
The information word or message is a vector made of L
sections, s = [s1 : sL]. Each section is a B-d vector with
a single non-zero component equal to 1. B is the section
size (or alphabet size) and we set N = LB. For exam-
ple if (B = 3, L = 4),
then a valid message could be
s = [001, 010, 100, 010]. We consider random linear codes
generated by a ﬁxed coding matrix F ∈ RM×N drawn from the
ensemble of random matrices with i.i.d real Gaussian entries
with distribution N (·|0, 1/L). The codeword Fs ∈ RM and
the cardinality of the code is BL. Hence, the (design) rate
is R = L log2(B)/M = N log2(B)/(M B). The code is
thus speciﬁed by (M, R, B). The rate R can be linked to
the “measurement rate” α, used in the compressive sensing
literature [8], by α := M/N = log2(B)/(BR).

receiver obtains the noisy channel observation y.

channel Pout(y|Fs) = (cid:81)M

We want to communicate through a known memoryless
channel W . This requires to map the codeword components
[Fs]µ ∈ R onto the input alphabet of W . Call π this map (see
Sec. V for various examples). The concatenation of π and W
can be seen as an effective memoryless channel Pout, such that
Pout(yµ|[Fs]µ) := W (yµ|π([Fs]µ)). In the present framework,
it is more convenient to work with this effective memoryless
µ=1 Pout(yµ|[Fs]µ), from which the
We now present the spatially coupled ensemble of SS
codes. We consider SC codes based on coding matrices in
RM×N made of Γ × Γ blocks indexed by (r, c), each with
N/Γ columns and M/Γ = αN/Γ rows. This ensemble
of matrices is parametrized by (M, R, B, Γ, w, gw), where
w is the coupling window and gw is the design function.
This is any function verifying gw(x) = 0 if |x| > 1 and
gw(x) ≥ g0 > 0 else, which is Lipschitz continuous on its
support with Lipschitz constant g∗ independent of w. From

ance normalization(cid:80)Γ

gw, we construct the variances of the blocks: the i.i.d entries
inside the block (r, c) are distributed as N (0, Jr,c/L), where
Jr,c := γrΓgw((r− c)/w)/(2w + 1). Here γr enforces the ari-
c=1 Jr,c/Γ = 1 ∀ r. This normalization
induces homogeneous power over the codeword components,
i.e. [Fs]2
µ → 1 ∀ µ as L → ∞. The detailed SC construction
is explained in [1].
The SC matrix structure naturally induces a block structure
in the message, s = [s1 : sΓ]. In each of these blocks there
are L/Γ sections. We assume that the sections in the ﬁrst and
last 4w blocks of the message are known by the decoder. This
seed initiates a decoding wave in the SC code that propagates
inward through the entire message. The seed induces a rate
loss in the effective rate Reff = R(1 − 8w
Γ ) of the code, but
this loss vanishes as Γ → ∞.
III. STATE EVOLUTION AND POTENTIAL FORMULATION
The decoder is the GAMP algorithm, a generalization of
AMP to generic memoryless channels, introduced for esti-
mation of scalar signals with i.i.d components [14]. In the
present context the message components are correlated through
p0(sl), therefore we extend GAMP to cover this vectorial
setting (similarly to [7] for AMP). We ﬁrst give the SE
equations associated with the underlying and SC ensembles.
SE is conjectured to track the performance of the vectorial
extension of the GAMP decoder (see Sec. VI). We then deﬁne
an appropriate potential function for each ensemble.

A. State evolution

(cid:80)L
l=1 (cid:107)ˆs(t)

The goal is to iteratively compute the average mean square
2] of the GAMP

error (MSE) ˜E(t) := Es,y[ 1
estimate ˆs(t) at iteration t. We ﬁrst need some deﬁnitions.

l − sl(cid:107)2

Deﬁnition 3.1 (Effective noise): Let us deﬁne the effective

L

noise variance Σ(E)2 by the relation

Σ(E)−2 :=

Ep|E[F(p|E)]

,

R

(cid:90)

(1)

(2)

where the expectation Ep|E is w.r.t N (p|0, 1 − E) and

dyf (y|p, E)(∂x ln f (y|x, E))2

x=p

F(p|E) :=

f (y|p, E) :=(cid:82) duPout(y|u)N (u|p, E).

is the Fisher information of p associated with the distribution

the Fisher

Proof: Positivity of

Lemma 3.2: Σ(E)2 is non negative and increasing with E.
information implies
Σ(E)2 ≥ 0. The proof that it is increasing is a straightfor-
ward application of the data processing inequality for Fisher
information (Corollary 6 in [15]).
From now on, s ∼ p0(s) and z ∼ N (z|0, I) are B-d random
vectors and z ∼ N (z|0, 1), with expectations noted Es,z, Ez.
Deﬁnition 3.3 (Denoiser): The denoiser gin,i(s, z, Σ) is
the MMSE estimator of the i-th component of a section
s sent
through an effective AWGN channel with a noise
N (ξ|0, I Σ2/ log2(B)). Note that the effective AWGN channel
is induced by the code construction and depends on the

effective channel Pout only through Σ. For any B-d prior, we
have for i ∈ {1 : B}

(cid:82) dx p0(x)θ(x, s, z, Σ)xi
(cid:82) dx p0(x)θ(x, s, z, Σ)
(cid:81)B−1
− (cid:107)x−(s+zΣ/√log2(B))(cid:107)2

2Σ2/ log2(B)

,

2

(3)

(cid:1). Using

gin,i(s, z, Σ) :=

where θ(x, s, z, Σ) := exp(cid:0)
(cid:80)B

B

i=1 δxi,1

the prior p0(x) = 1
denoiser of SS codes [1].

j(cid:54)=i δxj ,0, one recovers the
Deﬁnition 3.4 (SE of the underlying system): The SE oper-
ator of the underlying system is the average MSE associated
with the MMSE estimator of the effective channel,

(cid:104) B(cid:88)

i=1

(gin,i(s, z, Σ(E)) − si)2(cid:105)

.

(4)

Tu(E) := Es,z

u

u

u

u

u

The SE tracking the performance of the GAMP decoder is
˜E(t+1) = Tu( ˜E(t)) for t ≥ 0 and is initialized with ˜E(0) = 1.
The existence of a ﬁxed point is ensured by the monotonic-
ity and boundedness of the SE iterations, see Sec. IV.

(cid:9).

(E) = E0

Deﬁnition 3.5 (MSE Floor): The MSE ﬂoor E0 is the ﬁxed

Deﬁnition 3.6 (Bassin of attraction): The basin of attraction

point reached from trivial initial condition, E0 = T (∞)

of the MSE ﬂoor E0 is V0 :=(cid:8)E | T (∞)

Deﬁnition 3.7 (Threshold of underlying ensemble): The
GAMP threshold is Ru := sup{R > 0 | T (∞)
(1) = E0}.
For the present system, one can show that the only two
(0) and T (∞)
possible ﬁxed points are T (∞)
(1). For R <
Ru, there is only one ﬁxed point, namely the “good” one
T (∞)
(0) = E0, and as the section size B increases E0 and
u
the section error rate (that is the fraction of wrongly decoded
sections) vanish. Instead if R > Ru, the GAMP decoder is
blocked by the “bad” ﬁxed point T (∞)

(0).

L

:= Es,y[ Γ

(cid:80)
l∈c (cid:107)ˆs(t)

For a SC system, the performance of GAMP is described by
an average MSE proﬁle [ ˜E(t)
| c = 1 : Γ] along the “spatial
c
dimension” indexed by the blocks of the message. To reﬂect
the seeding at the boundaries, we enforce the pinning condition
˜E(t)
c = 0 for c ∈ {1 : 4w} ∪ {Γ − 4w + 1 : Γ}, at all times.
Elsewhere, ˜E(t)
2], where the sum
c
(cid:80)Γ
l ∈ c is over the set of indices of the L/Γ sections composing
the c-th block of s. It turns out that the change of variables
E(t)
c makes the problem mathematically
r
more tractable. E is called a proﬁle. The pinning condition
becomes E(t)
r = 0 for r ∈ R := {1 : 3w}∪{Γ− 3w + 1 : Γ},
and at all times. In order to deﬁne the SE of the SC system,
we need ﬁrst the following deﬁnition.

l − sl(cid:107)2

c=1 Jr,c ˜E(t)

:= 1
Γ

Deﬁnition 3.8 (Per-block effective noise): The per-block
effective noise variance Σc(E)2 is ∀ c ∈ {1 : Γ} deﬁned by
Ep|Er [F(p|Er)].
Σc(E)−2 :=
(5)

ΓΣ(Er)2 =

Γ(cid:88)

Jr,c

(1) (cid:54)= E0.

u

Deﬁnition 3.9 (SE of the coupled system): The vector valued

coupled SE operator is deﬁned componentwise as

r=1

Jr,c
RΓ

Γ(cid:88)
(cid:104) B(cid:88)
(gin,i(s, z, Σc(E)) − si)2(cid:105)

i=1

[Tc(E)]r :=

Jr,c
Γ

Es,z

r=1

Γ(cid:88)

c=1

. (6)

r

r = 1 for r /∈ R.

= [Tc(E(t))]r for t ≥ 0.
r = 0 is enforced at all

The SE for r /∈ R then reads E(t+1)
For r ∈ R, the pinning condition E(t)
times. SE is initialized with E(0)
Let E0 := [Er = E0 | r = 1 : Γ] be the MSE ﬂoor proﬁle.
Deﬁnition 3.10 (Threshold of coupled ensemble): The
GAMP threshold of the SC system is deﬁned as Rc
:=
(1) ≺ E0} where 1 is the all
lim inf Γ,w→∞sup{R > 0 | T (∞)
ones vector. Here the lim inf Γ,w→∞ is taken along sequences
where ﬁrst Γ → ∞ and then w → ∞ (see Deﬁnition 4.1 for
the meaning of ≺).
B. Potential formulation

c

Uu(E) := −

E

R

The ﬁxed point equations associated with SE can be refor-
mulated as stationary point equations of potential functions
(obtained from the replica method [5] or integrating SE).

c=1 Su(Σc(E)).

2 ln(2)Σ(E)2 − 1

Deﬁnition 3.11 (Potentials): The potential of the underlying

(cid:40)
Ez[(cid:82) dy φ log2(φ)],
ensemble is Fu(E) := Uu(E) − Su(Σ(E)), with
Su(Σ(E)) := Es,z[logB((cid:82) dx p0(x)θ(x, s, z, Σ(E)))],
where φ(y|z, E) := (cid:82) dxPout(y|x)N (x|z√1 − E, E). The
Uc(E) :=(cid:80)Γ

potential of the SC ensemble is Fc(E) := Uc(E)−Sc(E) where
Deﬁnition 3.12 (Free energy gap): The free energy gap is
∆Fu := inf E /∈V0(Fu(E) − Fu(E0)), with the convention that
the inﬁmum over the empty set is ∞ (i.e. when R < Ru).
Deﬁnition 3.13 (Potential threshold): The potential thresh-
old is Rpot := sup{R > 0 | ∆Fu > 0}.

r=1 Uu(Er) and Sc(E) :=(cid:80)Γ

The next Lemma links the potential and SE formulations.
Lemma 3.14: One can show that if Tu( ˚E) = ˚E, then
∂E | ˚E = 0. Similarly for the SC system, if [Tc(˚E)]r = ˚Er
∀ r ∈ Rc = {3w + 1 : Γ − 3w} then ∂Fc
∂Er |˚E = 0 ∀ r ∈ Rc.
We end this section by pointing out that the terms com-
posing the potentials have natural interpretations in terms of

effective channels. The term Ez[(cid:82) dy φ log2(φ)] in Uu(E) is
minus the conditional entropy H(Y |Z) for the concatenation
of the channels N (x|z√1 − E, E) and Pout(y|x) with a stan-
dardised input N (z|0, 1). The term Su(Σ(E)) log2(B) is equal
to minus the mutual information I(S; Y) for the Gaussian
channel N (y|s, I Σ2/ log2(B)) and input distribution p0(s),
up to a constant factor −(2 ln(2))−1.
IV. SKETCH OF THE PROOF OF THRESHOLD SATURATION
Monotonicity properties of the SE operators Tu and Tc are

∂Fu

key elements in the analysis.

Deﬁnition 4.1 (Degradation): A proﬁle E is degraded (resp.
strictly degraded) w.r.t another one G, denoted as E (cid:23) G (resp.
E (cid:31) G), if Er ≥ Gr ∀ r (resp. if E (cid:23) G and there exists some
r such that Er > Gr).
Lemma 4.2: The SE operator of
the coupled system
if E (cid:23) G,
maintains degradation in space,
then
i.e.
Tc(E) (cid:23) Tc(G). It also maintains degradation in time, i.e.
Tc(E(t)) (cid:22) E(t) ⇒ Tc(E(t+1)) (cid:22) E(t+1). Similarly Tc(E(t)) (cid:23)
E(t) ⇒ Tc(E(t+1)) (cid:23) E(t+1). Furthermore, the limiting proﬁle

Fig. 1. A ﬁxed point proﬁle E∗ of the coupled SE (solid) is null ∀ r ≤ 3w and
increases until Emax ∈ [0, 1] at rmax ∈ {3w+1 : Γ−3w}. Then it decreases
and is null ∀ r ≥ Γ − 3w + 1. The associated saturated proﬁle E (dashed)
starts at E0 ∀ r ≤ r∗, where r∗ is deﬁned by: E∗
r ≤ E0 ∀ r ≤ r∗ and
r(cid:48) > E0 ∀ r(cid:48) > r∗. Then it matches E∗ ∀ r ∈ {r∗ : rmax} and saturates
E∗
Emax ∀ r ≥ rmax. By construction E is non decreasing and E (cid:31) E∗.

E(∞) := T (∞)
Tu for a scalar error as well.

c

(E(0)) exists. These properties are veriﬁed by

Proof: Combining Lemma 3.2 with (5) implies that if
E (cid:23) G, then Σc(E) ≥ Σc(G) ∀ c. The rest of the proof is
similar to the one of Lemma 4.2 and Corollary 4.3 in [1].
The pinning condition together with the monotonicity prop-
erties of the coupled SE imply that its ﬁxed point proﬁle E∗
must adopt a shape similar to Fig. 1. We associate to E∗ a
saturated proﬁle E (see Fig. 1) that veriﬁes by construction
E (cid:31) E∗. Thus E serves as an upper bound in our proof.
Deﬁnition 4.3 (Shift operator): The shift operator is deﬁned
componentwise as [S(E)]1 := E0, [S(E)]r := Er−1.
Lemma 4.4: Let E be a saturated proﬁle. Then the coupled
potential veriﬁes |Fc(S(E)) − Fc(E)| < K/w, where K is
independent of w and Γ.
Proof: The proof uses Lemmas 5.2, 5.3 and 5.4 of [1],
where Lemma 5.2 is implied by the present Lemma 3.14
and Lemma 5.3 remains valid as it depends only on the
SC contruction. Lemma 5.4 can be shown to be true for
any memoryless channel Pout such that the function gout :=

∂p ln((cid:82) dxPout(y|x)N (x|p, v)) [14] is Lipschitz continuous in

p with Lipschitz constant independent of the coupling window.

Lemma 4.5: Let E be a saturated proﬁle such that E (cid:31) E0.
Then Fc(S(E)) − Fc(E) ≤ −∆Fu.
Proof: See the proof of Lemma 5.6 in [1].
Theorem 4.6: Assume a spatially coupled SS code ensemble
is used for communication through a memoryless channel. Fix
R < Rpot, w > K/∆Fu (K is independent of w and Γ) and
Γ > 8w (such that the code is well deﬁned). Then any ﬁxed
point proﬁle E∗ of the coupled SE satisﬁes E∗ ≺ E0.
Proof: It follows from Lemma 4.4 and 4.5 as in [1].
Corollary 4.7: By ﬁrst taking Γ → ∞ and then w → ∞, the
GAMP threshold of the coupled ensemble satisﬁes Rc ≥ Rpot.
This result is a direct consequence of Theorem 4.6 and
Deﬁnition 3.10. It says that
the GAMP threshold of the
coupled SS codes saturates to the potential threshold.

We emphasize that Theorem 4.6 and Corollary 4.7 hold for
a large class of estimation problems with random linear mixing
[14]. Both the SE and potential formulations of Sec. III as well
as the proof sketched in the present section are not restricted to
SS codes. Indeed all the deﬁnitions and results are obtained
for any memoryless channel Pout and any factorizable (over
B-d sections, B ∈ N) prior over the message (or signal) s.

3wrE0r⇤Emaxrmax  3w+1  3w+1Let A and B be the input and output alphabet of W
respectively, where A,B ⊆ R are deﬁned over discrete
or continuous supports. Call P the capacity-achieving input
distribution associated with W . Choose π : R → A such that
i) Pout(y|z) = W (y|π(z)) and ii) if z ∼ N (z|0, 1), then
π(z) ∼ P. This map converts a standard Gaussian random
variable z onto a channel-input random variable π(z) = a
with capacity-achieving distribution P(a). Note that π can be
viewed equivalently as part of the code or of the channel.
DzW (y|π(z)) =
(cid:17)

Now using the relation(cid:82)
(cid:82) daP(a)W (y|a), (9) can be expressed equivalently as

DzPout(y|z) =(cid:82)
(cid:16)(cid:90)
(cid:16)

d˜aP(˜a)W (y|˜a)

(cid:90)
(cid:90)

dydaP(a)W (y|a) log2
dydaP(a)W (y|a) log2

R∞pot = −
+

.

(10)

W (y|a)

(cid:17)

The ﬁrst term in (10) is nothing but the Shannon entropy
H(Y ) of the channel output-distribution, while the second
term is the negative of the conditional entropy H(Y |A) of
the channel-output distribution given the input A = π(Z), that
has capacity-achieving distribution. Thus, R∞pot is the Shannon
capacity of W . Combining this result with Corollary 4.7, we
can assert that SC-SS codes allow to communicate reliably
up to Shannon’s capacity over any memoryless channel under
low complexity GAMP decoding.

But how to ﬁnd the proper map π for a given memoryless
channel? In the case of discrete input memoryless symmetric
channels, Shannon’s capacity can be attained by inducing
a uniform input distribution P = UA. Let us call q the
cardinality of A = {a1 : aq}. In this case the mapping π is
simply π(z) = ai if z ∈ ]z(i−1)/q, zi/q], where zi/q is the ith q-
quantile of the Gaussian distribution, with z0 = −∞, z1 = ∞.
For asymmetric channels, one can use some standard methods
such as Gallager’s mapping or more advanced ones [17] that
introduce bias in the channel-input distribution in order to
match the capacity-achieving one. We now illustrate these
ﬁndings, depicted for various channels in Fig. 3 and Fig. 4.
AWGN channel: We start showing that our results for the
AWGN channel [1] are a special case of the present general
framework. No map π is required and the Shannon capacity
is directly obtained from (9) because the capacity-achieving
input distribution for the AWGN channel is Gaussian. Thus,
by plugging Pout(y|z) = N (y|z, 1/snr) in (9), one recovers

Fig. 3. Large alphabet limits of the capacities and GAMP thresholds for the
BSC (left) and AWGN (right) channels.

Fig. 2. The large alphabet potential ϕu(E) (7) as a function of the MSE
for the BSC (left) and AWGN (right) channels with  = 0.1 and snr = 10
respectively. ϕu(E) is scaled such that ϕu(0) = 0. For R below the GAMP
threshold Ru, there is a unique minimum at E = 0 while just above Ru,
this minimum coexists with a local one at E = 1. At the optimal threshold
of the code, that coincides with the Shannon capacity, the two minima are
equal. Then, for R > C the minimum at E = 1 becomes the global one,
and thus decoding is impossible.

V. LARGE ALPHABET SIZE ANALYSIS AND
CONNECTION WITH SHANNON’S CAPACITY

We now show that as the alphabet size B increases, the
potential threshold of SS codes approaches Shannon’s capacity
R∞pot := limB→∞ Rpot = C, and also that limB→∞ E0 = 0.
Note that these are static properties of the code independent
of the decoder. But note also that the threshold saturation
established in Corollary 4.7 for SC-SS codes implies that
optimal decoding can actually be performed using the GAMP
decoder, i.e. limB→∞ Rc = C, since Rc ≤ C.
Lemma 3.14 implies that the underlying system’s potential
contains all the information about Rpot and Ru. Hence, we
proceed by computing ϕu(E) := limB→∞ Fu(E) [16],
.

(cid:16)

(cid:17)

(7)

1

ϕu(E) = Uu(E) − max

2 ln(2)Σ(E)2

0, 1 −

The analysis of ϕu(E) for E ∈ [0, 1] shows that the only
possible minima are at E = 0 and E = 1, which implies that
the error ﬂoor E0 vanishes as B increases (Fig. 2). One can
show that if Σ(E)2 < (2 ln(2))−1 ∀ E ∈ [0, 1], which cor-
responds to the region R < (2 ln(2))−1Ep|1[F(p|1)] for any
ﬁxed memoryless channel, then ϕu(E) has a unique minimum
at E = 0. Similarly for R > (2 ln(2))−1Ep|0[F(p|0)] there is
a unique minimum at E = 1. In the intermediate region both
minima coexist. Therefore, we identify
Ep|1[F(p|1)]

(8)

Ru =

.

= F(0|1)
2 ln(2)

R∞u := lim
B→∞

2 ln(2)

Since Rpot is deﬁned by the point where ∆Fu switches sign
(Deﬁnition 3.13), R∞pot can be obtained by equating the two
minima of ϕu(E). Setting ϕu(1) = ϕu(0) yields

D˜zPout(y|˜z)

(cid:17)

R∞pot = −
+

dyDzPout(y|z) log2
dyDzPout(y|z) log2

(9)
where Dz is a standard Gaussian distribution. We will now
recognize that this expression is the Shannon capacity of W
for a proper choice of the map π.

Pout(y|z)

,

(cid:90)
(cid:90)

(cid:16)(cid:90)
(cid:16)

(cid:17)

00.20.40.60.81E-0.200.20.40.60.81ϕu(E)R<RuRu<R<1−h2(ǫ)R=1−h2(ǫ)R>1−h2(ǫ)00.20.40.60.81E00.511.52ϕu(E)R<RuRu<R<12log2(1+snr)R=12log2(1+snr)R>12log2(1+snr)00.20.40.60.81ǫ00.10.20.30.40.50.60.70.80.91C=1−h2(ǫ)Ru=(1−2ǫ)2(πln(2))−110-410-310-210-1100101snr−101234567C=12log2(1+snr)Ru=[2ln(2)(1+snr−1)]−1point
is to estimate at what rate the error ﬂoor vanishes
when B increases. Finally, the ﬁnite size effects should be
considered in order to assess the real potential of these codes.
We plan to settle these questions in future works.

ACKNOWLEDGMENTS

J.B and M.D acknowledge funding from the Swiss National
Science Foundation grant num. 200021-156672. We thank
Florent Krzakala, Rüdiger Urbanke and Christophe Schülke
for helpful discussions.

REFERENCES

[1] J. Barbier, M. Dia, and N. Macris, “Proof of Threshold Saturation for
Spatially Coupled Sparse Superposition Codes,” ArXiv e-prints, Mar.
2016. [Online]. Available: http://arxiv.org/pdf/1603.01817v1.pdf

[2] A. Barron and A. Joseph, “Toward fast reliable communication at rates
near capacity with gaussian noise,” in Information Theory Proceedings
(ISIT), 2010 IEEE International Symposium on, June 2010, pp. 315–319.
[3] A. Joseph and A. R. Barron, “Fast sparse superposition codes have
near exponential error probability for R<C,” IEEE Trans. on Information
Theory, vol. 60, no. 2, pp. 919–942, 2014.

[4] A. R. Barron and S. Cho, “High-rate sparse superposition codes with
iteratively optimal estimates,” in Information Theory Proceedings (ISIT),
2012 IEEE International Symposium on.

IEEE, 2012, pp. 120–124.

[5] J. Barbier and F. Krzakala, “Replica analysis and approximate message
passing decoder for superposition codes,” in Information Theory Pro-
ceedings (ISIT), 2014 IEEE International Symposium on, 2014.

[6] J. Barbier, C. Schülke, and F. Krzakala, “Approximate message-passing
with spatially coupled structured operators, with applications to com-
pressed sensing and sparse superposition codes,” Journal of Statistical
Mechanics: Theory and Experiment, vol. 2015, no. 5, 2015.

[7] J. Barbier and F. Krzakala, “Approximate message-passing decoder
and capacity-achieving sparse superposition codes,” 2015. [Online].
Available: http://arxiv.org/abs/1503.08040

[8] F. Krzakala, M. Mézard, F. Sausset, Y. Sun, and L. Zdeborová,
“Probabilistic reconstruction in compressed sensing: Algorithms, phase
diagrams, and threshold achieving matrices,” Journal of Statistical
Mechanics: Theory and Experiment, vol. P08009, 2012.

[9] F. Caltagirone and L. Zdeborová, “Properties of spatial coupling in

compressed sensing,” CoRR, vol. abs/1401.6380, 2014.

[10] C. Rush, A. Greig, and R. Venkataramanan, “Capacity-achieving sparse
regression codes via approximate message passing decoding,” in Infor-
mation Theory (ISIT), 2015 IEEE International Symposium on, June
2015, pp. 2016–2020.

[11] A. Yedla, Y.-Y. Jian, P. S. Nguyen, and H. D. Pﬁster, “A simple
proof of threshold saturation for coupled scalar recursions,” in 7th
International Symposium on Turbo Codes and Iterative Information
Processing (ISTC), 2012, pp. 51–55.

[12] S. Kumar, A. J. Young, N. Macris, and H. D. Pﬁster, “Threshold
saturation for spatially-coupled ldpc and ldgm codes on bms channels,”
IEEE Trans. on Information Theory, vol. 60, pp. 7389–7415, 2013.

[13] A. Yedla, Y.-Y. Jian, P. Nguyen, and H. Pﬁster, “A simple proof of
maxwell saturation for coupled scalar recursions,” Information Theory,
IEEE Trans. on, vol. 60, no. 11, pp. 6943–6965, 2014.

[14] S. Rangan, “Generalized approximate message passing for estimation
with random linear mixing,” in Information Theory Proceedings (ISIT),
2011 IEEE International Symposium on.
IEEE, 2011, pp. 2168–2172.
[15] P. Zegers, “Fisher information properties,” Entropy, vol. 17, no. 7, p.
4918, 2015. [Online]. Available: http://mdpi.com/1099-4300/17/7/4918
[16] J. Barbier, “Statistical physics and approximate message-passing
algorithms for sparse linear estimation problems in signal processing
and coding theory,” Ph.D. dissertation, Université Paris Diderot, 2015.
[Online]. Available: http://arxiv.org/abs/1511.01650

[17] M. Mondelli, R. Urbanke, and S. H. Hassani, “How to achieve the
capacity of asymmetric channels,” in Communication, Control, and
Computing, 2014 Allerton Conference on.
IEEE, 2014, pp. 789–796.
[18] M. Bayati and A. Montanari, “The dynamics of message passing on
dense graphs, with applications to compressed sensing,” IEEE Trans.
on Information Theory, vol. 57, no. 2, pp. 764 –785, 2011.

Fig. 4. Capacity and GAMP threshold of the Z channel. C(p∗
are the values under capacity-achieving input distribution, whereas C( 1
Ru( 1

2 ) are the values under uniform distribution.

1) and Ru(p∗
1)
2 ) and

the Shannon capacity R∞pot = 1
one obtains R∞u = [2 ln(2)(1 + 1/snr)]−1.

2 log2(1 + snr). Furthermore,

BSC channel: The binary symmetric channel (BSC) with
ﬂip probability  has transition probability W (y|a) = (1 −
)δ(y − a) + δ(y + a), where both y, a ∈ {−1, 1}. The
proper map is π(z) = sign(z) since it induces uniform input
distribution UA = 1/2. So by plugging W and UA in (10), or
equivalently Pout(y|z) = (1 − )δ(y − π(z)) + δ(y + π(z))
into (9), one obtains the Shannon capacity of the BSC channel
R∞pot = 1−h2() where h2 is the binary entropy function. This
map also gives R∞u = (π ln(2))−1(1 − 2)2.
BEC channel: Note that the binary erasure channel (BEC) is
also symmetric. Therefore, the same mapping π(z) = sign(z)
is used and leads to the Shannon capacity R∞pot = 1−, where
 is the erasure probability, and R∞u = (π ln(2))−1(1 − ).
Z channel: The Z channel is the extremal discrete asym-
metric channel, in the sense that it represents the “worst”
one. It has binary input and output ∈ {−1, 1} with transition
probability W (y|a) = δ(a − 1)δ(y − a) + δ(a + 1)[(1 −
)δ(y − a) + δ(y + a)], where  is the ﬂip probability of
the −1 input. The map π(z) = sign(z) leads to the symmetric
capacity of the Z channel R∞pot = h2((1 − )/2) − h()/2,
that is the input-output mutual information when the input is
uniformly distributed, and R∞u = [π ln(2)(1 + )]−1(1 − ).
This expression differs from Shannon’s capacity. However, one
can introduce bias in the input distribution and hence match
the capacity-achieving one. To do so, the proper map deﬁned
in terms of the Q-function is π(z) = sign(z−Q−1(p1)), where
p1 is the input probability of the bit 1. By optimizing over p1,
one can obtain the Shannon’s capacity of the Z channel for
p∗1 = 1 − [(1 − )(1 + 2h2()/(1−))]−1.

VI. OPEN CHALLENGES

We end up pointing some open problems. In order to have a
fully rigorous capacity achieving scheme over any memoryless
channel, using SC-SS codes and GAMP decoding, it must
be shown that the SE tracks the asymptotic performance of
GAMP. We conjecture that it is indeed the case and that the
proof follows from the method of [18], then extended in [10]
for power allocated SS codes. It is also desirable to consider
practical coding schemes, using Hadamard-based operators or
more generally, row-othogonal matrices. Another important

00.20.40.60.81ǫ00.10.20.30.40.50.60.70.80.91C(p∗1)C(12)Ru(p∗1)Ru(12)