6
1
0
2

 
r
a

M
 
8
1

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
7
6
8
5
0

.

3
0
6
1
:
v
i
X
r
a

Optimal regularized inverse matrices

for

inverse problems

Julianne Chung ∗

Matthias Chung †

March 21, 2016

Abstract

In this paper, we consider optimal low-rank regularized inverse matrix approxi-
mations and their applications to inverse problems. We give an explicit solution to a
generalized rank-constrained regularized inverse approximation problem, where the key
novelties are that we allow for updates to existing approximations and we can incorpo-
rate additional probability distribution information. Since computing optimal regular-
ized inverse matrices under rank constraints can be challenging, especially for problems
where matrices are large and sparse or are only accessable via function call, we pro-
pose an eﬃcient rank-update approach that decomposes the problem into a sequence of
smaller rank problems. Using examples from image deblurring, we demonstrate that
more accurate solutions to inverse problems can be achieved by using rank-updates
to existing regularized inverse approximations. Furthermore, we show the potential
beneﬁts of using optimal regularized inverse matrix updates for solving perturbed to-
mographic reconstruction problems.

Keywords: ill-posed inverse problems, low-rank matrix approximation, regularization, Bayes
risk
AMS: 65F22, 15A09, 15A29

1

Introduction

Optimal low-rank inverse approximations play a critical role in many scientiﬁc applications
such as matrix completion, machine learning, and data analysis [38, 14, 26]. Recent theoret-
ical and computational developments on regularized low-rank inverse matrices have enabled

∗Department of Mathematics, Virginia Tech, Blacksburg, VA
†Department of Mathematics, Virginia Tech, Blacksburg, VA

(cid:66) jmchung@vt.edu (cid:109) www.math.vt.edu/people/jmchung/

(cid:66) mcchung@vt.edu (cid:109) www.math.vt.edu/people/mcchung/

1

new applications, such as for solving inverse problems [9]. In this paper, we develop theoret-
ical results for a general case for ﬁnding optimal regularized inverse matrices (ORIMs), and
we propose novel uses of these matrices for solving linear ill-posed inverse problems of the
form,

b = Aξ + δ,

(1)
where ξ ∈ Rn is the desired solution, A ∈ Rm×n models the forward process, δ ∈ Rm is
additive noise, and b ∈ Rm is the observed data. We assume that A is very large and sparse,
or that A cannot be formed explicitly, but matrix vector multiplications with A are feasible
(e.g., A can be an object or function handle). Furthermore, we are interested in ill-posed
inverse problems, whereby small errors in the data may result in large errors in the solution
[19, 23, 37], and regularization is needed to stabilize the solution.

Next, we provide a brief introduction to regularization and ORIMs, followed by a sum-
mary of the main contributions of this work. Various forms of regularization have been
proposed in the literature, including variational methods [31, 35] and iterative regulariza-
tion, where early termination of an iterative methods provides a regularized solution [21, 20].
Optimal regularized inverse matrices have been proposed for solving inverse problems and
have been studied in both the Bayes and empirical Bayes framework [8, 6, 9]. Let P ∈ Rn×m
be an initial approximation matrix (e.g., P = 0n×m in previous works). Then treating ξ

and δ as random variables, the goal is to ﬁnd a matrix (cid:98)Z ∈ Rn×m that gives a small re-
construction error. That is, ρ((P +(cid:98)Z)b − ξ) should be small for some given error measure
optimal matrix (cid:98)Z that minimizes the expected value of the errors with respect to the joint
distribution of ξ and δ. Hence, the problem of ﬁnding an ORIM (cid:98)Z can be formulated as

0 . In this paper, we consider ρ to be the squared Euclidean norm, and we seek an

ρ : Rn → R+

(cid:98)Z = arg min

Z

E (cid:107)((P + Z)A − In)ξ + Zδ(cid:107)2
2 .

(2)

This problem is often referred to as a Bayes risk minimization problem [4, 36]. Especially
for large scale problems, it may be advisable to include further constraints on Z such as
sparsity, symmetry, block or cyclic structure, or low-rank structure. Here, we will focus on

matrices Z of low-rank. Once computed, ORIM (cid:98)Z has mainly been used to eﬃciently solve
only a matrix-vector multiplication (P +(cid:98)Z)b.

linear inverse problems in an online phase as data b becomes available and requires therefore

Overview of our contributions First, we derive a closed-form solution for problem (2)
under rank constraints with uniqueness conditions. The two key novelties are that we include
matrix P, thereby allowing for updates to existing regularized inverse matrices, and we
incorporate additional information regarding the distribution of ξ. More speciﬁcally, we
allow non-zero mean for the distribution of ξ and show that our results reduce to previous
results in [9] that assume zero mean and P = 0n×m. These extension are not trivial and
require a diﬀerent approach than [9] for the proof. Second, we describe an eﬃcient rank-
update approach for computing a global minimizer of (2) under rank constraints, that is
related to but diﬀerent than the approach described in [7] where training data was used as a

2

substitute for knowledge of the forward model. We demonstrate the eﬃciency and accuracy of
the rank-update approach, compared to standard SVD-based methods, for solving a sequence
of ill-posed problems.

Third, we propose novel uses of ORIM updates in the context of solving inverse problems.
An example from image deblurring demonstrates that updates to existing regularized inverse
matrix approximations such as the Tikhonov reconstruction matrix can lead to more accurate
solutions. Also, we use an example from tomographic image reconstruction to show that
ORIM updates can be used to eﬃciently and accurately solve perturbed inverse problems.
This contribution has signiﬁcant implications for further research development, ranging from
use within nonlinear optimization schemes to preconditioner updates.

The key beneﬁts of using ORIMs for solution updates and for solving inverse problems
are that (1) we approximate the regularized inverse directly, so reconstruction or application
requires only a matrix-vector multiplication rather than a linear solve; (2) our matrix inher-
ently incorporates regularization; (3) ORIMs and ORIM updates can be computed for any
general rectangular matrix A, even if A is only available via a function call, making it ideal
for large-scale problems.

The paper is organized as follows. In Section 2, we provide preliminaries to establish
notation and summarize important results from the literature. Then, in Section 3, we de-
rive a closed form solution to problem (2) under rank constraints and provide uniqueness
conditions (see Theorem 3.3 for the main result). For large-scale problems, computing an
ORIM according to Theorem 3.3 may be computationally prohibitive, so in Section 4, we
describe a rank-update approach for eﬃcient computation. Finally, in Section 5 we provide
numerical examples from image processing that demonstrate the beneﬁts of ORIM updates.
Conclusions and discussions are provided in Section 6.

2 Background

Given a matrix A ∈ Rm×n with rank k ≤ min(m, n), let A = UAΣAV(cid:62)

In this section, we begin with preliminaries to establish notation.
A denote the
singular value decomposition (SVD) of A, where UA = [u1, . . . , um] ∈ Rm×m and VA =
[v1, . . . , vn] ∈ Rn×n are orthogonal matrices that contain the left and right singular vectors
of A, respectively. Diagonal matrix ΣA = diag(σ1(A), . . . , σk(A), 0, . . . , 0) ∈ Rm×n contains
the singular values σ1(A) ≥ ··· ≥ σk(A) > 0 and zeros on its main diagonal. The truncated
SVD approximation of rank r ≤ k of A is denoted by Ar = UA,rΣA,rV(cid:62)
A,r ∈ Rm×n where
UA,r and VA,r contain the ﬁrst r vectors of UA and VA respectively, and ΣA,r is the principal
r × r submatrix of ΣA. The TSVD approximation is unique if and only if σr(A) > σr+1(A).
Furthermore, the Moore-Penrose pseudoinverse of A is given by A† = VA,kΣ−1

A,kU(cid:62)

A,k.

Next we show that the problem of ﬁnding an optimal regularized inverse matrix (ORIM)
(i.e., a solution to (2)) is equivalent to solving a matrix approximation problem. That is,
assuming ξ and δ are random variables, the goal is to ﬁnd a matrix Z such that we minimize
the expected value of the squared 2-norm error, i.e., minZ f (Z), where

f (Z) = E (cid:107)(P + Z)b − ξ(cid:107)2

2 = E (cid:107)(P + Z)(Aξ + δ) − ξ(cid:107)2

2

3

f (Z) = µ(cid:62)

ξ ((P + Z)A − In)(cid:62)((P + Z)A − In)µξ

+ tr(cid:0)((P + Z)A − In)(cid:62)((P + Z)A − In)MξM(cid:62)
f (Z) =(cid:13)(cid:13)((P + Z)A − In)µξ

with MξM(cid:62)
the cyclic property of the trace leads to

(cid:13)(cid:13)2

ξ

(cid:1) + η2 tr(cid:0)(P + Z)(cid:62)(P + Z)(cid:1)

ξ = Γξ being any symmetric factorization, e.g., Cholesky factorization. Using

is often referred to as the Bayes risk.

Lets further assume that ξ and δ are independent random variables with E[ξ] = µξ, the
covariance matrix Cov[ξ] = Γξ is symmetric positive deﬁnite, E[δ] = 0m×1, and Cov[δ] =
η2Im. First, due to the independence of ξ and δ and since E[δ] = 0m×1, we can rewrite the
Bayes risk as

Then using the property of the quadratic form [33], E(cid:2)(cid:62)Λ(cid:3) = tr(ΛΣ) + µ(cid:62)

f (Z) = E(cid:2)(cid:107)((P + Z)A − In)ξ(cid:107)2

(cid:3) + E(cid:2)(cid:107)(P + Z)δ(cid:107)2

tr(·) denotes the trace, Λ is symmetric, E[] = µ and Cov[] = Σ,

 Λµ, where

(cid:3) .

2

2

where (cid:107)·(cid:107)F denotes the Frobenius norm. Next we rewrite f (Z) in terms of only one Frobenius

(cid:3) ∈ Rn×(n+1), then using the identities of the Frobenius and the

norm. Let M =(cid:2)Mξ µξ

2

+ (cid:107)((P + Z)A − In)Mξ(cid:107)2

F + η2 (cid:107)(P + Z)(cid:107)2
F ,

vector 2-norm, as well as applying Kronecker product properties, we get

f (Z) =(cid:13)(cid:13)Z(cid:2)AM ηIm

(cid:3) −(cid:2)M − PAM −ηP(cid:3)(cid:13)(cid:13)2

F .

(3)

Thus, minimizing the Bayes risk in problem (2) is equivalent to minimizing (3). Notice
that so far we have not imposed any constraints on Z. Although various constraints can be
imposed on Z, here we consider Z to be of low-rank, i.e., rank (Z) ≤ r for some r ≤ rank (A).
Hence the low-rank matrix approximation problem of interest in this paper is

f (Z) =(cid:13)(cid:13)Z(cid:2)AM ηIm

(cid:3) −(cid:2)M − PAM −ηP(cid:3)(cid:13)(cid:13)2

(4)

F .

min

rank(Z)≤r

regularized inverse matrix of at most rank r reduces to a truncated-Tikhonov matrix [9],

We will provide a closed form solution for (4) in Section 3, but it is important to remark that
special cases of this problem have been previously studied in the literature. For example,
a solution for the case where P = 0n×m and µξ = 0n×1 was provided in [9] that uses

the generalized SVD of (cid:8)A, M−1

(cid:9). If, in addition, we assume Mξ = In, then an optimal
(cid:98)Z = VA,rΨA,rU(cid:62)

(cid:17)
. Moreover, this(cid:98)Z is the unique global minimizer

(cid:16) σ1(A)

1(A)+η2 , . . . ,
σ2

σr(A)
σ2
r (A)+η2

A,r,

(5)

ξ

where ΨA,r = diag
for

min

rank(Z)≤r

(cid:107)ZA − In(cid:107)2

F + η2 (cid:107)Z(cid:107)2
F ,

(6)

if and only if σr(A) > σr+1(A).

4

3 Low-rank optimization problem

The goal of this section is to derive the unique global minimizer for problem (4), under
suitable conditions. We actually consider a more general problem, as stated in Theorem 3.3,
where M ∈ Rn×p with rank (M) = n ≤ p. Our proof uses a special case of Theorem 2.1 from
Friedland & Torokhti [16] that is provided here for completeness.
Theorem 3.1. Let matrices B ∈ Rm×n and C ∈ Rq×n with k = rank (C) be given. Then

(cid:98)Z =(cid:0)BVC,kV(cid:62)

(cid:1)

C†

C,k

r

is a solution to the minimization problem

min

rank(Z)≤r

(cid:107)ZC − B(cid:107)2
F ,

having a minimal (cid:107)Z(cid:107)F. This solution is unique if and only if either

or

1 ≤ r < rank(cid:0)BVC,kV(cid:62)

C,k

Proof. See [16].

(cid:1)

r ≥ rank(cid:0)BVC,kV(cid:62)
(cid:1)

C,k

and σr(BVC,kV(cid:62)

C,k) > σr+1(BVC,kV(cid:62)

C,k).

To get to our main result we ﬁrst provide the following Lemma.

Lemma 3.2. Let B = [A η Im] with A ∈ Rm×n and parameter η ≥ 0, nonzero if rank (A) <
max{m, n}. Let further DA ∈ Rm×m with DA = diag
for m ≥ n and DA = diag
B is given by B = UBΣBV(cid:62)

(cid:16)(cid:112)σ2
1(A) + η2, . . . ,(cid:112)σ2

for m < n. Then the SVD of

m(A) + η2

n(A) + η2, η, . . . , η

B, where

(cid:17)

(cid:16)(cid:112)σ2
1(A) + η2, . . . ,(cid:112)σ2
(cid:17)
(cid:20)VAΣ(cid:62)

AD−1
η UAD−1

A V12
A V22

(cid:21)

,

UB = UA, ΣB = [DA 0m×n]

and VB =

σj(B) =(cid:112)λj(BB(cid:62)), where λj(BB(cid:62)) deﬁnes the j-th eigenvalue of the matrix BB(cid:62) with

with arbitrary V12 and V22 satisfying V(cid:62)
Proof. Let the SVD of A = UAΣAV(cid:62)
λ1(BB(cid:62)) ≥ ··· ≥ λn(BB(cid:62)). Since the eigenvalue decomposition of BB(cid:62) is given by

12V12 + V(cid:62)
A be given. First, notice that the singular values

22V22 = In and AV12 + ηV22 = 0m×n.

BB(cid:62) = UA(ΣAΣ(cid:62)

A + η2Im)U(cid:62)

A

(7)

we have

ΣB = [DA 0m×n]

5

with DA, where

DA = diag

(cid:18)(cid:113)

and

DA = diag

(cid:19)

if m ≥ n,

if m < n.

(cid:19)

n(A) + η2, η, . . . , η

m(A) + η2

Notice that, DA is invertible if η > 0 or rank (A) = max{m, n}. By equation (7) the left
singular vectors of B correspond to the left singular vectors of A, i.e., UB = UA. As for the
right singular vectors let

σ2

σ2

1(A) + η2, . . . ,(cid:112)σ2
(cid:18)(cid:113)
1(A) + η2, . . . ,(cid:112)σ2
(cid:20)V11 V12
(cid:21)
(cid:20)V(cid:62)

V21 V22

VB =

(cid:21)

with V11 ∈ Rn×m, V21 ∈ Rm×m, V12 ∈ Rn×n, and V22 ∈ Rm×n. Then

B = [A η Im] = UA [DA 0m×n]

= [UADDAV(cid:62)

11 UADAV(cid:62)

21m]

11 V(cid:62)
V(cid:62)
12 V(cid:62)
A and V21 = η UAD−1
22V22 = In and V(cid:62)

21

22

and V11 = VAΣ(cid:62)
satisfying V(cid:62)
ηV22 = 0m×n.

AD−1
12V12 + V(cid:62)

A . The matrices V12 and V22 are any matrices
11V12 + V(cid:62)
21V22 = 0m×n or equivalently AV12 +

Next, we provide a main result of our paper.

Theorem 3.3. Given matrices A ∈ Rm×n, M ∈ Rn×p, and P ∈ Rn×m, with rank (A) = k ≤
n ≤ m, rank (M) = n ≤ p, let index r ≤ k and parameter η ≥ 0, nonzero if r < m. Deﬁne

F = (In − PA)MM(cid:62)A(cid:62) − η2P. If rank (F) ≥ r, then a global minimizer (cid:98)Z ∈ Rn×m of the

(cid:3) −(cid:2)M − PAM −ηP(cid:3)(cid:13)(cid:13)2

F

(8)

problem

is given by

min

rank(Z)≤r

f (Z) =(cid:13)(cid:13)Z(cid:2)AM ηIm
(cid:98)Z = UH,rU(cid:62)

H,rF(AMM(cid:62)A(cid:62) + η2I)−1,

(9)
where symmetric matrix H = F(AMM(cid:62)A(cid:62) + η2I)−1F(cid:62) has eigenvalue decomposition H =
ﬁrst r columns of UH. Moreover, (cid:98)Z is the unique global minimizer of (8) if and only if
H with eigenvalues ordered so that λj ≥ λi for j < i ≤ n, and UH,r contains the
UHΛHU(cid:62)

λr > λr+1.
Proof. We will use Theorem 3.1 where B = [(In − PA) M − ηP] and C = [AM ηIm].
Let

U(cid:62)AG = Σ and V(cid:62)M(cid:62)G = S

with

Σ =

(cid:20)diag(σ1, . . . , σn)

(cid:21)

0(m−n)×n

(cid:20)diag(s1, . . . , sn)
(cid:21)

0(p−n)×n

and S =

6

with

and

denote the generalized SVD of (cid:8)A, M(cid:62)(cid:9) and let L be deﬁned by L = ΣG−1G−(cid:62)S(cid:62) with

its SVD given by L = ULΣLV(cid:62)
VAM = VVL. Using Lemma 3.2, the SVD of C is given by

L . Then AM = UAMΣLV(cid:62)

AM, where UAM = UUL and

(cid:3)
UC = UAM, ΣC =(cid:2)DAM 0m×p
1(AM) + η2, . . . ,(cid:112)σ2
1(AM) + η2, . . . ,(cid:112)σ2

(cid:18)(cid:113)
(cid:18)(cid:113)

DAM = diag

DAM = diag

σ2

σ2

and VC =

L D−1
η UAMD−1

AM V12
AM V22

(cid:35)

,

n(AM) + η2, η, . . . , η

for m ≥ p,

,

m(AM) + η2

,

for m < p,

(cid:34)VAMΣ(cid:62)
(cid:19)

(cid:19)

and appropriately deﬁned V12 and V22. Notice that DAM is invertible and rank (C) = m, if
either η > 0 or rank (AM) = m. Also acknowledge that D2
L + η2Im. Thus, the
pseudoinverse of C is given by

AM = ΣLΣ(cid:62)

(cid:21)(cid:20) Σ(cid:62)

(cid:21)

L
η Im

C† =

(cid:20)VAM 0p×m
(cid:34)VAMΣ(cid:62)

0m×p UAM

D−2
AMU(cid:62)

AM

(cid:35)

.

Let F = (In − PA)MVAMΣ(cid:62)

VC,mV(cid:62)

C,m =

L D−2
AMΣLV(cid:62)
η UAMD−2
AMΣLV(cid:62)
AM − η2 P, then
L U(cid:62)
C,m = FUAMD−2
K = BVC,mV(cid:62)

AM

AM η VAMΣ(cid:62)

L D−2
η2 UAMD−2

AMU(cid:62)
AMU(cid:62)

AM

AM

(cid:2)ΣLV(cid:62)

AM η U(cid:62)

(cid:3) .

(10)
Notice that rank (K) ≥ r, since rank (F) ≥ r by assumption. Then, let symmetric
matrix H = KK(cid:62) = FUAMD−2
AMF(cid:62) have eigenvalue decomposition H = UHΛHU(cid:62)
with eigenvalues ordered so that λj ≥ λi, for j < i ≤ n. Next we proceed to get an SVD of
K,

AMU(cid:62)

AM

AM

H

with

K = UH

(cid:105)
H | 0n×(m+p−n)
(cid:21)
Λ1/2

(cid:104)
(cid:20)V11 V12 V13

V21 V22 V23

,

V(cid:62)

K

VK =

where V11 ∈ Rp×r, V21 ∈ Rm×r, V12 ∈ Rp×(n−r), and remaining matrices are deﬁned accord-
ingly. Then equating the SVD of K with (10) and using a similar argument as in Lemma 3.2,
we get

HFUAMD−2
U(cid:62)

AMΣLV(cid:62)

AM = Λ1/2

H

and

ηU(cid:62)

HFUAMD−2

AMU(cid:62)

AM = Λ1/2

H

7

(cid:20)V(cid:62)
(cid:21)
(cid:20)V(cid:62)

V(cid:62)

11

12

21

V(cid:62)

22

(cid:21)

.

K,r =(cid:2)V(cid:62)

Since ΛH,r (the principal r × r submatrix of ΛH) is invertible, the transpose of the ﬁrst r
columns of VK have the form,
11 | V(cid:62)
−1/2
H,r
−1/2
H,r U(cid:62)

(cid:2)ΣLV(cid:62)
(cid:3)
AM | η U(cid:62)

(cid:3)
(cid:2)Ir | 0r×(n−r)

(cid:2)ΣLV(cid:62)

H,rFUAMD−2

HFUAMD−2

(cid:3) U(cid:62)

AM | η U(cid:62)

V(cid:62)

(cid:3)

= Λ

= Λ

AM

AM

21

AM

AM

and the best rank r approximation of K is given by

Kr = UH,rΛ1/2
= UH,rU(cid:62)

H,rV(cid:62)
H,rFUAMD−2

K,r

AM [ΣL | η Im]

(cid:20)V(cid:62)

AM 0p×m
0m×p U(cid:62)
AM

(cid:21)

.

Finally, using Theorem 3.1 we ﬁnd that all global minimizers of f with rank at most r can
be written as

(cid:98)Z = KrC†

(cid:0)ΣLΣ(cid:62)

(cid:1) D−2

AMU(cid:62)

AM

= UH,rU(cid:62)
= UH,rU(cid:62)

H,rFUAMD−2
H,rF(AMM(cid:62)A(cid:62) + η2I)−1,

AM

L + η2Im

makes the choice of UH,r unique.

where (cid:98)Z is a unique global minimizer of (8) if and only if λr > λr+1 since this condition
4 Eﬃcient methods to compute ORIM (cid:98)Z
The computational cost to compute a global minimizer(cid:98)Z according to Theorem 3.3 requires
the computation of a GSVD of (cid:8)A, M(cid:62)(cid:9), an SVD of L, and a partial eigenvalue decom-
an alternative approach to eﬃciently compute ORIM (cid:98)Z.

position of H. For large-scale problems this may be computational prohibitive, so we seek
In the following we decompose
the optimization problem into smaller subproblems and use eﬃcient methods to solve the
subproblems. The optimality of our update approach is veriﬁed by the following corollary
of Theorem 3.3.

Corollary. Assume all conditions of Theorem 3.3 are fulﬁlled. Let (cid:98)Zr be a global minimizer
of (8) of maximal rank r and let (cid:98)Zr+(cid:96) be a global minimizer of (8) of maximal rank r + (cid:96).
Then ˜Z(cid:96) =(cid:98)Zr+(cid:96) −(cid:98)Zr is of maximal rank (cid:96) and the global minimizer of

(cid:13)(cid:13)(cid:13)(cid:16)(cid:98)Zr + Z

(cid:17)(cid:2)AM η Im

(cid:3) −(cid:2)M − PAM −η P(cid:3)(cid:13)(cid:13)(cid:13)2

F

.

(11)

˜Z(cid:96) = arg min
rank(Z)≤(cid:96)

Furthermore, ˜Z(cid:96) is the unique global minimizer if and only if λr > λr+1 and λr+(cid:96) > λr+(cid:96)+1.

8

The signiﬁcance of the corollary is as follows. Assume we are given a rank r approximation

(cid:98)Zr and we are interested in updating our approximation to a rank r + (cid:96) approximation(cid:98)Zr+(cid:96).
To calculate the optimal rank r + (cid:96) approximation (cid:98)Zr+(cid:96), we just need to solve a rank (cid:96)
optimization problem of the form (11) and then update the solution, (cid:98)Zr+(cid:96) =(cid:98)Zr + ˜Z(cid:96). Thus,
computing a rank r ORIM matrix (cid:98)Zr can be achieved by solving a sequence of smaller

rank problems and updating the solutions. Algorithm 1 describes such an rank-1 update
approach.

2: while stopping criteria not reached do

Algorithm 1 (rank-1 update approach)
Require: A, M, P, η

3:

1: set (cid:98)Z0 = 0n×m, r = 0
(cid:13)(cid:13)(cid:13)(cid:16)(cid:98)Zr + Z
(cid:98)Zr+1 =(cid:98)Zr + ˜Zr
Ensure: optimal (cid:98)Zr

4:
r = r + 1
5:
6: end while

˜Zr = arg min
rank(Z)≤1

(cid:17)(cid:2)AM η Im

(cid:3) −(cid:2)M − PAM −η P(cid:3)(cid:13)(cid:13)(cid:13)2

F

(cid:98)Zr = XrY(cid:62)

The main question in Algorithm 1 is how to eﬃciently solve the optimization problem
in line 3. First, we reformulate the rank-1 constraint by letting Z = xy(cid:62), where x ∈ Rn
and y ∈ Rm and deﬁning Xr = [x1, . . . , xr] ∈ Rn×r and Yr = [y1, . . . , yr] ∈ Rm×r. Then

r , and the optimization problem in line 3 of Algorithm 1 reads

(cid:13)(cid:13)(cid:0)XrY(cid:62)

r + xy(cid:62)(cid:1)(cid:2)AM η Im

(cid:3) −(cid:2)M − PAM −η P(cid:3)(cid:13)(cid:13)2

F . (12)

(xr+1, yr+1) = arg min

(x,y)

Although standard optimization methods could be used, care must be taken since this quartic
problem is of dimension n + m and ill-posed since the decomposition Z = xy(cid:62) is not unique.
Notice that for ﬁxed y, optimization problem (12) is quadratic and convex in x and vise
versa. Thus, we propose to use an alternating direction optimization approach. Assume
x (cid:54)= 0n×1, y (cid:54)= 0m×1, and η > 0, then the partial optimization problems resulting from (12)
are ensured to have unique minimizers

for ﬁxed y,

(13)

MM(cid:62)A(cid:62)y − (P + XrY(cid:62)

y(cid:62) (AMM(cid:62)A(cid:62) + η2Im) y

r )(cid:0)AMM(cid:62)A(cid:62) + η2Im
(cid:1) y
(cid:1)−1 AMM(cid:62)x − (P + XrY(cid:62)

r )(cid:62)x

(cid:0)AMM(cid:62)A(cid:62) + η2Im

(cid:98)x =
(cid:98)y =

and

Notice that computing (cid:98)x in (13) only requires matrix-vector products, while computing (cid:98)y

requires a linear solve. Since decomposition Z = xy(cid:62) is not unique, we propose to select the

for ﬁxed x.

x(cid:62)x

9

simpliﬁed formula for(cid:98)y, i.e.,
computationally convenient decomposition where (cid:107)x(cid:107)2 = 1 and x ⊥ Xr. This results in a

AMM(cid:62)x − P(cid:62)x.

(14)

Noticing that (14) is just the normal equations solution to the following least squares prob-
lem,

(cid:98)y =(cid:0)AMM(cid:62)A(cid:62) + η2Im
(cid:13)(cid:13)(cid:13)(cid:13)(cid:20)M(cid:62)A(cid:62)

(cid:1)−1
(cid:20)M(cid:62)x − M(cid:62)A(cid:62)P(cid:62)x

−ηP(cid:62)x

y −

(cid:21)

min

y

η Im

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)2

,

(15)

we propose to use a computationally eﬃcient least squares solver such as LSQR [29, 30],
where various methods can be used to exploit the fact that the coeﬃcient matrix remains
constant [5, 3]. In addition, quasi Newton methods may improve eﬃciency by taking advan-
tage of a good initial guess and a good approximation on the inverse Hessian [28], but such
comparisons are beyond the scope of this paper.

The alternating direction approach to compute a rank-1 update is provided in Algo-

rithm 2.

Algorithm 2 (alternating direction approach to compute rank-1 update)
Require: A, M, η, Z, P, r

2: while stopping criteria not reached do
3:
4:
5:
6:
7: end while

1: set(cid:98)y = 1m×1
get(cid:98)x by (13)
normalize(cid:98)x =(cid:98)x/(cid:107)(cid:98)x(cid:107)2
orthogonalize by(cid:98)x =(cid:98)x − XrX(cid:62)
r(cid:98)x
get(cid:98)y by solving (15)
8: xr+1 =(cid:98)x and yr+1 =(cid:98)y
In summary, our proposed method to compute low-rank ORIM(cid:98)Z combines Algorithms 1

Ensure: optimal xr+1 and yr+1

and 2. An eﬃcient Matlab implementation can be found at the following website:

https://github.com/juliannechung/ORIM.git

Before providing illustrations and examples of our method, we make a few remarks regarding
numerical implementation.

1. Storage. Algorithmically(cid:98)Zr need never be constructed, as we only require matrices Xr
2. Stopping criteria. For Algorithm 1, the speciﬁc rank r for (cid:98)Zr may be user-deﬁned,

and Yr. This decomposition is storage preserving as long as r ≤ mn
m+n and is ideal for
problems where Z is too large to compute or A can only be accessed via function call.

but oftentimes such information is not available a priori. However, the rank-1 update

10

approach allows us to track the improvement in the function value from rank r to
rank r + 1. Then an approximation of rank r is deemed suﬃcient when f (Zr−1) −
f (Zr) < tol · f (Zr), where our default tolerance is tol = 10−6. Standard stopping
criteria [17] can be used for Algorithm 2. In particular, we track improvement in the
function values f (XrY(cid:62)
iteration. Our default tolerance is 10−6.

r ), track changes in the arguments(cid:98)x and(cid:98)y, and set a maximum

3. Eﬃcient function evaluations. Rather than computing the function value f (XrY(cid:62)
r )
from scratch at each iteration (e.g., for determining stopping criteria), eﬃcient updates
can be done by observing that
r+1) =f (XrY(cid:62)
r )

f (Xr+1Y(cid:62)

(cid:1)(cid:0)y + 2P(cid:62)x(cid:1) − 2y(cid:62)AMM(cid:62)x ,

+ y(cid:62)(cid:0)AMM(cid:62)A(cid:62) + η2Im

where f (0n×m) = (cid:107)(In − PA)M(cid:107)2
F. Since function evaluations are only
relevant for the stopping criteria, they can be discarded, if desired, or approximated
using trace estimators [1].

F + η2 (cid:107)P(cid:107)2

4. Initialization. Equation (13) requires an initial guess for y. One uninformed choice may
be y = 1m×1, and another option is to select y orthogonal to Yr, i.e., y = (Im−YrY(cid:62)
r )r
with r ∈ Rm chosen at random.

to compute a symmetric ORIM (cid:98)Zr = XrX(cid:62)

If A and P are symmetric, our rank-1 update approach could be used
r , but the alternating direction approach

5. Symmetry.

should be replaced by an appropriate method for minimizing a quartic in x.

6. Covariance matrix. Since M in our rank update approach only occurs in the product
MM(cid:62) and since MM(cid:62) = MξM(cid:62)
ξ , our algorithm can work
directly with the covariance matrix. Thus, a symmetric factorization does not need to
be computed, which is important for various classes of covariance kernels [32].

ξ = Γξ + µξµ(cid:62)

ξ + µξµ(cid:62)

5 Numerical Results

In this section, we provide three experiments that not only highlight the beneﬁts of ORIM
updates but also demonstrate new approaches for solving inverse problems that use ORIM
updates.
In Experiment 1, we use an inverse heat equation to investigate the eﬃciency
and accuracy of our update approach. Then in Experiment 2, we use an image deblurring
example to show that more accurate solutions to inverse problems can be achieved by using
ORIM rank-updates to existing regularized inverse matrices. Lastly, in Experiment 3, we
show that ORIM updates can be used in scenarios where perturbed inverse problems need
to be solved eﬃciently and accurately.

11

5.1 Experiment 1: Eﬃciency of ORIM rank update approach

The goal of this example is to highlight our new result in Theorem 3.3 and to verify the
accuracy and eﬃciency of the update approach described in Section 4. We consider a dis-
cretized (ill-posed) inverse heat equation derived from a Volterra integral equation of the
ﬁrst kind on [0, 1] with kernel a(s, t) = k(s − t), where k(t) = t−3/2
√
4κ2t . Coeﬃcient matrix
πκe
A is 1,000 × 1,000 and is signiﬁcantly ill-posed for κ ∈ [1, 2]. We generate A using the
Regularization Tools package [22].

As a ﬁrst study, we compare ORIM (cid:98)Z with other commonly used regularized inverse
matrices. Notice that (cid:98)Z is fully determined by A, η, M, and P.
compute ORIM(cid:98)Z as in Equation (9) for various ranks r and plot the function values f ((cid:98)Z) in

For this illustration, we select P and M to be realizations of random matrices whose
entries are i.i.d. standard normal N (0, 1), and we select κ = 1 and η = 0.02. Then we

− 1

2

Figure 1. For comparison, we also provide function values for other commonly used rank-r
reconstruction matrices, including the TSVD matrix, A†
r, the truncated Tikhonov matrix (5)
(TTik), and the matrix provided from Theorem 1 of [9], here referred to as ORIM0. Notice
that TTik and ORIM0 matrices are just special cases of ORIM where M = [ In 0n×1 ] and
P = 0n×m for TTik and M = [ Mξ 0n×1 ] and P = 0n×m for ORIM0. Figure 1 shows that,
as expected, the function values for ORIM are smallest for all computed ranks.

Figure 1: Comparison of the function values f (Z) where Z corresponds to diﬀerent recon-
struction matrices. The dotted line refers to TSVD, the dashed line to truncated-Tikhonov,
the dash-dotted line to ORIM0 (i.e., ORIM where Mξ = In and µξ = 0n×1), and the solid

line to ORIM (cid:98)Z. Results correspond to a discretized Volterra integral equation.

We also veriﬁed our proposed rank-update approach by comparing function values com-
puted with the rank update approach to those from Theorem 3.3. We observed that the
relative absolute errors remained below 2.9485 · 10−3 for all computed ranks r, making the
plot of the function values for the update approach indistinguishable from the solid line in

12

5101520253035404550rankr0.9511.051.11.151.21.25objectivefunctionf(!Zr)×106TSVDTTikORIM0ORIMFigure 1. Thus, we omit it for clarity of presentation.

Next, we illustrate the eﬃciency of our rank update approach for solving a sequence
of ill-posed inverse problems. Such scenarios commonly occur in nonlinear optimization
problems such as variable projection methods where nonlinear parameters are moderately
changing during the optimization process [28, 18]. Consider again the inverse heat equation,
and assume that we are given a sequence of matrices A(κj) ∈ Rn×n, where the matrices
depend nonlinearly on parameter κj, and we are interested in solving a sequence of problems,
b(κj) = A(κj)ξ + δj for various κj.
VA(κj )ΨA(κj )U(cid:62)

For each problem in the sequence, one could compute a Tikhonov solution ξTik(κj) =

A(κj )b(κj), where

(cid:18) σ1(A(κj))

ΨA(κj ) = diag

1(A(κj)) + η2 , . . . ,
σ2

σn(A(κj))
n(A(κj)) + η2
σ2

(cid:19)

,

but this approach requires an SVD of A(κj) for each κj. We consider an alternate approach,
where the SVD is computed once for a ﬁxed κj and then ORIM updates are used to obtain
improved regularized inverse matrices for other κj’s. This approach relies on the fact that
small perturbations in A(κj) lead to small rank updates in its inverse [34].
Again for the inverse heat equation we use n = 1,000 and η = 0.02 and choose M = In
and µ = 0n×1. We select equidistant values for κj ∈ [1, 2], j = 1, . . . , 100, and let P(1) =
VA(κ1)ΨA(κ1)U(cid:62)
A(κ1) be the Tikhonov reconstruction matrix corresponding to κ1. Then for
all other problems in the sequence, we compute reconstructions as

where P(j+1) = P(j) + X(j+1)(cid:0)Y(j+1)(cid:1)(cid:62)

ξORIM(κj+1) = P(j+1)b(κj+1)

, where X(j+1) and Y(j+1) are the low rank ORIM
updates corresponding to A(κj+1). We use a tolerance tol = 10−3.
In Figure 2, we re-
port computational timings for the ORIM rank update approach, compared to the SVD,
and in Figure 3 we provide corresponding relative reconstruction errors, computed as rel =
(cid:107)ξ(cid:63) − ξtrue(cid:107)2 /(cid:107)ξtrue(cid:107)2, where ξ(cid:63) is and approximation of ξ (here, ξORIM(κj) and ξTik(κj)).
We observe that the ORIM update approach requires approximately half the required CPU
time compared to the SVD, and the ORIM update approach can produce relative reconstruc-
tion errors that are comparable to and even slightly better than Tikhonov. However, we also
note potential disadvantages of our approach. In particular, the SVD can be more eﬃcient
for small n, although ORIM updates are signiﬁcantly faster for larger problems (results not
shown). Also, using diﬀerent noise levels η in each problem or taking larger changes in κj
may result in higher CPU times and/or higher reconstruction errors for the update approach.
We assume that the noise levels and problems are not changing signiﬁcantly.

5.2 Experiment 2: ORIM Updates to Tikhonov

Here we consider a classic image deblurring problem, where the model is given in (1) where ξ
represents the desired image, A models the blurring process, and b is the blurred, observed

13

Figure 2: CPU times for computing a regularized inverse matrix using ORIM updates (solid
line) and for computing the SVD to get a Tikhonov solution (dotted line) for a sequence of
inverse problems varying in κ. We repeated the experiment 50 times and report the median
as well as the 25-75th percentiles.

image. The true image was taken to be the 15-th slice of the 3D MRI image dataset that
is provided in MATLAB, which is 256 × 256 pixels. We assume spatially invariant blur,
where the point spread function (PSF) is a 11 × 11 box-car blur. We assume reﬂexive
boundary conditions for the image. Since the PSF is doubly symmetric, blur matrix A is
highly structured and its singular value decomposition is given by A = UAΣAV(cid:62)
A, where
here V(cid:62)
A and UA represent the 2D discrete cosine transform (DCT) matrix and inverse 2D
DCT matrix respectively [24]. Here we use the RestoreTools package [27]. Noise δ was
generated from a normal distribution, with zero mean, and scaled such that the noise level
was (cid:107)δ(cid:107)2
2 = 0.01. The true and observed images, along with the PSF, are provided
in Figure 4.
As an initial regularized inverse approximation, we use a Tikhonov reconstruction matrix,
P = VA(Σ(cid:62)
A, where regularization parameter η was selected to provide
minimal reconstruction error. That is, we used η = 2.831 · 10−2, which corresponded to
the minimum of error function, (cid:107)Pb − ξ(cid:107)2. Although this approach uses the true image
(which is not known in practice), our goal here is to demonstrate the improvement that
can be obtained using the rank-update approach.
In practice, a standard regularization
parameter selection method such as the generalized cross-validation could be used, which
for this problem gave η = 2.713 · 10−2. The Tikhonov reconstruction, Pb, is provided in
Figure 5(a) along with the computed relative reconstruction error.

AΣA + η2I)−1Σ−1

2 /(cid:107)Aξ(cid:107)2

A U(cid:62)

Next we consider various ORIM updates to P and evaluate corresponding reconstructions.
For the mean vector µξ, we use the image shown in Figure 5(b), which was obtained by
averaging images slices 8–22 of the MRI stack (omitting slice 15, the image of interest). For
eﬃcient computations and simplicity, we assume Γξ is diagonal with variances proportional

(cid:1); the matrix Mξ is deﬁned accordingly. We compute ORIM

to µξ, we choose, Γξ = diag(cid:0)µξ

14

Figure 3: Relative reconstruction errors for reconstructions obtained using ORIM updates
(solid line) and using Tikhonov regularization (dotted line). We report the median as well
as the 25-75th percentiles for each κ after repeating the experiment 50 times.

(a) True image

(b) Observed, blurred image

(c) Point spread function

Figure 4:
observed, blurred image is provided in (b), and the PSF is provided in (c).

Image deblurring example. The true (desired) MRI image is given (a). The

updates to P according to Algorithm 1 for the following cases of M:

M(1) =(cid:2)In µξ

(cid:3) , M(2) =(cid:2)Mξ 0n×1

(cid:3) ,

and M(3) =(cid:2)Mξ µξ

(cid:3) .

We refer to these matrix updates as (cid:98)Z(1), (cid:98)Z(2), and (cid:98)Z(3) respectively, where (cid:98)Z(1) is a rank-1
matrix and (cid:98)Z(2) and (cid:98)Z(3) are matrices of rank 5. Image reconstructions were obtained via

(16)

matrix-vector multiplication,

ξ(j) = Pb +(cid:98)Z(j)b,

for j = 1, 2, 3,

and are provided in Figure 5(c)–(e). Corresponding relative reconstruction errors are also
provided. Furthermore, absolute error images (in inverted colormap so that black corre-
sponds to larger reconstruction error) in Figure 6 show that the errors for the ORIM updated

15

(a) Tikhonov, Pb, rel = 0.2247

(b) Mean image, µξ

(c) ξ(1), rel = 0.1938

(d) ξ(2), rel = 0.2179

(e) ξ(3), rel = 0.1904

Figure 5: Initial Tikhonov reconstruction is provided in (a). The mean image, µ, provided in
(b), was taken to be the average of images slices 8-22 of the MRI image stack (omitting slice
15, the image of interest). Image reconstructions in (c)-(e) correspond to ORIM updates to
the initial Tikhonov reconstruction, for the various choices for M provided in (16). Relative
reconstruction errors are provided.

16

(a) Tikhonov

(b) ξ3

Figure 6: Error images (in inverted colormap where white corresponds to 0) for the initial
Tikhonov reconstruction and the ORIM updated solution ξ(3) which corresponds to M(3)
(i.e., nonzero mean and covariance matrix for ξ).

solution ξ(3) have smaller and more localized errors than the initial Tikhonov reconstruction.

We repeated this experiment 20,000 times, each time with a diﬀerent noise realization in b
and provide the distribution of the corresponding relative reconstruction errors in Figure 7.
Additionally, for each of these approaches, we provide the average reconstruction error,
along with the standard deviation over all noise realizations in Table 5.2. It is evident from
these experiments that ORIM rank-updates to the Tikhonov reconstruction matrix can lead
to reconstructions with smaller relative errors and allows users to easily incorporate prior
knowledge regarding the distributions of ξ and δ.

Figure 7: Distributions of relative reconstruction errors

17

Table 1: Comparison of average relative reconstruction error and standard deviation for
1,000 noise realizations.

Tikhonov
ORIM update, M(1)
ORIM update, M(2)
ORIM update, M(3)

mean ± standard deviation
1.1215 · 10−5 ± 3.4665 · 10−8
9.6881 · 10−6 ± 3.2040 · 10−8
1.0880 · 10−5 ± 3.4402 · 10−8
9.5254 · 10−6 ± 3.1541 · 10−8

We then applied our reconstruction matrices, P +(cid:98)Z(j), to the other images in the MRI

stack and provide the relative reconstruction errors in Figure 8. We observe that in general,
all of the reconstruction matrices provide fairly good reconstructions, with smaller relative
errors corresponding to images that are most similar to the mean image. Some of the true
images were indeed included in the mean image. Regardless, our goal here is to illustrate
that ORIM update matrices can be eﬀective and eﬃcient, if a good mean image and/or
covariance matrix are provided. Other covariance matrices can be easily incorporated in this
framework, but comparisons are beyond the scope of this work.

Figure 8: Reconstructions of diﬀerent slices from the MRI image stack using the initial
Tikhonov reconstruction matrix, as well as the ORIM-updated reconstruction matrices.

5.3 Experiment 3: ORIM updates for perturbed problems

Last, we consider an example where ORIM updates to existing regularized inverse matri-
ces can be used to eﬃciently solve perturbed problems. That is, consider a linear inverse
problem such as (1) where a good regularized inverse matrix denoted by P can be obtained.

18

510152025Imageinstack0.160.180.20.220.240.26RelativereconstructionerrorTikhonovORIMupdate,M(1)ORIMupdate,M(2)ORIMupdate,M(3)Now, suppose A is modiﬁed slightly (e.g., due to equipment setup or a change in model
parameters), and a perturbed linear inverse problem

(cid:101)b = (cid:101)Aξ +(cid:101)δ

(17)

must be solved. We will show that as long as the perturbation is not too large, a good
solution to the perturbed problem can be obtained using low-rank ORIM updates to P.
This is similar to the scenario described in Experiment 1, but here we use an example from
2D tomographic imaging, where the goal is to estimate an image or object f (x, y), given
measured projection data. The Radon transform can be used to model the forward process,
where the Radon transform of f (x, y) is given by

b(ξ, φ) =

f (x, y)δ(x cos φ + y sin φ − ξ) dx dy

(18)

(cid:90)

where δ is the Dirac delta function. Figure 9 illustrates the basic tomographic process.

Figure 9: Experiment 3: Illustration of 2D tomography problem setup, where f (x, y) is the
desired object and projection data is obtained by x-ray transmission at various angles around
the object.

The goal of the inverse problem is to compute a (discretized) reconstruction of the image
f (x, y), given projection data that is collected at various angles around the object. The
projection data, when stored as an image, gives the sinogram. In Figure 10 (a), we provide
the true image which is a 128× 128 image of the Shepp-Logan phantom, and two sinograms
are provided in Figure 10 (b) and (c), where the rows of the image contain projection data
at various angles. In particular, for this example, we take 60 projection images at 3 degree
intervals from 0 to 177 degrees (i.e., the sinogram contains 60 rows). In order to deal with
boundary artifacts, we pad the original image with zeros.

The discrete tomographic reconstruction problem can be modeled as (1) where ξ rep-
resents the (vectorized) desired image, A models the tomographic process, and b is the

19

(a) True image

(b) Sinogram 1

(c) Sinogram 2

Figure 10: Tomography Problem. The true image is shown in (a), the observed sinogram for
the initial problem is given in (b) and the sinogram corresponding to the perturbed problem
is given in (c).

(vectorized) observed sinogram. For this example, we construct

 RS(1)

...

 ,

RS(60)

A =

where S(j) is a sparse matrix that represents rotation of the image for the j-th angle, whose
entries were computed using bilinear interpolation as described in [10, 11], and R is a Kro-
necker product that approximates the integration operation. It is worth mentioning that
in typical tomography problems, A is never created, but rather accessed via projection and
backprojection operations [15]. Our methods also work for scenarios where A represents a
function call or object, but our current approach allows us to to build the sparse matrix
directly. White noise is added to the problem at relative noise level 0.005.

Since A has no obvious structure to exploit, we use iterative reconstruction methods
to get an initial reconstruction matrix. This mimics a growing trend in tomography where
reconstruction methods have shifted from ﬁltered back projection approaches to iterative re-
construction methods [25, 2]. Furthermore, these iterative approaches are ideal for problems
such as limited angle tomogography or tomosynthesis, where the goal is to obtain high qual-
ity images while reducing the amount of radiation to the patient [13, 12]. In this paper, we
deﬁne a regularized inverse matrix P in terms of a partial Golub-Kahan bidiagonalization.
That is, given a matrix A and vector b, the Golub-Kahan process iteratively transforms ma-
trix [b A] to upper-bidiagonal form [β1e1 B(k)], with initializations β1 = (cid:107)b(cid:107)2, w1 = b/β1
and α1q1 = A(cid:62)w1. After k steps of the Golub-Kahan bidiagonalization process, we have

(cid:3) ∈ Rm×k, and bidiagonal

(cid:3) ∈ Rn×k, W(k) = (cid:2)w1

matrices Q(k) = (cid:2)q1

. . . qk

matrix

B(k) =

α1
β2 α2
. . .



. . . wk

 ∈ R(k+1)×k,

. . .
βk

20

αk
βk+1

such that

AQ(k) = W(k+1)B(k).

(19)

It is worth noting that in exact arithmetic, the k-th LSQR [29, 30] iterate is given by xLSQR =
Q(k)(B(k))†(W(k+1))(cid:62)b. Thus, we deﬁne P = Q(k)(B(k))†(W(k+1))(cid:62) to be a regularized
inverse matrix for the original problem, where k = 46 corresponds to minimal reconstruction
error (cid:107)xLSQR − xtrue(cid:107)2 /(cid:107)xtrue(cid:107)2 = 0.2641 for the original problem. See Figure 11 for the
relative error plot for the original problem.

Figure 11: Relative reconstruction errors for LSQR on the original tomography problem,
where the bullet • corresponds to minimal reconstruction error.

The goal of this illustration is to show that a low-rank ORIM update to P can be used

to solve a perturbed problem. Thus, we created a perturbed problem (17), where (cid:101)b and (cid:101)A
perturbed data: P(cid:101)b. This reconstruction is provided in the top left corner of Figure 12, and

were created with slightly shifted projection angles. Again, we take 60 projection images at
3 degree intervals, but this time the angles ranged from 1 to 178 degrees. The corresponding
sinogram is given in Figure 10(c). A ﬁrst approach would be to use P to reconstruct the

it is evident that this is not a very good reconstruction. After a rank-4 update to P, where
µξ = 0n×1, Mξ = In and η = 0.08, we get a signiﬁcantly better reconstruction (middle col-
umn of Figure 12). For comparison purposes, we provide in the last column the best LSQR
reconstruction for the perturbed problem (i.e., corresponding to minimal reconstruction er-
ror). Relative reconstruction errors are provided, and corresponding absolute error images
are presented on the same scale and with inverted colormap.

6 Conclusions

In this paper, we provide an explicit solution for a generalized rank-constrained matrix in-
verse approximation problem. We deﬁne the solution to be an optimal regularized inverse

21

01020304050607080iteration0.250.30.350.40.450.5relative errorOriginalProblemminimumInitial, rel = 1.438

ORIM, rel = 0.287

LSQR, rel = 0.267

Figure 12: Tomographic reconstructions for the perturbed problem, with corresponding error

images. The reconstruction in the ﬁrst column was obtained as P(cid:101)b, the reconstruction in
(P +(cid:98)Z)(cid:101)b. The reconstruction in the last column corresponds to the LSQR reconstruction

the second column was obtained using a rank-4 ORIM update to P and was computed as

for the perturbed problem corresponding to minimal reconstruction error.

22

matrix (ORIM), where we include regularization terms, rank constraints, and a more gen-
eral weighting matrix. Two main distinctions from previous results are that we can include
updates to an existing matrix inverse approximation, and in the Bayes risk minimization
framework, we can incorporate additional information regarding the probability distribution
of ξ. For large scale problems, obtaining an ORIM according to Theorem 3.3 can be computa-
tionally prohibitive, so we described an eﬃcient rank-update approach that decomposes the
optimization problem into smaller rank subproblems and uses gradient-based methods that
can exploit linearity. Using examples from image processing, we showed that ORIM updates
can be used to compute more accurate solutions to inverse problems and can be used to
eﬃciently solve perturbed systems, which opens the door to new applications and investiga-
tions. In particular, our current research is on incorporating ORIM updates within nonlinear
optimization schemes such as variable projection methods, as well as on investigating its use
for updating preconditioners for slightly changing systems.

References

[1] H. Avron and S. Toledo, Randomized algorithms for estimating the trace of an im-
plicit symmetric positive semi-deﬁnite matrix, Journal of the ACM (JACM), 58 (2011),
pp. 8:1–8:17.

[2] M. Beister, D. Kolditz, and W. A. Kalender, Iterative reconstruction methods

in x-ray ct, Physica medica, 28 (2012), pp. 94–108.

[3] M. Benzi, Preconditioning techniques for large linear systems: a survey, Journal of

Computational Physics, 182 (2002), pp. 418–477.

[4] B. Carlin and T. Louis, Bayes and Empirical Bayes Methods for Data Analysis,

Chapman and Hall/CRC, Boca Raton, 2 ed., 2000.

[5] K. Chen, Matrix Preconditioning Techniques and Applications, vol. 19, Cambridge

University Press, Cambridge, 2005.

[6] J. Chung and M. Chung, Computing optimal low-rank matrix approximations for
image processing, in IEEE Proceedings of the Asilomar Conference on Signals, Systems,
and Computers. November 3-6, 2013, Paciﬁc Grove, CA, USA, 2013.

[7] J. Chung and M. Chung, An eﬃcient approach for computing optimal low-rank

regularized inverse matrices, Inverse Problems, 30 (2014), pp. 1–19.

[8] J. Chung, M. Chung, and D. O’Leary, Designing optimal ﬁlters for ill-posed

inverse problems, SIAM Journal on Scientiﬁc Computing, 33 (2011), pp. 3132–3152.

[9] J. Chung, M. Chung, and D. P. O’Leary, Optimal regularized low rank inverse

approximation, Linear Algebra and its Applications, 468 (2015), pp. 260–269.

23

[10] J. Chung, E. Haber, and J. G. Nagy, Numerical methods for coupled super-

resolution, Inverse Problems, 22 (2006), pp. 1261–1272.

[11] J. Chung and J. Nagy, An eﬃcient iterative approach for large-scale separable non-
linear inverse problems, SIAM Journal on Scientiﬁc Computing, 31 (2010), pp. 4654–
4674.

[12] J. Chung, J. Nagy, and I. Sechopoulos, Numerical algorithms for polyenergetic
digital breast tomosynthesis reconstruction, SIAM Journal on Imaging Sciences, 3 (2010),
pp. 133–152.

[13] J. T. Dobbins III and D. J. Godfrey, Digital x-ray tomosynthesis: current state

of the art and clinical potential, Physics in medicine and biology, 48 (2003), p. R65.

[14] P. Drineas, R. Kannan, and M. Mahoney, Fast Monte Carlo algorithms for matri-
ces II: Computing a low-rank approximation to a matrix, SIAM Journal on Computing,
36 (2007), pp. 158–183.

[15] T. G. Feeman, Mathematics of Medical Imaging, Springer, 2015.

[16] S. Friedland and A. Torokhti, Generalized rank-constrained matrix approxima-

tions, SIAM Journal on Matrix Analysis and Applications, 29 (2007), pp. 656–659.

[17] P. Gill, W. Murray, and M. Wright, Practical Optimization, Emerald Group

Publishing, Bingley, UK, 1981.

[18] G. Golub and V. Pereyra, The diﬀerentiation of pseudo-inverses and nonlinear
least squares whose variables separate, SIAM J. Numer. Anal., 10 (1973), pp. 413–432.

[19] J. Hadamard, Lectures on Cauchy’s Problem in Linear Diﬀerential Equations, Yale

University Press, New Haven, 1923.

[20] M. Hanke, Conjugate Gradient Type Methods for Ill-Posed Problems, Pitman Research

Notes in Mathematics, Longman Scientiﬁc & Technical, Harlow, Essex, 1995.

[21] M. Hanke and P. Hansen, Regularization methods for large-scale problems, Surveys

on Mathematics for Industry, 3 (1993), pp. 253–315.

[22] P. Hansen, Regularization tools: A MATLAB package for analysis and solution of

discrete ill-posed problems, Numerical Algorithms, 6 (1994), pp. 1–35.

[23] P. Hansen, Discrete Inverse Problems: Insight and Algorithms, SIAM, Philadelphia,

2010.

[24] P. Hansen, J. Nagy, and D. O’Leary, Deblurring Images: Matrices, Spectra and

Filtering, SIAM, Philadelphia, 2006.

24

[25] J. Hsieh, Computed tomography: principles, design, artifacts, and recent advances,

SPIE Bellingham, WA, 2009.

[26] I. Markovsky, Low Rank Approximation: Algorithms, Implementation, Applications,

Springer, New York, 2012.

[27] J. Nagy, K. Palmer, and L. Perrone, Iterative methods for image deblurring: A

Matlab object oriented approach, Numerical Algorithms, 36 (2004), pp. 73–93.

[28] J. Nocedal and S. Wright, Numerical Optimization, Springer, New York, 1999.

[29] C. Paige and M. Saunders, LSQR: An algorithm for sparse linear equations and
sparse least squares, ACM Transactions on Mathematical Software, 8 (1982), pp. 43–71.

[30] C. C. Paige and M. A. Saunders, Algorithm 583, LSQR: Sparse linear equations

and least-squares problems, ACM Trans. Math. Soft., 8 (1982), pp. 195–209.

[31] L. Rudin, S. Osher, and E. Fatemi, Nonlinear total variation based noise removal

algorithms, Physica D, 60 (1992), pp. 259–268.

[32] A. K. Saibaba, S. Ambikasaran, J. Yue Li, P. K. Kitanidis, and E. F. Darve,
Application of hierarchical matrices to linear inverse problems in geostatistics, Oil and
Gas Science and Technology-Revue de l’IFP-Institut Francais du Petrole, 67 (2012),
p. 857.

[33] G. A. F. Seber and A. J. Lee, Linear Regression Analysis, vol. 936, John Wiley &

Sons, San Francisco, 2012.

[34] G. W. Stewart, Matrix Algorithms: Volume 2. Eigensystems, vol. 2, SIAM, Philadel-

phia, 2001.

[35] A. Tikhonov and V. Arsenin, Solutions of Ill-posed Problems, Winston, 1977.

[36] V. Vapnik, Statistical Learning Theory, Wiley, San Francisco, 1998.

[37] C. R. Vogel, Computational Methods for Inverse Problems (Frontiers in Applied

Mathematics), SIAM, Philadelphia, 1987.

[38] J. Ye, Generalized low rank approximations of matrices, Machine Learning, 61 (2005),

pp. 167–191.

25

