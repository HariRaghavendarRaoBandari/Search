6
1
0
2

 
r
a

M
2

 

 
 
]

G
L
.
s
c
[
 
 

1
v
4
6
5
0
0

.

3
0
6
1
:
v
i
X
r
a

Asymptotic behavior of ℓp-based Laplacian
regularization in semi-supervised learning

Ahmed El Alaoui⋆ Xiang Cheng⋆ Aaditya Ramdas⋆,†

Martin J. Wainwright⋆,† Michael I. Jordan⋆,†

Department of Electrical Engineering and Computer Sciences⋆, and

Department of Statistics†,

UC Berkeley, Berkeley, CA 94720.

Abstract

Given a weighted graph with N vertices, consider a real-valued regression problem in a
semi-supervised setting, where one observes n labeled vertices, and the task is to label the
remaining ones. We present a theoretical study of ℓp-based Laplacian regularization under
a d-dimensional geometric random graph model. We provide a variational characterization
of the performance of this regularized learner as N grows to inﬁnity while n stays constant;
the associated optimality conditions lead to a partial diﬀerential equation that must be

transition in its smoothness at the threshold p = d + 1; and (b) a tradeoﬀ between
smoothness and sensitivity to the underlying unlabeled data distribution P . Thus, over

satisﬁed by the associated function estimate bf . From this formulation we derive several
predictions on the limiting behavior the d-dimensional function bf , including (a) a phase
the range p ≤ d, the function estimate bf is degenerate and “spiky,” whereas for p ≥
d + 1, the function estimate bf is smooth. We show that the eﬀect of the underlying
so-called Absolutely Minimal Lipschitz Extension, the estimate bf is independent of the

density vanishes monotonically with p, such that in the limit p = ∞, corresponding to the
distribution P . Under the assumption of semi-supervised smoothness, ignoring P can lead
to poor statistical performance; in particular, we construct a speciﬁc example for d = 1 to
demonstrate that p = 2 has lower risk than p = ∞ due to the former penalty adapting to
P and the latter ignoring it. We also provide simulations that verify the accuracy of our
predictions for ﬁnite sample sizes. Together, these properties show that p = d + 1 is an

optimal choice, yielding a function estimate bf that is both smooth and non-degenerate,

while remaining maximally sensitive to P .

Keywords: ℓp-based Laplacian regularization; semi-supervised learning; asymptotic behav-
ior; geometric random graph model; absolutely minimal Lipschitz extension; phase transition.

1

Introduction

Semi-supervised learning is a research ﬁeld of growing interest in machine learning.
It is
attractive due to the availability of large amounts of unlabeled data, and the growing desire
to exploit it in order to improve the quality of predictions and inference in downstream
applications. Although many proposed methods have been successful empirically, a formal
understanding of the pros and cons of diﬀerent semi-supervised methods is still incomplete.
The goal of this paper is to study the tradeoﬀs between some recently proposed Laplacian
regularization algorithms for graph-based semi-supervised learning. In the noiseless setting,
the problem amounts to a particular form of interpolation of a graph-based function. More
precisely, consider a graph G = (V, E, w) where V = {v1,··· , vN} is a set of N vertices, and

1

E is the set of edges equipped with a set w = (we)e∈E of non-negative edge weights. For some
subset O ⊂ V of the vertex set, say with cardinality |O| = n ≪ N , and an unknown function
f∗ : V → R, suppose that we are given observations (yi = f∗(vi))i∈O of the function at the
speciﬁed subset of vertices. Our goal is to use the observed values to make predictions of the
function values at the remaining vertices in a way that agrees with f∗ as much as possible.

In order to render this problem well-posed, the behavior of the function f∗ must be tied to
the properties of the graph G. In a statistical context, one such requirement is such that the
marginal distribution of the points vi be related to the regression function f∗—for instance,
by requiring that f∗ be smooth on regions of high density. This assumption and variants
thereof are collectively referred to as the cluster assumption in the semi-supervised learning
literature.

Under such a graph-based smoothness assumption, one reasonable method for extrapola-
tion is to penalize the change of the function value between neighboring vertices while agreeing
with the observations. A widely used approach involves using the ℓ2-based Laplacian as a
regularizer; doing so leads to the objective

min

f Xij∈E

wij(cid:0)f (vi) − f (vj)(cid:1)2

subject to f (vi) = yi for all i ∈ O,

(1)

where the penalization is enforced by the quadratic form given by the graph Laplacian [ZGL03].
This method is closely tied to heat diﬀusion on the graph and has a probabilistic interpreta-
tion in terms of a random walk on the graph. Unfortunately, solutions of this objective are
badly behaved in the sense that they tend to be constant everywhere except for the points
{vi}i∈O associated with observations [NSZ09]. The solution must then have sharp variations
near those points in order to respect the measurement constraints.
Given this undesirable property, several alternative methods have been proposed in recent
work (e.g., [AL11, BZ13, ZB11, KRSS15]). One such formulation is based on interpolating the
observed points exactly while penalizing the maximal gradient value on neighboring vertices:

min

f

max
ij∈E

wij |f (vi) − f (vj)|

subject to f (vi) = yi for all i ∈ O.

(2)

Any solution to this variational problem is known as an inf-minimizer.
[KRSS15] recently
proposed a fast algorithm for solving the optimization prolblem (2). In fact, their algorithm
ﬁnds a speciﬁc solution that not only minimizes the maximum gradient value, but also the
second largest one among all minimizers of the former and so on—that is to say, they ﬁnd
a solution such that the gradient vector (wij |f (vi) − f (vj)|)(i,j)∈E is minimal in the lexico-
graphic ordering, which they refer to as the lex-minimizer. They observed empirically that
when |V | = N grows to inﬁnity while both the degree of the graph and number of observations
are held ﬁxed, the lex-minimizer is a better behaved solution than its 2-Laplacian counterpart.
More precisely, the observed advantage is twofold: (a) the solution is a better interpolation of
nPn
the observed values; and (b) the average ℓ1-error 1
i=1 |f (vi)− f∗(vi)| for the lex-minimizer
remains stable, while it quickly diverges with N when f is the 2-Laplacian minimizer. Their
experiments together with the known limitations of the 2-Laplacian regularization method
point to the possible superiority of ℓ∞-based formulation (2) over the ℓ2-based (1) in a semi-
supervised setting. However, we currently lack a theoretical understanding of this assertion.
Accordingly, we aim to ﬁll this gap in the theoretical understanding of Laplacian-based reg-
ularization by studying both formulations in the asymptotic limit as the graph size goes to
inﬁnity.

2

We conduct our investigation in the context of a more general objective that encompasses
the approaches (1) and (2) as special cases. In particular, for a positive integer p ≥ 2, we
consider the variational problem

Jp(f ) = Xij∈E

wp
ij |f (vi) − f (vj)|p .

(3)

The objective Jp is referred to as the (discrete) p-Laplacian of the graph G in the literature;
for instance, see the papers by [ZS05] and [BH09], as well as references therein. It oﬀers a
way to interpolate between the 2-Laplacian regularization method and the inf-minimization
approach. It is then natural to consider the general family of interpolation problems based
on p-Laplacian regularization—namely

min

f

Jp(f )

subject to f (vi) = yi for i ∈ O.

(4)

Formulations (1) and (2) are recovered respectively with p = 2 and p → ∞. Indeed, in the
wij |f (vi) − f (vj)|. Moreover, under certain
latter case, observe that lim
p→∞
regularity assumptions ([EH90, KRSS15]), it follows that the lex-minimizer is the limit of the
(unique) minimizers of Jp as p grows to inﬁnity1—that is, we have the equivalence

Jp(f )1/p = max
ij∈E

flex = lim
p→∞

arg min

u

Jp(u)

subject to u(vi) = yi for all i ∈ O.

(5)

Our contributions: We analyze the behavior of p-Laplacian interpolation when the un-
derlying graph G is drawn from a geometric random model. Our ﬁrst main result is to derive
a variational problem that is the almost-sure limit of the formulation (4) in the asymptotic
regime when the size of the graph grows to inﬁnity. For a twice diﬀerentiable function f ,
we use ∇f and ∇2f to denote its gradient and Hessian, respectively. Letting µ denote the
density of the vertices in a latent space, we show that for any even integer p ≥ 2, solutions of
this variational problem must satisfy the partial diﬀerential equation

∆2f (x) + 2h∇ log µ(x),∇f (x)i + (p − 2)∆∞f (x) = 0,

where ∆2f : = Tr(∇2f ) is the usual 2-Laplacian operator , while ∆∞f : = h∇f, ∇2f ∇fi
∞-Laplacian operator, which is deﬁned to be zero when ∇f = 0.
This theory then yields several predictions on the behavior of these regularization methods
when the number of labeled examples is ﬁxed while the number of unlabeled examples becomes
inﬁnite: the method leads to degenerate solutions when p ≤ d; i.e., they are discontinuous, a
manifestation of the curse of dimensionality. On the other hand, the solution is continuous
when p ≥ d + 1. The solution is dependent on the underlying distribution of the data for
all ﬁnite values of p; however, when p = ∞, the solution is not dependent on the underlying
density µ. Consequently, as the graph size increases, the lex- and inf-minimizers end up
interpolating the observed values without exploiting the additional knowledge of the density
µ of the features that is provided by the abundance of unlabeled data.

h∇f,∇fi

is the

In order to illustrate the consequences of this last property, we study a simple one-
dimensional regression problem whose intrinsic diﬃculty is controlled by a parameter ǫ > 0.
We show that the 2-Laplacian method has an estimation rate independent of ǫ while the

1The construction of this sequence of minimizers is known as the P´olya algorithm, and the study of its rate

of convergence is a classical problem in approximation theory ([DLT83, ET87, LT89, EH90]).

3

inﬁnity-minimization approach has a rate that is increasing in 1/ǫ. As shown by our anal-
ysis, this important diﬀerence can be traced back to whether or not the method leverages
the knowledge of µ. We also provide an array of experiments that illustrate some pros and
cons of each method, and show that our theory predicts these behaviors accurately. Overall,
our theory lends support to using intermediate value of p that will lead to non-degenerate
solutions while remaining sensitive to the underlying data distribution.

2 Generative Model

use GN,h to denote the random graph generated in this way.

h

We follow the popular assumption in the semi-supervised learning literature that the graph
represents the metric properties of a cloud point in d-dimensional Euclidean space ([ZGL03,
BCH03, BN04, Hei06a, NSZ09, ZB11]). More precisely, suppose that we are given a prob-
ability distribution on the unit hypercube [0, 1]d having a smooth density µ with respect to
the Lebesgue measure, as well as a bounded decreasing function ϕ : R+ → R+ such that
limz→∞ ϕ(z) = 0. We then draw an i.i.d. sequence (xi)N
i=1 of samples from µ; these vectors
will be identiﬁed with the vertices of the graph G: xi ≡ vi. Finally, we associate to each pair

(cid:17), where h > 0 is a bandwidth parameter. We
of vertices the edge weight wij = ϕ(cid:16)kxi−xjk2
Degree asymptotics Given a sequence of graphs {GN,h}∞N =1 generated as above, we study
the behavior of the minimizers of Jp in the limit N → ∞. A ﬁrst step is to understand the
behavior of the graph itself, and in particular its degree distribution in this limit. In order
to gain intuition for this issue. consider the special case when ϕ(z) = 1{z ≤ 1}.
If the
bandwidth parameter h > 0 is held ﬁxed, then any sequence xi1,··· , xik of points that fall in
a ball of radius h will form a clique. Thus, the graph will contain roughly 1/hd cliques, each
with approximately N hd vertices. It is typically desired that the sequence of graphs be sparse
with an appropriate degree growth (e.g., constant or logarithmic growth) so that it converges
to the underlying manifold. In order to enforce this behavior, the bandwidth parameter h
should tend to zero as the sample size N increases.

Under this assumption, it can be shown that the scaled degree at any vertex x, given by

d(x) =

1

N hd

NXi=1

ϕ(cid:18)kxi − xk2

h

(cid:19) ,

concentrates around µ(x). A precise statement can be found in [Hei06a]; roughly speaking,
it follows from the fact that for any ﬁxed point x in [0, 1]d, as h goes to zero and under a
smoothness assumption on the density µ, the probability that a random vector xi ∼ µ falls

in the h-neighborhood of x scales as Pr(kxi − xk2 ≤ h) =Rkz−xk2<h µ(z)dz ∼ hdµ(x).

3 Variational problem and related PDE

In this section, our main goal is to study the behavior of the solution in the limit as the
sample size N → ∞ and the bandwidth h → 0. As discussed above, it is natural to consider
scalings under which h decreases in parallel with the increase in N . However, for simplicity, we
follow [NSZ09] and ﬁrst take the sample size N to inﬁnity with the bandwidth held constant,

4

1

0.5

0

-0.5

s
e
u
a
v

l

-1

-5

Mixture of Gaussians 1D
2-Laplacian
∞-Laplacian

0

vertices

5

10

Figure 1: A mixture of two 1-dimensional Gaussians N (0, 1) and N (4, 1) with equal weights.
500 points are drawn i.i.d. from each component. We added one point at 0 with label -1 and
one point at 4 with label +1. The similarity graph is constructed with an RBF kernel with
bandwidth .4.

and then let h go to zero.2 Our ﬁrst result characterizes the asymptotic behavior of the
objective Jp.

Theorem 3.1. Let f be continuously diﬀerentiable with a bounded derivative, and let µ be a
bounded density. Then for any even integer p ≥ 2, we have

Ip(f ) : = lim
h→0

lim
N→∞

1

N 2 hp+d Jp(f ) = CpZ k∇f (x)kp

2µ2(x)dx,

(6)

where Cp : = 1
We provide the proof in Appendix A; it involves applying the strong law for U -statistics, as
well as an auxiliary result on the isotropic nature of the integral of a rank-one tensor.

dp/2R kzkp

2ϕ(cid:0)kzk2(cid:1)pdz.

Based on Theorem 3.1, the asymptotic limit of the semi-supervised learning problem is
a supervised non-parametric estimation problem with a regularization term given by the
functional Ip—namely, the problem

inf

g Z k∇g(x)kp

2 µ2(x)dx subject to g(xi) = yi for all i ∈ O.

(7)

Our next main result characterizes the solutions of this optimization problem in terms of a
partial diﬀerential equation known as the (weighted) p-Laplacian equation. Here the word
“weighted” refers to the term µ2 in the functional (7) ([HKM12, Obe13]).

Let us introduce various pieces of notation that are useful in the sequel. Given a vector
i=1 ∂xiFi to denote denote its divergence. For a

ﬁeld F : Rd → Rd, we use div(F ) : = Pd
scalar-valued function f : Rd → R, we let
∆2f = div(cid:0)∇f(cid:1) =

dXi=1

∂2
xif,

and ∆∞f = h∇f, ∇2f ∇fi

h∇f,∇fi

=

1

k∇fk2

2

dXi,j=1

∂xif · ∂xi,xj f · ∂xj f

denote the (standard) 2-Laplacian operator and the ∞-Laplacian operator, respectively.

2In the case p = 2, it is known that the same limiting objects are recovered when a joint limit in N and
h is taken with h → 0 but N hd/ log N → ∞ ([Hei06a]). Based on the discussion above, this scaling implies a
super-logarithmic degree sequence. It is still to be veriﬁed if the same result holds for all p.

5

Theorem 3.2. Suppose that the density µ is bounded and continuously diﬀerentiable. Then
any twice-diﬀerentiable minimizer f of the functional (7) must satisfy the Euler-Lagrange
equation

If moreover the distribution µ has full support, then equation (8a) is equivalent to

div(cid:0)µ2(x)k∇f (x)kp−2

2 ∇f (x)(cid:1) = 0.

∆2f (x) + 2h∇ log µ(x),∇f (x)i + (p − 2)∆∞f (x) = 0.

(8a)

(8b)

The proof employs standard tools from calculus of variations [GF63]. We note here that
f does not need to be twice diﬀerentiable for the above result to hold ([HKM12]) in which
case equations (8a) and (8b) have to be understood in the viscosity sense ([CEG01, AS10]).
Twice diﬀerentiability is assumed so only for ease of the proof (see Appendix B).

When µ is the uniform distribution, equation (8b) reduces to the partial diﬀerential equa-

tion (PDE)

∆2f (x) + (p − 2)∆∞f (x) = 0,

which is known as the p-Laplacian equation and often studied in the PDE literature ([HKM12,
Obe13]). If one divides by p and lets p → ∞, one obtains the inﬁnity-Laplacian equation

∆∞f (x) = 0,

(9)

subject to measurement constraints f (xi) = yi for i ∈ O. This problem has been studied by
various authors (e.g., [CEG01, ACJ04, PSSW09, AS10]). Note that in dimension d = 1, we
have ∆∞ = ∆2 = d2

dx2 , and equation (8b) reduces to

(p − 1)µ(x)f ′′(x) + 2µ′(x)f′(x) = 0.

Therefore, if we specialize to the case p = 2, the 2-Laplacian regularization method solves the
diﬀerential equation µ(x)f ′′(x) + 2µ′(x)f′(x) = 0, whereas if we specialize to p = ∞, then the
inf-minimization method solves the diﬀerential equation f ′′ = 0. Note that the two equations
coincide only when µ is the uniform distribution, in which case µ′ is uniformly zero.

4

Insights and predictions

Our theory from the previous section allows us to make a number of predictions about the
behavior of diﬀerent regularization methods, which we explore in this section.

4.1

Inf-minimization is insensitive to µ

Observe that the eﬀect of the data-generating distribution µ has disappeared in equation (9).
One could also see this by taking the limit p → ∞ in the objective to be minimized in Theorem
3.1—in particular, assuming that µ has full support, we have Ip(f )1/p → sup[0,1]d k∇f (x)k2.
From the observations above, one can see that in the limit of inﬁnite unlabeled data—i.e.,
once the distribution µ is available—the 2-Laplacian regularization method, as well as any
p-Laplacian method based on ﬁnite (even) p, incorporates knowledge of µ in computing the
solution; in contrast, for p = ∞, the inf-minimization method does not (see Figure 2). On
the other hand, it has been shown that the 2-Laplacian method is badly behaved for d ≥ 2

6

1

s
e
u
a
v

l

0.5

0

-0.5

-1

-5

1

s
e
u
a
v

l

0.5

0

-0.5

1D uniform vs Gaussian: 2-Laplacian

1D uniform vs Gaussian: inf-Laplacian

uniform
Gaussian

uniform
Gaussian

5

10

-1

-5

0

vertices
(a)

5

10

0

vertices
(b)

Figure 2: Behavior of the 2- and inﬁnity- Laplacian solutions under a change of input density.
The latter is either a mixture of two 1-dimensional Gaussians N (0, 1) and N (4, 1) or a mixture
of two uniform distributions U ([−3, 3]) and U ([1, 7]) with equal weights. In each case, 500
points are drawn i.i.d. from each component. The methods are given two observations (0,−1)
and (4, 1). (a) ℓ2-based solution. (b) ℓ∞-based solution.

in the sense that the solution tends to be uninformative (constant) everywhere except on the
points of observation. The solution must then have sharp variations on those points in order
to respect the measurement constraints (see Figure 1 for an illustration of this phenomenon).
We show in the next section that this problem plagues the p-Laplacian minimization approach
as well whenever p ≤ d + 1.

4.2

p-Laplacian regularization is degenerate for p ≤ d

In this section, we show that p-Laplacian regularization is degenerate for all p ≤ d. This
issue was originally addressed by [NSZ09], who provided an example that demonstrates the
degeneracy for p = 2 for d ≥ 2. Here we show that the underlying idea generalizes to all pairs
(p, d) with p ≤ d. Recall that Theorem 3.1 guarantees that

Ip(f ) : = lim
h→0

lim
N→∞

1

N 2 hp+d Jp(f ) = CpZ k∇f (x)kp

2µ2(x)dx.

In the remainder of our analysis, we treat the cases p ≤ d − 1 and p = d separately.

Case p ≤ d − 1: Beginning with the case p ≤ d − 1, we ﬁrst set x0 = 0 and then let x1 be
any point on the unit sphere (i.e., kx1k2 = 1). Deﬁne the function fǫ(x) = min{kxk2/ǫ, 1} for
some ǫ ∈ (0, 1), and let the observed values be yj = fǫ(xj) for j ∈ {0, 1}. Using the fact that
∇kxk2 = x
and assuming that µ is uniformly upper bounded by µmax on [0, 1]d, we have
kxk2

Ip(fǫ) =ZB(0,ǫ)

µ2(x)
ǫp dx ≤

max

µ2
ǫp vol(B(0, ǫ)) = µ2

max vol(B(0, 1))ǫd−p,

where B(0, ǫ) denotes the Euclidean ball of radius ǫ centered at the origin, and vol denotes
the Lebesgue volume in Rd. Consequently, we have limǫ→0 Ip(fǫ) = 0, so the inﬁmum of Ip is
achieved for the trivial function that is 1 everywhere except at the origin, where it takes the

7

value 0. The key issue here is that k∇f (x)kp
“spike” in the gradient shrinks at a rate of ǫd.
Case p = d: On the other hand, when p = d, then we take fǫ(x) = log(cid:0)kxk2

for which we also have y0 = fǫ(x0) = 0 and y1 = fǫ(x1) = 1. With this choice, we have

2 grows at a rate of 1

2+ǫ
ǫ

(cid:1)/ log(cid:0) 1+ǫ
ǫ (cid:1),

ǫp while the measure of the

1

Ip(fǫ) =

≤

(i)
=

(ii)
=

(iii)

≤

max

2

2

µ2

max

µ2

ǫ (cid:1)dZB(0,1)
kxkd
2 + ǫ(cid:1)d µ2(x)dx
log(cid:0) 1+ǫ
(cid:0)kxk2
ǫ (cid:1)dZB(0,1)
kxkd
2 + ǫ(cid:1)d dx
(cid:0)kxk2
log(cid:0) 1+ǫ
ǫ (cid:1)d · vol(B(0, 1))Z 1
log(cid:0) 1+ǫ
Z 1
ǫ (cid:1)d
log(cid:0) 1+ǫ
ǫ (cid:1)d−1
2 log(cid:0) 1+ǫ

max vol(B(0, 1))

max vol(B(0, 1))

0
ud−1

(cid:0)r2 + ǫ(cid:1)d drd−1dr
(cid:0)u + ǫ(cid:1)d du

d µ2

d µ2

0

,

rd

where step (i) follows from a change of variables from x to the radial coordinate r; step (ii)
follows by the variable change u = r2; and step (iii) follows by upper-bounding u by u + ǫ in
the numerator inside the integral. Again, we ﬁnd that limǫ→0 Ip(fǫ) = 0. Thus, in order to
avoid degeneracies, it is necessary that p ≥ d + 1.
It is worth noting that [AL11] studied the problem of computing the so-called q-resistances
of a graph, which are a family of distances on the graph having a formulation similar —in
fact, dual— to the p-Laplacian regularization method considered in the present paper, and
where 1/p + 1/q = 1. They established a phase transition in the value of q for the geometric
random graph model, where above the threshold q∗∗ = 1 + 1/(d − 2), the q-resistances “[...]
depend on trivial local quantities and do not convey any useful information [...]” about the
graph, while below the threshold q∗ = 1 + 1/(d − 1), these resistances encode interesting
properties of the graph. They conclude by suggesting the use of p-Laplacian regularization
with 1/p + 1/q∗ = 1. The latter condition can be read p = d. However, as shown by the
examples above, this choice is still problematic, and in fact, the choice d + 1 is the smallest
admissible value for p.

We also note that the example for d ≥ p + 1 extends to an arbitrary number of labeled
points: one simply has a spike for each point. Undesirable behavior arises as long as the set
{xi | i ∈ O} of observed points is of measure zero. Finally, we note that both the above
example can be adapted to the case where the squared loss (f (xi) − yi)2 is optimized along
with the regularizer instead of imposing the hard constraint f (xi) = yi (see Appendix D).
The issue is that the regularizer is too weak and allows to choose the solution from a very
large class of functions.

4.3

p-Laplacian solution is smooth for p ≥ d + 1

At this point, a natural question is whether the condition p ≥ d + 1 is also suﬃcient to ensure
that the solution is well-behaved. In a speciﬁc setting to be described here, the answer is yes 3.

3Interestingly enough, the p-Laplacian equation has been extensively studied in non-linear potential theory.
It is in fact the prototypical example of a non-linear degenerate elliptic equation. The regularity of the solutions

8

The underlying reason is the Sobolev embedding theorem. More precisely, let W 1,p([0, 1]d)
denote the weighted Sobolev space of all (weakly) diﬀerentiable functions f on [0, 1]d such
that the semi-norm

kfk1,p : =(cid:18)Z k∇f (x)kp

2µ2(x)dx(cid:19)1/p

< ∞.

If, moreover, we assume µ is strictly positive almost everywhere and restrict the above class
to functions vanishing on the boundary, then k · k1,p actually deﬁnes a norm. When p > d,
and under additional regularity conditions on µ (e.g. upper- and lower-bounded by constants
a.e.), the space W 1,p can be embedded continuously into the space of H¨older functions of
exponent 1 − d
for all x, y ∈ [0, 1]d for
some dimension-dependent constant c. For details, see Theorem 11.34 of [Leo09] or Lemma
[BO92] provide some relaxed conditions on µ. Since the minimizer f of Ip
5.17 of [AF03].
is such that Ip(f ) = R k∇f (x)kp
2µ2(x)dx < ∞, the function f is in the Sobolev class W 1,p,
and therefore it automatically inherits the H¨older smoothness property, i.e. the p-Laplacian
solution is smooth for p ≥ d + 1, asymptotically as N → ∞, h → 04. Incidentally, via the
examples in the previous section, it is clear that no such embedding exists if p ≤ d.

p , i.e. functions u such that |u(x) − u(y)| ≤ ckx − yk

1− d
2

p

4.4 An example where inf-minimization interpolates well

By extension to the case p → ∞, the inﬁnity-Laplacian solutions also enjoy continuity (this
solution is actually Lipschitz based on its interpretation as the absolutely minimal Lipschitz
extension of the observations ([ACJ04]). It was also argued [KRSS15], based on experimental
results, that the inf-minimization method has a better behavior in higher dimensions in terms
of faithfulness to the observations. We illustrate this point by considering a simple example,
similar to the one above, for which the ∞-Laplacian equation (9) produces a sensible solution.
With x0 = 0 and S : = {x ∈ Rd | kxk2 = 1} denoting the Euclidean unit sphere, suppose
that we limit ourselves to functions satisfying the observation constraints y0 = f (0) = 0 and
y(x) = 1 for all x ∈ S.
Without any further information on the data-generating process, a reasonable ﬁt is the
function ¯f (x) = kxk2. We claim that it is the only radially symmetric solution to the diﬀer-
ential equation ∆∞f = 0 with the boundary constraints f (x0) = y0 and f (x) = y(x) for all
x ∈ S. In order to verify this claim, let f (x) = g(kxk2) for a function g : R+ → R. For any
non-zero x ∈ Rd, we have
∇f (x) = g′(kxk2)

and ∇2f (x) = g′′(kxk2)

+ g′(kxk2)

I

.

kxk2 − g′(kxk2)

xx⊺
kxk3

2

,

x
kxk2

xx⊺
kxk2

2

Then

∆∞f (x) =

1

k∇f (x)k2

2∇f (x)⊺∇2f (x)∇f (x) = g′′(kxk2).

Given the boundary conditions on f , the only solution to ∆∞f = 0, is given by g(r) = r,
meaning that f (x) = kxk2. On the other hand, the latter is not a solution to ∆2f = 0, unless
d = 1.

is well understood for any real number 1 < p < ∞ (see e.g. [HKM12]). For our purposes however, we do not
need the full power of this theory.

4If the graph is ﬁnite, then the solution might still contain small spikes as apparent in Figures 1 and 2.

9

In summary, this section reveals a trade-oﬀ between smoothness and sensitivity to the
data-generating density µ in p-Laplacian regularization: the solution is strongly sensitive to
µ but is non-smooth for small values of p, while it is smooth but weakly dependent on µ
for large and inﬁnite values of p. The transition from degeneracy to smoothness happens at
a sharp threshold p∗ = d + 1, while the dependence on µ weakens with larger and larger p
without a threshold.

While the property of smoothness is an obvious quality in an estimation setting, and may
lead to improved statistical rates if assumed ﬁrst hand —especially if the signal one wishes
to recover is itself smooth— it is less obvious how to quantify the advantages entailed by the
sensitivity to the underlying data-generating density; especially when the latter is available
and is to be incorporated in the design of an estimator. We provide in the next section a
simple, one-dimensional regression example where the regression function is tied to the density
µ via the cluster assumption, and where a diﬀerence in estimation rates between the ℓ2 and
ℓ∞ methods is exhibited. This diﬀerence is explicitly due to the fact that the ℓ2 method
leverages the knowledge of µ while ℓ∞ does not.

5 The price of “forgetting” µ

We consider in this section a simple estimation example in one dimension where the asymptotic
formulation of 2-Laplacian regularization method achieves a better rate of convergence than
that of the inf-minimization method. Such an advantage of the ℓ2 method over the ℓ∞ method
should be conceivable under the cluster assumption: the regularizer I2 =R f′2µ2 will encodes
information about the target function via µ while the regularizer I∞ = sup|f′| does not.
Let the target function f∗ and the data-generating density µ be supported on the interval
[−1, 1]. For some small ǫ > 0, we construct a density µ that takes a uniformly small value over
the interval [−ǫ, ǫ], and takes and a large value on the complementary set [−1,−ǫ)∪ (ǫ, 1]. We
also let f∗ have a high Lipschitz constant on the interval [−ǫ, ǫ] and be constant otherwise.
More precisely, the density µ and function f∗ are constructed as follows:

1
ǫ2 b2dx = 2b2/ǫ.

−ǫ

10

(10)

a x ∈ [−1,−ǫ) ∪ (ǫ, 1],

µ(x) =(b x ∈ [−ǫ, ǫ],

x ∈ [−1,−ǫ),

−1
x/ǫ x ∈ [−ǫ, ǫ],
1
x ∈ (ǫ, 1].

The constants a and b are related by the equation (1 − ǫ)a + ǫb = 1/2 so that the density
µ integrates to 1, and we think of b as being much smaller than a, i.e. b ≪ a. Consider the
following two classes of functions corresponding to the regularizer Ip for p = 2 and p = ∞
respectively:

f∗(x) =
H : =nf : [−1, 1] → R , f absolutely continuous, odd and Z 1
L : =nf : [−1, 1] → R , f absolutely continuous, odd and sup
We deﬁne the associated norms kfkH : =(cid:0)R 1
x∈[−1,1]|f′(x)|
on H and L respectively. Observe that kf∗kL = 1/ǫ while kf∗kH is upper bounded by a con-
stant:
[(f∗)′(x)]2µ2(x)dx =Z ǫ

−1 f′(x)2µ2(x)dx(cid:1)1/2 and kfkL : = sup

f′(x)2µ2(x)dx < ∞o,

−1

|x|≤1|f′(x)| < ∞o.

Z 1

−1

Taking b = √ǫ, the above integral is bounded above by 2.

We draw n points (xi)n

i=1 independently from µ and observe the responses yi = f∗(xi)+σ2ξi
where ξi ∼ N (0, 1) are i.i.d. standard normal random variables, σ > 0. We compare the
following two M-estimators:

f

1
n

nXi=1(cid:0)yi − f (xi)(cid:1)2

bfH = arg min
in terms of the rate of decay of the error(cid:13)(cid:13)bf − f∗(cid:13)(cid:13)2

s.t. f ∈ H , kfkH ≤ 2,

and

i=1 f 2(xi). Note that
this error can in principle tend to zero as n grows to inﬁnity since the target f∗ belongs to
the hypothesis class in both of the considered cases, i.e. there is no approximation error.

n, where kfk2

nXi=1(cid:0)yi − f (xi)(cid:1)2

s.t. f ∈ L , kfkL ≤ 1/ǫ,

1
n

f

bfL = arg min
nPn

n = 1

Theorem 5.1. There are universal constants (c0, c1, c2, c3) such that for any ǫ ∈ (0, 1/2), the
ℓ2 estimator satisﬁes the bound

2

(11)

the bound

n(cid:19)2/3

n ≤ c1 (cid:18) σ2

(cid:13)(cid:13)(cid:13) ˆfH − f∗(cid:13)(cid:13)(cid:13)
σ2(cid:1)1/3(cid:9). On the other hand, the ℓ∞ estimator satisﬁes
with probability at least 1 − exp(cid:8)−c0(cid:0) n
n ≤ c3(cid:16) σ2
(cid:13)(cid:13)bfL − f∗(cid:13)(cid:13)2
σ2(cid:1)1/3(cid:9).
with probability at least 1 − exp(cid:8)−c2(cid:0)ǫ2 n

We can thus compare upper bounds on the rate of estimation of the ℓ2 and ℓ∞ methods
respectively. The upper bound (12) shows a dependence in 1/ǫ2/3, while the bound (11) shows
no dependence on ǫ. One might ask if these bounds are tight. In particular, a question of
interest is whether the ℓ∞ estimator adapts to the target f∗ without the need to know the
density µ, in which case the corresponding estimator would achieve a better rate. While
we think our bounds could be sharpened, we strongly suspect that the ℓ∞ estimator cannot
achieve a rate independent of ǫ (in contrast to the ℓ2 estimator). We provide an array of
simulations showing that the rate of ℓ∞ deteriorates as ǫ gets small, where the rate of the ℓ2
method stays the same (see Figure 3).

ǫ n(cid:17)2/3

(12)

5.1 Main ideas of the proof

The ﬁrst non-asymptotic bound (12) in Theorem 5.1 follows in a straightforward way from
known results on the minimax rate of estimation on the class of Lipschitz functions. Indeed,

the rate of estimation on this class with Lipschitz constant L is(cid:0)Lσ2/n(cid:1)2/3, and in our case

L = 1/ǫ. On the other hand, the second result (11) follows by recognizing that the class H is
a weighted Sobolev space of order 1, which is a Reproducing Kernel Hilbert Space (RKHS).
The associated kernel, as identiﬁed by [NSZ09], is given by

K(x, y) =

1

4Z 1

−1

dt/µ2(t) −

1

2(cid:12)(cid:12)(cid:12)(cid:12)Z y

x

dt/µ2(t)(cid:12)(cid:12)(cid:12)(cid:12) ,

11

for all x, y ∈ [−1, 1].

(13)

E
S
M

0.014
0.012
0.01
0.008
0.006
0.004
0.002
0
0

0.02

0.015

E
S
M

0.01

0.005

0
0

2-Lap MSE

ǫ=0.01
ǫ=0.05
ǫ=0.1
ǫ=0.5

2000

4000

6000

number of labelled points

E
S
M

0.014
0.012
0.01
0.008
0.006
0.004
0.002
0
0

inf-Lap MSE

ǫ=0.01
ǫ=0.05
ǫ=0.1
ǫ=0.5

2000

4000

number of labelled points

6000

(a)

(b)

MSE for epsilon = 1/n

2-Laplacian
∞-Laplacian

MSE*n2/3 for epsilon = 1/n

2-Laplacian
∞-Laplacian

2

1.5

e
u
a
v

l

1

0.5

2000

4000

number of labelled points

6000

0
0

2000

4000

number of labelled points

6000

(c)

(d)

Figure 3: Plots of mean-squared-error (MSE) against number of labeled samples. The regu-
larization parameter is determined using cross-validation (thereby producing estimators of

superior performance than bfH and bfL). The samples xi are drawn according to µ and
yi = f∗(xi) + ξi, and ξi are i.i.d. N (0, 0.05). Panels (a) and (b): MSE of ℓ2 and ℓ∞ methods
respectively for various values of ǫ. As expected, the MSE of the ℓ2 method is independent
of ǫ while that of the ℓ∞ method is increasing in 1/ǫ. Panels (c) and (d): Plots of the MSE
and MSE × n2/3, respectively versus the sample sample size n for both methods. Both plots
correspond to sequences of problems with ǫ = 1/n; panel (d) shows that in this regime, the
rate of the ℓ2 method is roughly n−2/3, thereby providing evidence that the upper bound (11)
is tight, while the rate of the ℓ∞ method, call it r(ǫ, n), is such that r( 1

n , n) ≫ n−2/3.

It is known that the rate of estimation on a ball of radius R of an RKHS is tightly
related to the decay of the eigenvalues of the kernel. More precisely, the rate of estimation is
upper-bounded with high probability by the smallest solution δ > 0 to the inequality

 2

n

∞Xj=0

min(cid:8)γj, δ2(cid:9)

1/2

R
σ

≤

δ2,

(14)

where (γj)j≥0 is the sequence of eigenvalues of the kernel K ([Kol06, Men02, BBM02, vdG00]).

12

Our next result upper-bounds the rate of decay of these eigenvalues.

Lemma 5.1. For any ǫ ∈ (0, 1/2), the eigenvalues of the kernel K form a doubly indexed
sequence (γk,j) with 0 ≤ k ≤ 2k0 − 1, j ≥ 0, k0 = ⌊√2ǫ−3/4⌋. This sequence satisﬁes the upper
bound

γk,j ≤

1.26

2√2

(cid:0) k

1

+jǫ−3/4(cid:1)2

π2

if k = j = 0
otherwise.

Plugging these estimates in equation (14) leads to the rates we claim in Theorem 5.1. The

full details are given in Appendix C.

6 Related work

The discrete graph p-Laplacian was introduced by [ZS05] as a form of regularization gener-
alizing classical Laplacian regularization in semi-supervised learning [ZGL03]. We mention
however that the continuous p-Lapalcian has been extensively studied much earlier in PDE
theory [HKM12, ACJ04]. It was also used and analyzed for spectral clustering, where it pro-
vides a family of relaxations to the normalized cut problem [Amg03, BH09, LHDN10]. A dual
version of the problem (the q-resistances or q-volatges problem) was investigated and shown
to yield improved classiﬁcation performance in [BZ13]. [AL11] prove the existence of a phase
transition under the geometric graph model roughly similar to the one exhibited in this paper,
although the thresholds are slightly diﬀerent. The exact nature of the connection is still un-
clear however. On the other hand, a game-theoretic interpretation of the p-Laplacian solution
is studied in [PS08, PSSW09], and a similar transition at p = d+1 in the behavior of the game
is found. The assumption that the graph entails a geometric structure is popular in the anal-
ysis of semi-supervised learning algorithms [BN01, BCH03, BN04, Hei06a, Hei06b, NSZ09].
This line of work have mostly focused on the 2-Laplacian formulation and its convergence
properties to a diﬀerential operator on the limiting manifold.

Among other approaches that circumvent the degeneracy issue discussed in the paper,
we mention higher order regularization [ZB11], where instead of only penalizing the ﬁrst
derivative of the function, one can penalize up to l derivatives. This approach considers
solutions in a higher order Sobolev space W l,2 which, via the Sobolev embedding theorem, only
contains smooth functions if l > d/2 (see [AF03, Leo09]). This approach can be implemented
algorithmically using the discrete iterated Laplacian [ZB11, WSST15].

Results on statistical rates for semi-supervised learning problems are very sparse. The
ﬁrst results are covered in [CC96] in the context of mixture models, [Rig07] in the context
of classiﬁcation, and [LW07] for regression. A recent line of work considers the setting where
the graph is ﬁxed while the set of vertices where the labels are available is random [AZ07,
JZ07, JZ08, SB14, SCS+15]. The methods studied in this setting generalize the 2-Laplacian
method by penalizing by the quadratic form given by a general positive semideﬁnite kernel.
The derived rates depend on the structural properties of the graph and/or the kernel used.
It is shown in particular that using the normalized Laplacian instead of the regular one leads
to a better statistical bound, and on the other hand, one can obtain rates depending on the
Lov´asz Theta function of the graph by choosing the regularization kernel optimally.

13

7 Conclusion and open problems

In this paper, we used techniques and ideas from PDE theory to yield insight into the behavior
of various methods for semi-supervised learning on graphs. The d-dimensional geometric
random graph model analyzed in this paper is a common one in the literature, and the most
common Laplacian penalization technique is for p = 2, though the choice p = ∞ has also
attracted some recent attention. Our paper sheds light on both of these options, as well as
the full range of p in between. From our asymptotic analysis, we see that for a d-dimensional
problem, degenerate solutions occur whenever p ≤ d, whereas at the other extreme, the choice
p = ∞ leads to solutions that are totally insensitive to the input data distribution. Hence,
the choice p = d + 1 seems like a prudent one, trading oﬀ degeneracy of the solution with
sensitivity to the unlabeled data.

An important companion problem is the unconstrained version of the problem, in which we
penalize a (weighted) sum of two terms, the p-Laplacian term and a sum of squared losses on
the labeled data. One can see that under our asymptotics, we can make the same conclusions
about the unconstrained solution. Hence, the conclusions of this paper do not hinge on the
fact that we modeled the problem with equality constraints, and do apply more generally. For
completeness, we outline this argument in Appendix D.

There are perhaps two important assumptions which must be relaxed in future work.
The ﬁrst is the asymptotics we consider, in which the number of labeled points is ﬁxed as the
unlabeled points become inﬁnite. An interesting situation is when both labeled and unlabeled
points grow at a relative rate. We showed that in the ﬁrst situation, a certain class of methods,
namely all p-Laplacian methods with p ≤ d, behave poorly.

An interesting direction is to understand what set of methods are appropriate in diﬀerent
regimes of relative growth rate.
In particular, we suspect that most of our results should
continue to hold as long as the number of unlabeled points grow at a much faster rate than the
number of labeled points. The second assumption is about the geometric random graph model,
and how much our results are tied to this model. Finite sample rates and non-asymptotic
arguments are necessary to understand how soon we can expect to see these eﬀects on general
graphs in practice. Finally, the model selection problem of what p to use in practice is very
important, since we may not know the underlying dimensionality of the data from which our
graph was formed.

Acknowledgments: We thank Jason Lee, Kevin Jamieson, Anup Rao, Sushant Sachdeva
and Ryan Tibshirani for discussions in the early phases of this work. This work was partially
supported by Air Force Oﬃce of Scientiﬁc Research grant FA9550-14-1-0016 and Oﬃce of
Naval Research grant DOD-ONR-N00014 to MJW. This material is based upon work sup-
ported in part by the Oﬃce of Naval Research under grant number N00014-11-1-0688 and
by the Army Research Laboratory and the U. S. Army Research Oﬃce under grant number
W911NF-11-1-0391to MIJ.

References

[ACJ04] Gunnar Aronsson, Michael Crandall, and Petri Juutinen. A tour of the theory of
absolutely minimizing functions. Bulletin of the American mathematical society,
41(4):439–505, 2004.

14

[AF03] Robert A Adams and John JF Fournier. Sobolev spaces, volume 140. Academic

press, 2003.

[AL11] Morteza Alamgir and Ulrike V Luxburg. Phase transition in the family of p-
resistances. In Advances in Neural Information Processing Systems, pages 379–
387, 2011.

[Amg03] S Amghibech. Eigenvalues of the discrete p-Laplacian for graphs. Ars Combina-

toria, 67:283–302, 2003.

[AS10] Scott N Armstrong and Charles K Smart. An easy proof of Jensen’s theorem on
the uniqueness of inﬁnity harmonic functions. Calculus of Variations and Partial
Diﬀerential Equations, 37(3-4):381–384, 2010.

[AZ07] Rie Kubota Ando and Tong Zhang. Learning on graph with Laplacian regulariza-

tion. Advances in neural information processing systems, 19:25, 2007.

[BBM02] Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized Rademacher

complexities. In Computational Learning Theory, pages 44–58. Springer, 2002.

[BCH03] Olivier Bousquet, Olivier Chapelle, and Matthias Hein. Measure based regular-
ization. In Advances in Neural Information Processing Systems, pages 1221–1228,
2003.

[BH09] Thomas B¨uhler and Matthias Hein. Spectral clustering based on the graph p-
Laplacian. In Proceedings of the 26th Annual International Conference on Machine
Learning, pages 81–88. ACM, 2009.

[BN01] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques
for embedding and clustering. In Advances in Neural Information Processing Sys-
tems, volume 14, pages 585–591, 2001.

[BN04] Mikhail Belkin and Partha Niyogi. Semi-supervised learning on Riemannian man-

ifolds. Machine learning, 56(1-3):209–239, 2004.

[BO92] RC Brown and B Opic. Embeddings of weighted sobolev spaces into spaces of con-
tinuous functions. In Proceedings of the Royal Society of London A: Mathematical,
Physical and Engineering Sciences, volume 439, pages 279–296. The Royal Society,
1992.

[BZ13] Nick Bridle and Xiaojin Zhu.

p-voltages: Laplacian regularization for semi-
supervised learning on high-dimensional data. In Eleventh Workshop on Mining
and Learning with Graphs (MLG2013), 2013.

[CC96] Vittori Castelli and Thomas M Cover. The relative value of labeled and unla-
beled samples in pattern recognition with an unknown mixing parameter. IEEE
Transactions on Information Theory, 42(6):2102–2117, 1996.

[CEG01] Michael G Crandall, Lawrence C Evans, and Ronald F Gariepy. Optimal Lips-
chitz extensions and the inﬁnity Laplacian. Calculus of Variations and Partial
Diﬀerential Equations, 13(2):123–139, 2001.

15

[DLT83] RB Darst, DA Legg, and DW Townsend. The P´olya algorithm in L∞ approxima-

tion. Journal of Approximation Theory, 38(3):209–220, 1983.

[Dud99] R. M. Dudley. Uniform central limit theorems. Cambridge university press, 1999.

[EH90] Alan Egger and Robert Huotari. Rate of convergence of the discrete P´olya algo-

rithm. Journal of approximation theory, 60(1):24–30, 1990.

[ET87] AG Egger and GD Taylor. Dependence on p of the best Lp approximation operator.

Journal of approximation theory, 49(3):274–282, 1987.

[GF63] IM Gelfand and SV Fomin. Calculus of variations. revised english edition trans-

lated and edited by richard a. silverman, 1963.

[Hei06a] Matthias Hein. Geometrical aspects of statistical learning theory. PhD thesis, TU

Darmstadt, 2006.

[Hei06b] Matthias Hein. Uniform convergence of adaptive graph-based regularization. In

Learning Theory, pages 50–64. Springer, 2006.

[HKM12] Juha Heinonen, Tero Kilpel¨ainen, and Olli Martio. Nonlinear potential theory of

degenerate elliptic equations. Courier Corporation, 2012.

[JZ07] Rie Johnson and Tong Zhang. On the eﬀectiveness of Laplacian normalization
for graph semi-supervised learning. Journal of Machine Learning Research, 8(4),
2007.

[JZ08] Rie Johnson and Tong Zhang. Graph-based semi-supervised learning and spectral

kernel design. Information Theory, IEEE Transactions on, 54(1):275–288, 2008.

[Kol06] Vladimir Koltchinskii. Local Rademacher complexities and oracle inequalities in

risk minimization. The Annals of Statistics, 34(6):2593–2656, 2006.

[KRSS15] Rasmus Kyng, Anup Rao, Sushant Sachdeva, and Daniel A Spielman. Algorithms
for Lipschitz learning on graphs. Proceedings of The 28th Conference on Learning
Theory, pages 1190–1223, 2015.

[Leo09] Giovanni Leoni. A ﬁrst course in Sobolev spaces, volume 105. American Mathe-

matical Society Providence, RI, 2009.

[LHDN10] Dijun Luo, Heng Huang, Chris Ding, and Feiping Nie. On the eigenvectors of

p-Laplacian. Machine Learning, 81(1):37–51, 2010.

[LT89] David A Legg and Douglas W Townsend. The P´olya algorithm for convex ap-
proximation. Journal of mathematical analysis and applications, 141(2):431–441,
1989.

[LW07] John Laﬀerty and Larry Wasserman. Statistical analysis of semi-supervised regres-
sion. In Advances in Neural Information Processing Systems 20, pages 801–808.
Curran Associates, Inc., 2007.

[Men02] Shahar Mendelson. Geometric parameters of kernel machines. In Computational

Learning Theory, pages 29–43. Springer, 2002.

16

[NSZ09] Boaz Nadler, Nathan Srebro, and Xueyuan Zhou. Semi-supervised learning with
the graph laplacian: The limit of inﬁnite unlabelled data. In Neural Information
Processing Systems, pages 1330–1338, 2009.

[Obe13] Adam M Oberman. Finite diﬀerence methods for the inﬁnity Laplace and p-
Laplace equations. Journal of Computational and Applied Mathematics, 254:65–
80, 2013.

[PS08] Yuval Peres and Scott Sheﬃeld. Tug-of-war with noise: A game-theoretic view of

the p-laplacian. Duke Mathematical Journal, 145(1):91–120, 2008.

[PSSW09] Yuval Peres, Oded Schramm, Scott Sheﬃeld, and David Wilson. Tug-of-war and
the inﬁnity Laplacian. Journal of the American Mathematical Society, 22(1):167–
210, 2009.

[Rig07] Philippe Rigollet. Generalization error bounds in semi-supervised classiﬁcation
under the cluster assumption. Journal of Machine Learning Research, 8:2183–
2206, 2007.

[SB14] Rakesh Shivanna and Chiranjib Bhattacharyya. Learning on graphs using or-
thonormal representation is statistically consistent. In Advances in Neural Infor-
mation Processing Systems, pages 3635–3643, 2014.

[SCS+15] Rakesh Shivanna, Bibaswan K Chatterjee, Raman Sankaran, Chiranjib Bhat-
tacharyya, and Francis Bach. Spectral norm regularization of orthonormal repre-
sentations for graph transduction. In Advances in Neural Information Processing
Systems, pages 2206–2214, 2015.

[Ser09] Robert J Serﬂing. Approximation theorems of mathematical statistics, volume 162.

John Wiley & Sons, 2009.

[vdG00] Sara van de Geer. Empirical processes in M-estimation. Cambridge University

Press Cambridge, 2000.

[WSST15] Yu-Xiang Wang, James Sharpnack, Alex Smola, and Ryan J Tibshirani. Trend
ﬁltering on graphs. In Proceedings of the Eighteenth International Conference on
Artiﬁcial Intelligence and Statistics,, page 1042–1050, 2015.

[ZB11] Xueyuan Zhou and Mikhail Belkin. Semi-supervised learning by higher order
regularization. In International Conference on Artiﬁcial Intelligence and Statistics,
pages 892–900, 2011.

[ZGL03] Xiaojin Zhu, Zoubin Ghahramani, and John Laﬀerty. Semi-supervised learning
using Gaussian ﬁelds and harmonic functions. In Proceedings of The 31st Inter-
national Conference on Machine Learning, volume 3, pages 912–919, 2003.

[ZS05] Dengyong Zhou and Bernhard Sch¨olkopf. Regularization on discrete spaces. In

Pattern Recognition, pages 361–368. Springer, 2005.

17

A Proof of Theorem 3.1

almost surely.

Let x and x′ be i.i.d. draws from µ. Since f is a bounded function, the ﬁrst moment E(cid:2)|f (x)−
f (x′)|q(cid:3) is ﬁnite. Therefore, by the strong law of large numbers for U-statistics ([Ser09]), we

have

1

N 2 Jp(f ) =Z Z ϕ(cid:18)kx − x′k2

h

lim
N→∞

(cid:19)p(cid:12)(cid:12)f (x) − f (x′)(cid:12)(cid:12)p µ(x)µ(x′)dxdx′,

Writing x′ = x + hz for some scalar h > 0 and vector z, the second integral simpliﬁes to

Z ϕ(cid:18)kx − x′k

h

(cid:19)p(cid:12)(cid:12)f (x) − f (x′)(cid:12)(cid:12)p µ(x′)dx′ = hdZ ϕ(cid:0)kzk2(cid:1)p |f (x) − f (x + hz)|p µ(x + hz)dz.

We now divide by hd+p and consider the behavior as the bandwidth parameter h tends to
zero. Since the functions f , f′ and µ are all bounded on a compact domain, the dominated
convergence theorem implies that

µ(x)Z ϕ(cid:0)kzk2(cid:1)p |h∇f (x), zi|p dz = µ(x)(cid:10)∇f (x)⊗p,Z ϕ(cid:0)kzk2(cid:1)pz⊗pdz(cid:11).

Note the above inner product involves tensors of order p, and recall that we have assumed
that p ≥ 2 is even. Since the function ϕ depends only on the norm of z in the above integral,
the latter should also be isotropic. The precise statement is as follows:

Lemma A.1. For any function w : R+ → R+ and vector u ∈ Rd, we have

(cid:10)u⊗p , Z w(cid:0)kzk2(cid:1)z⊗pdz(cid:11) =( 1

0

dp/2(cid:0)R w(cid:0)kzk2(cid:1)kzkp

2dz(cid:1) · kukp

2

if p ≥ 2 is even
if p is odd.

(15)

Applying Lemma A.1 with the function w(kzk2) : = ϕ(kzk2)p and then simplifying yields

1

N 2hp+d Jp(f ) =

lim
N→∞

1

dp/2Z kzkp

2ϕ(cid:0)kzk2(cid:1)pdz ·Z k∇f (x)kp

2µ2(x)dx,

which concludes the proof of the theorem.

The only remaining detail is to prove Lemma A.1.

Proof of Lemma A.1 We proceed by induction on the integer p. In the base case (p = 2),
we have

(cid:10)uu⊺ , Z w(cid:0)kzk2(cid:1)zz⊺dz(cid:11) = u⊺(cid:0)Z w(cid:0)kzk2(cid:1)zz⊺dz(cid:1)u.

Since the function w depends only on kzk2, the matrix between the parentheses above is
proportional to the identity, the proportionality constant can be determined by taking a

trace. We end up with R w(cid:0)kzk2(cid:1)zz⊺dz = 1
2dz(cid:1)I. This establishes the base
case.5 Now assume that for a given even p ≥ 2, and for all function non-negative maps w,

d(cid:0)R w(cid:0)kzk2(cid:1)kzk2

5The case p = 2 is also proven in Proposition 4.1 of the paper [BCH03]

18

one has (15). We prove that the same is true for p + 2. Deﬁne Tp : =R w(cid:0)kzk2(cid:1)z⊗pdz, and
for any vector u ∈ Rd, let ¯Tp be the partial contraction of Tp+2 by u ⊗ u, namely

The tensor ¯T is of order p, and the map z → w(kzk2)hu, zi2 is non-negative, so by the
induction hypothesis, for every v ∈ Rd, we have

¯Tp : =Tp+2(u ⊗ u) =Z w(cid:0)kzk2(cid:1)z⊗phu, zi2dz.
2hu, zi2dz(cid:19) kvkp

dp/2(cid:18)Z w (kzk2)kzkp

1

2.

By recourse to the base case, the quadratic form between the parentheses is equal to
1

2. Taking u = v completes the proof of the lemma.

hv⊗p , ¯Tpi =
dz(cid:17) kuk2

2

B Proof of Theorem 3.2

d(cid:16)R w(cid:0)kzk2(cid:1)kzkp+2
Recall the shorthand notation Ip(f ) : =R k∇f (x)kp

2µ2(x)dx. By convexity, the function f is a
minimizer of the functional Ip if for all test functions h and all suﬃciently small real numbers
ǫ > 0, we have Ip(f + ǫh) ≥ Ip(f ). Moreover, by a Taylor series expansion, we have
2 µ2(x)dx + O(ǫ2),

Ip(f + ǫh) = Ip(f ) + pǫZ h∇f (x),∇h(x)i · k∇f (x)kp−2

where the O(ǫ2) term is non-negative by convexity of Ip. Hence, the function f is a minimizer
if and only if

Z h∇f (x),∇h(x)i · k∇f (x)kp−2

2 µ2(x)dx = 0

= µ2(x)k∇f (x)kp−2

2

19

for all testing functions h. By integrating by parts and choosing h to vanish on the boundary
of the set [0, 1]d, we ﬁnd that the above quantity is equal to

2 ∇f (x)(cid:1)h(x)dx.

We now further manipulate this equation so as to obtain the p-Laplacian equation. In

This expression has to vanish for all test functions h (that vanish on the boundary, which
implies the Euler-Lagrange equation

particular, some straightforward computations yield

Z h∇f (x),∇h(x)i · k∇f (x)kp−2

2 µ2(x)dx = −Z div(cid:0)µ2(x)k∇f (x)kp−2
div(cid:0)µ2(x)k∇f (x)kp−2
2 ∂xif(cid:1)(x) = ∂xi(cid:0)µ2(x)k∇f (x)kp−2

2 ∇f (x)(cid:1) = 0.
(cid:1)∂xif (x) + µ2(x)k∇f (x)kp−2
∂xi(cid:0)µ2k∇fkp−2
∂xi(cid:0)µ2k∇fkp−2(cid:1)(x) = 2∂xiµ(x) · µ(x)k∇f (x)kp−2 + µ2(x)(p − 2)(cid:0) dXj=1
div(cid:0)µ2(x)k∇f (x)kp−2

2 ∇f (x)(cid:1) = 2µ(x)k∇f (x)kp−2

Now summing these terms yield

2

2

+ (p − 2)µ2(x)k∇f (x)kp−4

2

2 ∂2

xif (x),

and

∂xi,xj f ∂xj f(cid:1) · k∇f (x)kp−4.

2 ∆2f (x)

h∇µ(x),∇f (x)i + µ2(x)k∇f (x)kp−2
∂xif · ∂xi,xj f · ∂xj f(cid:1)(x)
µ(x)h∇µ(x),∇f (x)i + (p − 2)∆∞f (x)(cid:1).

(cid:0) dXi,j=1
·(cid:0)∆2f (x) +

2

From the derivation above, the Euler-Lagrange equation (8a) is equivalent to

∆2f (x) + 2h∇ log µ(x),∇f (x)i + (p − 2)∆∞f (x) = 0,

as claimed.

C Proof of Theorem 5.1

Bounding the error of M -estimators is a classical problem in statistics and learning theory.
Optimal rates typically follow by deriving uniform convergence bounds over a small ball
localized around the true regression function f∗; for instance, see the book [vdG00] as well
as the papers [Kol06, BBM02]). Uniform convergence is established by upper-bounding the
Rademacher or Gaussian complexity of this small ball via generic covering number arguments,
or by leveraging the special structure of the ball. The ﬁrst approach will be used to analyze
In this
latter case, the analysis is based on the study of the spectrum of a certain integral operator
associated to the kernel k (13) that generates the space H.

the rate of the estimator bfL, and the second approach to analyze the rate of bfH.
C.1 Proof of the error bound (12) on bfL

For a given metric space (F, ρ), we let N (t,F, ρ) be the covering number of a metric space F
in the metric ρ at resolution t. Now consider the shifted function class

L∗ : = {f − f∗ | f ∈ L , kfkL ≤ 1/ǫ}
under the metric kfkL = supx |f′(x)|. By known results on metric entropy [Dud99], we have
ǫt(cid:1), using the fact that any function in L∗ must be 2/ǫ-Lipschitz.
log(N (t;L∗;k · kL)) = O(cid:0) 1
nPn
Now let kfk2
i=1 f 2(xi) be the squared empirical L2-norm, and consider the ball
Bn(δ,L∗) : = {f ∈ L∗ | kfkn < δ}.

n = 1

Since the sup norm is stronger than this empirical norm, we have the sequence of inequalities

log(N (t; Bn(δ;L∗);k · kn)) ≤ log(N (t;L∗;k · kn)) ≤ log(N (t;L∗;k · kL)) = O(cid:18) 1
ǫt(cid:19) .

Next consider the δ-localized Gaussian complexity of a function class F, given by

(16)

(17)

G(δ;F) = Ew sup

g∈F
kgkn≤δ

1
n

nXi=1

wig(xi) ,

where {wi}N
radius δn as the smallest δ that satisﬁes the master inequality

i=1 is an i.i.d. sequence of standard normal random variables. Deﬁne the critical

G(δ;L∗) ≤

δ2
2σ

.

(18)

With this set-up, it is known [vdG00] that the M -estimator bfL satisﬁes a bound of the form

(19)

P(cid:2)kbfL − f∗k2

n ≤ c1δ2

n(cid:3) ≥ 1 − e−c2

nδ2
n
2σ2 ,

20

where c1 and c2 are universal positive constants. By Dudley’s entropy integral, the critical
radius δn is upper bounded by any δ which satisﬁes

1

√nZ δ

δ2/2plog(N (t; Bn(δ;L∗);k · kn))dt ≤

δ2
σ

.

A little calculation shows that it suﬃces to choose δn such that

δ2

n ≤(cid:18) σ2

ǫn(cid:19)2/3

.

Note that this is in fact a global upper bound on δn, since the ﬁrst inequality of (16) holds
for any setting of the design points {xi}n

i=1.

C.2 Proof of the error bound (11) on bfH

As our starting point, we use the master inequality (18) with the shifted function class L∗
H =R f′2µ2 and R = 2 in
replaced by H∗ : = {f − f∗ | f ∈ H , kfkH ≤ R}. Recall that kfk2
our case. Using known bounds [Men02] on localized Gaussian complexity of H∗ in terms of
the eigenvalues of the kernel K, the master inequality takes the simpler form

 2

n

∞Xk,j=0

1/2

min(cid:8)γk,j, δ2(cid:9)
R2n(cid:19)1/3
δn = c1(cid:18) σ2

R
σ

≤

δ2.

(20)

(21)

We claim that this master inequality is satisﬁed by

where c1 is an absolute constant, independent of n, ǫ. Given this choice, the overall claim
follows by applying the non-asymptotic error bound (19).

It remains to show that the choice (21) is valid, and we do so by using the bounds on the

eigenvalues from Lemma 5.1. For δ ∈ (0,√2/π), let k∗, j∗ be the largest integers such that
γk,j ≥ δ2, or equivalently such that

k/(2√2) + jǫ−3/4 ≤ 1/(πδ).
δπ ⌋ and k∗ = ⌊2√2ǫ−3/4(cid:0) ǫ3/4

δπ ⌋(cid:1)⌋. We note that for
One can verify that j∗ = ⌊ ǫ3/4
δ < √2/π < 1, j∗ and k∗ cannot simultaneously be zero. Using these cut-oﬀ points, the
number of eigenvalues (γk,j) that are larger than δ2 is at most 2k0j∗ + k∗ + 1. Moreover, we
have

δπ − ⌊ ǫ3/4

∞Xk,j=0

min(cid:8)γk,j, δ2(cid:9) ≤ (2k0j∗ + k∗ + 1)δ2 +
δπ . Moreover, we have k∗ ≤ 2√2

Next, we control each term in the above expression. For the ﬁrst term, note that k0 ≤
√2ǫ−3/4 and j∗ ≤ ǫ3/4
πδ . Therefore, the ﬁrst term is bounded as
2k0j∗ + k∗ + 1 ≤ 4√2/(δπ) + 1.

2k0−1Xk=k∗+1

γk,j∗ + Xj≥j∗+1

2k0−1Xk=0

γk,j.

(22)

21

As for the second term,

1

1

1

(i)

=

≤

2√2

2√2

2√2

γk,j∗ ≤

2k0−1Xk=k∗+1

2k0−1Xk=k∗+1
(cid:0) k∗+1
(cid:0) k∗+1
(cid:0) k∗+1
(cid:0) k∗+1
(cid:0) k∗+1

(cid:0) k
+ j∗ǫ−3/4(cid:1)2π2
+ j∗ǫ−3/4(cid:1)2π2
+ j∗ǫ−3/4(cid:1)2π2
+ j∗ǫ−3/4(cid:1)2π2
+ j∗ǫ−3/4(cid:1)π2
the discrete sum by an integral: Pk0

4√2

2√2

2√2

2√2

≤

≤

(ii)

=

1

1

.

1

+

2√2

+ j∗ǫ−3/4(cid:1)2π2
2k0−1Xk=k∗+2
(cid:0) k
+Z 2k0−1
(cid:0) t
+ j∗ǫ−3/4(cid:1)π2 −
(cid:0) k∗+1
(cid:0) k∗+1
+ j∗ǫ−3/4(cid:1)π2

+ j∗ǫ−3/4(cid:1)2π2
+ j∗ǫ−3/4(cid:1)2π2
(cid:0) 2k0−1

2√2
2√2

2√2

2√2

2√2

2√2

k∗+1

dt

+

+

2√2

+ j∗ǫ−3/4(cid:1)π2

The ﬁrst inequality is obtained using Lemma 5.1. Inequality (i) follows by upper-bounding
k∗ dt/t (this argument will be used once
more to control of the third sum). Inequality (ii) is obtained simply by factorizing and noting
that k∗+1
πδ . Therefore,
2√2

k=k∗+1 1/k2 ≤R k0

. By deﬁnition of j∗ and k∗, we have k∗+1
2√2

+j∗ǫ−3/4 > 1

+j∗ǫ−3/4 ≥ 1
2√2

2k0−1Xk=k∗+1

γk,j∗ ≤

4√2
π

δ.

=

4√2

(cid:0) 1
δπ(cid:1)π2

On the other hand, using the same techniques above, the third term is upper bounded as

Xj≥j∗+1

2k0−1Xk=0

γk,j ≤ Xj≥j∗+1

2k0

(jǫ−3/4)2π2

=

≤

=

2k0ǫ3/2

(j∗ + 1)2π2 + Xj≥j∗+2
(j∗ + 1)2π2 +Z ∞

2k0ǫ3/2

2k0ǫ3/2
j2π2

2k0ǫ3/2
s2π2 ds

j∗+1
2k0ǫ3/2

(j∗ + 1)π2

2k0ǫ3/2
(j∗ + 1)2π2 +

2k0ǫ3/2
(j∗ + 1)π2 .

Since j∗ = ⌊ ǫ3/4

≤ 2
δπ . Therefore, using k0 = ⌊√2ǫ−3/4⌋, we have
δπ ⌋, we have j∗ + 1 > ǫ3/4
2k0−1Xk=0

Xj≥j∗+1

4√2
π

γk,j ≤

4k0ǫ3/4

δ ≤

δ.

π

22

Putting these estimates together, inequality (22) yields
8√2
π

4√2
δπ

+ 1)δ2 +

min(cid:8)γk,j, δ2(cid:9) ≤ (

∞Xk,j=0

δ ≤ δ2 +

12√2
π

δ.

Therefore, the master inequality (20) becomes

r 2
n(cid:16)δ2 +

12√2
π

δ(cid:17)1/2

R
σ

≤

δ2.

Finally, only considering solutions δ < 1, it can be veriﬁed that the speciﬁed choice (21) is
adequate, as claimed.

C.3 Proof of Lemma 5.1

We ﬁrst characterize the eigenvalues of the kernel K as solutions to a certain non-linear
equation:

Lemma C.1. The eigenvalues of the kernel K from equation (13) are given by the solutions
λ > 0 of the non-linear equation

tan(cid:18) ǫ

√λb(cid:19) tan(cid:18) 1 − ǫ

√λa(cid:19) =(cid:18) b

a(cid:19)3/2

.

(23)

The proof of this lemma is deferred to the next subsection. Our next step is to understand

the solutions to tan(cid:0) ǫ√b
For our purposes, we only need to consider the regime where b = √ǫ ≪ a and the quantity

a(cid:1)3/2 with x = 1/√λ.

x(cid:1) tan(cid:0) 1−ǫ√a x(cid:1) =(cid:0) b

ǫ is close to zero. We then assume that the ratio of the two periods 1−ǫ√a / ǫ√b
: = k0 is an
integer; note that this assumption can always be satisﬁed by choosing ǫ appropriately. This
assumption prevents complicated oscillatory phenomena, and thus makes the analysis simpler.

Deﬁne the function φ(x) : = tan(cid:0) ǫ√b

x(cid:1) tan(cid:0) 1−ǫ√a x(cid:1). We ﬁrst exploit the periodicity of the

function φ so as to simplify the reduce the problem φ is even, and by our assumption on k0,
√b
it has a period of
ǫ π]. We divide this
√b
ǫ π] and we study the behavior of

√a
1−ǫ π. Therefore, we only study its behavior on [0,

√b
ǫ π/2) and ¯I = [

√b
ǫ π/2,

interval into two intervals I = [0,
φ on each of them separately.

√a

√a
1−ǫ π/2, (k +1)

and increasing. In addition, the last one varies from 0 to ∞. Therefore, φ spans the entire

Divide I into smaller subintervals Ik : =(cid:2)k
each interval Ik, both functions x → tan(cid:0) 1−ǫ√a x(cid:1) and x → tan(cid:0) ǫ√b
half line [0,∞) on Ik. Consequently, it must cross the line y =(cid:0) b
interval Ik. We denote the coordinate of intersection by xk, i.e. φ(xk) =(cid:0) b
of φ, we have φ(−xk) = (cid:0) b
a(cid:1)3/2, and by periodicity, ¯xk = −xk +
φ(¯xk) =(cid:0) b

1−ǫ π/2(cid:1) with 0 ≤ k ≤ k0−1. On
x(cid:1) are continuous, positive,
a(cid:1)3/2 exactly once in each
a(cid:1)3/2 and xk ∈ Ik.
√b
ǫ π − Ik. We observe that by parity
√b
ǫ π ∈ ¯Ik also veriﬁes
a(cid:1)3/2. The sequence of numbers x0 < x1 < . . . < xk0−1 < ¯xk0−1 < ¯xk0−2 < . . . < ¯x0

Similarly, we divide ¯I into regular subintervals ¯Ik =

√b
ǫ π]. Then by periodicity, we

correspond to the entire set of solutions on the interval [0,

23

√b
obtain all positive solutions by translating the above sequence by multiples of the period
ǫ π.
Therefore the eigenvalues of the kernel K form a doubly-indexed sequence (γk,j) such that

γk,j =

1.(cid:0)xk + j
1.(cid:0)√b

√b

ǫ π(cid:1)2

ǫ π − x2k0−k−1 + j

√b

k ∈ [0, k0 − 1],

ǫ π(cid:1)2 k ∈ [k0, 2k0 − 1].

Now, we can upper bound γk,j by using the fact xk ∈ Ik when either k or j is greater than

√a
1−ǫ π/2. Likewise, when k0 ≤ k ≤ 2k0 − 1,

1: observe that when k ≤ k0 − 1, xk ≥ k

√b
ǫ

√b
ǫ

π − x2k0−k−1 ≥

π − ((2k0 − k − 1) + 1)

π/2 = k

π/2.

√a
1 − ǫ

√a
1 − ǫ

more, recalling that a = 1/2−ǫ3/2
we have

Thus, we have shown that γk,j ≤ 1.(cid:0)k
√b
ǫ (cid:17)2

γk,j ≤ 1.(cid:16) k

2√2

1−ǫ

+ j

Note in passing that k0 = 1−ǫ√a ǫ−3/4 ≤ √2ǫ−3/4.

, we notice that

√a
1−ǫ /2 + j
√a

√b
ǫ (cid:1)2π2 for all 1 ≤ k ≤ 2k0 − 1. Further-
1−ǫ ≥ 1/√2 when ǫ < 1/2. Consequently,

π2

whenever k ≥ 1 or j ≥ 1.

√b

The case k = j = 0 needs extra care. We have x0 ∈ [0,

√a
1−ǫ π/2). Since φ vanishes at
0 and is strictly increasing on this interval, it is clear that x0 > 0. Now we proceed by
an approximation argument valid in the limit ǫ → 0, and then invoke monotonicity of the
solution x0 in ǫ. For ǫ suﬃciently small and by using b = √ǫ, we can uniformly approximate

x for 0 < x < 1. Then, the equation φ(x) = (cid:0) b

x(cid:1) by ǫ√b
a(cid:1)3/2 becomes
the function tan(cid:0) ǫ√b
x tan(cid:0) 1−ǫ√a x(cid:1) =
ǫ (cid:0) b
a(cid:1)3/2 = 1
In this regime a ≃ 1/2 and the equation can be further
a3/2 .
approximated by tan(√2x) = 2√2
x . A numerical inspection shows that the latter has a unique
solution .892 ≤ x∗ ≤ .899 on [0, 1]. Moreover we also numerically observe that the solution
x0(ǫ) to the equation φ(x) = (cid:0) b
a(cid:1)3/2 on [0, 1] is an increasing function of ǫ, hence x0 ≥
limǫ→0 x0(ǫ) = x∗. Therefore, the ﬁrst eigenvalue γ0,0 = 1/x2
independent of ǫ—that is, we have γ0,0 ≤ 1/x∗2 ≤ 1.26, as claimed.
C.4 Proof of Lemma C.1

0 is upper bounded by a constant

Letting P be the distribution associated with the density µ, we study the eigenvalues and
eigenfunctions of the integral operator T : L2(P ) → L2(P ) given by

f (t)K(t, x)µ(t)dt.
Here the reader should recall that the density µ takes the form

−1

T f (x) : =Z 1
µ(x) =(b

if x ∈ [−ǫ, ǫ]

a if x ∈ [−1,−ǫ) ∪ (ǫ, 1].

where the parameters (a, b, ǫ) are related by the equations (1 − ǫ)a + ǫb = 1/2, and b = √ǫ.
The kernel K is given by

K(x, y) =

1

4Z 1

−1

dt
µ2(t) −

1

2(cid:12)(cid:12)(cid:12)(cid:12)Z y

x

24

dt

µ2(t)(cid:12)(cid:12)(cid:12)(cid:12) ,

for x, y ∈ [−1, 1].

The eigenvalue equation associated with the operator T can be written as T ϕλ = λϕλ,
where ϕλ is the eigenfunction associated with the eigenvalue λ ≥ 0. Diﬀerentiating this
equation twice yields a system of diﬀerential equations for the eigenfunctions:

Lemma C.2. All eigenfunctions of T must satisfy the following system of diﬀerential equa-
tions:

λb ϕ′′λ + ϕλ = 0 on [−ǫ, ǫ],
λa ϕ′′λ + ϕλ = 0

and

on [−1,−ǫ) ∪ (ǫ, 1].

Solving this system of diﬀerential equations yields that any eigenfunction must be of the

form

ϕλ(x) =

A1 sin(cid:0) x√λb(cid:1)
A2 sin(cid:0) x√λa(cid:1) + B2 cos(cid:0) x√λa(cid:1)
A2 sin(cid:0) x√λa(cid:1) − B2 cos(cid:0) x√λa(cid:1)

for x ∈ [−ǫ, ǫ],
for x ∈ [−1,−ǫ), and
for x ∈ (ǫ, 1],

where we already exploited the fact that ϕλ has to be odd. Of course, not all functions of
the above form are eigenfunctions of T , since we lost information by taking two derivatives.
In order to show that ϕλ is actually an eigenfunction, we need to verify that it is continuous

continuous, and satisﬁes the relations (cid:0)T ϕλ(cid:1)′ = λϕ′λ and T ϕλ = λϕ. Actually, the last

condition will be satisﬁed when the ﬁrst two are. Together, these conditions will provide
enough constraints to specify the four parameters A1, A2, B2 and λ in an unambiguous, modulo
a global multiplicative constant for the ﬁrst three.

By imposing continuity on the solutions for ±ǫ, we obtain an equation relating the pa-

rameters of the problem:

A2 sin(cid:18) ǫ

√λa(cid:19) − B2 cos(cid:18) ǫ

√λa(cid:19) = A1 sin(cid:18) ǫ

√λb(cid:19) .

(24)

Next we verify that the condition(cid:0)T ϕλ(cid:1)′ = λϕ′λ holds on [−ǫ, ǫ] and [−1,−ǫ)∪(ǫ, 1] separately.

Since the density µ is even, we have K(−x, y) = K(x,−y) for all x, y ∈ [−1, 1]. Therefore, for
any odd function f , we have

Z −ǫ

−1

f (t)K(t, x)µ(t)dt = −Z 1

ǫ

f (t)K(t,−x)µ(t)dt,

so one we can study the problem on the interval (ǫ, 1] and automatically obtain the corre-
sponding results on [−1,−ǫ).

Case x ∈ (ǫ, 1]: For any odd function f , we have

T f (x) = −

1

2Z −ǫ

−1

af (t)(cid:0)−ǫ − t

a2 +

2ǫ
b2 +

x − ǫ

a2 (cid:1)dt −

25

1

2Z ǫ

−ǫ

bf (t)(cid:0) ǫ − t

b2 +

1

2Z 1

ǫ

−

x − t

a2 (cid:1)dt
af (t)(cid:0)|t − x|

a2

(cid:1)dt.

Diﬀerentiating with respect to x and setting f = ϕλ yields

1

1

−

2a(cid:8)A2(cid:2) −

2a(cid:8)A2(cid:2) −

(cid:0)T ϕλ(cid:1)′(x) = −

√λa cos(cid:0) t
−1 + B2(cid:2)√λa sin(cid:0) t
−1(cid:9)
√λa(cid:1)(cid:3)−ǫ
√λa(cid:1)(cid:3)−ǫ
√λa cos(cid:0) t
ǫ − B2(cid:2)√λa sin(cid:0) t
ǫ(cid:9)
√λa(cid:1)(cid:3)x
√λa(cid:1)(cid:3)x
x − B2(cid:2)√λa sin(cid:0) t
√λa cos(cid:0) t
√λa(cid:1)(cid:3)1
2a(cid:8)A2(cid:2) −
Since we must have(cid:0)T ϕλ(cid:1)′ = λϕλ, some algebra then leads to
√λa(cid:1) = 0.

√λa(cid:1) + B2 sin(cid:0) 1

A2 cos(cid:0) 1

+

1

x(cid:9).
√λa(cid:1)(cid:3)1

(25)

Equations (24) and (25) form a linear system in the coeﬃcients A2 and B2, and solving this
system yields

A2 = A1

B2 = −A1

sin(cid:0) 1√λa(cid:1) sin(cid:0) ǫ√λb(cid:1)
cos(cid:0) 1√λa(cid:1) sin(cid:0) ǫ√λb(cid:1)

sin(cid:0) 1√λa(cid:1) sin(cid:0) ǫ√λa(cid:1) + cos(cid:0) 1√λa(cid:1) cos(cid:0) ǫ√λa(cid:1) ,
sin(cid:0) 1√λa(cid:1) sin(cid:0) ǫ√λa(cid:1) + cos(cid:0) 1√λa(cid:1) cos(cid:0) ǫ√λa(cid:1) .

Case x ∈ [−ǫ, ǫ]: For an odd function f , we have

1

2Z ǫ

−ǫ

1

2Z −ǫ

−1

ǫ

−

b2
1

x + ǫ

b2 +

a2 +

T f (x) = −

b2 (cid:1)dt −

af (t)(cid:0)−ǫ − t

Following an argument similar to the previous case, we ﬁnd that

(cid:1)dt
bf (t)(cid:0)|x − t|
2Z 1
af (t)(cid:0) ǫ − x
√λa sin(cid:0) t
−1(cid:9)
√λa(cid:1)(cid:3)−ǫ
−1 + B2(cid:2) −
√λb cos(cid:0) t
√λb cos(cid:0) t
−ǫ + A1(cid:2) −
√λb(cid:1)(cid:3)x
Imposing the constraint(cid:0)T ϕλ(cid:1)′ = λϕλ leads to the equation
A2(cid:0) cos(cid:0) ǫ
√λa(cid:1) − sin(cid:0) 1

√λa(cid:1)(cid:1) + B2(cid:0) sin(cid:0) ǫ

√λa(cid:1)(cid:3)−ǫ
2b(cid:8)A1(cid:2) −

√λa(cid:1) − cos(cid:0) 1

(cid:0)T ϕλ(cid:1)′(x) = −

a(cid:1)3/2A1 cos(cid:0) ǫ
√λb(cid:1).

√λa cos(cid:0) t

√λa(cid:1)(cid:1) =(cid:0) b

b2(cid:8)A2(cid:2) −

a

1

−

t − ǫ

a2 (cid:1)dt.

x(cid:9).
√λb(cid:1)(cid:3)ǫ

(26)

Finally, plugging the expressions of A2 and B2 in the above equation and simplifying yields
the claimed equation (23).

D Reduction of least squares to constrained optimization

In this appendix, we show that in the regime p ≤ d, minimizing an objective function that

is a weighted sum of a least-squares cost with a regularization term(cid:0)R k∇f (x)kpµ2(x)dx(cid:1)1/p

26

will have degenerate solutions, just like the constrained formulation. Consider a least-squares
problem of the form

min

f nXi∈O

(f (xi) − yi)2 + λR(f )o,

(27)

where R(f ) is some arbitrary regularization term. We show that the above has the same
solution as a constrained optimization problem. Let

f nXi∈O

(f (xi) − yi)2 + λR(f )o.

bf = arg min
Then bf is equal to the minimizer of
To see this, suppose that the optimizer of (28) is g 6= bf , then it must be that R(g) < R(bf )
andPi∈O(g(xi) − yi)2 =Pi∈O(bf (xi) − yi)2, in which case the function g achieves a smaller
value of the cost (27) than bf .
By setting R(f ) = (cid:0)R k∇f (x)kpµ2(x)dx(cid:1)1/p, we know from Section 4.2 that the solu-

tion to the optimization problem (28) must be degenerate, and so must the solution to the
problem (27).

subject to f (xi) = bf (xi), i ∈ O.

min

R(f )

f

(28)

27

