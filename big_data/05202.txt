6
1
0
2

 
r
a

 

M
6
1

 
 
]

.

G
M
h
t
a
m

[
 
 

1
v
2
0
2
5
0

.

3
0
6
1
:
v
i
X
r
a

Packing, coding, and ground states

Henry Cohn

Packing, coding, and ground states

Preface
Acknowledgments

Lecture 1. Sphere packing

Introduction

1.
2. Motivation
3. Phenomena
4. Constructions
5. Diﬃculty of sphere packing
6. Finding dense packings
7. Computational problems

Lecture 2. Symmetry and ground states

Introduction

1.
2. Potential energy minimization
3. Families and universal optimality
4. Optimality of simplices

Lecture 3.

Interlude: Spherical harmonics

1. Fourier series
2. Fourier series on a torus
3. Spherical harmonics

Lecture 4. Energy and packing bounds on spheres

Introduction

1.
2. Linear programming bounds
3. Applying linear programming bounds
4. Spherical codes and the kissing problem
5. Ultraspherical polynomials

Lecture 5. Packing bounds in Euclidean space

Introduction

1.
2. Poisson summation
3. Linear programming bounds
4. Optimization and conjectures

Bibliography

i

Contents

1

3
3
3

5
5
6
8
10
12
13
15

17
17
18
19
23

27
27
29
31

35
35
37
39
40
41

47
47
49
50
52

57

Packing, coding, and ground states

Henry Cohn

Packing, coding, and ground states

Henry Cohn

Preface

In these lectures, we’ll study simple models of materials from several diﬀerent
perspectives: geometry (packing problems), information theory (error-correcting
codes), and physics (ground states of interacting particle systems). These per-
spectives each shed light on some of the same problems and phenomena, while
highlighting diﬀerent techniques and connections.

One noteworthy phenomenon is the exceptional symmetry that is found in
certain special cases, and we’ll examine when and why it occurs. The overall theme
of the lectures is thus order vs. disorder. How much symmetry can we expect to
see in optimal geometric structures?

The style of these lecture notes is deliberately brief and informal. See Conway
and Sloane’s book Sphere packing, lattices and groups [26] for far more information
about many of the mathematical objects we’ll discuss, as well as the references cited
in the notes for omitted details.

I’ve included a dozen exercises for the reader, which cover things I think it’s
most useful to do for oneself. The exercises vary in diﬃculty, from routine veriﬁ-
cations to trickier computations. There’s no need to solve them if you are willing
to take a few things on faith, but I highly recommend engaging actively with this
material, and the exercises would be a good way to get started.

These notes are based on my PCMI lectures from 2014 and were written before
Viazovska [80] found a remarkable solution to the sphere packing problem in R8
using linear programming bounds. The only updates to reﬂect this development
are a few footnotes.

Acknowledgments

I am grateful to Matthew de Courcy-Ireland for serving as the teaching assistant
for this course and for providing feedback on the lecture notes.

Microsoft Research New England, One Memorial Drive, Cambridge, MA 02142
E-mail address: cohn@microsoft.com

3

LECTURE 1

Sphere packing

1. Introduction

The sphere packing problem asks for the densest packing of congruent spheres in Rn.
In other words, how can we cover the greatest fraction of space using congruent
balls that do not overlap (i.e., that have disjoint interiors)? The density is the
fraction of space covered. Finding the densest sphere packing sounds simple, but
it turns out to be a surprisingly deep and subtle problem.

Before we dive into the sphere packing problem, it’s worth thinking about how
to write down a rigorous deﬁnition. Although pathological packings may not have
well-deﬁned densities, everything we could reasonably hope for is true: we can
deﬁne the optimal density by taking a suitable limit, and there is a packing that
achieves this density. Speciﬁcally, given a packing P, a point x ∈ Rn, and a positive
real number r, let

∆r,x(P) =

vol(Br(x) ∩ P)

vol Br(x)

be the fraction of the ball Br(x) of radius r centered at x that is covered by P. If
we deﬁne the optimal packing density ∆n in Rn by

∆n = lim sup
r→∞

sup
P

∆r,0(P),

then there exists a single packing P for which

lim
r→∞

∆r,x(P) = ∆n

uniformly for all x ∈ Rn. See [37] for a proof.
What are the optimal sphere packings in low dimensions? In one dimension, we
have the interval packing problem on the line, which is trivial. In two dimensions,
the answer is pretty clearly the hexagonal packing, with each disk surrounded by
six others:

However, proving optimality takes a genuine idea. For example, one can show that
the Voronoi cells (the sets of points closer to each sphere center than to the others)
in the hexagonal packing are as small as possible in any packing. See [74] for the
ﬁrst proof of optimality, [63, 35] for subsequent proofs, and [38] for a particularly
short proof.

5

6

HENRY COHN, PACKING, CODING, AND GROUND STATES

In three dimensions, the sphere packing problem is much more diﬃcult. There
is a natural guess for the solution, namely stacking hexagonal layers as densely
as possible, so that each is nestled into the gaps in the neighboring layers. Such
a packing is known to be optimal, via an elaborate proof [39] that depends on
computer calculations. The original proof was so long and complex that it was
diﬃcult to check carefully, but it has recently been veriﬁed at the level of formal
logic [40].

In four or more dimensions, the optimal sphere packing density is not known,

although there are upper and lower bounds.1

Exercise 1.1. How can hexagonal layers be stacked to form dense packings in
R3? Show that there are an uncountable number of diﬀerent ways to do so, even
if you consider two packings the same when they are related by a rigid motion
of space. Can you extend this analysis to R4? Which packings can you get by
stacking optimal three-dimensional packings as densely as possible? How many can
you ﬁnd? How dense are they? What about R5? R6? How high can you go?

Feel free to give up after four dimensions, but the further you go, the more
interesting phenomena you’ll run into. By R10, this iterated stacking process will
no longer produce the densest possible sphere packings, but nobody knows whether
it fails before that. See [25] for more details on what happens in dimensions two
through ten.

2. Motivation

There are several reasons why we should care about sphere packing. One is that
it’s a natural geometric problem: humanity ought to know the answer to such a
simple and natural question.

Another reason is that the problem has interesting solutions. Sometimes it’s
diﬃcult to judge how interesting a problem is in the abstract, before taking a look
at the phenomena that occur. Sphere packing is full of rich, intricate structures that
are themselves of intrinsic interest, and this makes the problem far more appealing
than it would have been if the answers had been less exciting.

A third reason to care about sphere packing is that it is a toy model of granular
materials. Of course no real material consists of identical perfect spheres, and the
sphere packing problem also neglects forces and dynamics. However, sphere packing
is at least a ﬁrst step towards understanding the density of an idealized material.
(See [52] for a statistical physics perspective on packing.)

The most important practical reason to study sphere packing is also one of the
most surprising reasons: high-dimensional sphere packings are essential for commu-
nication over noisy channels, as we’ll spend the rest of this section understanding.
This is really an assertion about information theory, Claude Shannon’s great dis-
covery from his famous 1948 paper A mathematical theory of communication [70].
Sphere packing per se again deals with an idealized scenario, but it illustrates some
of the fundamental principles underlying information theory.

The setting works as follows. Suppose we are sending messages over some
communication channel. We will represent the signals by points in a bounded sub-
set of Rn, say the ball of radius R about the origin (the precise subset is not so

1Viazovska [80] has recently solved the sphere packing problem in R8 using linear programming
bounds.

LECTURE 1. SPHERE PACKING

7

important). In this model, each coordinate represents some measurement used to
describe the signal. For example, for a radio signal we could measure the amplitude
at diﬀerent frequencies. There is no reason to expect the number of measurements
to be small, and realistic channels can involve hundreds or even thousands of coor-
dinates. Thus, the signal space for the channel will be high-dimensional. Note that
this dimensionality has nothing to do with the physical space we are working in;
instead, it simply represents the number of independent measurements we make on
the signals.

Each signal will be an individual transmission over the channel at a given time,
and we will send a stream of signals as time passes. Of course, the big diﬃculty
with communication is noise:
if we send a signal s, then the received signal r at
the other end will generally not be exactly equal to s. Instead, it will have been
perturbed by channel noise. In a useful channel, the noise level will be fairly low,
and we can expect that |r − s| < ε for some ﬁxed ε (the noise level of the channel).
Thus, we can imagine an open error ball of radius ε about each signal sent,
which shows how it could be received after adding noise:

ε

r

s

This is a simplistic model of noise, since we assume that the noise has no direc-
tionality or structure, and that every perturbation up to radius ε could plausibly
occur but nothing beyond that limit. In practice, engineers use more sophisticated
noise models; for example, cell phones have to take into account all sorts of other
phenomena, such as interference from reﬂected signals. However, our basic noise
model is a good illustration of the essential principles.

How can we arrange our communications so as to remove the eﬀects of noise?
We will build a vocabulary S ⊆ Rn of signals and only send signals in S. This is
called an error-correcting code. If two distinct signals s1, s2 ∈ S satisfy |s1 − s2| <
2ε, then the received signal could be ambiguous:

s1

r

s2

Therefore, we will keep all signals in S at least 2ε apart, so that the error balls are
disjoint:

2ε

s1

s2

This is exactly the sphere packing problem. We want the signal set S to be
as large as possible, since having more signals available increases the rate at which
we can transmit information, but the ε-balls about the signals in S are not allowed

8

HENRY COHN, PACKING, CODING, AND GROUND STATES

to overlap. How large can we make S subject to this constraint? Recall that the
only available subset of Rn in our model is the ball of radius R. Thus, the question
becomes how many ε-balls we can pack into a ball of radius R + ε. (The radius is
R + ε, rather than R, because the error balls can stick out over the edge.) That’s a
ﬁnite version of sphere packing, and we recover the usual version in all of Rn in the
limit when R is much larger than ε. That limit is exactly the situation we expect,
since the channel is not very useful if ε is on the same scale as R.

It is remarkable that although high-dimensional packing sounds utterly abstract
and impractical, it turns out to be particularly important for applications.
In
these lectures we will focus on the theory behind sphere packing, rather than the
applications, but it is helpful to keep in mind that a high-dimensional packing is a
tool for communicating over a noisy channel.

3. Phenomena

Relatively little is understood about the sphere packing problem. One might hope
for a systematic solution that works in every dimension, but that just doesn’t seem
possible. Instead, each dimension has its own idiosyncrasies. Getting a feeling for
how R8 diﬀers from R7 or R9 is part of the charm of the subject, but these diﬀerences
mean the packing problem is much more subtle than it sounds. In two or three
dimensions, we can rely on our spatial intuition and summarize the procedure as
“just do the obvious thing,” but there is no obvious thing to do in Rn.

Good constructions are known in low dimensions, and there is little doubt that
humanity has found the optimal density through at least the ﬁrst eight dimensions.
However, we have absolutely no idea what the best high-dimensional packings look
like. For example, we do not know whether to expect them to be ordered and
crystalline, or disordered and pseudorandom. Many researchers expect disorder,
perhaps on the grounds that this is the default when there is no reason to expect
order. However, we lack the theoretical tools to analyze this question.

All we know in general are upper and lower bounds for the optimal density,
and these bounds are distressingly far apart. For example, in R36 they diﬀer by
a multiplicative factor of 58: if you take the densest known packing in R36, then
the best we can say is that you couldn’t ﬁt in any more than 58 times as many
spheres if you rearranged them. The ratio of the upper and lower bounds in Rn
grows exponentially as n → ∞.
At ﬁrst, this gap sounds absurd. How could our bounds possibly be oﬀ by an
exponential factor? One way to think about it is that volume scales exponentially
in high dimensions, because the volume of a hypercube of side length ℓ in Rn is
ℓn, which is exponential in n. If you take a packing in Rn and move the sphere
centers 1% further apart, then you lower the density by a factor of 1.01n. In low
dimensions this factor is insigniﬁcant, but in high dimensions it is enormous. Thus,
even a little bit of uncertainty in the sphere locations translates to an exponential
uncertainty in the density.

On a scale from one to inﬁnity, a million is small, but we know almost nothing
about sphere packing in a million dimensions. The best we can say is that the op-
timal density is at least a little larger than 2−1000000. More generally, the following
greedy argument gives a surprisingly easy lower bound of 2−n in Rn.

Consider a saturated packing in Rn, i.e., a packing such that no further spheres
can be added without overlap. Such packings certainly exist, because one can

LECTURE 1. SPHERE PACKING

9

Figure 1. Any point not covered by the double-radius spheres
could be used as the center of a new sphere (shaded above).

obtain a saturated packing by iteratively adding spheres as close to the origin as
possible. Alternatively, there are saturated packings on ﬂat tori because there is
room for only ﬁnitely many spheres, and unrolling such a packing yields a saturated
periodic packing in Euclidean space.

Proposition 3.1. Every saturated sphere packing in Rn has density at least 2−n.

Proof. No point in Rn can have distance at least 2 from all the sphere centers
in a saturated packing with unit spheres, because we could center a new sphere at
such a point without creating any overlap (see Figure 1). In other words, doubling
the radius of the spheres in a saturated packing would cover space completely.
Doubling the radius increases the volume by a factor of 2n, and so the original
spheres must occupy at least a 2−n fraction of Rn. Thus, every saturated packing
has density at least 2−n.
(cid:3)

In R1 there are saturated packings with density arbitrarily close to 1/2, but
that is the only case in which Proposition 3.1 is sharp, because the bound is sharp
exactly when the double-radius balls can tile Rn. One way to improve it is to prove
a lower bound for how ineﬃcient a sphere covering in Rn must be. For example,
using the Coxeter-Few-Rogers theorem on sphere covering [27] improves the bound
to e−3/2n · 2−n asymptotically.
At ﬁrst 2−n sounds like a rather weak bound, which must be far from the truth.
However, nobody has been able to obtain any exponential improvement to it, and
perhaps it is closer to the truth than one would guess. In any case, it is nearly all
we know regarding density lower bounds in high dimensions. A long sequence of
improvements ground to a halt with Ball’s bound of 2(n−1)·2−n in 1992 [7], before
progress began again nearly twenty years later. Vance proved a lower bound asymp-
totic to 6n/e· 2−n in 2011 [77], which improves on Ball’s bound because e < 3, and
Venkatesh followed that with a much larger constant-factor improvement as well
as a bound proportional to n log log n · 2−n for a certain sparse sequence of dimen-
sions [78]. This last bound is particularly exciting because it is the ﬁrst superlinear
improvement on 2−n, but on an exponential scale all of these improvements are
small. For comparison, the best upper bound known is 2−(0.5990...+o(1))n, due to
Kabatiansky and Levenshtein [41] in 1978, with a constant-factor improvement by
Cohn and Zhao [23] in 2014.

Note that the greedy argument is nonconstructive, and the same is true of the
improvements mentioned above. For large n, all known bounds anywhere near 2−n

10

HENRY COHN, PACKING, CODING, AND GROUND STATES

are nonconstructive, while every packing anyone has described explicitly is terrible
in high dimensions. For example, one natural attempt is to center spheres of radius
1/2 at the integer lattice points Zn. That yields a packing of density

πn/2

(n/2)!2n ,

and the factorial in the denominator ruins the density. (Note that when n is odd,
(n/2)! means Γ(n/2 + 1).)

There are some patterns in low dimensions, but they quickly stop working. For
example, natural generalizations of the face-centered cubic packing from R3 work
well in R4 and R5, but not in higher dimensions, as we will see in the next section.
In R10, the best packing known is based on a periodic arrangement with 40 spheres
in each fundamental cell [26, p. 140].

Crystalline packings work beautifully in low dimensions, but they become in-
creasingly diﬃcult to ﬁnd in high dimensions. Perhaps they just aren’t optimal?
It’s natural to speculate about amorphous packings, but nobody really knows. In
high dimensions, we can analyze only random or typical packings, and we simply
do not know how close they are to the very best.

One philosophical quandary is that too much structure seems to make high-
dimensional packings bad, but the known lower bounds all rely on some sort of
heavy structure. Vance’s and Venkatesh’s techniques give the best density, but
they involve the most structure, namely lattices with nontrivial symmetry groups
acting on them. The trade-oﬀ is that structure seemingly hurts density but helps
in analyzing packings.

The most remarkable packings are the E8 root lattice in R8 and the Leech
lattice Λ24 in R24. They are incredibly symmetrical and dense packings of spheres,
and they must be optimal, although this has not yet been proved.2 What makes
them exciting is that they turn out to be connected with many areas in mathematics
and physics, such as string theory, hyperbolic geometry, and ﬁnite simple groups.
See [26] and [32] for more information about these wonderful objects, as well as
the next section for a construction of E8.

4. Constructions

How can we form a sphere packing? The simplest structure we could use is a lattice,
the integer span of n linearly independent vectors in Rn. In other words, given a
basis v1, . . . , vn, we center the spheres at the points

{a1v1 + a2v2 + ··· + anvn | a1, . . . , an ∈ Z}.

The packing radius of a lattice is half the shortest nonzero vector length, since that
is the largest radius for which the spheres do not overlap. Given a lattice basis
v1, . . . , vn, the corresponding fundamental cell is the parallelotope
{x1v1 + x2v2 + ··· + xnvn | x1, . . . , xn ∈ [0, 1)}.
The translates of the fundamental cell by lattice vectors tile space.

In a lattice packing, there is one sphere per translate of the fundamental cell,
and the density is the volume ratio of the sphere and cell. More generally, we
could form a periodic packing, which is the union of ﬁnitely many translates of a

2Until very recently in [80] for n = 8.

LECTURE 1. SPHERE PACKING

11

lattice. Equivalently, there can be several spheres per cell, which are then translated
throughout space by the lattice vectors. There is no reason to believe that one
sphere per cell is the best choice, and indeed periodic packings oﬀer considerably
more ﬂexibility.

One confusing issue is that physicists use the term “lattice” to mean periodic
packing, while they call lattices “Bravais lattices.” We will stick with the standard
mathematical terminology.

There is no reason to believe that periodic packings achieve the greatest possible
density. This is an open question above three dimensions, and it is plausibly false
in high dimensions. However, periodic packings always come arbitrarily close to the
optimal density. To see why, consider an optimal packing, and imagine intersecting
it with a large box. If we try to repeat the part in the box periodically through
space, then the only place overlap could occur is along the boundary of the box.
We can ﬁx any problems by removing the spheres next to the boundary. Shaving
the packing in this way produces a periodic packing without overlap, at the cost of
slightly lowering the density. The decrease in density becomes arbitrarily small if
we use a suﬃciently large box, and thus periodic packings come arbitrarily close to
the optimal packing density.

By contrast, lattices probably do not approach the optimal density in high
dimensions. The problem is that unlike periodic packings, lattices have limited
ﬂexibility. A lattice is completely determined by a basis, and thus a lattice in Rn
can be speciﬁed by n2 parameters (in fact, fewer if we take the quotient by rigid
motions). Quadratically many parameters just don’t give enough ﬂexibility to ﬁll
all the gaps in an exponential amount of space. It’s natural to guess that when
n is large enough, no lattice packing in Rn is ever saturated, but this conjecture
remains out of reach.

The best sphere packings currently known are not always lattice packings (R10
is the ﬁrst case in which lattices seem to be suboptimal), but many good packings
are. The simplest lattice is Zn, but it is a lousy packing when n > 1, as discussed
above. Instead, the “checkerboard” packing

Dn = {(x1, . . . , xn) ∈ Zn | x1 + ··· + xn is even}.

is better for n ≥ 3. In fact, D3, D4, and D5 are the best packings known in their
dimensions, and provably the best lattice packings (see [26] for more information).
However, they are suboptimal for n ≥ 6.
What goes wrong for n ≥ 6 is that the holes in Dn grow larger and larger. A
hole in a lattice Λ in Rn is a point in Rn that is a local maximum for distance from
the nearest point in Λ. There are two classes of holes in Dn for n ≥ 3, represented
by (1, 0, . . . , 0), which is at distance 1 from Dn, and (1/2, 1/2, . . . , 1/2), which is at
distance

s(cid:18) 1
2(cid:19)2

2(cid:19)2
+ ··· +(cid:18) 1

=r n

4

.

More generally, the translates of these points by Dn are also holes, as are the
translates of (1/2, 1/2, . . . , 1/2,−1/2).

When n > 4 we call (1, 0, . . . , 0) a shallow hole in Dn and (1/2, . . . ,±1/2) a
enormous. For comparison, note that the spheres in the Dn packing have radius

deep hole, becausepn/4 > 1. When n is large, the depthpn/4 of a deep hole is

12

HENRY COHN, PACKING, CODING, AND GROUND STATES

√2/2, because the nearest lattice points are

(0, 0, . . . , 0)

and

(1, 1, 0, . . . , 0),

at distance √2. When n is large, the holes are much larger than the spheres in the
packing, and Dn is not even saturated, let alone an optimal packing.

right at the transition point. When n = 8, the radiuspn/4 of a deep hole equals

This transition occurs at dimension eight, and something wonderful happens
the distance √2 between adjacent lattice points. Thus, we can slip another copy
of D8 into the holes, which doubles the packing density, and the new spheres ﬁt
perfectly into place. The resulting packing is called the E8 root lattice.

This construction of E8 appears asymmetric, with two diﬀerent types of spheres,
namely the original spheres and the ones that were added. However, they are
indistinguishable, because E8 is a lattice and thus all the spheres are equivalent
under translation.

Exercise 4.1. Check that E8 is in fact a lattice.

The E6 and E7 lattices are certain cross sections of E8. The E6, E7, and E8
lattices are the densest lattice packings in R6 through R8, and they are almost
certainly the densest sphere packings.

The Leech lattice Λ24 in R24 is similar in spirit, but with a more elaborate
construction. See [32] for an elegant treatment of the Leech lattice, as well as the
theory of root lattices.

The kissing number in Rn is the greatest number of spheres that can touch a
central sphere, if they all have the same size and cannot overlap except tangentially.
It is known to be 6 in R2, 12 in R3, 24 in R4, 240 in R8, and 196560 in R24, but is
not known in any other dimensions. The case of R2 is easy, but R3 is not [66], and
R4 is yet more diﬃcult [55]. Surprisingly, R8 and R24 are quite a bit simpler than
R3 or R4 are [59, 50], and we will settle them in the fourth lecture.

Exercise 4.2. What are the shortest nonzero vectors in Dn? In E8? This will give
optimal kissing conﬁgurations in R3, R4, and R8.

Exercise 4.3. The vertices of a cross polytope centered at the origin in Rn consist
of n pairs of orthogonal vectors of the same length (it’s a generalized octahedron).
Show how to decompose the vertices of a hypercube in R4 into two cross polytopes.
Find a symmetry of the hypercube that interchanges them.

Exercise 4.4. Show how to decompose the minimal vectors in D4 into three disjoint
cross polytopes, and ﬁnd a symmetry of D4 that cyclically permutes these cross
polytopes.

This symmetry is called triality, and it makes D4 more symmetrical than any
of its siblings. When n 6= 4, the symmetries of Dn are simply permutations and
sign changes of the coordinates, while D4 has all those plus triality.

5. Diﬃculty of sphere packing

Why is the sphere packing problem hard? There are several reasons for this. One
is that there are many local optima. For example, among lattices in R8, there are
2408 local maxima for density [73]. This number seems to grow rapidly in high
dimensions, and it means the structure of the space of packings is complicated.

LECTURE 1. SPHERE PACKING

13

There is lots of space to move in, with complicated geometrical conﬁgurations, and
it is diﬃcult to rule out implausible conﬁgurations rigorously.

To get a feeling for the diﬃculties, it is useful to think about the geometry of

high dimensions. Let’s start by looking at the n-dimensional cube

{(x1, . . . , xn) | |xi| ≤ 1 for all i}

of side length 2. It has 2n vertices (±1, . . . ,±1), each at distance √12 + ··· + 12 =
√n from the center. When n = 106, the number of vertices is absurdly large, and
they are each 1000 units from the center, despite the fact that the side length is
only 2. These facts are amazingly diﬀerent from our intuition in low dimensions.
I like to imagine the vertices as vast numbers of tiny ﬁngers stretching out from
the center of the cube. I ﬁnd it diﬃcult to imagine that the result is convex, but
somehow a million dimensions has enough space to accommodate such a convex
body. The reason why cubes pack much better than spheres is that the vertices
stick out far enough to ﬁll in all the gaps.

One of the most important insights in high-dimensional geometry is the fol-
lowing principle: almost all the volume of a high-dimensional body is concentrated
near its boundary. To see why, imagine shrinking such a body by 1%, leaving just
a thin fringe near the boundary. The volume of the shrunken copy is lower by a
factor of (99/100)n, which tends exponentially to zero as n → ∞. Thus, virtually
all of the volume lies in that boundary fringe. There is of course nothing special
about 1%. The appropriate shrinkage scale in Rn to capture a constant fraction of
the volume is on the order of 1/n, because (1 − c/n)n converges to e−c.
Boundaries, where all the interaction takes place, become increasingly impor-
tant as dimension rises. This helps explain the diﬃculty of sphere packing, because
avoiding overlap is all about interaction along boundaries.

This principle reverses our intuition from low dimensions. We typically think
of boundaries as small and exceptional, but in high dimensions there’s practically
nothing but boundary, and this changes everything. For example, suppose we are
analyzing a numerical algorithm that uses many variables and thus operates in a
high-dimensional space. If it works eﬃciently throughout a certain region except
near the boundary, then that sounds good until we realize that almost all the region
is near the boundary.

6. Finding dense packings

In this section we’ll examine how record-setting sphere packings can be found. The
high and low-dimensional cases are handled very diﬀerently in practice. First, we’ll
look at the averaging techniques used in high dimensions, and then we’ll brieﬂy
discuss how computer searches can be used in low dimensions.

The key technique used in the most recent papers in high dimensions [77, 78]
is the Siegel mean value theorem [72], which lets us average suitable functions
over the space of lattices. To carry out such an averaging we need a probability
measure, and indeed there is a canonical probability measure on lattices with ﬁxed
determinant (i.e., fundamental cell volume). Speciﬁcally, it’s the unique SLn(R)-
invariant probability measure on this space. The existence of an SLn(R)-invariant
measure follows from general results on Haar measure [56], but it takes a calculation
to show that it has ﬁnite volume and can thus be normalized to yield a probability
measure.

over all lattices Λ of determinant 1 equals

f (x)

Xx∈Λ\{0}
ZRn

f (x) dx.

14

HENRY COHN, PACKING, CODING, AND GROUND STATES

Once we have this probability measure on lattices, we can ask various statistical
questions. For example, what does the average pair correlation function look like?
In other words, what can we say about the average number of neighbors at each
distance in a random lattice? The Siegel mean value theorem says that these
pair correlations are exactly the same as for a Poisson distribution (i.e., uniformly
scattered points). More precisely, it says that for a suﬃciently well-behaved function
f : Rn → R with n > 1, the average of

Intuitively, averaging over a random lattice blurs the sum into an integral.

The reason why the Siegel mean value theorem holds is that there is enough
symmetry to rule out any other possible answer. Speciﬁcally, by linearity the

answer must be R f dµ for some measure µ on Rn \ {0} that is invariant under

SLn(R). There is only one such measure up to scaling when n > 1 (given a few mild
hypotheses), and some consistency checks determine the constant of proportionality.
The meta principle here is that averaging over all possible structures is the
same as having no structure at all. Of course this is not always true. It generally
depends on invariance under the action of a large enough group, and SLn(R) is
more than large enough.

It is not hard to deduce lower bounds for sphere packing density from the Siegel
mean value theorem. The following proposition is far from the state of the art, but
it illustrates the basic technique.
Proposition 6.1. The sphere packing density in Rn is at least 2 · 2−n.

Proof. Let B be a ball of volume 2 centered at the origin. For a random lattice
of determinant 1, the expected number of nonzero lattice points in B is vol(B) = 2,
by applying the Siegel mean value theorem to the characteristic function of B.
These lattice points come in pairs (negatives of each other), so the number is
always even. Since the average number is 2 and some lattices have many, other
lattices must have none. Such a lattice gives a packing with one copy of B/2 per
unit volume and density

vol(B/2) =

vol(B)
2n = 2 · 2−n,

as desired.

(cid:3)

Vance’s key idea [77] builds on the extra factor of 2 that arises because lattice
vectors occur in pairs of the same length. What if we impose additional symmetry?
The intuition is that the average number of neighbors remains the same, but now
they occur in bigger clumps, and so the chances of no nearby neighbors go up.
Vance used lattices with quaternion algebras acting on them, and Venkatesh [78]
obtained even stronger results by using cyclotomic ﬁelds.

Is this the best we can do? Only certain symmetry groups work here: we need
a big centralizer to get enough invariance for the Siegel mean value theorem proof,
and only division algebras will do. Cyclotomic ﬁelds are the best division algebras

LECTURE 1. SPHERE PACKING

15

for this purpose [54]. Other sorts of groups will distort the pair correlation function
away from Poisson statistics, but that could be good or bad. The area is wide open,
and it is unclear which sorts of constructions might help.

In low dimensions, one can obtain much better results through numerical
searches by computer. Several recent papers [43, 53, 42] have taken this ap-
proach and recovered the densest lattices known in up to 20 dimensions. So far
the computer searches have not yielded anything new, but they seem to be on the
threshold of doing so. Can we push the calculations further, to unknown territory?
What about periodic packings?

7. Computational problems

Lattices may sound down to earth, but they are full of computational diﬃculties.
For example, given a lattice basis it is hard to tell how dense the corresponding
sphere packing is. The diﬃculty is that to compute the density, we need to know
both the volume of a fundamental cell and the packing radius of the lattice. The
former is just the absolute value of the determinant of a basis matrix, which is
easy to compute, but computing the packing radius is not easy. We can see why as
follows.

Recall that the packing radius is half the shortest nonzero vector length in the
lattice. The problem is that the basis vectors may not be the shortest vectors in the
lattice, because some linear combination of them could be much shorter. There are
exponentially many linear combinations that could work, and there is no obvious
way to search eﬃciently. In fact, computing the shortest vector length is NP-hard
[1]. In other words, many other search problems can be reduced to it. No proof is
known that it cannot be solved eﬃciently (this is the famous problem of whether
P = NP), but that is almost certainly the case.

There are good algorithms for “lattice basis reduction,” such as the LLL algo-
rithm [49, 58], and they produce pretty short vectors. These vectors are generally
far from optimal, but they are short enough for some applications, particularly in
relatively low dimensions.

Shortest vector problems and their relatives come up in a surprising range
of topics. One beautiful application is cryptography. We’ll brieﬂy discuss the
Goldreich-Goldwasser-Halevi cryptosystem [36], which turns out to have weak-
nesses [57] but is a good illustration of how lattice problems can be used to build
cryptosystems. It’s a public key cryptosystem, in which the public key is a basis for
a high-dimensional lattice, while the private key is a secret nearly orthogonal basis
for the same lattice, which makes it easy to ﬁnd the nearest lattice point to any
given point in space (while this problem should be hard for anyone who does not
know the secret basis). We encode messages as lattice points. Anyone can encrypt
a message by adding a small random perturbation, thereby moving it oﬀ the lattice.
Decryption requires ﬁnding the nearest lattice point, which has no obvious solution
without the private key. As mentioned above, this system is not as secure as it was
intended to be [57], but there are other, stronger lattice-based systems. See [61]
for a survey of recent work in this area.

Recognizing algebraic numbers is a rather diﬀerent sort of application. The

number

α = −7.82646099323767402929927644895

16

HENRY COHN, PACKING, CODING, AND GROUND STATES

is a 30-digit approximation to a root of a ﬁfth-degree polynomial equation. Which
equation is it? Of course there are inﬁnitely many answers, but Occam’s razor
suggests we should seek the simplest one. One interpretation of “simplest” is that
the coeﬃcients should be small.

For comparison,

0.1345345345345345345345345345345

is clearly an approximation to 1/10 + 345/9990, and no other answer is nearly as
satisfying.

To identify the number α given above, let C = 1020 (chosen based on the

precision of α), and look at the lattice generated by the vectors

v0 = (1, 0, 0, 0, 0, 0, C),
v1 = (0, 1, 0, 0, 0, 0, Cα),
v2 = (0, 0, 1, 0, 0, 0, Cα2),
v3 = (0, 0, 0, 1, 0, 0, Cα3),
v4 = (0, 0, 0, 0, 1, 0, Cα4),
v5 = (0, 0, 0, 0, 0, 1, Cα5).

The lattice vectors are given by

a0v0 + ··· + a5v5 = a0, a1, a2, a3, a4, a5, C  5Xi=0

aiαi!!

with a0, . . . , a5 ∈ Z. Such a vector is small when the coeﬃcients ai are small and
i=0 aiαi is tiny, since C is huge. Thus, ﬁnding a short vector amounts to
i=0 aixi with small coeﬃcients such that α is nearly a root.

If we search for a short vector using the LLL algorithm, we ﬁnd

the sumP5
ﬁnding a polynomialP5

This tells us that

(71, −5, 12, −19, 13, 2, 0.000004135 . . . ).

71 − 5α + 12α2 − 19α3 + 13α4 + 2α5 ≈ 0.

(More precisely, it is about 0.000004135/C ≈ 4· 10−26.) In fact, this is the equation
I used to generate α.
More generally, we can use lattices to ﬁnd integral linear relations between any
real numbers, not just powers of α. I ﬁnd it really remarkable that the same sort
of mathematics arises in this problem as in communication over a noisy channel.

Symmetry and ground states

LECTURE 2

1. Introduction

One of the beautiful phenomena in sphere packing is the occurrence of spontaneous
order. There seems to be no reason to expect that an optimal sphere packing should
be highly structured, but this happens time and again, with the precise structure
being diﬃcult to predict a priori.

These questions of order vs. disorder ﬁt into a broader context. Where do
symmetry and structure come from? L´aszl´o Fejes T´oth played an important role in
formulating and attracting attention to this question. He drew a distinction between
the systematology of the regular ﬁgures, which amounts to classifying the possible
symmetries that could occur, and the genetics of the regular ﬁgures, which studies
when and why they do occur. He sought to explain the genetics of the regular
ﬁgures via optimization principles, and he made considerable progress towards this
goal. In his vision [34, p. x], “regular arrangements are generated from unarranged,
chaotic sets by the ordering eﬀect of an economy principle, in the widest sense of
the word.”

Typically the optimization problem has certain symmetries, but it is far from
obvious when its solutions will inherit these symmetries. Steiner trees are an at-
tractive illustration of this issue. What is the minimal-length path connecting the
vertices of a square? One obvious guess is an X, which inherits all the symmetries
of the square:

However, the optimal solution turns out to look like this, or its rotation by 90◦:

There is partial symmetry breaking, in that the set of all solutions is of course
invariant under the full symmetry group of the square, but each individual solution
is invariant under just a subgroup.

This behavior occurs generically for optimization problems. For example, in
the sphere packing problem the full symmetry group of the optimization problem
consists of all rigid motions of Euclidean space, while each optimal sphere packing

17

18

HENRY COHN, PACKING, CODING, AND GROUND STATES

will be invariant under a much smaller subgroup, consisting of just a discrete set
of motions. The diﬃculty lies in predicting what that subgroup will be. Which
materials crystallize beautifully, and which remain amorphous?

From this perspective, we would like to understand which optimization prob-
lems admit highly symmetrical solutions, such as lattices or regular polytopes. Can
we explain why E8 and the Leech lattice are so much more symmetrical than the
best packing known in R10?

2. Potential energy minimization

There’s no hope of developing a comprehensive theory of symmetry in optimization
problems, because optimization is just too broad a topic. If you choose an arbitrary
function to optimize, then you can make literally anything happen to the optima.
To make progress, we must restrict the class of functions under consideration. In
this lecture we will take a look at point particles with pairwise forces acting on
them.

Given a collection of particles interacting according to some potential function,
what do they do? For example, the Thomson problem deals with charged particles
on the surface of the unit sphere S2 in R3. Each pair of particles at Euclidean
distance r has potential energy 1/r, and the total potential energy is the sum over
all the pairs.

The simplest question is what the ground states are. In other words, what are
the minimal-energy conﬁgurations? They describe the behavior of the system at
zero temperature. This is a simple question, but the ground states in the Thomson
problem are far from obvious, and in fact not fully known in general.

More generally, we can ask about dynamics or the behavior at positive temper-
ature. These questions are more subtle, and we will generally restrict our attention
to ground states. After all, if we can’t even understand the ground states, then
there is little hope of analyzing anything more involved than that.

Before we restrict our attention to ground states, though, it’s worth putting
everything in the context of Gibbs measures. They are a canonical way of putting a
probability measure on the states of a system based on nothing except their energies
and the system’s temperature. Of course one can’t possibly capture the behavior
of every system based on so little information, but Gibbs measures do a good job of
describing a system that is in equilibrium with a heat bath (a neighboring system
that is so much larger that its temperature is unaﬀected by the smaller system).

For simplicity, imagine that our system has only n states, labeled 1 through n,
where state i has energy Ei. (To handle continuous systems we can simply replace
If we are given the average energy E of the
sums over states with integrals.)
system, we determine the corresponding probability distribution on the states by

ﬁnding probabilities p1, . . . , pn so thatPi piEi = E and the entropy Pi −pi log pi

is maximized, where we interpret 0 log 0 as 0.
disordered as possible, subject to having a certain average energy.

In other words, the system is as

It is not diﬃcult to solve this optimization problem via Lagrange multipliers,

and the result is that

for some constants α and β. Thus, we can write

log(1/pi) = α + βEi

pi =

e−βEi

Z

,

LECTURE 2. SYMMETRY AND GROUND STATES

19

where the partition function Z =Pi e−βEi ensures thatPi pi = 1 (it is also eα).

Such a probability distribution is called a Gibbs distribution.

In physics terms, β turns out to be proportional to the reciprocal of tempera-
ture. As the temperature tends to zero, β tends to inﬁnity and the Gibbs distri-
bution becomes concentrated on the ground states. As the temperature tends to
inﬁnity, β tends to zero and the Gibbs distribution becomes equidistributed among
all the states.

One question we have not yet addressed is whyPi −pi log pi deserves the name

entropy. In fact, it is an excellent measure of disorder, essentially because it mea-
sures how surprising the probability distribution is on average. Consider how sur-
prised we should be by an event of probability p. Call this surprise function S(p),
and think of it as a measure of how much you learn from seeing this event happen.
(Information theory makes this intuition precise.)

Clearly S should be a decreasing function: the higher the probability is, the
less surprising it is and the less you learn from seeing it happen. Furthermore, we
should have

S(pq) = S(p) + S(q).

In other words, the amount you learn from independent events is additive. This
makes good sense intuitively: if you learn one bit of information from a coin ﬂip,
then you learn two bits from two independent coin ﬂips.

These conditions uniquely determine the function S up to a constant factor, as

S(p) = − log p. Now the entropy isPi piS(pi), and this quantity measures disorder

by telling us how surprised we’ll be on average by the outcome.

Part of the beauty of mathematics is that concepts are connected in ways one
would never guess. Gibbs measures are not just a construction from statistical
physics, but rather occur throughout mathematics. For example, Dyson recognized
that they describe eigenvalues of random matrices [31], as follows.

Haar measure gives a canonical probability measure on the unitary group U (n).
What does a random n × n unitary matrix chosen from this distribution look like?
It has n eigenvalues z1, . . . , zn on the unit circle, and the Weyl integral formula tells
us that the probability density function for these eigenvalues is proportional to

Yi<j

|zi − zj|2.

If we call the constant of proportionality 1/Z, then we can rewrite this formula as

−2 Pi<j log

1

|zi−zj | .

1
Z

e

In other words, the eigenvalue distribution of a random unitary matrix is a Gibbs
distribution for a certain potential function between the eigenvalues. Speciﬁcally,
they repel each other according to the potential function x 7→ log(1/|x|). This
function is harmonic on R2 \ {(0, 0)}, just as the Coulomb potential x 7→ 1/|x| is
harmonic on R3 \ {(0, 0, 0)}. Thus, the eigenvalues of a random unitary matrix
literally repel each other via electrostatic interactions in two dimensions, with the
2 in the exponent specifying the temperature of this system.

3. Families and universal optimality

Given that we are going to study particles interacting via pairwise potential func-
tions, what do we hope to learn from it? There are many possibilities:

20

HENRY COHN, PACKING, CODING, AND GROUND STATES

(1) We may care about the ground states for their own sake, as part of pure

mathematics or physics (see [11] for many examples in physics).

(2) We may seek a highly uniform point distribution so that we can discretize

the ambient space.

(3) We may wish to construct error-correcting codes by letting the codewords

repel each other, so that they become well separated.

(4) We may seek well-distributed sample points for numerical integration.

To account for these and other goals, we will have to look at a broad range of
potential functions.

There are also many spaces we could work in, such as spheres, projective spaces,
Grassmannians, Euclidean spaces, hyperbolic spaces, and even discrete spaces such
as the Hamming cube {0, 1}n. All of these possibilities are interesting, but in this
lecture we will focus on spheres. (For comparison, [24] and [20] examine spaces
that are rather diﬀerent from spheres.)

Thus, we will focus on the question of what energy minima on spheres look like
for a variety of potential functions. As we vary the potential function, how do the
optimal conﬁgurations change? They vary in some family, and we would like to
understand these families. Note that our perspective here is broader than is typical
for physics, where the potential function is usually ﬁxed in advance.

The simplest case is that the optimal conﬁgurations never vary, at least for rea-
sonable potential functions, such as inverse power laws.1 For example, 4 points on
S2 always form a regular tetrahedron. Abhinav Kumar and I named this property
universal optimality [17].

More generally, we can ask for a parameter count for the family, which is 0
for universal optima. As we vary the potential function (say, among all smooth
functions), what is the dimension of the space of conﬁgurations attained as ground
states? There is little hope of proving much about this quantity in general. How-
ever, we can try to estimate it from numerical data [8]. These parameter counts
can be diﬃcult to predict, because they take into account how well the number
of points accommodates diﬀerent sorts of symmetry. For example, 44 points on
S2 vary in a one-parameter family near the putative Coulomb minimizer when we
perturb the potential function, while 43 points vary in a 21-parameter family. See
Figure 1 for an illustration. What this means is that the 44-point conﬁguration is
nearly determined by symmetry, with just one degree of freedom remaining to be
speciﬁed by the choice of potential function, while the 43-point conﬁguration is far
more complex.

To give a precise deﬁnition of universal optimality, we must specify the class of
potential functions. For a ﬁnite subset C ⊂ Sn−1 and a function f : (0, 4] → R, we
deﬁne the energy of C with respect to the potential function f to be

1

Ef (C) =

2 Xx,y∈C

x6=y

f(cid:0)|x − y|2(cid:1).

The factor of 1/2 simply corrects for counting each pair twice and is not important.
The use of squared Euclidean distance similarly doesn’t matter in principle, since

1Of course it is impossible for a conﬁguration of more than one point to be a ground state for
literally every potential function, since minimizing f is the same as maximizing −f . We must
restrict the class of potential functions at least somewhat.

LECTURE 2. SYMMETRY AND GROUND STATES

21

43 points, 21 parameters

Klein four-group symmetry

44 points, 1 parameter

cubic symmetry

(ﬁxed point = double circle, orbit black)

(square faces shaded)

Figure 1. Putative ground states for Coulomb energy on S2, and
the number of parameters for the families they lie in.

the squaring could be incorporated into the potential function, but it turns out to
be a surprisingly useful convention.

A function f is completely monotonic if it is inﬁnitely diﬀerentiable and

(−1)kf (k) ≥ 0

for all k ≥ 0 (i.e., its derivatives alternate in sign, as in inverse power laws). We
say C is universally optimal if it minimizes Ef (C) for all completely monotonic f ,
compared with all |C|-point conﬁgurations on Sn−1.
It’s not obvious that completely monotonic functions are the right class of
functions to use, but they turn out to be. The fact that f is decreasing means the
force is repulsive, and convexity means the force grows stronger at short distances.
Complete monotonicity is a natural generalization of these conditions, and the
results and examples in [17] give evidence that it is the right generalization (see
pages 101 and 107–108). Note in particular that inverse power laws are completely
monotonic, so universal optima must minimize energy for all inverse power laws.

In the circle S1, there is a universal optimum of each size, namely the regular
polygon. This is not as straightforward to prove as it sounds, but it follows from
Theorem 1.2 in [17], which we will state as Theorem 3.3 in the fourth lecture. In
S2, the complete list of universal optima with more than one point is as follows:

(1) Two antipodal points (2 points)
(2) Equilateral triangle on equator (3 points)
(3) Regular tetrahedron (4 points)
(4) Regular octahedron (6 points)
(5) Regular icosahedron (12 points)

See Figure 2. Universal optimality again follows from Theorem 1.2 in [17] (after
special cases were proved in [81, 44, 2, 45, 3]), while completeness follows from a
theorem of Leech in [48].

22

HENRY COHN, PACKING, CODING, AND GROUND STATES

tetrahedron
4 vertices

octahedron
6 vertices

icosahedron
12 vertices

Figure 2. Platonic solids whose vertices form universally optimal codes.

The cube and regular dodecahedron are conspicuously missing from this list.
The cube cannot be universally optimal, because rotating one face moves its corners
further from those of the opposite face, and the dodecahedron fails similarly. Square
and pentagonal faces are not particularly favorable shapes for energy minimization,
although cubes and dodecahedra can occur for unusual potential functions [19].

Five points are the ﬁrst case without universal optimality, and they are surpris-
ingly subtle. There are two natural ways to arrange the particles: we could include
the north and south poles together with an equilateral triangle on the equator (a
triangular bipyramid ), or the north pole together with a square at constant lati-
tude in the southern hemisphere (a square pyramid ). The square pyramid lies in
a one-parameter family, where the latitude of the square depends on the choice of
potential function. By contrast, the triangular bipyramid is in equilibrium for every
potential function, but it becomes an unstable equilibrium for steep inverse power
laws.

Conjecture 3.1. For each completely monotonic potential function, either the tri-
angular bipyramid or a square pyramid minimizes energy for 5 points in S2.

This conjecture really feels like it ought to be provable. Specifying ﬁve points
on S2 takes ten degrees of freedom, three of which are lost if we take the quotient
by symmetries. Thus, we are faced with a calculus problem in just seven variables.
However, despite a number of partial results [30, 67, 10, 68], no complete solution
is known.

The known universal optima in spheres are listed in Table 1. Each of them is
an exciting mathematical object that predates the study of universal optimality.
For example, the 27 points in R6 correspond to the classical conﬁguration of 27
lines on a cubic surface. One way of thinking about universal optimality is that it
highlights similarities between various exceptional structure and helps characterize
what’s so special about them. See [17] for descriptions of these objects and how
they are related. We’ll discuss the proof techniques in the fourth lecture, while [17]
contains detailed proofs.

One important source of universal optima is regular polytopes, the higher-
dimensional generalizations of the Platonic solids. As in three dimensions, only
some of them are universally optimal, speciﬁcally the ones with simplicial facets.2

2Surprisingly, the minimal vectors of D4 are not universally optimal [14], despite their beauty and
symmetry. They are the vertices of a regular polytope with octahedral facets, called the regular
24-cell.

LECTURE 2. SYMMETRY AND GROUND STATES

23

Table 1. Known universal optima with N points on Sn−1.

n

2
n
n
3
4
5
6
7
8
21
21
22
22
22
23
23
24

N

N

Description

N -gon

N ≤ n + 1

2n
12
120
16
27
56
240
112
162
100
275
891
552
4600

196560

simplex (generalized tetrahedron)

cross polytope (generalized octahedron)

icosahedron

regular 600-cell

hemicube

Schl¨aﬂi graph

equiangular lines
E8 root system

isotropic subspaces

strongly regular graph

Higman-Sims graph
McLaughlin graph
isotropic subspaces
equiangular lines

kissing conﬁguration of next line

Leech lattice minimal vectors

q(q3 + 1)/(q + 1)

(q + 1)(q3 + 1)

isotropic subspaces (q is a prime power)

The shortest vectors in the E8 lattice (called the E8 root system) also form a
universally optimal conﬁguration, as do the shortest vectors in the Leech lattice.

It is diﬃcult to depict high-dimensional objects on a two-dimensional page,
but Figure 3 shows how the E8 root system appears when viewed from random
directions. It is so regular and symmetrical that even these random views display
considerable structure. For comparison, Figure 4 shows similar projections of a
random point conﬁguration.

In up to 24 dimensions, all of the known universal optima are regular polytopes
or cross sections of the E8 or Leech conﬁgurations. However, the last line of Table 1
shows that there are more examples coming from ﬁnite geometry. It’s not plausible
that Table 1 is the complete list of universal optima, and in fact [8] constructs two
conjectural examples (40 points in R10 and 64 points in R14), but it seems diﬃcult
to ﬁnd or analyze further universal optima.

The gap between 8 and 21 dimensions in Table 1 is puzzling. Are the dimensions
in between not favored by universal optimality, or do we just lack the imagination
to construct new universal optima in these dimensions?

4. Optimality of simplices

It is not diﬃcult to explore energy minimization via numerical optimization, but it
is far from obvious how to prove anything about it. Developing proof techniques
will occupy most of the remaining lectures, and we will start here by analyzing
regular simplices, i.e., conﬁgurations of equidistant points.

In particular, we will study the spherical code problem: how can we maximize
the closest distance between N points on the unit sphere Sn−1? This is an important

24

HENRY COHN, PACKING, CODING, AND GROUND STATES

Figure 3. Four views of the E8 root system, after orthogonal
projection onto randomly chosen planes.

Figure 4. A random 240-point conﬁguration in S7, orthogonally
projected onto randomly chosen planes.

problem in both geometry and information theory.3 It is a version of the sphere
packing problem in spherical geometry, i.e., for spherical caps on the surface of a
sphere. Furthermore, it is a degenerate case of energy minimization. If we look
at the limit of increasingly steep potential functions, then asymptotically only the
minimal distance matters and we obtain an optimal spherical code.

3If we represent radio signals by vectors in Rn by measuring the amplitudes at diﬀerent frequencies,
then the squared vector length is proportional to the power of the radio signal. If we transmit
a constant-power signal, then we need an error-correcting code on the surface of a sphere, i.e., a
spherical code.

LECTURE 2. SYMMETRY AND GROUND STATES

25

When N ≤ n + 1, we will see shortly that the optimal solution is a regular
simplex. In other words, the points are all equidistant from each other, forming
an n-dimensional analogue of the equilateral triangle or regular tetrahedron. The
cutoﬀ at n + 1 simply reﬂects the fact that Rn cannot contain more than n + 1
equidistant points.

Let hx, yi denote the inner product of x and y. Inner products can be used to

measure distances on the unit sphere, since

|x − y|2 = hx − y, x − yi = |x|2 + |y|2 − 2hx, yi = 2 − 2hx, yi

when |x| = |y| = 1. Thus, maximizing the distance |x − y| is equivalent to mini-
mizing the inner product hx, yi.
Note that if x1, . . . , xN are unit vectors forming the vertices of a regular simplex
centered at the origin, then all the inner products between them must be −1/(N−1).
To see why, observe that x1 + ··· + xN = 0 and hence

0 = |x1 + ··· + xN|2 = N +Xi6=j

hxi, xji,

while all the N (N−1) inner products in this sum are equal. This calculation already
contains all the ingredients needed to prove that regular simplices are optimal
spherical codes:
Proposition 4.1. If N ≤ n + 1, then the unique optimal N -point spherical code in
Sn−1 is the regular simplex centered at the origin.

Of course it is unique only up to rigid motions.

Proof. Suppose x1, . . . , xN are points on Sn−1. The fundamental inequality

we’ll use is

2

≥ 0.

hxi, xji ≥ 0,

xi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
NXi=1
N +Xi6=j
N (N − 1)Xi6=j

1

hxi, xji ≥ −

1

N − 1

.

Using |xi|2 = 1, this inequality expands to

which amounts to

In other words, the average inner product is at least −1/(N − 1), and hence the
maximal inner product (which corresponds to the minimal distance) must be at least

that large. Equality holds iﬀ all the inner products are the same andPi xi = 0.

This condition is equivalent to all the points being equidistant with centroid at the
origin, which can be achieved iﬀ N ≤ n + 1.
(cid:3)
Exercise 4.2. Prove that regular simplices are universally optimal, and more gen-
erally that they minimize Ef for every decreasing, convex potential function f .

Our discussion here may give the impression that the existence of regular sim-
plices is trivial, while their optimality is a little more subtle. This impression is
reasonable for Euclidean space, but in projective spaces or Grassmannians the ex-
istence of regular simplices is far more mysterious. See, for example, [20].

26

HENRY COHN, PACKING, CODING, AND GROUND STATES

The inequality

is useful for analyzing simplices, but it is not obvious at a glance what its signiﬁcance
is or how it ﬁts into a broader theory. It turns out to be a special case of Delsarte’s
linear programming bounds, which are also equivalent to the nonnegativity of the
structure factor in statistical physics. In the upcoming lectures, we’ll look at these
connections. The fundamental theme will be geometrical constraints on correlation
functions.

NXi=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≥ 0

2

xi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Interlude: Spherical harmonics

LECTURE 3

Spherical harmonics are a spherical generalization of Fourier series and a fun-
damental tool for understanding particle conﬁgurations on the surface of a sphere.
Despite their importance in mathematics, they are not nearly as well known as
Fourier series are, so this lecture will be devoted to the basic theory. We’ll begin
with a quick review of Fourier series, to establish notation and fundamental con-
cepts, and then we’ll do the same things in higher dimensions. Our discussion will
start oﬀ in a rather elementary fashion, but then gradually increase in sophistica-
tion. We won’t go through complete proofs of basic facts such as convergence of
Fourier series under the L2 norm, but we will at least see an outline of what is true
and why, to a level of detail at which the proofs could be completed using standard
facts from introductory graduate classes.

1. Fourier series

We will identify the circle S1 with the quotient R/2πZ via arc length (i.e., the
quotient of the real line in which we wrap around after 2π units). In other words,
a function on the circle is the same as a function on R with period 2π.

We know from basic analysis that every suﬃciently nice function f from R/2πZ

to C can be expanded in a Fourier series

(1.1)

f (x) =Xk∈Z

akeikx.

Of course we could replace the complex exponentials with trigonometric functions
by writing eikx = cos kx + i sin kx, but the exponentials will be more pleasant.

The coeﬃcients aℓ are determined by orthogonality via

because we can interchange the sum (1.1) with the integral and apply

f (x)e−iℓx dx,

aℓ =

0

1

2πZ 2π
ei(k−ℓ)x dx =(1

0

if k = ℓ, and
otherwise.

(1.2)

1

2πZ 2π

0

on S1, i.e.,

The right setting for Fourier series is the space of square-integrable functions

L2(S1) =(cid:26)f : R/2πZ → C(cid:12)(cid:12)(cid:12) Z 2π

0

|f (x)|2 dx < ∞(cid:27) .

This is a Hilbert space under the inner product h·,·i deﬁned by

hf, gi =

f (x)g(x) dx,

1

2πZ 2π

0

27

28

HENRY COHN, PACKING, CODING, AND GROUND STATES

which corresponds to the L2 norm k · k2 deﬁned by

kfk2 =phf, fi =s 1

2πZ 2π

0

|f (x)|2 dx.

The exponential functions are orthonormal in L2(S1): if fk is the function deﬁned
by fk(x) = eikx, then (1.2) amounts to

hfk, fℓi =(1

0

if k = ℓ, and
otherwise.

Furthermore, these functions form an orthonormal basis of L2(S1).

We can express this fact algebraically as follows. If Vk consists of the complex

multiples of the function fk, then

L2(S1) =dMk∈Z

Vk.

(Here ⊕ is the orthogonal direct sum. The hat indicates a Hilbert space completion;
without the hat, the direct sum would contain only sums of ﬁnitely many expo-
nentials.) In other words, the partial sums of the Fourier series of an L2 function
converge to that function under the L2 norm. However, it’s important to keep in
mind that they needn’t converge pointwise.

The most important property of the decomposition

L2(S1) =dMk∈Z

Vk.

is that it is compatible with the symmetries of S1 (i.e., the rigid motions that
preserve S1), as we will see shortly. Recall that the symmetry group O(2) of
S1 consists of rotations and reﬂections that ﬁx the center of the circle, with the
subgroup SO(2) consisting of just the rotations. The notation is based on the fact
that these symmetries can be written in terms of orthogonal matrices, but we do
not need that perspective here.

Each symmetry g of S1 acts on functions f : S1 → C by sending f to the
function gf deﬁned by (gf )(x) = f (g−1x). The inverse ensures that the associative
law (gh)f = g(hf ) holds. For motivation, recall that moving the graph of a function
f (x) one unit to the right amounts to graphing f (x − 1), not f (x + 1). Similarly,
the graph of gf is simply the graph of f transformed according to g.
Under this action, L2(S1) is a representation of the group O(2). In other words,
the group O(2) acts on L2(S1) by linear transformations. In fact, it is a unitary
representation, which means that symmetries of S1 preserve the L2 norm. We
would like to decompose L2(S1) into irreducible representations of O(2) or SO(2).
In other words, we would like to break it apart into orthogonal subspaces preserved
by these groups, with the subspaces being as small as possible.

For the rotation group SO(2), we’re already done.

In the R/2πZ picture,
rotations of S1 correspond to translations of R. The exponential functions are
already invariant: if we translate x 7→ eikx by t, we get

eik(x−t) = e−ikteikx,

LECTURE 3. INTERLUDE: SPHERICAL HARMONICS

29

which is the original function x 7→ eikx multiplied by the constant e−ikt. In other
words, Vk is itself a representation of SO(2), and

L2(S1) =dMk∈Z

Vk

is the complete decomposition of L2(S1) under this group action. Each summand
must be irreducible, since it’s one-dimensional.

There are many ways to restate this decomposition, such as:

(1) The Fourier basis simultaneously diagonalizes the translation operators

on L2(R/2πZ) (i.e., rotations of L2(S1)).

(2) The exponential functions are simultaneous eigenfunctions for the trans-

lation operators.

It turns out that the reason why the Fourier decomposition is particularly simple,
with one-dimensional summands, is that the rotation group SO(2) is abelian.

But what about the full symmetry group O(2)? It is generated by SO(2) and
any one reﬂection, because all the reﬂections are conjugate by rotations. In the
R/2πZ picture, we can use the reﬂection x 7→ −x. The nonconstant exponential
functions are not preserved by this reﬂection, because it takes x 7→ eikx to x 7→
e−ikx. In other words, it interchanges k with −k.
Instead of keeping the representations Vk and
V−k separate, we combine them to form Wk = Vk ⊕ V−k when k > 0 (while we
take W0 = V0). Now Wk is the span of x 7→ eikx and x 7→ e−ikx, or equivalently
x 7→ cos kx and x 7→ sin kx if we expand e±ikx = cos kx ± i sin kx. These spaces
Wk are preserved by O(2), because this group is generated by SO(2) and x 7→ −x.
Thus, the decomposition of L2(S1) into irreducible representations of O(2) is

However, this is no big deal.

L2(S1) =dMk≥0

Wk.

This decomposition is just slightly more complicated than the one for SO(2), be-
cause dim Wk = 2 when k > 0.

Another way to think of this equation is as the spectral decomposition of the

Laplacian operator d2/dx2. Speciﬁcally,

d2
dx2 eikx = −k2eikx.

Thus, Wk is the eigenspace with eigenvalue −k2. The Laplacian plays a fundamental
role, since it is invariant under the action of O(2). (In other words, translating or
reﬂecting a function commutes with taking its Laplacian.) In fact, the Laplacian
generates the algebra of isometry-invariant diﬀerential operators on S1, but that’s
going somewhat far aﬁeld from anything we will need.

2. Fourier series on a torus

The S1 theory generalizes pretty straightforwardly if we think of S1 as a one-
dimensional torus. We can view a higher-dimensional ﬂat torus as Rn/Λ, where
Λ is a lattice in Rn. In other words, we simply take a fundamental cell for Λ and
wrap around whenever we cross the boundary. When we looked at S1, we wrote it
as R1/Λ with Λ = 2πZ, and it’s worth keeping this example in mind.

30

HENRY COHN, PACKING, CODING, AND GROUND STATES

We can decompose L2(Rn/Λ) into exponential functions in exactly the same
way as we did for S1. It works out particularly simply since Rn/Λ is an abelian
group. To write this decomposition down, we need to ﬁgure out which exponential
functions are periodic modulo Λ. Suppose y ∈ Rn, and consider the exponential
function

from Rn to C. Here h·,·i denotes the usual inner product on Rn (not the inner
product on functions used in the previous section). This formula deﬁnes a function
on Rn/Λ if and only if it is invariant under translation by vectors in Λ.

x 7→ e2πihx,yi

What happens if we translate the function x 7→ e2πihx,yi by a vector z? It
gets multiplied by e−2πihz,yi, and so it is always an eigenfunction of the translation
operator. Furthermore, it is invariant under translation by vectors in Λ if and only
if y satisﬁes

e2πihz,yi = 1

Let

for all z ∈ Λ, which is equivalent to hz, yi ∈ Z for all z ∈ Λ.
Λ∗ = {y ∈ Rn | hz, yi ∈ Z for all z ∈ Λ}

be the dual lattice to Λ. Thus, the exponential functions that are periodic modulo
Λ are parameterized by Λ∗.

Exercise 2.1. Prove that Λ∗ is a lattice. Speciﬁcally, prove that if v1, . . . , vn is
any basis of Λ, then Λ∗ has v∗
n as a basis, where these vectors are the dual
basis vectors satisfying

1 , . . . , v∗

j i =(1

0

hvi, v∗

if i = j, and
otherwise.

Deduce also that (Λ∗)∗ = Λ.

Let Vy be the complex multiples of x 7→ e2πihx,yi. Then

L2(Rn/Λ) = dMy∈Λ∗

Vy,

which is the decomposition into irreducible representations under the translation
action.

When n = 1, the lattice Λ is determined up to scaling. In the previous section
we took Λ = 2πZ, in which case Λ∗ = (2π)−1Z. The elements of Λ∗ are (2π)−1k,
where k is an integer, and V(2π)−1k is spanned by x 7→ eikx. Thus, we recover exactly
the same theory as in the previous section, except that we now write V(2π)−1k instead
of Vk. It’s arguably prettier to take Λ = Z and use the functions x 7→ e2πikx, but
this is a matter of taste.
The higher-dimensional analogue of the O(2) theory is a little more subtle.
The map x 7→ −x is always a symmetry of Rn/Λ, and taking it into account means
combining Vy with V−y as before. Generically, all the symmetries of Rn/Λ are
generated by translations and x 7→ −x. However, particularly nice lattices may
have further symmetries. If G is the automorphism group of the lattice itself, then
the full group of isometries of Rn/Λ is the semidirect product of G with the additive
group Rn/Λ. What eﬀect this has on the decomposition of L2(Rn/Λ) depends on
the representation theory of G. However, for many purposes this is not important,
and the decomposition under translations alone will suﬃce.

LECTURE 3. INTERLUDE: SPHERICAL HARMONICS

31

3. Spherical harmonics

If we think of S1 as a one-dimensional sphere, rather than a one-dimensional torus,
then it is less clear how to generalize Fourier series. Instead of exponential functions,
we’ll have to use spherical harmonics.

The symmetry group of the unit sphere

is the orthogonal group O(n), which consists of n × n orthogonal matrices. As
before, L2(Sn−1) is a Hilbert space under the inner product

Sn−1 = {x ∈ Rn | |x|2 = 1}

hf, gi =ZSn−1

f (x)g(x) dx,

where the integral is taken with respect to the surface measure on Sn−1, and
L2(Sn−1) is a unitary representation of O(n). We would like to decompose it
into irreducible representations of O(n).

To get a handle on L2(Sn−1), we will study the polynomials on Sn−1. Let Pk
be the subset of L2(Sn−1) consisting of polynomials on Rn of total degree at most
k. (Strictly speaking, it consists of the restrictions of these polynomials to Sn−1,
since two diﬀerent polynomials can deﬁne the same function on the unit sphere.)
Then

P0 ⊆ P1 ⊆ P2 ⊆ . . . ,

and each Pk is a representation of O(n). To see why, note that rotating or reﬂecting
a polynomial gives another polynomial of the same degree; in fact, this is true for
any invertible linear transformation.

Let W0 = P0, and for k > 0 let Wk be the orthogonal complement of Pk−1 in
Pk. Then Wk is a representation of O(n), because Pk−1 and Pk are representations
and the inner product in L2(Sn−1) is O(n)-invariant. Iterating this decomposition
shows that

Furthermore,Sk Pk is dense1 in L2(Sn−1), and hence

Pk = W0 ⊕ W1 ⊕ ··· ⊕ Wk.

L2(Sn−1) =dMk≥0

Wk.

We have thus decomposed L2(Sn−1) into ﬁnite-dimensional representations of O(n).
In fact they are irreducible, as we will see in the next lecture, but that fact is by
no means obvious.

This decomposition may sound abstract, but it’s actually quite elementary,
since it is simply given by polynomials. Let’s check that it agrees with what we did
for S1. Polynomials on R2 can be written in terms of the coordinate variables x
and y, and in the R/2πZ picture we have x = cos θ and y = sin θ with θ ∈ R/2πZ.
Thus, Pk consists of polynomials of degree at most k in the functions θ 7→ cos θ and
θ 7→ sin θ. If we write cos θ = (eiθ + e−iθ)/2 and sin θ = (eiθ − e−iθ)/(2i), then we
ﬁnd that the elements of Pk involve powers of eiθ ranging from −k to k, and every
such power is in Pk. In other words,

Pk = V−k ⊕ V−(k−1) ⊕ ··· ⊕ Vk−1 ⊕ Vk

1Continuous functions are dense in L2(S n−1), and the Stone-Weierstrass theorem tells us that
polynomials are dense in the space of continuous functions.

32

HENRY COHN, PACKING, CODING, AND GROUND STATES

in the notation from §1. In particular, the orthogonal complement Wk of Pk−1 in
Pk is indeed V−k ⊕ Vk when k > 0, which agrees with our previous construction.
Returning to L2(Sn−1), we call the elements of Wk spherical harmonics of
degree k. Note that the word “harmonic” generalizes the term from music theory
for a note whose frequency is an integer multiple of the base frequency; this term
literally describes Wk when n = 2, and it is applied by analogy in higher dimensions.
Writing Wk down explicitly is a little subtle, because two diﬀerent polynomials
on Rn can restrict to the same function on Sn−1. For example, x2
n and 1
are indistinguishable on the unit sphere. To resolve this ambiguity, we will choose
a canonical representative for each equivalence class.

1 + ··· + x2

Lemma 3.1. For each polynomial on Rn, there is a unique harmonic polynomial
on Rn with the same restriction to Sn−1.

Recall that harmonic means ∆g = 0, where

∆ =

∂2
∂x2
1

+ ··· +

∂2
∂x2
n

is the Laplacian on Rn. If f is a harmonic polynomial with f|Sn−1 = g|Sn−1, then
f is called a harmonic representative for g.
The main fact we’ll need about harmonic functions is the maximum principle
[4, p. 7]: the maximum of a harmonic function on a domain D cannot occur in the
interior of D (instead, it must occur on the boundary). Of course, multiplying the
function by −1 shows that the same is true for the minimum.

Proof. Uniqueness follows immediately from the maximum principle: if

g1|Sn−1 = g2|Sn−1

with both g1 and g2 harmonic, then g1 − g2 is a harmonic function that vanishes
on Sn−1. It must therefore vanish inside the sphere as well (since its minimum and
maximum over the ball must be attained on the sphere), which implies that g1 = g2
because they are polynomials.

Proving existence of a harmonic representative is only slightly trickier. Let Qk
denote the space of polynomials of degree at most k on Rn. Note that the diﬀerence
between Pk and Qk is that Pk consists of the restrictions to Sn−1, and thus Pk is
the quotient of Qk by the polynomials whose restrictions vanish. Multiplication by
x2
1 + ··· + x2
n − 1 maps Qk−2 injectively to Qk, and its image vanishes on Sn−1, so

On the other hand, ∆ maps Qk to Qk−2, and hence

dimPk ≤ dimQk − dim Qk−2.

dim ker ∆|Qk ≥ dimQk − dim Qk−2 ≥ dimPk.

By uniqueness, the restriction map from ker ∆|Qk to Pk is injective, and thus the
inequality dim ker ∆|Qk ≥ dimPk implies that each polynomial in Pk must have a
harmonic representative (and dim ker ∆|Qk = dim Pk).

(cid:3)

Another way to understand spherical harmonics is as eigenfunctions of the
spherical Laplacian ∆Sn−1, which acts on C2 functions on the sphere (i.e., twice
continuously diﬀerentiable functions). The right setting for this operator is the
theory of Laplace-Beltrami operators in Riemannian geometry, but we can give a

LECTURE 3. INTERLUDE: SPHERICAL HARMONICS

33

quick, ad hoc deﬁnition as follows. Given a function f on Sn−1, extend it to a
radially constant function fradial on Rn \ {0}. Then we deﬁne ∆Sn−1 by

∆Sn−1f =(cid:0)∆fradial(cid:1)(cid:12)(cid:12)Sn−1.

In other words, ∆Sn−1 f measures the Laplacian of f when there is no radial change.
It is often notationally convenient to extend the operator ∆Sn−1 to apply to
functions f : Rn \ {0} → R, rather than just functions deﬁned on the unit sphere.
We can do so by rescaling everything to the unit sphere. More precisely, to deﬁne
∆Sn−1f at the point x, we consider the function g : Sn−1 → R deﬁned by g(y) =
f (|x|y), and we let

∆Sn−1f (x) = ∆Sn−1g(x/|x|).

The advantage of being able to apply ∆Sn−1 to functions on Rn \ {0} is that it

becomes the angular part of the Euclidean Laplacian in spherical coordinates:

Exercise 3.2. Prove that if r denotes the distance to the origin and ∂/∂r is the
radial derivative, then for every C2 function f : Rn → R,
1
(3.1)
r2 ∆Sn−1 f

∂2f
∂r2 +

n − 1

∂f
∂r

∆f =

+

r

when r 6= 0.

If f is homogeneous of degree k, then (3.1) becomes

∆f =

k(k − 1)f

r2

+

(n − 1)kf

r2

+

1
r2 ∆Sn−1f.

Then ∆f = 0 is equivalent to ∆Sn−1f = −k(k + n− 2)f . In other words, harmonic
functions that are homogeneous of degree k are eigenfunctions of the spherical
Laplacian with eigenvalue −k(k + n − 2). We will see shortly that the spherical
harmonics in W k are all homogeneous of degree k, and thus that the spaces Wk are
the eigenspaces of ∆Sn−1.

First note, that the Euclidean Laplacian maps homogeneous polynomials of
degree k to homogeneous polynomials of degree k − 2. Thus, every harmonic poly-
nomial is the sum of homogeneous harmonics.
In terms of spherical harmonics, Pk is the sum of the eigenspaces of ∆Sn−1
with eigenvalues −ℓ(ℓ + n− 2) for ℓ = 0, 1, . . . , k. These eigenspaces are orthogonal,
because the spherical Laplacian is symmetric:

Lemma 3.3. For C2 functions f and g on Sn−1,

hf, ∆Sn−1gi = h∆Sn−1f, gi.

Proof. This identity is well-known for the Laplace-Beltrami operator, but
verifying it using our ad hoc deﬁnition takes a short calculation. Replace f and g
with their radial extensions to Rn \ {0}, and let

Equation (3.1) implies that

Ω = {x ∈ Rn | 1/2 ≤ |x| ≤ 2}.
ωnrn−3 dr =ZΩ

(cid:0)hf, ∆Sn−1gi − h∆Sn−1 f, gi(cid:1)Z 2

1/2

f ∆g − g∆f,

where the integral over Ω is with respect to Lebesgue measure and ωn is the surface
area of Sn−1. In this equation, ωnrn−3 combines the volume factor from spherical
coordinates with the 1/r2 factor multiplying ∆Sn−1f in (3.1).

34

HENRY COHN, PACKING, CODING, AND GROUND STATES

Now Green’s identity tells us that

ZΩ

f ∆g − g∆f =Z∂Ω

f

∂g
∂n − g

∂f
∂n

,

where ∂/∂n denotes the normal derivative and the integral over ∂Ω is with respect
to surface measure. It vanishes because ∂f /∂n = ∂g/∂n = 0 by construction. (cid:3)
Because Pk is the sum of the eigenspaces of ∆Sn−1 with eigenvalues −ℓ(ℓ +
n − 2) for ℓ = 0, 1, . . . , k and these eigenspaces are orthogonal, the orthogonal
complement of Pk−1 in Pk must be the −k(k + n − 2) eigenspace. Thus, Wk
consists of the harmonic polynomials that are homogeneous of degree k. This gives
a rather concrete, if cumbersome, description of the space of spherical harmonics.
By contrast, people sometimes make spherical harmonics look unnecessarily exotic
by writing them in spherical coordinates as eigenfunctions of the Laplacian.

Exercise 3.4. Compute the homogeneous harmonic polynomials explicitly when
n = 2, and check that this computation agrees with our earlier analysis of S1.

We will see in the next lecture that Wk is an irreducible representation of
O(n). Thus, we have found the complete decomposition of L2(Sn−1) into irreducible
representations, as well as the spectral decomposition of the Laplacian. The biggest
conceptual diﬀerence from S1 is that the space Wk of degree k spherical harmonics
has much higher dimension than 2 in general, but that’s not an obstacle to using
this theory.

Energy and packing bounds on spheres

LECTURE 4

1. Introduction

In this lecture, we will use spherical harmonics to prove bounds for packing and
energy minimization on spheres.1 Our technique will be essentially the same as in
the proof of Proposition 4.1 from the second lecture, but the bounds will be more
sophisticated algebraically and much more powerful. By the end of the lecture we
will be able to solve the kissing problem in R8 and R24, as well as analyze almost
all of the known cases of universal optimality. In the next lecture we will tackle
Euclidean space using much the same approach, but the analytic technicalities will
be greater and it will be useful to have looked at the spherical case ﬁrst.

Everything we do will be based on studying the distances that occur between
pairs of points. Motivated by error-correcting codes, we call a ﬁnite subset C of
Sn−1 a code. The distance distribution of a code measures how often each pairwise
distance occurs. For −1 ≤ t ≤ 1, deﬁne the distance distribution A of C by

At = #(cid:8)(x, y) ∈ C2 | hx, yi = t(cid:9),

where h·,·i denotes the usual inner product on Rn. Recall that |x− y|2 = 2 − hx, yi
when x and y are unit vectors; thus, At counts the number of pairs at distance
√2 − 2t, but inner products are a more convenient way to index these distances.

In physics terms [75, p. 63], the distance distribution is equivalent to the pair
correlation function, although it is formulated a little diﬀerently.

We can express the energy for a pair potential function f in terms of the

distance distribution via

(1.1)

Xx,y∈C

x6=y

f (|x − y|2) = X−1≤t<1

f (2 − 2t)At.

(In the sum on the right, there are uncountably many values of t, but only ﬁnitely
many of the summands are nonzero. Note that the restriction to t < 1 is to
avoid self-interactions; it corresponds to x 6= y on the left.) Thus, ﬁguring out
which energies can be attained amounts to understanding what the possible pair
correlation functions are. Which constraints must they satisfy?

We have made an important trade-oﬀ here. The dependence of energy on the
distance distribution is as simple as possible, because the right side of (1.1) is
a linear function of the variables At. However, the nonlinearity in this problem
cannot simply disappear. Instead, it reappears in the question of which distance
distributions occur for actual point conﬁgurations.

1Analogous techniques work in various other settings, such as projective spaces or Grassmannians.

35

Xt

Att = Xx,y∈C

hx, yi =*Xx∈C

x,Xy∈C

2

≥ 0.

y+ =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Xx∈C

x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

36

HENRY COHN, PACKING, CODING, AND GROUND STATES

There are some obvious constraints for an N -point code: At ≥ 0 for all t,

A1 = N , andPt At = N 2. They follow trivially from the deﬁnition

At = #(cid:8)(x, y) ∈ C2 | hx, yi = t(cid:9).

Another obvious constraint is that At must be an integer for each t, but we will
generally ignore this constraint, because optimization theory does not handle inte-
grality constraints as seamlessly as it handles inequalities.

There are also less obvious constraints, such as

To see why this inequality holds, note that

Att ≥ 0.

Xt
Att = Xx,y∈C

Xt

hx, yi,

because At counts how often t occurs as an inner product between points in C.
Thus,

Recall that this is the inequality we used to analyze simplices at the end of the
second lecture.

Delsarte discovered an inﬁnite sequence of linear inequalities generalizing this
one.2 The factor of t above is replaced with certain special functions, namely
Gegenbauer or ultraspherical polynomials, which are a family P n
k of polynomials in
one variable with deg(P n
k ) = k. The Delsarte inequalities then say that whenever
A is the distance distribution of a conﬁguration in Sn−1,

(1.2)

Xt

AtP n

k (t) ≥ 0

1 (t) = t, from which we recover the previous inequality,

for all k. In particular, P n
and P n

0 (t) = 1, while the higher-degree polynomials depend on n.

The Delsarte inequalities are far from a complete characterization of the dis-
tance distributions of codes. However, they are particularly beautiful and important
constraints on these distance distributions.

An equivalent reformulation of (1.2) is that for every ﬁnite set C ⊂ Sn−1,

Xx,y∈C

P n
k (hx, yi) ≥ 0.

We will return in §5 to what ultraspherical polynomials are and why they have this
property. In the meantime, we will treat them as a black box while we explore how
the Delsarte inequalities are used to prove bounds.

2Delsarte’s initial discovery was in a discrete setting [28], but analogous techniques apply to
spheres [29, 41].

LECTURE 4. ENERGY AND PACKING BOUNDS ON SPHERES

37

2. Linear programming bounds

The energy of a code is given by the linear function

of its distance distribution, and the Delsarte inequalities

1

2 X−1≤t<1
Xt

f (2 − 2t)At

AtP n

k (t) ≥ 0

are linear in A as well. The linear programming bounds minimize the energy subject
to these linear constraints.3 Because of the linearity, these bounds are particularly
well behaved and useful. The only computational diﬃculty is that there are inﬁn-
itely many variables At.

Let’s write down the linear programming bounds more precisely. To begin with,
we are given the dimension n, number N of points, and potential function f . Then
linear programming bounds attempt to choose At for −1 ≤ t ≤ 1 so as to minimize

subject to

1

2 X−1≤t<1

Atf (2 − 2t)

A1 = N,
At ≥ 0 for −1 ≤ t ≤ 1,
At = N 2, and

AtP n

k (t) ≥ 0 for all k ≥ 1.

Xt

Xt

This optimization problem gives us a lower bound for the energy of codes in Sn−1,
because every code has a corresponding distance distribution. However, there is
no reason to expect the bound to be sharp in general: the optimal choice of At
will usually not even be integral, let alone come from an actual code. Of course
one could improve the bound by imposing integrality, but then the optimization
problem would become far less tractable. In particular, it would no longer be a
convex optimization problem.

Linear programming bounds are well suited to computer calculations, but they
have not yet been fully optimized. Any given case can be solved numerically, but
the general pattern is unclear. In particular, for most n, N , and f we do not know
the optimal solution.

In practice, it is useful to apply linear programming duality, in which we try
to prove bounds on energy by taking linear combinations of the constraints. If we
multiply the Delsarte inequalities

Xt

AtP n

k (t) ≥ 0

by constants hk and then sum over k, we obtain the following theorem.

3Recall that “linear programming” means optimizing a linear function subject to linear constraints.
There are eﬃcient algorithms to solve ﬁnite linear programs.

38

HENRY COHN, PACKING, CODING, AND GROUND STATES

Theorem 2.1 (Yudin [81]). Suppose h = Pk hkP n

k with hk ≥ 0 for k ≥ 1, and
suppose h(t) ≤ f (2 − 2t) for t ∈ [−1, 1). Then every N -point conﬁguration C on
Sn−1 satisﬁes

Xx,y∈C

x6=y

f (|x − y|2) ≥ N 2h0 − N h(1).

suﬃciently large k, but convergence ofPk hkP n

The auxiliary function h is generally a polynomial, in which case hk = 0 for all
k on [−1, 1] suﬃces. (It turns out
k (1) on [−1, 1], and hence the convergence is automatically absolute

k | ≤ P n
that |P n
and uniform.)

Proof. We have

Xx,y∈C

x6=y

x6=y

h(hx, yi)

f (|x − y|2) ≥ Xx,y∈C
= Xx,y∈C
= N 2h0 − N h(1) +Xk≥1
≥ N 2h0 − N h(1),

h(hx, yi) − N h(1)

as desired.

(because f (2 − 2t) ≥ h(t) pointwise)

hk Xx,y∈C

P n
k (hx, yi)

(cid:3)

Note that the proof rests on the fundamental inequality

Xx,y∈C

P n
k (hx, yi) ≥ 0.

Xx,y∈C

P n
k (hx, yi)

The proof technique might seem extraordinarily wasteful, since it involves throwing
away many terms in our sum. However, P n
k (hx, yi) averages to zero over the whole
sphere when k ≥ 1, which suggests that the double sums

may not be so large after all when C is well distributed over the sphere.
Theorem 2.1 tells us that to prove a lower bound for f -energy, all we need is
a lower bound h for the potential function f such that h has non-negative ultra-
spherical coeﬃcients. Such an auxiliary function is a convenient certiﬁcate for a
lower bound.

Outside of a few special cases, nobody knows the optimal h for a given f .
However, numerical optimization is an eﬀective way to compute approximations
to it. One can use more sophisticated techniques such as sums of squares and
semideﬁnite programming, but even the most straightforward approach works well
in practice: let h be a polynomial of degree d, and instead of imposing the inequality
h(t) ≤ f (2 − 2t) for all t, impose it just at ﬁnitely many locations (chosen fairly
densely in [−1, 1), of course). Then we are left with a ﬁnite linear program, i.e.,
a linear optimization problem with only ﬁnitely many variables and constraints,
which is easily solved numerically using standard software. The resulting auxiliary
function h might not satisfy h(t) ≤ f (2 − 2t) everywhere, but any violations will
be small, and we can eliminate them by adjusting the constant term h0 without
substantially changing the energy bound.

LECTURE 4. ENERGY AND PACKING BOUNDS ON SPHERES

39

3. Applying linear programming bounds

Linear programming bounds are behind almost every case in which universal opti-
mality, or indeed any sharp bound on energy, is known. As mentioned above, they
are generally far from sharp, but for certain codes they miraculously give sharp
bounds. This is the case for all the universal optima listed in Table 1 from the
second lecture.

When could the bound be sharp for a conﬁguration C? Equality holds in
Theorem 2.1 iﬀ every term we throw away in the proof is actually already zero.
Inspecting the proof leads to the following criteria:

Lemma 3.1. The energy lower bound in Theorem 2.1 is attained by a code C if
and only if f (|x − y|2) = h(hx, yi) for all x, y ∈ C with x 6= y, and

Xx,y∈C

P n
k (hx, yi) = 0

for all k ≥ 1 for which hk > 0.

The ﬁrst condition says that h(t) = f (2 − 2t) whenever t = hx, yi with x, y ∈ C
and x 6= y. Because h(t) ≤ f (2 − 2t) for all t, the functions h and f cannot cross.
Instead, they must agree to order at least 2 whenever they touch.
In practice, sharp bounds are usually obtained in the simplest possible way
based on this tangency constraint. We choose h to be a polynomial of as low a
degree as possible subject to agreeing with f to order 2 at each inner product
that occurs between distinct points in C. This speciﬁes a choice of h, but it is
not obvious that it has any of the desired properties. For example, the inequality
h(t) ≤ f (2− 2t) might be violated in between the points at which we force equality,
and there is no obvious reason to expect the ultraspherical coeﬃcients hk to be
nonnegative.

This construction of h is generally far from optimal when it works at all, but
for particularly beautiful codes it does remarkably well at proving sharp bounds.
For example, let’s show that regular simplices are universally optimal, which was
Exercise 4.2 from the second lecture. Recall that for N ≤ n+1, the N -point regular
simplex C in Sn−1 has all inner products equal to −1/(N − 1).
Proposition 3.2. For N ≤ n+1, the N -point regular simplex is universally optimal
in Sn−1.

We’ll describe the proof in terms of linear programming bounds, but one could

reword it to use just the inequality

(as was intended in Exercise 4.2 from the second lecture).

2

≥ 0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Xx∈C

x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Proof. We will show that the simplex in fact minimizes energy for every
decreasing, convex potential function f , which is an even stronger property than
universal optimality. Let h(t) be the tangent line to f (2 − 2t) at t = −1/(N − 1);
in other words,

h(t) = f(cid:0)2 + 2/(N − 1)(cid:1) − 2f ′(cid:0)2 + 2/(N − 1)(cid:1)(cid:0)t + 1/(N − 1)(cid:1).

40

HENRY COHN, PACKING, CODING, AND GROUND STATES

This function is the lowest-degree polynomial that agrees with f (2 − 2t) to order 2
at all the inner products occurring in the regular simplex, which makes it a special
case of the construction outlined above.

Because f is convex, h(t) ≤ f (2−2t) for all t. Thus, the ﬁrst inequality we need
for h does in fact hold. To check the nonnegativity of the ultraspherical coeﬃcients
(aside from the constant term), note that the ﬁrst two ultraspherical polynomials
are 1 and t. If we express h(t) in terms of this basis, then the coeﬃcient of t is

−2f ′(cid:0)2 + 2/(N − 1)(cid:1), which is nonnegative since f is decreasing. Thus, h satisﬁes

the hypotheses of Theorem 2.1. Furthermore, h(t) = f (2−2t) when t = −1/(N −1)
by construction, and

Xx,y∈C

hx, yi =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Xx∈C

x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

2

= 0.

These are the conditions for a sharp bound in Lemma 3.1, and so we conclude
that our energy bound is equal to the energy of the regular simplex. Hence regular
simplices minimize energy for all decreasing, convex potential functions, and in
particular they are universally optimal.
(cid:3)

Codes with more inner products are more complicated to handle, but in any
given case one can ﬁgure out whether this approach works.
If one analyzes the
technique in suﬃcient generality, it proves the following theorem, which extends a
theorem of Levenshtein [51].

Theorem 3.3 (Cohn and Kumar [17]). Every m-distance set that is a spherical
(2m − 1)-design is universally optimal.

Here an m-distance set is a set in which m distances occur between distinct
points, and a spherical k-design is a ﬁnite subset D of the sphere Sn−1 such that
for every polynomial p : Rn → R of total degree at most k, the average of p over D
is equal to its average over the entire sphere Sn−1. In other words, averaging at the
points of D is an exact numerical integration formula for polynomials up to degree
k, which means these points are exceedingly well distributed over the sphere.
This theorem suﬃces to handle every known universal optimum on the surface
of a sphere (see Table 1 in the second lecture) except the regular 600-cell, which
is dealt with in §7 of [17]. Surely that’s not the only exception, but it is unclear
where to ﬁnd other universal optima that go beyond Theorem 3.3.

4. Spherical codes and the kissing problem

Recall that the spherical code problem asks whether N points can be arranged on
Sn−1 so that no two are closer than angle θ to each other along the great circle
connecting them. In other words, the minimal angle between the points is at least
θ. This is a packing problem: how many spherical caps of angular radius θ/2 can
we pack on the surface of a sphere?

The most famous special case is the kissing problem discussed in the ﬁrst lec-
ture. Given a central unit ball, the kissing problem asks how many non-overlapping
unit balls can be arranged tangent to it. Equivalently, the points of tangency should
form a spherical code with minimal angle at least 60◦ (see Figure 1).

Linear programming bounds apply to this problem. In fact, packing problems
were the original application for these bounds [28], before Yudin applied them to
energy minimization [81].

LECTURE 4. ENERGY AND PACKING BOUNDS ON SPHERES

41

no overlap

≥ 60◦

Figure 1. An angle of 60◦ or more between tangent spheres is
equivalent to avoiding overlap between them.

Theorem 4.1. Suppose h = Pk hkP n

k with hk ≥ 0 for k ≥ 0 and h0 > 0, and
suppose h(t) ≤ 0 for t ∈ [−1, cos θ]. Then every code C in Sn−1 with minimal angle
at least θ satisﬁes

|C| ≤ h(1)/h0.

Proof. We have

|C|h(1) ≥ Xx,y∈C

h(hx, yi) =Xk

hk Xx,y∈C

k (hx, yi) ≥ |C|2h0.
P n

(cid:3)

As in the case of energy minimization, this bound is generally not sharp, but
on rare occasions we are lucky enough to get a sharp bound. The most famous case
is the kissing problem in R8 and R24, which was solved independent by Levenshtein
[50] and Odlyzko and Sloane [59]. In particular, the kissing number is 240 in R8
and 196560 in R24, as achieved by the E8 lattice and the Leech lattice. It is not so
diﬃcult to prove these upper bounds using Theorem 4.1. In particular, we take

in the R8 case, and

h(t) = (t + 1)(t + 1/2)2t2(t − 1/2)

h(t) = (t + 1)(t + 1/2)2(t + 1/4)2t2(t − 1/4)2(t − 1/2)

in the R24 case. (The roots correspond to the inner products that occur in the
kissing conﬁgurations.) Checking that these polynomials satisfy the hypotheses of
Theorem 4.1 and prove sharp bounds is a ﬁnite calculation. Of course presenting it
this way makes the proof look like a miracle, and explaining it conceptually requires
a deeper analysis [51].

5. Ultraspherical polynomials

So far, we have treated ultraspherical polynomials as a black box and taken the
Delsarte inequalities on faith. In this section, we will ﬁnally examine where these
polynomials come from and why the inequalities hold.

42

HENRY COHN, PACKING, CODING, AND GROUND STATES

One simple (albeit unmotivated) description is that ultraspherical polynomials
for Sn−1 are orthogonal polynomials with respect to the measure (1 − t2)(n−3)/2 dt
on [−1, 1]. In other words,

Z 1

−1

P n
k (t)P n

ℓ (t)(1 − t2)(n−3)/2 dt = 0

for k 6= ℓ. Equivalently, P n
k is orthogonal to all polynomials of degree less than k
with respect to this measure, because all such polynomials are linear combinations
of P n
k−1. We’ll see shortly where the measure comes from and why this
orthogonality characterizes the ultraspherical polynomials, but ﬁrst let’s explore
its consequences.

0 , . . . , P n

Orthogonality uniquely determines the ultraspherical polynomials up to scaling
(and the scaling is irrelevant for our purposes, as long as we take P n
k (1) > 0 so as not
to ﬂip the Delsarte inequality). Speciﬁcally, we just apply Gram-Schmidt orthog-
onalization to 1, t, t2, . . . , which gives an algorithm to compute these polynomials
explicitly. It’s not the most eﬃcient method, but it works.

Although orthogonality may sound like an arcane property of a sequence of
polynomials, it has many wonderful and surprising consequences. For example, it
implies that P n
k changed
sign at only m points r1, . . . , rm in [−1, 1], with m < k. Then the polynomial

k has k distinct roots in [−1, 1]. To see why, suppose P n

would never change sign on [−1, 1], which would contradict

P n
k (t)(t − r1) . . . (t − rm)

Z 1

−1

P n
k (t)(t − r1) . . . (t − rm)(1 − t2)(n−3)/2 dt = 0

(which holds because (t− r1) . . . (t − rm) has degree less than k). Thus, m = k and
P n
k has k distinct roots in [−1, 1], which means it’s a highly oscillatory function.
Although ultraspherical polynomials can be characterized via orthogonality,
it’s not really a satisfactory explanation of where they come from. To explain that,
we will use spherical harmonics. Recall that as a representation of O(n), we can
decompose L2(Sn−1) as

L2(Sn−1) =dMk≥0

Wk,

where Wk consists of degree k spherical harmonics.

We can obtain ultraspherical polynomials by studying the evaluation map: let
x ∈ Sn−1, and consider the linear map that takes f ∈ Wk to f (x). By duality
for ﬁnite-dimensional vector spaces, this map must be the inner product with some
unique element wk,x of Wk, called a reproducing kernel. That is,

f (x) = hwk,x, fi

for all f ∈ Wk. Note that here h·,·i denotes the inner product on L2(Sn−1). We will
use the same notation for both this inner product and the standard inner product
on Rn; to distinguish between them, pay attention to which vector spaces their
arguments lie in.

The function wk,x on Sn−1 has considerable structure. For example, it is in-

variant under all symmetries of Sn−1 that ﬁx x:

Lemma 5.1. If T is an element of O(n) such that T x = x, then T wk,x = wk,x.

The reproducing kernel wk,x is a polynomial of degree k in several variables, because
it is a spherical harmonic in Wk, and thus P n
k must be a polynomial of degree k in
one variable. (Technically this deﬁnition is oﬀ by a constant factor from the special
case P n
1 (t) = t mentioned earlier, but we could easily rectify that by rescaling so
that P n
k (1) = 1.)

We have ﬁnally explained where ultraspherical polynomials come from. They
describe reproducing kernels for the spaces Wk, and the importance of reproducing
kernels is that they tell how to evaluate spherical harmonics at points.

The drawback of the reproducing kernel deﬁnition is that it does not make it
clear how to compute these polynomials in any reasonable way. In principle one
could choose a basis for the homogeneous harmonic polynomials of degree k, inte-
grate over the sphere to obtain the inner products of the basis vectors in Wk, write
down the evaluation map explicitly relative to this basis, and solve simultaneous
linear equations to obtain the reproducing kernel. However, that would be unpleas-
antly cumbersome. The beauty of the orthogonal polynomial characterization of
ultraspherical polynomials is that it is much more tractable, but we must still see
why it is true.

First, observe that wk,x and wℓ,x are orthogonal in L2(Sn−1) for k 6= ℓ, since

they are spherical harmonics of diﬀerent degrees. Thus,

(5.1)

P n
k (hx, yi)P n

ℓ (hx, yi) dµ(y) = 0,

ZSn−1

LECTURE 4. ENERGY AND PACKING BOUNDS ON SPHERES

43

Proof. This lemma follows easily from the invariance of the inner product on

Wk under O(n). We have hwk,x, fi = hT wk,x, fi for all f ∈ Wk, because

hwk,x, fi = f (x) = f (T x) = (T −1f )(x) = hwk,x, T −1fi = hT wk,x, fi,

and hence wk,x = T wk,x.

(cid:3)

Equivalently, wk,x(y) can depend only on the distance between x and y, and

therefore it must be a function of hx, yi alone. We deﬁne P n

k by

wk,x(y) = P n

k (hx, yi).

where µ is surface measure. We can now obtain the orthogonality of the ultras-
pherical polynomials from the following multivariate calculus exercise:

Exercise 5.2. Prove that under orthogonal projection from the surface of the
sphere onto a coordinate axis, the measure µ projects to a constant times the
measure (1 − t2)(n−3)/2 dt on [−1, 1]. (See [13, p. 2434] for a simple solution.)

If we apply this orthogonal projection onto the axis between the antipodal

points ±x, then (5.1) becomes
P n
k (t)P n

Z 1

−1

as desired.

ℓ (t)(1 − t2)(n−3)/2 dt = 0,

As a side comment, we can now see that Wk is an irreducible representation of
O(n). If it broke up further, then each summand would have its own reproducing
kernel, which would yield two diﬀerent polynomials of degree k that would be
orthogonal to each other as well as to lower degree polynomials. That’s impossible,
since the space of polynomials of degree at most k has dimension too low to contain
so many orthogonal polynomials.

44

HENRY COHN, PACKING, CODING, AND GROUND STATES

All that remains to prove is the Delsarte inequalities. The key observation is
k (hx, yi) can be written as the inner product of two vectors in Wk depending

that P n
only on x and y, namely the reproducing kernels:
Lemma 5.3. For all x, y ∈ Sn−1 and k ≥ 0,

P n
k (hx, yi) = hwk,x, wk,yi.

Proof. Recall that the reproducing kernel property means hwk,x, fi = f (x)
for all f ∈ Wk. In particular, taking f = wk,y yields hwk,x, wk,yi = wk,y(x). Now
wk,y(x) = P n

k (hx, yi) implies that
P n
k (hx, yi) = hwk,x, wk,yi,

as desired.
Corollary 5.4. For every ﬁnite subset C ⊂ Sn−1 and k ≥ 0,

P n

P n
k (hx, yi) ≥ 0.

Xx,y∈C
k (hx, yi) = Xx,y∈C
=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Xx∈C
wk,x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≥ 0,

2

hwk,x, wk,yi

Proof. We have

Xx,y∈C

as desired.

(cid:3)

(cid:3)

This argument is a perfect generalization of(cid:12)(cid:12)Px∈C x(cid:12)(cid:12)2

≥ 0, except instead of
summing the vectors x, we are summing vectors wk,x in the Hilbert space Wk. One
interpretation is that x 7→ wk,x maps Sn−1 into a sphere in the higher-dimensional
space Wk, and we’re combining the trivial inequality

2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Xx∈C

wk,x(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≥ 0

Xx,y∈C

P (hx, yi) ≥ 0

with that nontrivial mapping. When n = 2, the space Wk has dimension 2 for
k ≥ 1, and so up to scaling we are mapping S1 to itself. This map wraps S1 around
itself k times, while the analogues for n ≥ 3 are more subtle.
It’s natural to wonder whether ultraspherical polynomials span all the functions
P satisfying

for all C. In fact, they do not. Pfender has constructed further such functions and
used them to obtain improvements on linear programming bounds [62]. However,
the numerical improvements so far have been relatively modest.

Instead, Schoenberg proved that the ultraspherical polynomials span the space
of positive-deﬁnite kernels [64], i.e., functions P such that for all x1, . . . , xN ∈ Sn−1,
the N × N matrix with entries P (hxi, xji) is positive semideﬁnite. The reason
why ultraspherical polynomials are positive-deﬁnite is Lemma 5.3: the matrix with

LECTURE 4. ENERGY AND PACKING BOUNDS ON SPHERES

45

entries hwk,xi , wk,xji is a Gram matrix and is thus positive semideﬁnite. Positive-
deﬁnite kernels play an important role in representation theory, which contributes
to the importance of ultraspherical polynomials.

As a ﬁnal comment, everything we have done in this lecture has been restricted
to analyzing pairwise distance distributions. It’s natural to ask what happens if
one looks at triples of points instead of pairs, or even larger subconﬁgurations.
The Delsarte inequalities can be generalized to semideﬁnite constraints on these
higher-order correlation functions, and thus we can obtain semideﬁnite program-
ming bounds [65, 5, 47], which are a powerful and important extension of linear
programming bounds. For reasons that have not yet been understood, these higher-
order bounds seem less fruitful for obtaining sharp bounds, but several sharp cases
are known [6, 22] and others presumably remain to be discovered.

Packing bounds in Euclidean space

LECTURE 5

1. Introduction

In this lecture we will study linear programming bounds for the sphere packing
problem in Euclidean space. The basic principles are closely analogous to those we
saw for spherical codes in the fourth lecture. However, the way the bounds behave
in Euclidean space is far more mysterious. They almost certainly solve the sphere
packing problem in R8 and R24, by matching the densities of the E8 and Leech
lattices, but nobody has been able to prove it.

We will focus on the sphere packing problem, rather than energy minimization.
Everything we will do works just as well in the latter case (see §9 of [17]), but
sphere packing already illustrates the essential features of these bounds.
To begin, let’s review the statement and proof of linear programming bounds

for spherical codes, i.e., Theorem 4.1 from the last lecture:

Theorem 1.1. Suppose h = Pk hkP n

k with hk ≥ 0 for k ≥ 0 and h0 > 0, and
suppose h(t) ≤ 0 for t ∈ [−1, cos θ]. Then every code C in Sn−1 with minimal angle
at least θ satisﬁes

|C| ≤ h(1)/h0.

Proof. We have

|C|h(1) ≥ Xx,y∈C

h(hx, yi) =Xk

hk Xx,y∈C

P n
k (hx, yi) ≥ |C|2h0.

(cid:3)

How could we generalize this argument? First, we need functions on Euclidean
space that can play the same role as ultraspherical polynomials. In particular, we
need an analogue of the positivity property

Xx,y∈C

P n
k (hx, yi) ≥ 0.

As it turns out, the Euclidean functions are considerably more familiar, namely
exponentials x 7→ e2πiht,xi. If we apply them to two points via (x, y) 7→ e2πiht,x−yi,
then for every ﬁnite subset C of Rn,

(1.1)

Xx,y∈C

e2πiht,x−yi =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Xx∈C

e2πiht,xi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

2

≥ 0.

As in the third lecture, these functions have representation-theoretic origins, but
we will not take up that subject here.

In the same way we previously made use of nonnegative linear combinations of
ultraspherical polynomials, we will now need to use nonnegative linear combinations
of exponentials. The natural setting for linear combinations of exponentials is

47

whenever C is a ﬁnite subset of Rn, because

f (x − y) ≥ 0

Xx,y∈C
f (x − y) =ZRn bf (t)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Xx∈C

Xx,y∈C

2

dt

e2πiht,xi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

48

HENRY COHN, PACKING, CODING, AND GROUND STATES

the Fourier transform. Deﬁne the Fourier transform bf of an integrable function

f : Rn → R by

f (x)e−2πiht,xi dx.

bf (t) =ZRn
f (x) =ZRn bf (t)e2πiht,xi dt.

If f is continuous and bf is integrable as well, then Fourier inversion tells us that
In other words, the Fourier transform bf gives the coeﬃcients needed to express f
interested in functions f for which bf ≥ 0.
If bf (t) ≥ 0 for all t, then Fourier inversion implies that

as a continuous linear combination of exponentials. Thus, we will be particularly

by (1.1). Thus, functions with nonnegative Fourier transforms have exactly the
property we need to generalize the Delsarte inequalities to Euclidean space.

However, using these functions to prove sphere packing bounds requires some

ﬁnesse. In the spherical case, we looked at the double sum

and bounded it on both sides to get

|C|h(1) ≥ Xx,y∈C

hk Xx,y∈C

P n
k (hx, yi) ≥ |C|2h0.

In Euclidean space, the corresponding double sum would be

h(hx, yi)

Xx,y∈C
h(hx, yi) =Xk
Xx,y∈C

f (x − y),

where C is a dense sphere packing, or rather the set of sphere centers in such a
packing. Unfortunately, there’s an obvious problem with this approach: C will be
inﬁnite and the double sum will diverge. For example, if C is a lattice, then every
term in the sum occurs inﬁnitely often, because there are inﬁnitely many ways to
write each lattice vector as a diﬀerence of lattice vectors.

Can we somehow renormalize the double sum and use it to complete the proof?
The answer is yes if we’re careful; see the proof of Theorem 3.3 in [23], which
controls the sum over a packing by subtracting a uniform background distribution
of equal density. However, in this lecture we’ll take an arguably more fundamental
approach using the Poisson summation formula.

LECTURE 5. PACKING BOUNDS IN EUCLIDEAN SPACE

49

2. Poisson summation

Poisson summation is a remarkable duality between summing a function over a
lattice and summing its Fourier transform over the dual lattice.

We’ll take a somewhat cavalier attitude towards analytic technicalities: we will
manipulate sums and integrals however we like, and include enough hypotheses to
justify these manipulations. Speciﬁcally, we will deal with what we’ll call admissible

functions f : Rn → R, those for which |f (x)| = O(cid:0)(1 + |x|)−n−ε(cid:1) and |bf (t)| =
O(cid:0)(1+|t|)−n−ε(cid:1) for some ε > 0. This decay rate is fast enough for sums over lattices

to converge with room to spare. In practice, we can simply read “admissible” as
“suﬃciently rapidly decreasing and smooth for everything to work.”
Theorem 2.1 (Poisson summation). If f : Rn → R is an admissible function and
Λ is a lattice in Rn, then

f (x) =

Xx∈Λ

1

vol(Rn/Λ) Xt∈Λ∗ bf (t).

Here vol(Rn/Λ) is the volume of a fundamental cell of Λ, i.e., the determinant

of Λ, and

is the dual lattice (see Exercise 2.1 in the third lecture).

Λ∗ = {t ∈ Rn | hx, ti ∈ Z for all x ∈ Λ}

Proof. The key idea is to prove an even more general formula, by looking at

the Fourier expansion of the periodization of f under Λ. Let

F (y) =Xx∈Λ
F (y) = Xt∈Λ∗

f (x + y),

cte2πiht,yi

so that F is periodic modulo Λ. We can expand F as a Fourier series

for some coeﬃcients ct, where Λ∗ occurs because it speciﬁes the exponentials that
are periodic modulo Λ (see §2 in the third lecture).

Let D be a fundamental domain for Λ. By orthogonality,

F (y)e−2πiht,yi dy

F (y)e−2πiht,yi dy

ct =

=

=

=

=

=

1

1

vol(D)ZD
vol(Rn/Λ)ZD
vol(Rn/Λ)Xx∈ΛZD
vol(Rn/Λ)Xx∈ΛZD
vol(Rn/Λ)ZRn
vol(Rn/Λ)bf (t).

1

1

1

1

f (x + y)e−2πiht,yi dy

f (x + y)e−2πiht,x+yi dy

(x ∈ Λ and t ∈ Λ∗)

f (y)e−2πiht,yi dy

(translates of D tile Rn)

In other words, the Fourier coeﬃcients ct of the periodization of f are simply

proportional to bf (t), with constant of proportionality 1/ vol(Rn/Λ).

50

HENRY COHN, PACKING, CODING, AND GROUND STATES

and setting y = 0 yields Poisson summation.

(cid:3)

Thus,

f (x + y) =

The more general formula

f (x + y) =

Xx∈Λ

Xx∈Λ

1

vol(Rn/Λ) Xt∈Λ∗ bf (t)e2πiht,yi,
vol(Rn/Λ) Xt∈Λ∗ bf (t)e2πiht,yi

1

πn/2
(n/2)! ·

.

bf (0)

is important in its own right, not just as tool for proving Poisson summation. At
ﬁrst it looks considerably more general than Poisson summation, but it is simply
Poisson summation applied to the function x 7→ f (x + y) in place of f .
3. Linear programming bounds

We can now state and prove the linear programming bounds for Euclidean sphere
packings.
Theorem 3.1 (Cohn and Elkies [15]). Let f : Rn → R be an admissible function
density in Rn is at most

with f (x) ≤ 0 for |x| ≥ 2, bf (t) ≥ 0 for all t, and bf (0) > 0. Then the sphere packing

f (0)

As before, (n/2)! means Γ(n/2 + 1) when n is odd. The factor of πn/2/(n/2)!
is the volume of a unit ball. It occurs because we are looking at packing density,
rather than just the number of balls per unit volume in space.

We will prove Theorem 3.1 using Poisson summation [15]. Several other proofs
are known, but they are longer [12] or more delicate [23]. One advantage of the
proof in [23] is that it weakens the admissibility hypothesis, so that we can use a
more robust space of functions; however, it obscures when a sharp bound can be
obtained.

Before we turn to the proof, let’s compare Theorem 3.1 with Theorem 1.1, its
spherical analogue. One diﬀerence is that the Euclidean case involves a function
of n variables, as opposed to one variable in the spherical case. However, this
diﬀerence is illusory: we might as well radially symmetrize f in the Euclidean case
(since both the hypotheses and the bound are radially symmetric), after which it
becomes a function of one variable.

Table 1 gives a dictionary with which these theorems can be compared. They
really are fully analogous, with the biggest discrepancy being that we use inner
products to measure distances in the spherical case but Euclidean distance in the
Euclidean case.

Proof. As a warm-up, let’s prove the linear programming bounds for lattice
packings. Suppose Λ is a lattice packing with unit balls (since we can specify the
packing radius without loss of generality).
In other words, the minimal vector
length of the lattice Λ is at least 2.

By Poisson summation,

f (x) =

Xx∈Λ

1

vol(Rn/Λ) Xt∈Λ∗ bf (t).

because f (x) ≤ 0 for |x| ≥ 2, while

because bf (t) ≥ 0 for all t. Thus,

f (x)

f (0) ≥Xx∈Λ
Xt∈Λ∗ bf (t) ≥ bf (0)
bf (0)

vol(Rn/Λ)

f (0) ≥

.

πn/2
(n/2)! ·

,

f (0)

bf (0)

LECTURE 5. PACKING BOUNDS IN EUCLIDEAN SPACE

51

Table 1. A dictionary for comparing linear programming bounds
on spheres and in Euclidean space.

space
function
transform
balls don’t overlap
value at distance zero h(1)
bound

Sn−1
h
hk
t ∈ [−1, cos θ]
h(1)/h0

Rn
f

|x| ≥ 2
f (0)

bf (t)
f (0)/bf(0)

We will apply the contrasting inequalities f (x) ≤ 0 (for |x| ≥ 2) and bf (t) ≥ 0 to

this identity. We have

The number of balls per unit volume in the packing is 1/ vol(Rn/Λ), and its density
is therefore 1/ vol(Rn/Λ) times the volume of a unit ball. Thus, the density is at
most

as desired.

So far, we have done nothing but apply the given inequalities to both sides of
Poisson summation. Handling general packings will require a little more work, but
nothing too strenuous.

Without loss of generality, we can restrict our attention to periodic packings,
since they come arbitrarily close to the optimal packing density. In other words,
we can suppose our packing consists of N translates of a lattice Λ, namely

Λ + y1, . . . , Λ + yN .

Now the number of balls per unit volume in the packing is N/ vol(Rn/Λ), and the
condition that they should not overlap says that |x + yj − yk| ≥ 2 for x ∈ Λ as long
as x 6= 0 or j 6= k.

A little manipulation based on the translated Poisson summation formula

shows that

Xx∈Λ

NXj,k=1

f (x + y) =

Xx∈Λ

f (x + yj − yk) =

1

vol(Rn/Λ) Xt∈Λ∗ bf (t)e2πiht,yi
vol(Rn/Λ) Xt∈Λ∗ bf (t)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
NXj=1

1

2

.

e2πihyj ,ti(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

which completes the proof.

In this proof, the inequality

πn/2
(n/2)! ·

bf (0)
N 2bf (0)

Xx∈Λ

NXj,k=1

f (x + yj − yk) ≥

vol(Rn/Λ)

52

HENRY COHN, PACKING, CODING, AND GROUND STATES

The inequalities on f and bf show that the left side is at most N f (0) and the right
side is at least N 2bf (0)/ vol(Rn/Λ). It follows that

f (0)

N

,

πn/2
(n/2)! ·

vol(Rn/Λ) ≤

(cid:3)

for functions satisfying bf ≥ 0 plays the role of the Delsarte inequalities. Note that

the left side is essentially summing f over the distance distribution of the packing,
but renormalized so that the distances do not occur inﬁnitely often.

In physics terms [75, p. 72], this inequality says that the structure factor (the
Fourier transform of g2−1, where g2 is the pair correlation function) is nonnegative.
The structure factor plays an important role in the theory of scattering, and its
nonnegativity is a fundamental constraint on the pair correlations that can occur
in any material.

The Poisson summation approach to linear programming bounds generalizes
naturally to the Selberg trace formula [69, 33]. Speciﬁcally, one can use a pretrace
formula to prove density bounds in hyperbolic space [23]. However, several things
that are known in the Euclidean case remain mysterious in hyperbolic geometry.
In particular, the bounds based on the pretrace formula have been proved only for
periodic packings, which are not known to come arbitrarily close to the optimal
density in hyperbolic space, and it is not known how to decrease the hypotheses on
the auxiliary function f along the lines of the proof for Euclidean space in [23].

4. Optimization and conjectures

As in the spherical case, nobody knows how to choose the optimal auxiliary function
in the linear programming bounds. It is not diﬃcult to obtain a trivial bound as
follows:

Exercise 4.1. Let χB be the characteristic function of the unit ball centered at
the origin in Rn. Show that the convolution f = χB ∗ χB satisﬁes the hypotheses
of Theorem 3.1 and yields an upper bound of 1 for the sphere packing density.

Despite the triviality of the bound, this function is of some interest [71], but
better constructions are needed if we are to prove nontrivial bounds. See, for
example, §6 of [15] for constructions based on Bessel functions.
The behavior of the optimized linear programming bound in Rn as n → ∞ is
unclear. Cohn and Zhao [23] showed that it is at least as good as the Kabatiansky-
Levenshtein bound of 2−(0.5990...+o(1))n, while Torquato and Stillinger [76] showed
that it can be no better than 2−(0.7786...+o(1))n. In particular, it comes nowhere near
the density of 2−(1+o(1))n attained by the best sphere packings currently known,
although it might come much closer than the Kabatiansky-Levenshtein bound does.
Aside from these constraints, the asymptotics are a mystery.

We will focus instead on how the bounds behave in low dimensions, by which
I mean “not tending to inﬁnity” rather than low in the everyday sense. Linear

LECTURE 5. PACKING BOUNDS IN EUCLIDEAN SPACE

53

4

8

12

dimension
20
16

24

28

32

36

upper bound

best packing known

1
0

)
y
t
i
s
n
e
d
(
g
o
l

−14

Figure 1. The logarithm of sphere packing density as a function
of dimension. The upper curve is the linear programming bound,
while the lower curve is the best packing currently known. Vertical
lines mark conjectured equality above one dimension.

programming bounds are nearly the best bounds known in four or more dimensions
(there is a small improvement based on incorporating one more term from Poisson
summation [46]). As shown in Figure 1, they are not so far from the truth in eight
or fewer dimensions, but they gradually drift away from the current record densities
in high dimensions. Note that the jaggedness of the record densities reﬂects their
subtle dependence on the dimension.

The most remarkable feature of Figure 1 is that the curves appear to touch
in eight and twenty-four dimensions. If true, this would settle the sphere packing
problem in those dimensions, without the diﬃculties that plague three dimensions.

Conjecture 4.2 (Cohn and Elkies [15]). The linear programming bounds for sphere
packing density in Rn are sharp when n = 2, 8, or 24.

Equality holds to at least ﬁfty decimal places [21], but no proof is known.1 It is
furthermore conjectured that the linear programming bounds for energy are sharp,
which would lead to universal optimality in Euclidean space [17, §9].

Examining the proof of Theorem 3.1 shows that the auxiliary function f proves

a sharp bound for a lattice Λ iﬀ f (x) = 0 for all x ∈ Λ \ {0} and bf (t) = 0 for all
t ∈ Λ∗ \ {0}. In other words, all we have to do is to ensure that f and bf have

certain roots without developing any unwanted sign changes. That sounds like a
manageable problem, but unfortunately it seems diﬃcult to control the roots of a
function and its Fourier transform simultaneously.

1Viazovska [80] has recently proved the conjecture for n = 8.

54

HENRY COHN, PACKING, CODING, AND GROUND STATES

Linear programming bounds seem not to be sharp in Rn except when n = 1, 2,
8, or 24. We can’t rule out the possibility of sharp bounds in other dimensions, but
nobody has been able to identify any plausible candidates. The n = 1 case follows
from Exercise 4.1, but sharpness is not known even for n = 2, let alone 8 or 24.
That makes it seem all the more mysterious: eight or twenty-four dimensions could
be truly deep, but two dimensions cannot transcend human understanding.

The strongest evidence for the sharpness of these bounds is the numerics, but
there are also analogies with related bounds that are known to be sharp in these
dimensions, such as those for the kissing problem [50, 59].
It is worth noting
that the kissing bounds are not just sharp, but sharp to an unnecessary degree.
Because the kissing number is an integer, any bound with error less than 1 could
be truncated to the exact answer, but that turns out not to be necessary.
In
particular, the bound in Theorem 1.1 is generally not an integer, but for the kissing
problem it miraculously turns out to be integral in R8 and R24: it is exactly 240
in R8 and exactly 196560 in R24. This unexpected exactness raises the question
of whether sharp bounds hold also for quantities such as packing density, where
integrality does not apply, and indeed they do seem to.

The computations behind Figure 1 are based on numerical optimization within
a restricted class of auxiliary functions. Speciﬁcally, they use functions of the form
f (x) = p(|x|2)e−π|x|2
, where p is a polynomial of one variable. Such functions are
relatively tractable, while being dense among all reasonable radial functions.

To carry out explicit computations, it is convenient to write p in terms of an

eigenbasis of the Fourier transform:

Exercise 4.3. Let

Pk =nfunctions x 7→ p(|x|2)e−π|x|2

on Rn (cid:12)(cid:12) p is a polynomial and deg(p) ≤ ko .

Prove that Pk is closed under the Fourier transform. What are the eigenvalues of
the Fourier transform on Pk? Show that the polynomials p corresponding to an
eigenbasis are orthogonal with respect to a certain measure on [0,∞), and compute
that measure.

The most general approach to optimizing the linear programming bounds over

using semideﬁnite programming [60]. This technique will produce the best possible
polynomial p of any given degree. (Another approach is to force roots for f and

Pk is to impose the sign conditions on f and bf via sums of squares and then optimize
bf and then optimize the root locations [15].) However, we cannot obtain a sharp

bound by using polynomials, because they have only ﬁnitely many roots. The best
we can do is to approximate the optimal bound.

By contrast, in the spherical case we can prove sharp bounds using polynomials.
That is one reason why linear programming bounds are so much more tractable for
spheres than they are in Euclidean space.

Numerical computations have thus far shed little light on the high-dimensional
asymptotics of linear programming bounds. These bounds are diﬃcult to compute
precisely in high dimensions, because such computations seem to require using high-
degree polynomials. Computing the linear programming bound in R1000 to ﬁfteen
decimal places would be an impressive benchmark, which might be possible but
would not be easy. Even just a few decimal places would be interesting, as would
an order of magnitude estimate in R10000.

conjecture
−27/10
−3/2

−2.7000000000000000000000000000 . . .
−1.5000000000000000000000000000 . . .
−2.6276556776556776556776556776 . . . −14347/5460
−1.3141025641025641025641025641 . . .
−205/156
4.2167501240968298210998965628 . . .
−1.2397969070295980026220596589 . . .
3.8619903167183007758184168473 . . .
−0.7376727789015322303799539712 . . .

?
?
?
?

2
2
2
2
4
4
4
4

8
8
24
24
8
8
24
24

f

f

f

f

bf
bf
bf
bf

LECTURE 5. PACKING BOUNDS IN EUCLIDEAN SPACE

55

Table 2. Numerically computed Taylor series coeﬃcients of the
hypothetical sphere packing functions in Rn, normalized so f (0) =

bf (0) = 1.

n function order

coeﬃcient

Even without being able to prove that the linear programming bounds are sharp
in R8 and R24, they can be combined with further arguments to prove optimality
among lattices:

Theorem 4.4 (Cohn and Kumar [18]). The Leech lattice is the unique densest
lattice in R24.

See also the exposition in [16]. Aside from R24, the optimal lattices are known

only in up to eight dimensions [9, 79].

Cohn and Miller observed that the hypothetical auxiliary functions proving
sharp bounds in R8 and R24 have additional structure, which has not yet been
explained. The patterns are prettiest if we rescale the function f and its input so

Conjecture 4.5 (Cohn and Miller [21]). The quadratic Taylor coeﬃcients of the

minimizing the radius r such that f (x) ≤ 0 for |x| ≥ r (see Theorem 3.2 in [15]).
Then the quadratic Taylor coeﬃcients appear to be rational numbers:

that f (0) = bf (0) = 1, in which case the linear programming bounds amount to
optimal radial functions f and bf , normalized as above with f (0) = bf (0) = 1, are
Because f and bf are even functions, the odd-degree Taylor coeﬃcients vanish.

The fourth-degree coeﬃcients shown in Table 2 remain unidentiﬁed.
If all the
coeﬃcients could be determined, it would open the door to solving the sphere
packing problem in R8 and R24.

rational numbers when n = 8 or n = 24, as shown in Table 2.

Bibliography

[1] M. Ajtai, The shortest vector problem in L2 is NP-hard for randomized reductions, in Pro-
ceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, pp. 10–19,
Association for Computing Machinery, New York, 1998.

[2] N. N. Andreev, An extremal property of the icosahedron, East J. Approx. 2 (1996), 459–462.
[3] N. N. Andreev, Location of points on a sphere with minimal energy, Proc. Steklov Inst. Math.

219 (1997), 20–24.

[4] S. Axler, P. Bourdon, and W. Ramey, Harmonic function theory, second edition, Graduate

Texts in Mathematics 137, Springer-Verlag, New York, 2001.

[5] C. Bachoc and F. Vallentin, New upper bounds for kissing numbers from semideﬁnite pro-

gramming, J. Amer. Math. Soc. 21 (2008), 909–924.

[6] C. Bachoc and F. Vallentin, Optimality and uniqueness of the (4, 10, 1/6) spherical code, J.

Combin. Theory Ser. A 116 (2009), 195–204.

[7] K. Ball, A lower bound for the optimal density of lattice packings, Internat. Math. Res.

Notices 1992, 217–221.

[8] B. Ballinger, G. Blekherman, H. Cohn, N. Giansiracusa, E. Kelly, and A. Sch¨urmann, Exper-
imental study of energy-minimizing point conﬁgurations on spheres, Experiment. Math. 18
(2009), 257–283.

[9] H. F. Blichfeldt, The minimum values of positive quadratic forms in six, seven and eight

variables, Math. Z. 39 (1935), 1–15.

[10] A. V. Bondarenko, D. P. Hardin, and E. B. Saﬀ, Mesh ratios for best-packing and limits of

minimal energy conﬁgurations, Acta Math. Hungar. 142 (2014), 118–131.

[11] M. Bowick and L. Giomi, Two-dimensional matter: order, curvature and defects, Adv. in

Phys. 58 (2009), 449–563.

[12] H. Cohn, New upper bounds on sphere packings II, Geom. Topol. 6 (2002), 329–353.
[13] H. Cohn, Order and disorder in energy minimization, Proceedings of the International Con-
gress of Mathematicians, Hyderabad, August 19–27, 2010, Volume IV, pages 2416–2443,
Hindustan Book Agency, New Delhi, 2010.

[14] H. Cohn, J. H. Conway, N. D. Elkies, and A. Kumar, The D4 root system is not universally

optimal, Experiment. Math. 16 (2007), 313–320.

[15] H. Cohn and N. D. Elkies, New upper bounds on sphere packings I, Ann. of Math. (2) 157

(2003), 689–714.

[16] H. Cohn and A. Kumar, The densest lattice in twenty-four dimensions, Electron. Res. An-

nounc. Amer. Math. Soc. 10 (2004), 58–67.

[17] H. Cohn and A. Kumar, Universally optimal distribution of points on spheres, J. Amer.

Math. Soc. 20 (2007), 99–148.

[18] H. Cohn and A. Kumar, Optimality and uniqueness of the Leech lattice among lattices, Ann.

of Math. (2) 170 (2009), 1003–1050.

[19] H. Cohn and A. Kumar, Algorithmic design of self-assembling structures, Proc. Natl. Acad.

Sci. USA 106 (2009), 9570–9575.

[20] H. Cohn, A. Kumar, and G. Minton, Optimal simplices and codes in projective spaces, to

appear in Geometry and Topology, arXiv:1308.3188.

[21] H. Cohn and S. D. Miller, Some properties of optimal functions for sphere packing in dimen-

sions 8 and 24, preprint, 2016, arXiv:1603.04759.

[22] H. Cohn and J. Woo, Three-point bounds for energy minimization, J. Amer. Math. Soc. 25

(2012), 929–958.

[23] H. Cohn and Y. Zhao, Sphere packing bounds via spherical codes, Duke Math. J. 163 (2014),

1965–2002.

57

58

HENRY COHN, PACKING, CODING, AND GROUND STATES

[24] H. Cohn and Y. Zhao, Energy-minimizing error-correcting codes, IEEE Trans. Inform. Theory

60 (2014), 7442–7450.

[25] J. H. Conway and N. J. A. Sloane, What are all the best sphere packings in low dimensions?,

Discrete Comput. Geom. 13 (1995), 383–403.

[26] J. H. Conway and N. J. A. Sloane, Sphere packings, lattices and groups, third edition,

Grundlehren der Mathematischen Wissenschaften 290, Springer, New York, 1999.

[27] H. S. M. Coxeter, L. Few, and C. A. Rogers, Covering space with equal spheres, Mathematika

6 (1959), 147–157.

[28] P. Delsarte, Bounds for unrestricted codes, by linear programming, Philips Res. Rep. 27

(1972), 272–289.

[29] P. Delsarte, J. M. Goethals, and J. J. Seidel, Spherical codes and designs, Geom. Dedicata 6

(1977), 363–388.

[30] P. D. Dragnev, D. A. Legg, and D. W. Townsend, Discrete logarithmic energy on the sphere,

Paciﬁc J. Math. 207 (2002), 345–358.

[31] F. J. Dyson, A Brownian-motion model for the eigenvalues of a random matrix, J. Math.

Phys. 3 (1962), 1191–1198.

[32] W. Ebeling, Lattices and codes: a course partially based on lectures by Friedrich Hirzebruch,

third edition, Advanced Lectures in Mathematics, Springer Spektrum, Wiesbaden, 2013.

[33] J. Elstrodt, F. Grunewald, and J. Mennicke, Groups acting on hyperbolic space: harmonic

analysis and number theory, Springer-Verlag, Berlin, 1998.

[34] L. Fejes T´oth, Regular Figures, Pergamon Press, Macmillan, New York, 1964.
[35] L. Fejes T´oth, Lagerungen in der Ebene auf der Kugel und im Raum, second edition, Springer,

Berlin, 1972.

[36] O. Goldreich, S. Goldwasser, and S. Halevi, Public-key cryptosystems from lattice reduction
problems, in Advances in Cryptology – CRYPTO ’97, Lecture Notes in Computer Science,
volume 1294, pp. 112–131, Springer-Verlag, Berlin, 1997.

[37] H. Groemer, Existenzs¨atze f¨ur Lagerungen im Euklidischen Raum, Math. Z. 81 (1963), 260–

278.

[38] T. C. Hales, Cannonballs and honeycombs, Notices Amer. Math. Soc. 47 (2000), 440–449.
[39] T. C. Hales, A proof of the Kepler conjecture, Ann. of Math. (2) 162 (2005), 1065–1185.
[40] T. Hales, M. Adams, G. Bauer, D. T. Dang, J. Harrison, T. L. Hoang, C. Kaliszyk, V. Ma-
gron, S. McLaughlin, T. T. Nguyen, T. Q. Nguyen, T. Nipkow, S. Obua, J. Pleso, J. Rute,
A. Solovyev, A. H. T. Ta, T. N. Tran, D. T. Trieu, J. Urban, K. K. Vu, and R. Zumkeller, A
formal proof of the Kepler conjecture, preprint, 2015, arXiv:1501.02155.

[41] G. A. Kabatiansky and V. I. Levenshtein, Bounds for packings on a sphere and in space,

Probl. Inf. Transm. 14 (1978), 1–17.

[42] Y. Kallus, Statistical mechanics of the lattice sphere packing problem, Phys. Rev. E 87 (2013),

063307, 5 pp.

[43] Y. Kallus, V. Elser, and S. Gravel, Method for dense packing discovery, Phys. Rev. E 82

(2010), 056707, 14 pp.

[44] A. V. Kolushov and V. A. Yudin, On the Korkin-Zolotarev construction, Discrete Math.

Appl. 4 (1994), 143–146.

[45] A. V. Kolushov and V. A. Yudin, Extremal dispositions of points on the sphere, Anal. Math.

23 (1997), 25–34.

[46] D. de Laat, F. M. de Oliveira Filho, and F. Vallentin, Upper bounds for packings of spheres

of several radii, Forum Math. Sigma 2 (2014), e23, 42 pp.

[47] D. de Laat and F. Vallentin, A semideﬁnite programming hierarchy for packing problems in

discrete geometry, Math. Program. 151 (2015), Ser. B, 529–553.

[48] J. Leech, Equilibrium of sets of particles on a sphere, Math. Gaz. 41 (1957), 81–90.
[49] A. K. Lenstra, H. W. Lenstra, Jr., and L. Lov´asz, Factoring polynomials with rational coef-

ﬁcients, Math. Ann. 261 (1982), 515–534.

[50] V. I. Levenshtein, On bounds for packings in n-dimensional Euclidean space, Soviet Math.

Dokl. 20 (1979), 417–421.

[51] V. I. Levenshtein, Designs as maximum codes in polynomial metric spaces, Acta Appl. Math.

29 (1992), 1–82.

[52] H. L¨owen, Fun with hard spheres, in K. R. Mecke and D. Stoyan, eds., Statistical physics and
spatial statistics: the art of analyzing and modeling spatial structures and pattern formation,
Lecture Notes in Physics 554, Springer, New York, 2000, pp. 295–331.

59

BIBLIOGRAPHY

[53] ´E. Marcotte and S. Torquato, Eﬃcient linear programming algorithm to generate the densest

lattice sphere packings, Phys. Rev. E 87 (2013), 063303, 9 pp.

[54] G. Minton, unpublished notes, 2011.
[55] O. Musin, The kissing number in four dimensions, Ann. of Math. (2) 168 (2008), 1–32.
[56] L. Nachbin, The Haar integral, D. Van Nostrand Company, Inc., Princeton, NJ, 1965.
[57] P. Nguyen, Cryptanalysis of the Goldreich-Goldwasser-Halevi cryptosystem from Crypto ’97,
in Advances in Cryptology – CRYPTO ’99, Lecture Notes in Computer Science, volume 1666,
pp. 288–304, Springer-Verlag, Berlin, 1999.

[58] P. Q. Nguyen and B. Vall´ee, eds., The LLL algorithm: survey and applications, Springer-

Verlag, Berlin, 2010.

[59] A. M. Odlyzko and N. J. A. Sloane, New bounds on the number of unit spheres that can

touch a unit sphere in n dimensions, J. Combin. Theory Ser. A 26 (1979), 210–214.

[60] P. A. Parrilo, Semideﬁnite programming relaxations for semialgebraic problems, Math. Pro-

gram. 96 (2003), Ser. B, 293–320.

[61] C. Peikert, A decade of lattice cryptography, Cryptology ePrint Archive, Report 2015/939,

https://eprint.iacr.org/2015/939 .

[62] F. Pfender, Improved Delsarte bounds for spherical codes in small dimensions, J. Combin.

Theory Ser. A 114 (2007), 1133–1147.

[63] C. A. Rogers, The packing of equal spheres, Proc. London Math. Soc. (3) 8 (1958), 609–620.
[64] I. J. Schoenberg, Positive deﬁnite functions on spheres, Duke Math. J. 9 (1942), 96–108.
[65] A. Schrijver, New code upper bounds from the Terwilliger algebra and semideﬁnite program-

ming, IEEE Trans. Inform. Theory 51 (2005), 2859–2866.

[66] K. Sch¨utte and B. L. van der Waerden, Das Problem der dreizehn Kugeln, Math. Ann. 125

(1953), 325–334.

[67] R. E. Schwartz, The ﬁve-electron case of Thomson’s problem, Exp. Math. 22 (2013), 157–186.
[68] R. E. Schwartz, The triangular bi-pyramid minimizes a range of power law potentials,

preprint, 2015, arXiv:1512.04628.

[69] A. Selberg, Harmonic analysis and discontinuous groups in weakly symmetric Riemannian

spaces with applications to Dirichlet series, J. Indian Math. Soc. (N.S.) 20 (1956), 47–87.

[70] C. E. Shannon, A mathematical theory of communication, Bell System Tech. J. 27 (1948),

379–423 and 623–656.

[71] C. L. Siegel, ¨Uber Gitterpunkte in convexen K¨orpern und ein damit zusammenh¨angendes

Extremalproblem, Acta Math. 65 (1935), 307–323.

[72] C. L. Siegel, A mean value theorem in geometry of numbers, Ann. of Math. (2) 46 (1945),

340–347.

[73] M. Dutour Sikiri´c, A. Sch¨urmann, and F. Vallentin, Classiﬁcation of eight-dimensional perfect

forms, Electron. Res. Announc. Amer. Math. Soc. 13 (2007), 21–32.

[74] A. Thue, Om nogle geometrisk-taltheoretiske Theoremer, Forhandlingerne ved de Skandi-

naviske Naturforskeres 14 (1892), 352–353.

[75] S. Torquato, Random heterogeneous materials: microstructure and macroscopic properties,

Interdisciplinary Applied Mathematics 16, Springer-Verlag, New York, 2002.

[76] S. Torquato and F. Stillinger, New conjectural lower bounds on the optimal density of sphere

packings, Experiment. Math. 15 (2006), 307–331.

[77] S. Vance, Improved sphere packing lower bounds from Hurwitz lattices, Adv. Math. 227

(2011), 2144–2156.

[78] A. Venkatesh, A note on sphere packings in high dimension, Int. Math. Res. Not. 2013

(2013), 1628–1642.

[79] N. M. Vetˇcinkin, Uniqueness of classes of positive quadratic forms on which values of the

Hermite constant are attained for 6 ≤ n ≤ 8, Proc. Steklov Inst. Math. 152 (1982), 37–95.

[80] M. S. Viazovska, The sphere packing problem in dimensions 8, preprint,

2016,

arXiv:1603.04246.

[81] V. A. Yudin, Minimum potential energy of a point system of charges, Discrete Math. Appl.

3 (1993), 75–81.

