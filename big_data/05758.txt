6
1
0
2

 
r
a

 

M
8
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
8
5
7
5
0

.

3
0
6
1
:
v
i
X
r
a

Fast Covariance Estimation for Sparse Functional

Data

Luo Xiao1,∗, Cai Li1, Will Checkley2 and Ciprian M. Crainiceanu2

1 North Carolina State University and 2Johns Hopkins University

∗email: lxiao5@ncsu.edu

January 9, 2016

Abstract

We propose a novel covariance smoothing method and associated soft-

ware based on penalized spline smoothing. The proposed method is a bivari-

ate smoother that is designed for covariance smoothing and can be used for

sparse functional or longitudinal data. We propose a fast algorithm for covari-

ance smoothing using leave-one-subject-out cross validation. Our simulations

demonstrate that the proposed method compares favorably against several

commonly used methods. The method is applied to a study of child growth

led by one of coauthors and to a public dataset of longitudinal CD4 counts.

Keywords: bivariate smoothing; face; functional data analysis; fPCA; penal-

ized splines.

1

Introduction

The covariance function is a crucial ingredient in functional data analysis.

Sparse functional or longitudinal data are ubiquitous in scientiﬁc studies, while

functional principal component analysis has become one of the ﬁrst-line ap-

proaches to analyzing this type of data; see, e.g., Besse and Ramsay (1986);

1

Ramsay and Dalzell (1991); Kneip (1994); Besse et al. (1997); Staniswalis and

Lee (1998); Yao et al. (2003, 2005).

Given a sample of functions observed at a ﬁnite number of locations and,

often, with sizable measurement error, there are usually three approaches for

obtaining smooth functional principal components: 1) smooth the functional

principal components of the sample covariance function; 2) smooth each curve

and diagonalize the resulting sample covariance of the smoothed curves; and

3) smooth the sample covariance function and then diagonalize it.

Smoothing of the sample covariance function is typically done using bi-

variate smoothing. Local linear smoothers (Fan and Gijbels, 1996), tensor-

product bivariate P-splines (Eilers and Marx, 2003) and thin plate regression

splines (Wood, 2003) are among the popular methods for smoothing the sam-

ple covariance function. For example, the fpca.sc function in the R package

refund (Huang et al., 2015) uses the tensor-product bivariate P-splines. How-

ever, there are two known problems with these smoothers: 1) they are general-

purpose smoothers that are not designed speciﬁcally for covariance operators;

and 2) they ignore that the subject, instead of the observation, is the inde-

pendent sampling unit and assume that the empirical covariance surface is

the sum between an underlying smooth covariance surface and independent

random noise. The FACE smoothing approach proposed by Xiao et al. (2014)

was designed speciﬁcally for addressing these weaknesses of oﬀ-the-shelf covari-

ance smoothing software. The method is implemented in the function fpca.face

in the refund R package (Huang et al., 2015) and has proven to be reliable

and fast in a range of applications. However, FACE was developed for high-

dimensional dense functional data and the extension to sparse data is far from

obvious. One approach that attempts to solve these problems was proposed

by Yao et al. (2003).

In their paper they used leave-one-subject-out cross-

validation to choose the bandwidth for local polynomial smoothing methods.

This approach is theoretically sound, but computationally expensive. This

may be the reason why the practice is to either try multiple bandwidths and

visually inspect the results or completely ignore within-subject correlations.

2

Several alternative methods for covariance smoothing of sparse functional

data also exist in the literature: James et al. (2000) used reduced rank spline

mixed eﬀects models while Peng and Paul (2009) proposed a geometric ap-

proach under the framework of marginal maximum likelihood estimation.

This paper has two aims. First, we propose a new automatic bivariate

smoother that is speciﬁcally designed for covariance function estimation and

can be used for sparse functional data. Second, we propose a fast algorithm

for selecting the smoothing parameter using leave-one-subject-out cross vali-

dation. The R code for the proposed method will be made publicly available

in the face package.

2 Model

Suppose that the observed data take the form {(yij, tij), j = 1, . . . , mi, i =
1, . . . , n}, where tij is in the unit interval [0, 1], n is the number of subjects,
and mi is the number of observations for subject i. The model is

yij = f (tij) + ui(tij) + ij,

(1)

where f is a smooth mean function, ui(t) is generated from a zero-mean Gaus-
sian process with covariance operator C(s, t) = Cov{ui(s), ui(t)}, and ij is
white noise following a normal distribution N (0, σ2
 ). We assume that the

random terms are independent across subjects and from each other. In many

cases when data are sparse, mi’s are usually much smaller than n.

We are interested in estimating the covariance function C(s, t) and we

assume that a working estimator ˆf exists.

In our code, we use a P-spline

smoother (Eilers and Marx, 1996) with the smoothing parameter selected

by leave-one-subject-out cross validation. A standard procedure employed
for obtaining a smooth estimate of C(s, t) consists of two steps. In the ﬁrst
step, an empirical estimate of the covariance function is constructed: { ˆCij1j2 :
1 ≤ j1 (cid:54)= j2 ≤ mi, i = 1, . . . , n}, where ˆCij1j2 = rij1rij2 and rij = yij −
ˆf (tij). In the second step, the raw estimates are smoothed using a bivariate

3

smoother. Standard bivariate smoothers are local linear smoothers (Fan and

Gijbels, 1996), tensor-product bivariate P-splines (Eilers and Marx, 2003)

and thin plate regression splines (Wood, 2003). In the following section we

propose a statistically eﬃcient, computationally fast and automatic smoothing

procedure that serves as competitive alternative to standard approaches.

3 Method

(cid:80)

We model the covariance function C(s, t) as a tensor-product splines H(s, t) =
1≤κ≤c,1≤(cid:96)≤c θκ(cid:96)Bκ(s)B(cid:96)(t), where ΘΘΘ = (θκ(cid:96))1≤κ≤c,1≤(cid:96)≤c is a coeﬃcient ma-
trix, {B1(·), . . . , Bc(·)} is the collection of B-spline basis functions in the unit
interval, and c is the number of interior knots plus the order (degree plus 1) of

the B-splines. We use knots placed at the quantiles of observed t and enforce

the following constraint on ΘΘΘ:

θκ(cid:96) = θ(cid:96)κ, 1 ≤ κ, (cid:96) ≤ c.

With this constraint, H(s, t) is always symmetric in s and t, a desired prop-

erty for estimates of covariance functions. Unlike the standard approach for

estimating smooth covariance function, our method diﬀers in two directions:

ﬁrst, our approach applies a joint estimation of covariance function and er-

ror variance; second, we incorporate the correlation structure of the auxiliary
variables { ˆCij1j2 : 1 ≤ j1 ≤ j2 ≤ mi, i = 1, . . . , n} in a two-step procedure to
boost statistical eﬃciency. Because we use a relatively large number of knots,

estimating ΘΘΘ by least squares or weighted least squares tends to overﬁt. Thus,

we estimate ΘΘΘ by minimizing the following penalized weighted least squares

criterion

( ˆΘΘΘ, ˆσ2

 ) = arg min

ΘΘΘ:ΘΘΘ=ΘΘΘT ,σ2


n(cid:88)

i=1

(cid:16)

HHH i + δδδiσ2

 − ˆCCCi

(cid:17)T

WWW i

(cid:16)

HHH i + δδδiσ2

 − ˆCCCi

(cid:17)

+λ·tr(cid:0)ΘΘΘDDDDDDT ΘΘΘT(cid:1) ,

(2)

where ni = mi(mi+1)/2, ˆCCCi = ( ˆCCC
Rni, and δδδi = (δδδi1, δδδi2, . . . , δδδimi)T ∈ Rni with ˆCCCij = ( ˆCijj, ˆCij(j+1), . . . , ˆCijmi)T ∈
Rmi−j+1, HHH ij = {H(tij, tij),H(tij, ti(j+1)), . . . ,H(tij, timi)}T ∈ Rmi−j+1, and

i2, . . . , ˆCCC

imi)T ∈ Rni, HHH i = (HHH T

i1, HHH T

i2, . . . , HHH T

T

T

T

i1, ˆCCC

imi)T ∈

4

δδδij = (1, 0T

mi−j)T ∈ Rmi−j+1 for 1 ≤ j ≤ mi. Also WWW i is a weight matrix, tr(·)
is the sum of all diagonal entries of a square matrix, DDD is the second-order

diﬀerencing matrix (Eilers and Marx, 1996), and λ is a smoothing parameter

that balances model ﬁt and smoothness of the estimate. The penalty term

tr(ΘΘΘDDDDDDT ΘΘΘT ) can be interpreted as the row penalty in bivariate P-splines

(Eilers and Marx, 2003). Note that when ΘΘΘ is symmetric, as in our case, the

row and column penalties in bivariate P-splines become the same. Therefore,

our proposed method can be regarded as a special case of bivariate P-splines

that is designed speciﬁcally for covariance function estimation. It follows that

our estimate is given by ˜C(s, t) =(cid:80)

1≤κ≤c,1≤(cid:96)≤c

ˆθκ(cid:96)Bκ(s)B(cid:96)(t).

3.1 Estimation
Let bbb(t) = (B1(t), . . . , Bc(t))T be a column vector. Then H(s, t) = {bbb(t) ⊗
bbb(s)}T vec ΘΘΘ. Here vec(·) is an operator that stacks the columns of a matrix
into a column vector. Let θθθ = vech ΘΘΘ, where vech(·) is an operator that stacks

the columns of the lower triangle of a matrix into a column vector, and let GGGc

be the duplication matrix (page 246, Seber 2007) such that vec ΘΘΘ = GGGcθθθ. It
follows that H(s, t) = {bbb(t) ⊗ bbb(s)}T GGGcθθθ. Next we let BBBi = [BBBT
where BBBij = [bbb(tij), . . . , bbb(timi)] ⊗ bbb(tij). Then, we have

i1, . . . , BBBT

]T ,

imi

E( ˆCCCi) = HHH i + δδδiσ2



(cid:16)

=

BBBiGGGc

δδδi

= XXX iααα,



(cid:17) θθθ

σ2


(cid:16)

and

(cid:80)n

i=1

=

(cid:17)T

(cid:16)

 − ˆCCCi

HHH i + δδδiσ2
WWW i
( ˆCCC − XXXααα)T WWW ( ˆCCC − XXXααα),

HHH i + δδδiσ2

 − ˆCCCi

(cid:17)

(3)

(cid:16)

(cid:17)

where ˆCCC = ( ˆCCC

T

i , . . . , ˆCCC

T
n )T , BBB = [BBBT

n )T , XXX =
, and WWW = blockdiag(WWW 1,··· , WWW n). Next we can derive that (page

n ]T , δδδ = (δδδT

1 , . . . , BBBT

1 ,··· , δδδT

BBBGGGc

δδδ

5

241, Seber 2007)

tr(ΘΘΘDDDDDDT ΘΘΘT ) = (vec ΘΘΘ)T (III c ⊗ DDDDDDT )vec ΘΘΘ.

Because vec ΘΘΘ = GGGcθθθ, we obtain that

tr(ΘΘΘDDDDDDT ΘΘΘT ) = θθθT GGGT

(cid:16)

c (III c ⊗ DDDDDDT )GGGT
c θθθ

(cid:17)PPP 0

 θθθ



0 0

σ2


=

θθθT σ2


= αααT QQQααα,

(4)

where PPP = GGGT

c (III c ⊗ DDDDDDT )GGGT

c , QQQ is the block matrix containing PPP and zeros.

By (3) and (4), the objective function in (2) can be rewritten as

ˆααα = arg min

ααα

( ˆCCC − XXXααα)T WWW ( ˆCCC − XXXααα) + λαααT QQQααα.

(5)

Now we obtain an explicit form of ˆααα

 ˆθθθ

 = (XXX T WWWXXX + λQQQ)−1(XXX T WWW ˆCCC).

ˆααα =

ˆσ2


We need to specify the weight matrices WWW i’s. One sensible choice for WWW i
is the inverse of Cov( ˆCCCi). However Cov( ˆCCCi) may not be invertible. Thus, we

specify WWW i as

WWW −1

i = (1 − β)Cov( ˆCCCi) + βdiag{diag{Cov( ˆCCCi)}}, 1 ≤ i ≤ n,

for some constant 0 < β < 1. The above speciﬁcation ensures that WWW −1

i

exists. We will use β = 0.05, which works well in practice.

We now derive Cov( ˆCCCi) in terms of C and σ2

 . First note that

E(rij1rij2) = Cov(rij1, rij2) = C(tij1, tij2) + δj1j2σ2
 .

Proposition 1. Let MMM 1 = (C(tij1, tij3), δj1j3σ2
MMM(cid:48)
MMM(cid:48) = [MMM(cid:48)

1 = (C(tij2, tij4), δj2j4σ2
 )T , MMM(cid:48)
(cid:88)

2 = (C(tij2, tij3), δj2j3σ2
(cid:88)

2], then

1|MMM(cid:48)

(vec(MMM (cid:12) MMM(cid:48))) =

Cov( ˆCij1j2, ˆCij3j4) =

where (cid:12) is the Khatri-Rao product.

j1,j2,j3,j4

j1,j2,j3,j4

6

 )T , MMM 2 = (C(tij1, tij4), δj1j4σ2
 )T ;
 )T . MMM = [MMM 1|MMM 2],

(vec([MMM 1 ⊗ MMM(cid:48)

1|MMM 2 ⊗ MMM(cid:48)

2])),

The proof of Proposition 1 is provided in the appendix. Now we see that

 ). Hence, we employ a two-step estimation. We

WWW i also depends on (C, σ2
ﬁrst estimate (C, σ2
all i, then we obtain the plug-in estimate of WWW i and estimate (C, σ2
generalized least square (GLS). The algorithm for the two-step estimation is

 ) by ﬁtting ordinary least square (OLS), i.e., WWW i = III for

 ) using

summarized as Algorithm 1.

Algorithm 1 Estimation algorithm
ˆααα(0) = arg minααα( ˆCCC − XXXααα)T ( ˆCCC − XXXααα) + λαααTQQQααα

(cid:99)WWW = WWW (ˆααα(0))
ˆααα = arg minααα( ˆCCC − XXXααα)T(cid:99)WWW ( ˆCCC − XXXααα) + λαααTQQQααα

3.2 Selection of the smoothing parameter

For selecting the smoothing parameter, we use leave-one-subject-out cross

validation, a popular approach for correlated data; see, for example, Yao et al.

(2003), Reiss et al. (2010) and Xiao et al. (2015). Compared to the leave-

one-observation-out cross validation, which ignores the correlation, leave-one-

subject-out cross-validation was reported to be more robust against overﬁt.

However, such an approach is usually computationally expensive.

In this

section, we derive a fast algorithm for approximating the leave-one-subject-

out cross validation.

Let ˜CCC

[i]

i be the prediction of ˆCCCi by applying the proposed method to the

data without the data from the ith subject, then the cross-validated error is

n(cid:88)

iCV =

(cid:107) ˜CCC

[i]

i − ˆCCCi(cid:107)2.

(6)

i=1

There is a simple formula for iCV. First we let SSS = XXX(XXX T WWWXXX + λQQQ)−1XXX T WWW ,

which is the smoother matrix for the proposed method. SSS can be written as
(XXXAAA)[III +λdiag(sss)]−1(XXXAAA)T WWW for some orthogonal square matrix AAA satisfying

AAAT AAA = III and sss is a column vector; see, for example, Xiao et al. (2013). In

particular, both AAA and sss do not depend on λ.

7

Let SSSi = XXX i(XXX T WWWXXX +λQQQ)−1XXX T WWW and SSSii = XXX i(XXX T WWWXXX +λQQQ)−1XXX T

Then SSSi is of dimension ni × N , where N = (cid:80)n

i=1 ni, and SSSii is symmetric

i WWW i.

and of dimension ni × ni.

Lemma 1. The iCV in (6) can be simpliﬁed as
(cid:107)(III ni − SSSii)−1(SSSi

iCV =

ˆCCC − ˆCCCi)(cid:107)2.

n(cid:88)

i=1

The proof of Lemma 1 is the same as that of Lemma 3.1 in Xu and Huang

(2012) and thus is omitted. Similar to Xu and Huang (2012), we further
simplify iCV by using the approximation (III ni −SSSii)−2 = III ni + 2SSSii. Note that
SSSii is symmetric. This approximation leads to the generalized cross validation,

which we denote as iGCV,

n(cid:88)

iGCV =

ˆCCC − ˆCCCi)T (III ni + 2SSSii)(SSSi

ˆCCC − ˆCCCi)

(SSSi

i=1

= (cid:107) ˆCCC − SSS ˆCCC(cid:107)2 + 2

ˆCCC − ˆCCCi)T SSSii(SSSi

ˆCCC − ˆCCCi).

(SSSi

(7)

n(cid:88)

i=1

While iGCV in (7) is much easier to compute than iCV in (6), the for-

mula in (7) is still computationally expensive as the smoother matrix SSS is of
dimension N × N , where N = 2, 000 if n = 100 and mi = m = 5 for all i.
Thus, we further simplify iGCV.

Let FFF i = XXX iAAA, FFF = XXXAAA and ˜FFF = FFF T WWW . Deﬁne fff i = FFF T
i

ˆCCCi, fff = FFF T ˆCCC,

˜fff = ˜FFF ˆCCC, JJJ i = FFF T
denote [III + λdiag(sss)]−1 as ˜DDD, a symmetric matrix, and its diagonal as ˜ddd.

i WWW iFFF i. To simplify notation we will

ˆCCCi and LLLi = FFF T

i WWW i

T

where ggg =(cid:80)n

Proposition 2. The iGCV in (7) can be simpliﬁed as
iGCV = (cid:107) ˆCCC(cid:107)2−2˜ddd

(˜fff(cid:12)fff )+(˜fff(cid:12)˜ddd)T (FFF T FFF )(˜fff(cid:12)˜ddd)+2˜ddd
)◦(FFF T
i FFF i). Here (cid:12) is the row-wise Khatri-Rao product.
FFF T

i=1(JJJ i(cid:12)fff i), GGG1 =(cid:80)n

i=1{(JJJ i

˜fff

T

T

T

GGG1

ggg−4˜ddd

T
˜ddd+2˜ddd

i FFF i)}, and GGG2 =(cid:80)n

(cid:111)

(˜fff (cid:12) ˜ddd) ⊗ (˜fff (cid:12) ˜ddd)

,

(cid:110)
GGG2
i=1(LLLi(cid:12)

Remark 1. While the above formula looks complex, it can be eﬃciently com-

puted. Indeed, only the term ˜ddd depend on the smoothing parameter λ and it can

be easily computed; all other terms including ggg, GGG1, GGG2 can be pre-calculated

just for once.

The proof of Proposition 2 is provided in the appendix.

8

4 Prediction

In this section, we consider the prediction of Xi(t) = f (ti) + ui(tij), sub-

ject i’s curve. We assume that Xi(t) is generated from a Gaussian process.
Suppose we would like to predict Xi(t) at {si1, . . . , sim} for m ≥ 1. Let yi =
i = {f (ti1), . . . , f (timi)}T , and xi = {Xi(si1), . . . , Xi(sim)}T .
(yi1, . . . , yimi)T , fff o
i = {bbb(si1)T , . . . , bbb(sim)T}T , and HHH i =

Let HHH o

i = {bbb(ti1)T , . . . , bbb(timi)T}T , HHH n
, HHH n,T

]T . Then


 .

HHH o

i

i ΘΘΘHHH n,T
i + σ2

HHH n

i ΘΘΘHHH n,T

 III m

 III mi

i ΘΘΘHHH o,T

i + σ2
i ΘΘΘHHH o,T

i

HHH n

i

[HHH o,T

 yi

xi

i

 ∼ MVN


 fff o

i

fff n
i

We derive that

E(xi|yi) =

i ΘΘΘHHH o,T

i

i ) + fff n
i ,

where VVV i = HHH o

i ΘΘΘHHH o,T

i + σ2

Cov(xi|yi) = VVV n

HHH n

i ΘΘΘHHH o,T

i

HHH n

i ΘΘΘHHH o,T

i

 HHH o
 ,
(cid:16)
i −(cid:16)

 III mi, and

HHH n

(cid:17)

VVV −1
i (yi − fff o
(cid:16)
(cid:17)

VVV −1

i

(cid:17)T

,

where VVV n

i = HHH n

i ΘΘΘHHH n,T

i + σ2

 III m. Because f , ΘΘΘ and σ2

 are unknown, we need

to plug in their estimates ˆf , ˆΘΘΘ and ˆσ2

 , respectively, into the above equalities.

Thus, we could predict xi by

ˆxi = {ˆx(si1), . . . , ˆx(sim)}T =

(cid:16)

HHH n
i

ˆΘΘΘHHH o,T

i

(cid:17) ˆVVV

−1
i (yi − ˆfff

o

i ) + ˆfff

n
i ,

n

i = { ˆf (si1), . . . , ˆf (sim)}T , and ˆVVV i =

 III mi. Moreover, an approximate covariance matrix for ˆxi is

where ˆfff

HHH o
i

ˆΘΘΘHHH o,T

o

i = { ˆf (ti1), . . . , ˆf (timi)}T , ˆfff
i + ˆσ2

(cid:100)Cov(ˆxi|yi) = ˆVVV

i −(cid:16)

n

(cid:17) ˆVVV

−1
i

(cid:16)

(cid:17)T

HHH n
i

ˆΘΘΘHHH o,T

i

HHH n
i

ˆΘΘΘHHH o,T

i

Note that one may also use the standard Karhunen-Loeve decomposition

representation of Xi(t) for prediction; see, e.g., Yao et al. (2005). An advan-

tage of the above formulation is that we avoid the evaluation of the eigen-
functions extracted from the covariance function C; indeed, we just need to

compute the B-spline basis functions at the desired time points, which is com-

putationally simple.

9

5 Simulations

5.1 Simulation setting

We generate data using model (1). The number of observations for each ran-
dom curve is generated from a uniform distribution on either I1 = {3, 4, 5, 6, 7}
or I2 = {j : 5 ≤ j ≤ 15}, and then observations are sampled from a uniform
distribution in the unit interval. Therefore, on average, each curve has m = 5

or m = 10 observations. We use zero mean functions. For the covariance func-

tion C, we consider two cases. For case 1 we let C1(s, t) =(cid:80)3

(cid:96)=1 λ(cid:96)ψ(cid:96)(s)ψ(cid:96)(t),
where ψ(cid:96)’s are eigenfunctions and λ(cid:96)’s are eigenvalues. Here λ(cid:96) = 0.5(cid:96)−1
2 cos(4πt) and ψ3(t) =
for (cid:96) = 1, 2, 3 and ψ1(t) =
√
2 sin(4πt). For case 2 we consider the Matern covariance function

2 sin(2πt), ψ2(t) =

√

√

(cid:32)√

(cid:33)ν

(cid:32)√

(cid:33)

C(d; φ, ν) =

1

2ν−1Γ(ν)

2νd
φ

Kν

2νd
φ

with range φ = 0.07 and order ν = 1. Here Kν is the modiﬁed Bessel function

of order ν. The top two eigenvalues for this covariance function are 0.209 and

0.179, respectively. The noise term ij’s are assumed normal with mean zero

and variance σ2

 . We consider two levels of signal to noise ratio (SNR): 2 and

5. For example, if

σ2
 =

1
2

(cid:90) 1

(cid:90) 1

s=0

t=0

C(s, t)dsdt,

then the signal to noise ratio in the data is 2. The number of curves is n = 100,

200 or 400 and for each covariance function 200 datasets are drawn. Therefore,

we have 24 diﬀerent model conditions to examine.

5.2 Competing methods and evaluation criterion

We compare the proposed method (denoted by FACE) with the following

methods: 1) The fpca.sc method in Goldsmith et al. (2010), which uses tensor-

product bivariate P-splines (Eilers and Marx, 2003) for covariance smoothing

and is implemented in the R package refund; 2) a variant of fpca.sc that

uses thin plate regression splines for covariance smoothing, denoted by TPRS,

10

and is coded by the authors; 3) the MLE method in Peng and Paul (2009),

implemented in the R package fpca; and 4) the local polynomial method in

Yao et al. (2003), denoted by loc, and is implemented in the MATLAB toolbox

PACE. The underlying covariance smoothing functions for fpca.sc and TPRS

are the function gam in the R package mgcv (Wood, 2013). For fpca.sc, we

use its default setting, which uses 10 B-spline bases in each dimension and the

smoothing parameters are selected by “REML”. For TPRS, we also use the

default setting in gam, with the smoothing parameter selected by “REML”.

For the method MLE, we specify the range for the number of B-spline bases

to be [6, 10] and the range of possible ranks to be [2, 6]. We will not evaluate

the method using a reduced rank mixed eﬀects model (James et al., 2000)

because it has been shown in Peng and Paul (2009) that the MLE method is

more superior.

We evaluate the above methods using four criterions. The ﬁrst is the mean

integrated squared errors (MISE) for estimating the covariance function. The

function: C(s, t) = (cid:80)∞

next two criterions are based on the eigendecomposition of the covariance
(cid:96)=1 λ(cid:96)ψ(cid:96)(s)ψ(cid:96)(t), where λ1 ≥ λ2 ≥ . . . are eigenvalues
and ψ1(t), ψ2(t), . . . are the associated orthonormal eigenfunctions. The sec-

ond criterion is the mean integrated squared errors (ISE) for estimating the

top 3 eigenfunctions from the covariance function. Let ψ(t) be the true eigen-

function and ˆψ(t) be an estimate of ψ(t), then the mean integrated squared

{ψ(t) − ˆψ(t)}2dt,

{ψ(t) + ˆψ(t)}2dt

.

t=0

t=0

It is easy to show that the range of integrated squared error for eigenfunction

estimation is [0, 2]. Note that for the method MLE, if rank 2 is selected then

only two eigenfunctions can be extracted. In this case, to evaluate accuracy of

estimating the third eigenfunction, we will let ISE be 1 for a fair comparison.

The third criterion is the mean squared errors (MSE) for estimating the top

3 eigenvalues. The last criterion is the methods’ computation speed.

11

(cid:20)(cid:90) 1

error is

min

(cid:90) 1

(cid:21)

5.3 Simulation results

The results for estimating the true covariance function are summarized in

Table 1, Figure 1 and Figure 2. For most model conditions, FACE gives

the smallest medians of integrated squared errors and has the smallest IQRs.

MLE is the 2nd best for case 1 while loc is the 2nd best for case 2.

Table 1: Median and IQR (in parenthesis) of ISEs of ﬁve estimators for estimating

the covariance functions. The results are based on 200 replications.

Case

n m SNR

FACE

TPRS

fpca.sc

MLE

loc

Case 1

Case 2

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

2

2

2

2

2

2

5

5

5

5

5

5

2

2

2

2

2

2

5

5

5

5

5

5

0.169 (0.085)

0.460 (0.129)

0.305 (0.170)

0.244 (0.224)

0.302 (0.226)

0.100 (0.050)

0.380 (0.068)

0.206 (0.080)

0.153 (0.109)

0.302 (0.120)

0.060 (0.028)

0.319 (0.041)

0.122 (0.053)

0.102 (0.077)

0.307 (0.071)

0.094 (0.050)

0.363 (0.080)

0.184 (0.089)

0.143 (0.129)

0.221 (0.093)

0.058 (0.032)

0.318 (0.039)

0.108 (0.044)

0.092 (0.082)

0.232 (0.095)

0.034 (0.019)

0.285 (0.022)

0.057 (0.023)

0.069 (0.050)

0.226 (0.062)

0.116 (0.070)

0.419 (0.085)

0.255 (0.116)

0.144 (0.125)

0.302 (0.230)

0.059 (0.032)

0.342 (0.052)

0.155 (0.069)

0.129 (0.075)

0.287 (0.068)

0.034 (0.017)

0.305 (0.029)

0.094 (0.038)

0.079 (0.060)

0.313 (0.067)

0.068 (0.056)

0.346 (0.066)

0.154 (0.075)

0.125 (0.094)

0.203 (0.096)

0.035 (0.022)

0.302 (0.033)

0.086 (0.036)

0.080 (0.073)

0.218 (0.072)

0.018 (0.011)

0.280 (0.017)

0.047 (0.021)

0.063 (0.042)

0.229 (0.062)

0.047 (0.017)

0.061 (0.024)

0.070 (0.040)

0.090 (0.053)

0.049 (0.015)

0.029 (0.011)

0.044 (0.015)

0.046 (0.020)

0.045 (0.025)

0.038 (0.010)

0.019 (0.006)

0.031 (0.009)

0.030 (0.015)

0.028 (0.010)

0.029 (0.006)

0.025 (0.010)

0.041 (0.013)

0.047 (0.022)

0.040 (0.017)

0.034 (0.009)

0.016 (0.005)

0.030 (0.007)

0.028 (0.011)

0.024 (0.009)

0.025 (0.006)

0.009 (0.003)

0.022 (0.003)

0.016 (0.005)

0.015 (0.004)

0.021 (0.004)

0.038 (0.017)

0.054 (0.020)

0.060 (0.030)

0.066 (0.038)

0.043 (0.014)

0.022 (0.008)

0.038 (0.012)

0.038 (0.019)

0.037 (0.015)

0.033 (0.009)

0.014 (0.004)

0.027 (0.006)

0.024 (0.009)

0.023 (0.008)

0.027 (0.004)

0.020 (0.006)

0.035 (0.010)

0.038 (0.016)

0.033 (0.012)

0.032 (0.009)

0.012 (0.004)

0.025 (0.006)

0.022 (0.007)

0.019 (0.006)

0.024 (0.005)

0.007 (0.003)

0.020 (0.003)

0.013 (0.004)

0.013 (0.003)

0.020 (0.003)

Table 2 reports the results for estimating the 1st eigenfunction. The re-

12

Figure 1: Boxplots of ISEs of ﬁve estimators for estimating the covariance functions

of case 1, n = 100.

Figure 2: Boxplots of ISEs of ﬁve estimators for estimating the covariance functions

of case 2, n = 100.

13

FACETPRSfpca.scMLEloc0.00.20.40.60.81.0Covariance: SNR = 2, m = 50.00.10.20.30.40.50.60.7Covariance: SNR = 2, m = 10FACETPRSfpca.scMLEloc0.00.20.40.60.81.0Covariance: SNR = 5, m = 50.00.10.20.30.40.50.60.7Covariance: SNR = 5, m = 10FACETPRSfpca.scMLEloc0.000.050.100.150.20Covariance: SNR = 2, m = 50.000.020.040.060.080.10Covariance: SNR = 2, m = 10FACETPRSfpca.scMLEloc0.000.050.100.150.20Covariance: SNR = 5, m = 50.000.020.040.060.080.10Covariance: SNR = 5, m = 10sults for estimating the 2nd and 3rd eigenfunctions are shown in Table 3 and

Table 4, respectively. Figure 3 illustrates the superiority of FACE for estimat-

ing eigenfunctions when n = 100, m = 5. FACE tends to outperform other

approaches in most scenarios, while for the remaining scenarios, its perfor-

mance is still comparable with the best one. MLE performs well for case 1

but relatively poorly for case 2, while the opposite is true for loc. TPRS and

fpca.sc perform quite poorly for estimating the 2nd and 3rd eigenfunctions in

both case 1 and case 2.

Figure 3: Boxplots of ISEs of ﬁve estimators for estimating the top 3 eigenfunctions

when n = 100, m = 5. Note that the straight lines are the medians of FACE when

SN R = 5 and the dash lines are the medians of FACE when SN R = 2.

The results for estimation of eigenvalues are shown in Tables 5, 6, and 7.

We have the following ﬁndings: 1) FACE performs the best for estimating

the ﬁrst eigenvalue in case 1; 2) loc performs the best for estimating the ﬁrst

eigenvalue in case 2; 3) MLE performs overall the best for estimating 2nd

and 3rd eigenvalues in both cases, while the performance of FACE is very

close and can be better than MLE under some model scenarios; 4) TPRS,

fpca.sc and loc perform quite poorly for estimating the 2nd and 3rd eigenvalues

in most scenarios. We conclude that FACE shows overall very competitive

14

FACETPRSfpca.scMLElocFACETPRSfpca.scMLEloc 0.00.10.20.30.40.50.6Eigenfunction 1 of case 1SNR = 5SNR = 20.00.51.01.52.0Eigenfunction 2 of case 1SNR = 5SNR = 20.00.51.01.52.0Eigenfunction 3 of case 1SNR = 5SNR = 2FACETPRSfpca.scMLElocFACETPRSfpca.scMLEloc 0.00.51.01.52.0Eigenfunction 1 of case 2SNR = 5SNR = 20.00.51.01.52.0Eigenfunction 2 of case 2SNR = 5SNR = 20.00.51.01.52.0Eigenfunction 3 of case 2SNR = 5SNR = 2Table 2: Median and IQR (in parenthesis) of ISEs of ﬁve estimators for estimating

the 1st eigenfunction. The results are based on 200 replications.

Case

n m SNR

FACE

TPRS

fpca.sc

MLE

loc

Case 1

Case 2

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

2

2

2

2

2

2

5

5

5

5

5

5

2

2

2

2

2

2

5

5

5

5

5

5

0.038 (0.039)

0.042 (0.048)

0.071 (0.092)

0.050 (0.064)

0.017 (0.017)

0.024 (0.026)

0.029 (0.026)

0.051 (0.063)

0.030 (0.029)

0.026 (0.026)

0.013 (0.011)

0.015 (0.014)

0.025 (0.024) 0.012 (0.012)

0.018 (0.016)

0.020 (0.026)

0.026 (0.025)

0.058 (0.062)

0.029 (0.034)

0.025 (0.025)

0.015 (0.015)

0.016 (0.013)

0.030 (0.030) 0.012 (0.011)

0.015 (0.014)

0.008 (0.009)

0.009 (0.007)

0.016 (0.016) 0.005 (0.007)

0.010 (0.008)

0.025 (0.024)

0.040 (0.041)

0.064 (0.080)

0.032 (0.046)

0.016 (0.020)

0.013 (0.013)

0.024 (0.019)

0.041 (0.042)

0.019 (0.019)

0.022 (0.019)

0.009 (0.010)

0.013 (0.010)

0.023 (0.024) 0.009 (0.010)

0.018 (0.016)

0.014 (0.017)

0.024 (0.025)

0.052 (0.060)

0.025 (0.028)

0.021 (0.025)

0.008 (0.009)

0.013 (0.010)

0.025 (0.025)

0.011 (0.011)

0.012 (0.013)

0.005 (0.006)

0.009 (0.007)

0.014 (0.015) 0.004 (0.004)

0.008 (0.008)

0.436 (0.748)

0.554 (0.916)

0.689 (0.981)

0.869 (0.959)

0.515 (0.781)

0.338 (0.722)

0.422 (0.794)

0.557 (0.854)

0.501 (0.808)

0.317 (0.573)

0.329 (0.556)

0.434 (0.698)

0.464 (0.730)

0.398 (0.718)

0.209 (0.455)

0.437 (0.610)

0.513 (0.709)

0.645 (0.713)

0.578 (0.696)

0.408 (0.638)

0.312 (0.649)

0.378 (0.690)

0.470 (0.743)

0.417 (0.682)

0.208 (0.375)

0.205 (0.517)

0.236 (0.491)

0.272 (0.516)

0.217 (0.457)

0.154 (0.333)

0.400 (0.701)

0.592 (0.720)

0.710 (0.777)

0.786 (0.859)

0.452 (0.701)

0.427 (0.627)

0.462 (0.759)

0.636 (0.792)

0.520 (0.815)

0.329 (0.625)

0.225 (0.496)

0.329 (0.477)

0.396 (0.644)

0.280 (0.604)

0.167 (0.271)

0.390 (0.643)

0.499 (0.735)

0.593 (0.828)

0.468 (0.668)

0.362 (0.666)

0.217 (0.445)

0.317 (0.537)

0.375 (0.511)

0.282 (0.448)

0.184 (0.401)

0.161 (0.327)

0.209 (0.323)

0.248 (0.327)

0.147 (0.298)

0.095 (0.210)

15

Table 3: Median and IQR (in parenthesis) of ISEs of ﬁve estimators for estimating

the 2nd eigenfunction. The results are based on 200 replications.

Case

n m SNR

FACE

TPRS

fpca.sc

MLE

loc

Case 1

Case 2

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

2

2

2

2

2

2

5

5

5

5

5

5

2

2

2

2

2

2

5

5

5

5

5

5

0.179 (0.300)

1.452 (0.783)

0.304 (0.427)

0.185 (0.249)

0.450 (0.573)

0.072 (0.085)

1.477 (0.641)

0.166 (0.142) 0.067 (0.078)

0.218 (0.291)

0.037 (0.037)

1.623 (0.468)

0.079 (0.071) 0.036 (0.034)

0.146 (0.127)

0.069 (0.079)

1.505 (0.583)

0.141 (0.147)

0.070 (0.089)

0.133 (0.136)

0.035 (0.040)

1.663 (0.433)

0.077 (0.057) 0.032 (0.040)

0.094 (0.075)

0.017 (0.021)

1.738 (0.323)

0.037 (0.036) 0.015 (0.014)

0.073 (0.047)

0.086 (0.117)

1.449 (0.647)

0.218 (0.258)

0.096 (0.111)

0.362 (0.505)

0.035 (0.045)

1.498 (0.563)

0.111 (0.103)

0.045 (0.042)

0.187 (0.172)

0.020 (0.024)

1.675 (0.419)

0.061 (0.063)

0.021 (0.022)

0.124 (0.107)

0.043 (0.058)

1.560 (0.475)

0.101 (0.114)

0.050 (0.051)

0.107 (0.120)

0.019 (0.023)

1.668 (0.448)

0.059 (0.048) 0.019 (0.025)

0.084 (0.059)

0.011 (0.012)

1.742 (0.309)

0.032 (0.030) 0.009 (0.009)

0.070 (0.036)

0.700 (0.856)

0.883 (0.967)

1.050 (0.892)

1.238 (0.862)

0.865 (0.959)

0.682 (0.992)

0.771 (0.875)

1.013 (0.935)

1.031 (0.931)

0.655 (0.867)

0.543 (0.823)

0.701 (0.946)

0.838 (0.962)

0.828 (0.782)

0.450 (0.686)

0.640 (0.985)

0.779 (0.887)

0.969 (0.960)

0.971 (0.941)

0.729 (0.860)

0.601 (0.920)

0.688 (0.903)

0.765 (0.887)

0.758 (0.899)

0.430 (0.552)

0.374 (0.658)

0.438 (0.785)

0.508 (0.760)

0.374 (0.583)

0.271 (0.424)

0.687 (0.951)

0.826 (0.901)

1.060 (0.901)

1.101 (0.900)

0.815 (0.935)

0.718 (0.932)

0.777 (1.028)

0.893 (0.983)

0.939 (0.897)

0.669 (0.903)

0.449 (0.676)

0.587 (0.704)

0.706 (0.841)

0.624 (0.934)

0.364 (0.425)

0.701 (0.963)

0.876 (0.986)

1.035 (0.906)

0.881 (0.878)

0.723 (0.861)

0.426 (0.585)

0.589 (0.913)

0.682 (0.944)

0.513 (0.636)

0.392 (0.629)

0.257 (0.446)

0.340 (0.492)

0.384 (0.579)

0.264 (0.437)

0.203 (0.288)

16

Table 4: Median and IQR (in parenthesis) of ISEs of ﬁve estimators for estimating

the 3rd eigenfunction. The results are based on 200 replications.

Case

n m SNR

FACE

TPRS

fpca.sc

MLE

loc

Case 1

Case 2

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

2

2

2

2

2

2

5

5

5

5

5

5

2

2

2

2

2

2

5

5

5

5

5

5

0.352 (0.455)

1.104 (0.998)

0.663 (0.670) 0.339 (0.462)

0.799 (1.020)

0.165 (0.156)

1.210 (0.920)

0.332 (0.346)

0.175 (0.161)

0.468 (0.621)

0.069 (0.061)

1.527 (0.553)

0.163 (0.114) 0.057 (0.058)

0.229 (0.248)

0.110 (0.124)

1.425 (0.784)

0.226 (0.165)

0.117 (0.120)

0.200 (0.251)

0.064 (0.065)

1.556 (0.558)

0.118 (0.088) 0.043 (0.042)

0.125 (0.126)

0.035 (0.040)

1.740 (0.355)

0.060 (0.044) 0.021 (0.017)

0.078 (0.070)

0.156 (0.160)

1.177 (0.813)

0.470 (0.466) 0.156 (0.164)

0.715 (0.827)

0.063 (0.054)

1.369 (0.708)

0.198 (0.179)

0.075 (0.072)

0.309 (0.363)

0.036 (0.029)

1.558 (0.505)

0.119 (0.089) 0.027 (0.025)

0.197 (0.194)

0.054 (0.066)

1.489 (0.629)

0.150 (0.143)

0.056 (0.061)

0.148 (0.188)

0.026 (0.026)

1.629 (0.532)

0.085 (0.078) 0.021 (0.026)

0.096 (0.090)

0.014 (0.013)

1.723 (0.372)

0.046 (0.030) 0.010 (0.009)

0.067 (0.052)

0.610 (0.900)

0.781 (0.992)

1.027 (0.823)

1.177 (0.852)

0.979 (0.827)

0.551 (0.989)

0.679 (0.751)

1.012 (0.760)

1.187 (0.849)

0.747 (0.811)

0.363 (0.506)

0.511 (0.728)

0.760 (0.795)

0.759 (0.795)

0.408 (0.489)

0.663 (0.952)

0.721 (0.917)

1.111 (0.869)

1.093 (0.907)

0.744 (0.826)

0.470 (0.595)

0.590 (0.746)

0.936 (0.919)

0.704 (0.680)

0.431 (0.494)

0.308 (0.471)

0.319 (0.491)

0.505 (0.704)

0.392 (0.598)

0.279 (0.242)

0.449 (0.894)

0.698 (1.010)

1.222 (0.972)

1.229 (0.757)

0.816 (0.910)

0.614 (0.700)

0.670 (0.905)

1.004 (0.866)

1.052 (0.841)

0.588 (0.588)

0.321 (0.582)

0.425 (0.646)

0.709 (0.698)

0.626 (0.725)

0.372 (0.362)

0.812 (0.898)

0.791 (0.858)

1.096 (0.789)

1.121 (0.846)

0.788 (0.823)

0.371 (0.532)

0.501 (0.630)

0.654 (0.759)

0.535 (0.657)

0.359 (0.365)

0.193 (0.252)

0.267 (0.375)

0.362 (0.455)

0.257 (0.300)

0.219 (0.199)

17

performance and never deviates much from the best performance. We use

Figure 4 to illustrate the patterns of eigenvalue estimation for n = 100, m = 5.

Table 5: 100× Median and IQR (in parenthesis) of SEs of ﬁve estimators for esti-

mating the 1st eigenvalue. The results are based on 200 replications.

Case

n m SNR

FACE

TPRS

fpca.sc

MLE

loc

Case 1

Case 2

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

2

2

2

2

2

2

5

5

5

5

5

5

2

2

2

2

2

2

5

5

5

5

5

5

1.736 (5.183)

2.914 (6.793)

2.335 (6.261)

3.877 (9.450)

5.749 (21.780)

0.624 (2.112)

1.507 (3.510)

1.057 (2.680)

4.502 (7.193)

9.324 (9.992)

0.501 (1.213)

0.624 (1.742)

0.603 (1.510)

4.911 (6.375)

11.312 (6.953)

1.212 (3.228)

1.772 (5.345)

1.486 (4.460)

4.009 (9.774)

4.366 (8.535)

0.795 (1.776)

1.210 (2.502)

1.250 (2.418)

4.336 (8.115)

7.887 (7.807)

0.258 (0.700)

0.290 (0.792)

0.284 (0.845)

4.840 (4.729)

8.184 (4.808)

2.087 (4.322)

1.945 (5.038) 1.463 (4.701)

2.708 (7.754)

5.927 (23.788)

0.550 (2.008)

0.774 (2.252)

0.657 (1.847)

5.297 (6.989)

8.499 (6.500)

0.370 (0.855)

0.602 (1.434)

0.540 (1.165)

4.783 (5.177)

11.382 (5.761)

1.387 (3.910)

1.582 (4.156) 1.337 (3.598)

4.295 (8.905)

4.576 (7.862)

0.523 (1.706)

0.778 (2.150)

0.763 (2.105)

4.904 (6.767)

7.577 (7.104)

0.192 (0.534)

0.311 (0.721)

0.308 (0.690)

4.888 (4.324)

8.222 (5.181)

0.265 (0.716)

0.567 (1.193)

0.816 (1.530)

1.359 (1.865) 0.211 (0.542)

0.132 (0.301)

0.245 (0.734)

0.341 (0.904)

0.447 (0.693) 0.124 (0.294)

0.071 (0.185)

0.104 (0.312)

0.156 (0.435)

0.180 (0.281) 0.047 (0.098)

0.115 (0.269)

0.241 (0.556)

0.417 (0.786)

0.307 (0.601) 0.084 (0.273)

0.057 (0.176)

0.111 (0.299)

0.146 (0.384)

0.112 (0.277)

0.062 (0.147)

0.032 (0.068)

0.046 (0.092)

0.056 (0.113)

0.045 (0.093) 0.026 (0.065)

0.127 (0.405)

0.331 (0.840)

0.537 (1.242)

0.783 (1.289)

0.134 (0.437)

0.076 (0.201)

0.162 (0.418)

0.275 (0.543)

0.236 (0.462) 0.062 (0.152)

0.034 (0.105)

0.069 (0.202)

0.095 (0.254)

0.113 (0.229) 0.033 (0.086)

0.063 (0.194)

0.154 (0.383)

0.225 (0.503)

0.207 (0.483)

0.074 (0.225)

0.039 (0.116)

0.077 (0.192)

0.095 (0.248)

0.087 (0.186)

0.042 (0.102)

0.028 (0.080)

0.031 (0.081)

0.038 (0.106)

0.046 (0.091) 0.023 (0.052)

Finally, Table 8 and Figure 5 summarize the run times. When m = 5,

FACE takes about three to six times the computation times of TPRS and

fpca.sc; but it is much faster than MLE and loc, the speed-up is about 20 and

70 fold, respectively. When m = 10, the run times of FACE and fpca.sc are

18

Table 6: 100× Median and IQR (in parenthesis) of SEs of ﬁve estimators for esti-

mating the 2nd eigenvalue. The results are based on 200 replications.

Case

n m SNR

FACE

TPRS

fpca.sc

MLE

loc

Case 1

Case 2

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

2

2

2

2

2

2

5

5

5

5

5

5

2

2

2

2

2

2

5

5

5

5

5

5

1.912 (2.943)

9.879 (6.997)

1.530 (3.118) 0.486 (1.564)

9.088 (4.855)

0.659 (1.710)

10.064 (5.502)

0.783 (1.760) 0.324 (0.857)

10.906 (3.227)

0.313 (0.889)

9.780 (3.503)

0.451 (0.927) 0.146 (0.304)

11.891 (2.358)

0.869 (1.573)

9.662 (4.534)

0.565 (1.554) 0.261 (0.736)

7.351 (3.697)

0.299 (0.978)

9.622 (3.784)

0.282 (0.740) 0.172 (0.488)

8.276 (3.345)

0.205 (0.444)

9.901 (2.740)

0.152 (0.435) 0.096 (0.273)

8.810 (2.439)

2.079 (2.585)

10.684 (5.510)

1.677 (3.346) 0.455 (1.420)

9.634 (5.118)

0.908 (1.556)

10.065 (4.927)

0.628 (1.560) 0.216 (0.583)

11.419 (2.904)

0.356 (0.769)

10.164 (3.484)

0.288 (0.762) 0.100 (0.292)

12.017 (1.920)

1.795 (1.835)

10.207 (4.581)

0.492 (1.327) 0.242 (0.681)

7.831 (4.057)

0.723 (0.965)

9.767 (3.036)

0.214 (0.599) 0.166 (0.453)

8.338 (2.999)

0.311 (0.441)

9.793 (1.964)

0.123 (0.295) 0.055 (0.175)

9.106 (2.132)

0.165 (0.333)

0.177 (0.447)

0.174 (0.430)

0.241 (0.596)

0.140 (0.333)

0.042 (0.112)

0.053 (0.134)

0.057 (0.200)

0.072 (0.211)

0.081 (0.187)

0.021 (0.060)

0.027 (0.065)

0.027 (0.081)

0.035 (0.077)

0.098 (0.196)

0.063 (0.158)

0.061 (0.146)

0.067 (0.207) 0.046 (0.144)

0.091 (0.212)

0.023 (0.062)

0.030 (0.094)

0.033 (0.090) 0.022 (0.060)

0.083 (0.174)

0.015 (0.035)

0.012 (0.050)

0.014 (0.054)

0.015 (0.040)

0.056 (0.121)

0.079 (0.252)

0.128 (0.316)

0.089 (0.270)

0.136 (0.302)

0.154 (0.367)

0.024 (0.102)

0.064 (0.166)

0.065 (0.169)

0.049 (0.141)

0.121 (0.233)

0.016 (0.042)

0.029 (0.071)

0.029 (0.069)

0.028 (0.069)

0.110 (0.216)

0.041 (0.102)

0.043 (0.115)

0.043 (0.125) 0.038 (0.098)

0.115 (0.211)

0.015 (0.044)

0.031 (0.066)

0.031 (0.069)

0.017 (0.043)

0.087 (0.161)

0.012 (0.037)

0.010 (0.034)

0.012 (0.033)

0.011 (0.036)

0.068 (0.126)

19

Table 7: 100× Median and IQR (in parenthesis) of SEs of ﬁve estimators for esti-

mating the 3rd eigenvalue. The results are based on 200 replications.

Case

n m SNR

FACE

TPRS

fpca.sc

MLE

loc

Case 1

Case 2

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

2

2

2

2

2

2

5

5

5

5

5

5

2

2

2

2

2

2

5

5

5

5

5

5

0.167 (0.491)

3.147 (2.115)

0.500 (1.095)

0.224 (0.711)

2.521 (1.915)

0.135 (0.276)

2.843 (2.330)

0.250 (0.710) 0.132 (0.383)

2.763 (1.365)

0.077 (0.197)

2.778 (1.909)

0.157 (0.406)

0.088 (0.218)

2.986 (0.959)

0.115 (0.325)

2.964 (1.778)

0.285 (0.552) 0.114 (0.371)

2.337 (1.448)

0.055 (0.137)

3.019 (1.679)

0.162 (0.331)

0.062 (0.203)

2.487 (0.934)

0.037 (0.084)

3.113 (1.452)

0.094 (0.186) 0.028 (0.087)

2.581 (0.833)

0.274 (0.537)

3.237 (2.045)

0.453 (0.914) 0.192 (0.518)

2.646 (1.684)

0.070 (0.203)

2.801 (2.020)

0.150 (0.459)

0.080 (0.221)

2.860 (1.167)

0.034 (0.084)

3.049 (1.690)

0.122 (0.290)

0.038 (0.086)

3.066 (0.808)

0.130 (0.288)

3.059 (1.669)

0.193 (0.427) 0.093 (0.224)

2.416 (1.112)

0.037 (0.127)

3.038 (1.704)

0.111 (0.245) 0.033 (0.118)

2.565 (0.879)

0.019 (0.054)

2.960 (1.349)

0.038 (0.116)

0.021 (0.063)

2.680 (0.590)

0.418 (1.067)

0.407 (0.826)

0.184 (0.501) 0.085 (0.230)

0.370 (0.474)

0.093 (0.283)

0.140 (0.373)

0.059 (0.163) 0.029 (0.083)

0.286 (0.373)

0.029 (0.083)

0.056 (0.138)

0.030 (0.067) 0.016 (0.039)

0.258 (0.311)

0.072 (0.154)

0.097 (0.188)

0.035 (0.100) 0.019 (0.065)

0.227 (0.294)

0.030 (0.078)

0.047 (0.102)

0.025 (0.065) 0.016 (0.049)

0.196 (0.248)

0.012 (0.032)

0.018 (0.048)

0.013 (0.037) 0.011 (0.027)

0.162 (0.151)

0.204 (0.547)

0.257 (0.478)

0.092 (0.294) 0.060 (0.203)

0.380 (0.443)

0.046 (0.140)

0.119 (0.279)

0.056 (0.142) 0.029 (0.069)

0.291 (0.321)

0.013 (0.041)

0.039 (0.090)

0.019 (0.051)

0.017 (0.048)

0.224 (0.236)

0.044 (0.102)

0.067 (0.149)

0.026 (0.072) 0.023 (0.054)

0.217 (0.251)

0.016 (0.037)

0.031 (0.063)

0.017 (0.042) 0.011 (0.032)

0.174 (0.208)

0.010 (0.028)

0.013 (0.037)

0.014 (0.029) 0.010 (0.025)

0.154 (0.159)

20

Figure 4: Boxplots of 100× SEs of ﬁve estimators for estimating the eigenfunctions

when n = 100, m = 5. Note that the straight lines are the medians of FACE when

SN R = 5 and the dash lines are the medians of FACE when SN R = 2.

quite close to each other, and FACE is only slower than TPRS; run times of

MLE and loc are over 13 and 160 fold of FACE, respectively. Because TPRS

and fpca.sc are naive covariance smoothers, their fast speed is oﬀset by their

tendency to have inferior performance in terms of estimation of covariance

functions, eigenfunctions, and eigenvalues.

To summarize, FACE is a relatively fast method coupled with competing

performance against the methods examined above.

6 Applications

6.1 Child growth data

The Contents study was conducted in Pampas de San Juan Miraﬂores and

Nuevo Paraso, two peri-urban shanty towns with high population density, 25

km south of central Lima. These peri-urban communities are comprised of

50,000 residents, the majority of whom are immigrants from rural areas of

the Peruvian Andes who settled nearly 35 years ago and later claimed unused

21

FACETPRSfpca.scMLElocFACETPRSfpca.scMLEloc 0102030405060Eigenvalue 1 of case 1SNR = 5SNR = 205101520Eigenvalue 2 of case 1SNR = 5SNR = 201234567Eigenvalue 3 of case 1SNR = 5SNR = 2FACETPRSfpca.scMLElocFACETPRSfpca.scMLEloc 012345Eigenvalue 1 of case 2SNR = 5SNR = 20.00.51.01.52.0Eigenvalue 2 of case 2SNR = 5SNR = 20.00.51.01.52.0Eigenvalue 3 of case 2SNR = 5SNR = 2Table 8: Median and IQR (in parenthesis) of computation times (in seconds) of ﬁve

estimators for estimating the covariance functions on a desktop with a 2.3 GHz CPU

and 8 GB of RAM. The results are based on 200 replications.
MLE

n m SNR

FACE

Case

TPRS

fpca.sc

loc

Case 1

Case 2

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

100

200

400

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

5

5

5

10

10

10

2

2

2

2

2

2

5

5

5

5

5

5

2

2

2

2

2

2

5

5

5

5

5

5

7.900 (0.100)

4.100 (1.200)

2.400 (1.200)

141.700 (52.700)

477.300 (43.800)

15.400 (0.600)

7.000 (0.300)

7.000 (0.500)

397.700 (52.000)

787.300 (407.100)

30.800 (6.700)

8.500 (2.900)

11.000 (7.100)

615.100 (209.600)

1304.800 (239.000)

14.200 (1.000)

7.500 (1.400)

12.200 (4.400)

153.100 (41.700)

1680.500 (258.900)

27.900 (1.100)

10.300 (0.700)

18.800 (2.700)

249.200 (35.100)

315.900 (39.600)

54.600 (4.900)

23.300 (4.000)

55.700 (17.500)

779.800 (149.300)

772.800 (31.500)

9.900 (0.300)

4.600 (0.200)

2.900 (0.300)

174.200 (26.000)

462.500 (83.800)

19.200 (3.400)

5.500 (0.200)

5.000 (0.300)

295.900 (44.800)

1779.900 (355.500)

32.000 (8.400)

10.000 (2.000)

14.100 (4.500)

767.300 (216.400)

2462.800 (387.200)

14.700 (0.800)

6.900 (1.500)

10.900 (3.300)

143.900 (46.500)

2781.000 (426.800)

28.700 (1.500)

12.100 (3.700)

23.800 (7.500)

361.600 (107.000)

344.400 (26.400)

45.800 (4.400)

23.600 (4.900)

55.600 (17.400)

758.300 (137.200)

486.000 (36.700)

8.900 (4.800)

4.100 (0.300)

2.300 (0.300)

166.600 (28.700)

1035.500 (112.500)

16.500 (0.700)

6.900 (0.500)

6.800 (0.900)

455.100 (51.500)

1173.200 (135.900)

37.100 (12.100)

8.900 (2.400)

11.400 (4.100)

783.900 (262.200)

2704.500 (1303.500)

13.200 (0.700)

6.200 (0.600)

9.600 (2.100)

150.300 (23.100)

2302.800 (461.400)

22.400 (1.400)

10.900 (0.500)

20.400 (1.400)

368.300 (33.100)

203.300 (50.200)

52.200 (16.700)

19.500 (3.700)

42.700 (13.700)

701.200 (117.800)

362.800 (63.300)

9.500 (0.300)

4.100 (0.200)

2.400 (0.200)

179.800 (17.800)

739.400 (60.100)

17.900 (2.400)

6.200 (1.200)

5.900 (1.400)

420.000 (104.600)

1195.800 (356.300)

29.600 (4.000)

9.000 (0.600)

13.400 (1.700)

835.100 (97.700)

2041.200 (981.400)

13.400 (0.900)

6.900 (0.300)

11.300 (1.100)

172.000 (13.000)

2719.900 (1005.600)

27.200 (1.800)

10.400 (3.000)

22.900 (8.200)

311.500 (128.100)

1601.700 (171.800)

45.300 (5.500)

22.100 (5.300)

52.900 (17.800)

772.800 (210.800)

1532.400 (247.800)

22

Figure 5: Boxplots of computation times (in seconds) of ﬁve estimators for esti-

mating the covariance functions when n = 400, SN R = 2.

land on the outskirts of Lima. In the last two decades Pampas has undergone

many economic and social developments. The study contains 197 children

with anthropomorphic measurements taken from birth. Here we focus on the

length curves from birth to 1 year. Each child has 10 to 32 measurements of

length, with 4320 data points in total. Figure 6 displays the sample length

trajectories of 4 children. We apply the proposed method to the data. The es-

timated population mean is plotted in Figure 6 as a solid line. The estimated

mean curve is generally increasing with age, which is expected. In Figure 7,

we plot the estimated variance and correlation functions. The estimated vari-

ance function is increasing as a function of age. The estimated correlation

function is shown as a heat map. Each point in the heat map represents the

estimated correlation between two days and the color corresponds to the cor-

relation values with red indicating higher correlation and blue indicating lower

correlation. The diagonal is dark red indicating perfect correlation, while the

minimum correlation is about 0.2. Given the estimated covariance function,

using the framework described in Section 4, we predict each child’s length

23

FACETPRSfpca.scMLElocTime(in seconds) of case 1: m = 517554032981Time(in seconds) of case 1: m = 1017554032981FACETPRSfpca.scMLElocTime(in seconds) of case 2: m = 517554032981Time(in seconds) of case 2: m = 1017554032981trajectories. For the 4 children in Figure 6, Figure 8 displays the predicted

length trajectories with point-wise conﬁdence bands. We see that the point-

wise conﬁdence intervals are very narrow due to that for these 4 children a

large number of observations are available.

Figure 6: Length trajectories of 4 children from birth to 1 year old. The estimated

population mean is the solid line.

6.2 CD4 data

CD4 cells are a type of white blood cells that could send signals to the human

body to activate the immune response when they detect viruses or bacte-

ria.Thus, the CD4 count is an important biomarker used for assessing the

health of HIV infected persons as HIV viruses attack and destroy the CD4

cells. The dataset analyzed here is from the Multicenter AIDS Cohort Study

(MACS) and is available in the refund R package (Huang et al., 2015). The

observations are CD4 cell counts for 366 infected males in a longitudinal study

(Kaslow et al., 1987). With a total of 1888 data points, each subject has be-

24

05010020030045556575Age (days)Length (cm)MeanChild 1Child 2Child 3Child 4Figure 7: Estimated variance function (left panel) and correlation function (right

panel) for the length of children from birth to 1 year old.

Figure 8: Predicted child-speciﬁc length trajectories from birth to 1 year old and

associated 95% conﬁdence bands for 4 children. The estimated population mean is

the dotted line.

25

01002003004.05.06.07.0Length: variance functionAge (days)Unit: cm^25015025035050150250350Length: correlation functionAge (days)Age (days)0.70.80.91.0010020030045556575Child 1Age (days)Length (cm)meanprediction010020030045556575Child 2Age (days)Length (cm)meanprediction010020030045556575Child 3Age (days)Length (cm)meanprediction010020030045556575Child 4Age (days)Length (cm)meanpredictiontween 1 and 11 observations. Statistical analysis based on this or related

datasets were done in Diggle et al. (1994), Yao et al. (2005), Peng and Paul

(2009) and Goldsmith et al. (2013).

For our analysis we consider log (CD4 count) since the counts are skewed.

We plot the data in Figure 9 where the x-axis is months since seroconversion

(i.e., the time at which HIV becomes detectable). The overall trend seem to

be decreasing, as can be conﬁrmed by the estimated mean function plotted

in Figure 9. The estimated variance and correlation functions are displayed

in Figure 10. It is interesting to see that the minimal value of the estimated

variance function occurs at month 0 since seroconversion. Finally we display

in Figure 11 the predicted trajectory of log (CD4 count) for 4 males and the

corresponding pointwise conﬁdence bands.

Figure 9: Observed log (CD4 count) trajectories of 366 HIV-infected males. The

estimated population mean is the black solid line.

26

−20−100102030404.55.56.57.5Months since seroconversionlog (CD4 count)Figure 10: Estimated variance function (left panel) and correlation function (right

panel) for the log (CD4 count).

Figure 11: Predicted subject-speciﬁc trajectories of log (CD4 count) and associated

95% conﬁdence bands for 4 males. The estimated population mean is the dotted

line.

27

−20010300.100.150.200.25CD4: variance functionMonths since seroconversion−2001030−2001030CD4: correlation functionMonths since seroconversionMonths since seroconversion0.20.40.60.81.0−20−1001020304045678Male 1Months since seroconversionlog (CD4 count)meanprediction−20−1001020304045678Male 2Months since seroconversionlog (CD4 count)meanprediction−20−1001020304045678Male 3Months since seroconversionlog (CD4 count)meanprediction−20−1001020304045678Male 4Months since seroconversionlog (CD4 count)meanprediction7 Discussion

Estimating and smoothing covariance operators is an old problem with many

proposed solutions. Automatic and fast covariance smoothing is not fully

developed and, in practice, one still does not have a method that is used

consistently. The reason why the practical solution to the problem has been

quite elusive is the lack of automatic covariance smoothing software. The

novelty of our proposal is that it directly tackles this problem from the point

of view of practicality. Here we proposed a method that we are already using

extensively in practice and which is becoming increasingly popular among

practitioners.

The ingredients of the proposed approach are not all new, but their com-

bination leads to a complete product that can be used in practice. The fun-

damentally novel contributions that make everything practical are: 1) use

a particular type of penalty that respects the covariance matrix format; 2)

provide a very fast ﬁtting algorithm for leave-one-subject-out cross valida-

tion; and 3) ensure the scalability of the approach by controlling the overall

complexity of the algorithm.

To make methods transparent and reproducible, the method will be made

publicly available soon in the face package and will be incorporated in the

function fpca.face in the refund package later. The current fpca.face function

(Xiao et al., 2014) deals with high-dimensional functional data observed on

the same grid and has been used extensively by our collaborators. We have a

long track-record of releasing functional data analysis software and the ﬁnal

form of the function will be part of the next release of refund.

Appendix

Proofs

Proof of Proposition 1: We will use the following Lemma (Isserlis, 1918) in

our proof.

28

Lemma 2. If (x1,··· , x2n) is a zero mean multivariate normal random vector,
then

E(x1,··· , x2n) =

(cid:88)(cid:89) E(xixj),

where xixj are all distinct pairs over x1, x2,··· , x2n.

First, we have

Cov( ˆCijj(cid:48), ˆCikk(cid:48)) = E( ˆCijj(cid:48) ˆCikk(cid:48)) − E( ˆCijj(cid:48))E( ˆCikk(cid:48)),

E( ˆCijj(cid:48)) = C(tij, tij(cid:48)) + δjj(cid:48)σ2
 ,
E( ˆCikk(cid:48)) = C(tik, tik(cid:48)) + δkk(cid:48)σ2
 .

Then,

E( ˆCijj(cid:48))E( ˆCikk(cid:48))

= C(tij, tij(cid:48))C(tik, tik(cid:48)) + C(tij, tij(cid:48))δkk(cid:48)σ2

 + C(tik, tik(cid:48))δjj(cid:48)σ2

 + δjj(cid:48)σ2

 δkk(cid:48)σ2
 .

Next, we derive E( ˆCijj(cid:48) ˆCikk(cid:48)) that

E( ˆCijj(cid:48) ˆCikk(cid:48))

= E(yijyij(cid:48)yikyik(cid:48))
= E{(xi(tij) + ij)(xi(tij(cid:48)) + ij(cid:48))(xi(tik) + ik)(xi(tik(cid:48)) + ik(cid:48))}
= E{xi(tij)xi(tij(cid:48))xi(tik)xi(tik(cid:48))} + E{xi(tij)xi(tij(cid:48))xi(tik)}E(ik(cid:48))
+ E{xi(tij)xi(tij(cid:48))xi(tik(cid:48))}E(ik) + E{xi(tij)xi(tij(cid:48))}E(ikik(cid:48))
+ E{xi(tij)xi(tik)xi(tik(cid:48))}E(ij(cid:48)) + E{xi(tij)xi(tik)}E(ij(cid:48)ik(cid:48))
+ E{xi(tij)xi(tik(cid:48))}E(ij(cid:48)ik) + E{xi(tij)}E(ij(cid:48)ikik(cid:48))
+ E{xi(tij(cid:48))xi(tik)xi(tik(cid:48))}E(ij) + E{xi(tij(cid:48))xi(tik)}E(ijik(cid:48))
+ E{xi(tij(cid:48))xi(tik(cid:48))}E(ijik) + E{xi(tij(cid:48))}E(ijikik(cid:48))
+ E{xi(tik)xi(tik(cid:48))}E(ijij(cid:48)) + E{xi(tik)}E(ijij(cid:48)ik(cid:48))
+ E{xi(tik(cid:48))}E(ijij(cid:48)ik) + E(ijij(cid:48)ikik(cid:48))
= E{xi(tij)xi(tij(cid:48))xi(tik)xi(tik(cid:48))} + E(ijij(cid:48)ikik(cid:48))
+ C(tij, tij(cid:48))δkk(cid:48)σ2
+ C(tij(cid:48), tik)δjk(cid:48)σ2

 + C(tij, tik(cid:48))δj(cid:48)kσ2
 + C(tik, tik(cid:48))δjj(cid:48)σ2
 .

 + C(tij, tik)δj(cid:48)k(cid:48)σ2
 + C(tij(cid:48), tik(cid:48))δjkσ2



29

Finally, with E( ˆCijj(cid:48))E( ˆCikk(cid:48)) and E( ˆCijj(cid:48) ˆCikk(cid:48)), we have
Cov( ˆCijj(cid:48), ˆCikk(cid:48)) = C(tij, tij(cid:48))C(tik, tik(cid:48)) + C(tij, tik)C(tij(cid:48), tik(cid:48)) + C(tij, tik(cid:48))C(tij(cid:48), tik)

 δkk(cid:48)σ2

 + δjkσ2

 δj(cid:48)k(cid:48)σ2

 δj(cid:48)kσ2

 + C(tij, tik(cid:48))δj(cid:48)kσ2
 + C(tik, tik(cid:48))δjj(cid:48)σ2

 + δjk(cid:48)σ2
 + C(tij, tik)δj(cid:48)k(cid:48)σ2
 + C(tij(cid:48), tik(cid:48))δjkσ2

+ δjj(cid:48)σ2
+ C(tij, tij(cid:48))δkk(cid:48)σ2
+ C(tij(cid:48), tik)δjk(cid:48)σ2
− C(tij, tij(cid:48))C(tik, tik(cid:48)) − C(tij, tij(cid:48))δkk(cid:48)σ2
= C(tij, tik)C(tij(cid:48), tik(cid:48)) + C(tij, tik(cid:48))C(tij(cid:48), tik) + δjkδj(cid:48)k(cid:48)σ4
 + C(tij(cid:48), tik)δjk(cid:48)σ2
+ C(tij, tik)δj(cid:48)k(cid:48)σ2

 + C(tij, tik(cid:48))δj(cid:48)kσ2

 − C(tik, tik(cid:48))δjj(cid:48)σ2





 δkk(cid:48)σ2


 − δjj(cid:48)σ2
 + δjk(cid:48)δj(cid:48)kσ4

 + C(tij(cid:48), tik(cid:48))δjkσ2
 .

which proves the proposition.

Proof of Proposition 2: We will use the following Lemma (page 241, Seber

2007).

Lemma 3. Let AAA, BBB and CCC and DDD be compatible matrices. Then

tr(AAABBBCCCDDD) = (vec DDD)T (AAA ⊗ CCCT )vec BBBT .

We ﬁrst write iGCV as a sum

iGCV = I + 2

n(cid:88)

i=1

where

(II i − 2III i + IV i),

(8)

I = (cid:107) ˆCCC − SSS ˆCCC(cid:107)2,

T
i SSSii

ˆCCCi,

II i = ˆCCC
III i = (SSSi
IV i = (SSSi

ˆCCC)T SSSii

ˆCCCi,

ˆCCC)T SSSii(SSSi

ˆCCC).

Note that we have the following equalities that will be used later:

SSS = (XXXAAA)[III + λdiag(sss)]−1(XXXAAA)T WWW

= FFF ˜DDD ˜FFF ,

SSSi = (XXX iAAA)[III + λdiag(sss)]−1(XXXAAA)T WWW

= FFF i

˜DDD ˜FFF ,

30

and

SSSii = (XXX iAAA)[III + λdiag(sss)]−1(XXX iAAA)T WWW i

= FFF i

˜DDDFFF T

i WWW i.

We ﬁrst compute I. We have

I = (cid:107) ˆCCC − SSS ˆCCC(cid:107)2
= (cid:107) ˆCCC − FFF ˜DDD˜fff(cid:107)2
= (cid:107) ˆCCC(cid:107)2 − 2fff T ˜DDD˜fff + ˜fff

T ˜DDDFFF T FFF ˜DDD˜fff .

Thus,

I = (cid:107) ˆCCC(cid:107)2 − 2˜ddd

T

(˜fff (cid:12) fff ) + (˜fff (cid:12) ˜ddd)T (FFF T FFF )(˜fff (cid:12) ˜ddd).

(9)

Second, we compute II i. We have

II i = ˆCCC

T
i SSSii

ˆCCCi = fff T
i

˜DDDFFF T

i WWW i

ˆCCCi = fff T
i

T
˜DDDJJJ i = ˜ddd

(JJJ i (cid:12) fff i).

(10)

Third, we compute III i. Note that SSSi

ˆCCC = FFF i

˜DDD˜fff and hence

ˆCCC)T SSSii

ˆCCCi

III i = (SSSi
= ˜fff

T ˜DDDFFF T

˜DDDJJJ i

i FFF i
T ˜DDDFFF T

˜fff
= tr(JJJ i
T{(JJJ i

= ˜ddd

T

˜fff

˜DDD)
i FFF i
) ◦ (FFF T
i FFF i)}˜ddd.

Thus we have

III i = ˜ddd

T{(JJJ i

˜fff

T

) ◦ (FFF T

i FFF i)}˜ddd.

(11)

Fourth, we compute IV i. We derive that

ˆCCC)T SSSii(SSSi

ˆCCC)

IV i = (SSSi
= ˜fff

T ˜DDDFFF T

i FFF i
T ˜DDDFFF T

= tr(˜fff

˜DDDFFF T

i WWW iFFF i

˜DDD˜fff

i FFF i

˜DDDFFF T

˜DDD˜fff )

i WWW iFFF i
T ˜DDD)

i FFF i

˜DDDLLLi

˜DDD˜fff ˜fff

= tr(FFF T
= tr( ˜DDDLLLi(˜fff (cid:12) ˜ddd)(˜fff (cid:12) ˜ddd)T FFF T
= ˜ddd

(cid:111) (cid:12)(cid:110)

T(cid:104)(cid:110)

LLLi(˜fff (cid:12) ˜ddd)

(FFF T

(cid:111)(cid:105)
i FFF i)
i FFF i)(˜fff (cid:12) ˜ddd)

.

31

Hence we obtain

T(cid:8)LLLi (cid:12) (FFF T

i FFF i)(cid:9)(cid:110)

IV i = ˜ddd

(cid:111)

.

(12)

(˜fff (cid:12) ˜ddd) ⊗ (˜fff (cid:12) ˜ddd)

Now with (8), (9), (10), (11) and (12), we obtain that

iGCV = (cid:107) ˆCCC(cid:107)2 − 2˜ddd

T

(˜fff (cid:12) fff ) + (˜fff (cid:12) ˜ddd)T (FFF T FFF )(˜fff (cid:12) ˜ddd) + 2

(JJJ i (cid:12) fff i)

+ 2

T

˜fff

) ◦ (FFF T

i FFF i)

(JJJ i

= (cid:107) ˆCCC(cid:107)2 − 2˜ddd

(˜fff (cid:12) fff ) + (˜fff (cid:12) ˜ddd)T (FFF T FFF )(˜fff (cid:12) ˜ddd) + 2˜ddd

JJJ i (cid:12) fff i

n(cid:88)

i=1

(cid:104)−2˜ddd
T(cid:110)
(cid:40) n(cid:88)

T

(JJJ i

˜fff

i=1

T
˜ddd

n(cid:88)
i FFF i)(cid:9)(cid:110)
(cid:111) ˜ddd + ˜ddd
T(cid:8)LLLi (cid:12) (FFF T
(cid:40) n(cid:88)
(cid:40) n(cid:88)
(cid:41)(cid:110)

i=1

T

LLLi (cid:12) (FFF T

i FFF i)

(cid:41)

(cid:111)(cid:105)

(cid:111)

,

(˜fff (cid:12) ˜ddd) ⊗ (˜fff (cid:12) ˜ddd)

(cid:41)

(˜fff (cid:12) ˜ddd) ⊗ (˜fff (cid:12) ˜ddd)

− 4˜ddd

T

T

) ◦ (FFF T

i FFF i)

T
˜ddd + 2˜ddd

i=1

i=1

which proves the proposition.

32

References

Besse, P., H. Cardot, and F. Ferraty (1997). Simultaneous nonparametric

regressions of unbalanced longitudinal data. Comput. Statist. Data Anal. 24,

255–270.

Besse, P. and J. O. Ramsay (1986). Principal components analysis of sampled

functions. Psychometrika 51, 285–311.

Diggle, P., P. Heagerty, K.-Y. Liang, and S. Zeger (1994). Analysis of longi-

tudinal data. Oxford, U.K.: Oxford University Press.

Eilers, P. and B. Marx (1996). Flexible smoothing with B-splines and penalties

(with Discussion). Statist. Sci. 11, 89–121.

Eilers, P. and B. Marx (2003). Multivariate calibration with temperature in-

teraction using two-dimensional penalized signal regression. Chemometrics

and Intelligent Laboratory Systems 66, 159–174.

Fan, J. and I. Gijbels (1996). Local polynomial modelling and its applications.

London: Chapman&Hall/CRC.

Goldsmith, J., J. Bobb, C. Crainiceanu, B. Caﬀo, and D. Reich (2010). Pe-

nalized functional regression. J. Comput. Graph. Statist. 20, 830–851.

Goldsmith, J., S. Greven, and C. Crainiceanu (2013). Corrected conﬁdence

bands for functional data using principal components. Biometrics 69 (1),

41–51.

Huang, L., F. Scheipl, J. Goldsmith, J. Gellar, J. Harezlak, M. Mclean,

B. Swihart, L. Xiao, C. Crainiceanu, P. Reiss, Y. Chen, S. Greven,

L. Huo, M. Kundu, and J. Wrobel (2015). R package mgcv: Methodol-

ogy for regretssion with functional data (version 0.1-13). URL: https:

//cran.r-project.org/web/packages/refund/index.html.

33

Isserlis, L. (1918, November). On a formula for the product-moment coeﬃcient

of any order of a normal frequency distribution in any number of variables.

Biometrika 12 (1-2), 134–139.

James, G., T. Hastie, and C. Sugar (2000). Principal component models for

sparse functional data. Biometrika 87, 587–602.

Kaslow, R. A., D. G. Ostrow, R. Detels, J. P. Phair, B. F. Polk, and C. R.

Rinaldo (1987). The multicenter aids cohort study: Rationale, organiza-

tion, and selected characteristics of the participants. American Journal of

Epidemiology 126 (2), 310–318.

Kneip, A. (1994). Nonparametric estimation of common regressors for similar

curve data. Ann. Statist. 22, 1386–1427.

Peng, J. and D. Paul (2009). A geometric approach to maximum likelihood es-

timation of functional principal components from sparse longitudinal data.

J. Comput. Graph. Stat. 18, 995–1015.

Ramsay, J. and C. J. Dalzell (1991). Some tools for functional data analysis

(with Discussion). J. R. Statist. Soc. B 53, 539–572.

Reiss, P., L. Huang, and M. Mennes (2010). Fast function-on-scalar regression

with penalized basis expansions. Int. J. Biostat. 6, 28.

Seber, G. (2007). A Matrix Handbook for Statisticians. New Jersey: Wiley-

Interscience.

Staniswalis, J. and J. Lee (1998). Nonparametric regression analysis of longi-

tudinal data. J. Amer. Statist. Assoc. 93, 1403–1418.

Wood, S. (2003). Thin plate regression splines. J. R. Statist. Soc. B 65,

95–114.

Wood, S. (2013). R package mgcv: Mixed GAM computation vehicle with

GCV/AIC/REML, smoothese estimation (version 1.7-24). URL:http://

cran.r-project.org/web/packages/mgcv/index.html.

34

Xiao, L., L. Huang, J. Schrack, L. Ferrucci, V. Zipunnikov, and C. Crainiceanu

(2015). Quantifying the life-time circadian rhythm of physical activity: a

covariate-dependent functional approach. Biostatistics 16, 352–367.

Xiao, L., Y. Li, and D. Ruppert (2013). Fast bivariate P-splines: the sandwich

smoother. J. R. Statist. Soc. B 75, 577–599.

Xiao, L., D. Ruppert, V. Zipunnikov, and C. Crainiceanu (2014). Fast co-

variance function estimation for high-dimensional functional data. Stat.

Comput. 26, 409–421.

Xu, G. and J. Huang (2012). Asymptotic optimality and eﬃcient computation

of the leave-subject-out cross-validation. Ann. Statist. 40, 3003–3030.

Yao, F., H. M¨uller, A. Cliﬀord, S. Dueker, J. Follett, Y. Lin, B. Buchholz,

and J. Vogel (2003). Shrinkage estimation for functional principal compo-

nent scores with application to the population kinetics of plasma folate.

Biometrics 20, 852–873.

Yao, F., H. M¨uller, and J. Wang (2005). Functional data analysis for sparse

longitudinal data. J. Amer. Statist. Assoc. 100, 577–590.

35

