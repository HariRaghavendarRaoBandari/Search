6
1
0
2

 
r
a

M
4

 

 
 
]
S
D
h
t
a
m

.

[
 
 

1
v
3
7
5
1
0

.

3
0
6
1
:
v
i
X
r
a

McCulloch-Pitts brains

and pseudorandom functions

Vaˇsek Chv´atal 1, Mark Goldsmith 2, and Nan Yang 3

Department of Computer Science and Software Engineering

Concordia University, Montreal

Abstract

In a pioneering classic, Warren McCulloch and Walter Pitts proposed a model
of the central nervous system. Motivated by EEG recordings of normal brain
activity, Chv´atal and Goldsmith asked whether or not these dynamical systems can
be engineered to produce trajectories which are irregular, disorderly, apparently
unpredictable. We show that they cannot build weak pseudorandom functions.

Electroencephalogram recordings of normal brain (or of an epileptic brain well
before a seizure) are usually irregular, disorderly, with no apparent pattern: see,
for instance, [21, 20, 7, 16, 4, 31, 2]. Chv´atal and Goldsmith [5] asked whether or
not the McCulloch-Pitts model of the brain can be engineered to exhibit similar
behaviour. The same question, although without its physiological interpretation,
was also asked in [10]. Let us begin by brieﬂy describing the McCulloch-Pitts
model.

A linear threshold function is a function f : Rn → {0, 1} such that, for some real
numbers w1, . . . , wn and θ,

f (x1, . . . , xn) = H (cid:16)Pn

j=1 wjxj − θ(cid:17)

where H is the Heaviside step function deﬁned by H(d) = 1 for all nonnegative d
and H(d) = 0 for all negative d. Warren McCulloch and Walter Pitts [22] proposed
a model of the central nervous system built from linear threshold functions. When
this system has n neurons and no peripheral aﬀerents, its McCulloch-Pitts model
is a mapping Φ : {0, 1}n → {0, 1}n deﬁned by

Φ(x) = (f1(x), . . . , fn(x))

1chvatal@cse.concordia.ca
2markgoldsmith@gmail.com
3nan.yang@me.com

1

for some linear threshold functions f1, . . . , fn. We will refer to such mappings Φ
as McCulloch-Pitts dynamical systems.

Chv´atal and Goldsmith [5] asked whether or not these dynamical systems can
produce trajectories which are irregular, disorderly, apparently unpredictable in
the sense of generating random numbers. In making the meaning of their question
precise, they took the point of view of the practitioners, who mean by a random
number generator any deterministic algorithm that, given a short sequence of
numbers, called a seed, returns a longer sequence of numbers; such a random
number generator is considered to be good if it passes statistical tests from some
commonly agreed on battery. (This point of view is expounded in [18, Chapter
3].)

In this note, we take the point of view of the theorists: we are going to prove that
McCulloch-Pitts dynamical systems cannot produce trajectories which are irregu-
lar, disorderly, apparently unpredictable in the sense of providing weak pseudoran-
dom functions. These have been introduced in [25] and subsume pseudorandom
functions, introduced in [13] under the original name of ‘poly-random collections’.
Roughly speaking, a weak pseudorandom function is a probability distribution
on a set Fn of functions from {0, 1}n to {0, 1}n with the following property:
if
x1, . . . , xm are chosen independently and uniformly at random from {0, 1}n, then
no polynomial-time randomized algorithm can distinguish with a non-negligible
probability between (i) a sequence (x1, f (x1), . . . , xm, f (xm)) where f is chosen
at random from Fn and (ii) a sequence (x1, y1, . . . , xm, ym) where y1, . . . , ym are
chosen independently and uniformly at random from {0, 1}n. (Distinguishing be-
tween (i) and (ii) is a trivial matter when f is known and that is why an unknown
f must be drawn from a probability distribution on Fn.) Our result shows that
weak pseudorandom functions cannot be built from McCulloch-Pitts dynamical
systems:

Theorem 1. There is a polynomial-time deterministic algorithm that, given a se-
quence (x1, y1, . . . , xm, ym) of n-bit vectors, returns either the message McCulloch-
Pitts or the message not McCulloch-Pitts in such a way that

(i) if y1 = Φ(x1), . . . , ym = Φ(xm) for some McCulloch-Pitts dynamical system

Φ, then the algorithm returns McCulloch-Pitts,

(ii) if x1, . . . , xm are chosen independently and uniformly at random from {0, 1}n,
if y1, . . . , ym are chosen independently and uniformly at random from {0, 1}n,
and if m ≥ (2 + ε)n for some positive constant ε, then the algorithm returns
not McCulloch-Pitts with probability at least 1−e−δn, where δ is a positive
constant depending only on ε.

2

A dichotomy of a set X is its partition into two disjoint sets. Unlike Cover [6], for
whom a dichotomy is an unordered pair of sets, we view every dichotomy as an
ordered pair of sets. A dichotomy (X +, X −) of a subset of Rn is linearly separable
if there are numbers y1, . . . , yn+1 such that

Pn
j=1xjyj > yn+1 whenever (x1, . . . , xn) ∈ X +,
Pn
j=1xjyj < yn+1 whenever (x1, . . . , xn) ∈ X −.

(1)

When f is a function from {0, 1}n to {0, 1} and x1, . . . , xm are points in {0, 1}n,
the dichotomy ({xi : f (xi) = 0}, {xi : f (xi) = 1}) is linearly separable if and only
if f is a threshold function. Our proof of Theorem 1 evolves from the propositions
that linearly separable dichotomies are easy to recognize and linearly separable
dichotomies are rare:

Lemma 1. Linearly separable dichotomies of m-point subsets of {0, 1}n can be
recognized in time polynomial in m and n.

Lemma 2. For every positive ε there is a positive γ with the following property:
If X is a ﬁnite subset of Rn such that |X| ≥ (2 + ε)n, then a dichotomy chosen
uniformly at random from all dichotomies of X is linearly separable with probability
at most e−γn.

Following the seminal report [34], the subject of learning a hyperplane that sep-
arates, or at least nearly separates, the two parts of a dichotomy received much
attention in the machine learning community. None of it is relevant to the follow-
ing standard argument, implicit in the linear programming proof of Minkowski’s
Separating Hyperplane Theorem for convex polytopes [37].

Deciding whether a prescribed dichotomy of an m-point
Proof of Lemma 1.
subset of {0, 1}n is linearly separable amounts to solving system (1) of m strict
linear inequalities in variables y1, . . . , yn+1, where each coeﬃcient xj is 0 or 1; the
epoch-making result of Khachiyan [17] guarantees that this can be done in time
polynomial in m and n.
(cid:3)

Proof of Lemma 2. Without loss of generality, we may assume that that 0 < ε ≤
1. Let m denote |X| and let p denote the probability that a dichotomy chosen
uniformly at random from all dichotomies of X is linearly separable.

at least implicit in [39] and [6]), and so

Of the 2m dichotomies of X, at most 2Pn
i=0(cid:0)m−1

p ≤ 2−m+1Pn

i (cid:1) ≤ 2−m+1Pn

i=0(cid:0)m
i(cid:1).

3

i=0(cid:0)m−1

i (cid:1) are linearly separable (this is

Since m ≥ (2 + ε)n and 0 < ε ≤ 1, we have n ≤ (0.5 − ε/6)m; a special case of the
well-known bound on the tail of the binomial distribution (see, for instance, [15,
Theorem 1]) guarantees that for every positive α smaller than 0.5 there is a positive
β such that

setting α = ε/6, we conclude that p ≤ 2e−βm, which proves the lemma.

Pi≤(0.5−α)m(cid:0)m

i(cid:1) ≤ 2me−βm;

(cid:3)

An alternative proof of Lemma 2, proposed by one of the reviewers, relies on
the Sauer-Shelah Lemma ([35], [36]): If a family of subsets of an m-point set

has Vapnik-Chervonenkis dimension d, then it includes at most Pd

other ingredient is the following corollary of Radon’s theorem [33]: If H is a family
of half-spaces in Rn and if X is a ﬁnite subset of Rn, then family {X ∩Y : Y ∈ H}
has Vapnik-Chervonenkis dimension at most n + 1. Putting the two together, we

i(cid:1) sets. Its

i=0(cid:0)m

conclude that X has at most 2Pn+1
upper bound, although weaker than our 2Pn

i(cid:1) linearly separable dichotomies. This
i (cid:1), also yields the lemma’s

i=0(cid:0)m−1

i=0 (cid:0)m

conclusion.

Proof of Theorem 1. The algorithm goes as follows: Let αi denote the ﬁrst bit of
yi and deﬁne

X + = {xi : 1 ≤ i ≤ m, αi = 1},
X − = {xi : 1 ≤ i ≤ m, αi = 0}.

If this dichotomy is linearly separable, then return McCulloch-Pitts; else return
not McCulloch-Pitts.

Lemma 1 guarantees that the algorithm can be implemented to run in polynomial
time.

To prove (i), assume that y1 = Φ(x1), . . . , ym = Φ(xm) for some McCulloch-Pitts
dynamical system Φ : {0, 1}n → {0, 1}n deﬁned by Φ(x) = (f1(x), . . . , fn(x)). Now
αi = f1(xi) for all i = 1, . . . , m, which means that f1 takes value 1 on all points of
X + and value 0 on all points of X −; since f1 is a threshold function, the dichotomy
(X +, X −) is linearly separable, and so the algorithm returns McCulloch-Pitts.

To prove (ii), assume that x1, . . . , xm are chosen independently and uniformly at
random from {0, 1}n, that y1, . . . , ym are chosen independently and uniformly at
random from {0, 1}n, and that m ≥ (2 + ε)n for some positive constant ε. Since
the probability that the algorithm returns not McCulloch-Pitts increases as m
increases, we may replace the assumption that m ≥ (2+ε)n by the assumption that
m = ⌈(2 + ε)n⌉. Write X = X + ∪ X −. Since x1, . . . , xm are chosen independently

4

and uniformly from {0, 1}n, they are pairwise distinct with probability 2n(2n −
1) · · · (2n − m + 1)/2nm; since

2n(2n − 1) · · · (2n − m + 1)

2nm

≥ (cid:18) 2n − m

2n (cid:19)m

= (cid:16)1 −

m

2n(cid:17)m

≥ 1 −

m2
2n ,

this probability is at least 1 − 5n22−n. When |X| = m, the assumption that
y1, . . . , ym are chosen independently and uniformly from {0, 1}n implies that the
dichotomy (X +, X −) of X is chosen uniformly from all dichotomies of X, in which
case Lemma 2 guarantees that (X +, X −) is linearly separable with probability at
most e−γn for some positive constant γ depending only on ε. We conclude that the
algorithm returns not McCulloch-Pitts with probability at least 1 − 5n22−n −
e−γn, which is at least 1 − e−δn for some positive constant δ depending only on ε.
(cid:3)

There is an obvious reﬁnement of the algorithm used in the proof of Theorem 1:
with yi

j standing for the j-th bit of yi, test each of the n dichotomies
(cid:0){xi : 1 ≤ i ≤ m, yi

j = 1}, {xi : 1 ≤ i ≤ m, yi

j = 0}(cid:1)

(j = 1, . . . , n)

and return McCulloch-Pitts if and only if all n of them are linearly separable. In
the context of distinguishing McCulloch-Pitts functions from truly random func-
tions, the extra work required in this reﬁnement is pointless. The probability of
returning McCulloch-Pitts when y1, . . . , ym are chosen independently and uni-
formly at random from {0, 1}n is at most e−δn in the original version and that is
good enough; reducing it further to e−δn2
in the reﬁnement is nice, but unneces-
sary. In addition, the assumption m ≥ (2 + ε)n cannot be signiﬁcantly relaxed
even in the reﬁnement: it is at least implicit in [39] and [6] that a dichotomy chosen
uniformly at random from all dichotomies of a set of fewer than (2 − ε)n points in
Rn is linearly separable with probability at least 1 − e−δn.

Theorem 1 implies that certain simple devices (namely, McCulloch-Pitts dynamical
systems) cannot generate pseudorandomness. In the opposite direction, it has been
proved that certain simple devices can generate pseudorandomness: examples can
be found in [24], [19], [27], [26], [3].

The question whether McCulloch-Pitts networks can produce trajectories which
are irregular, disorderly, apparently unpredictable remains open: all depends on
the interpretation of the terms “irregular, disorderly, apparently unpredictable”.
When clinical neurologists visually inspect an electroencephalogram, their vague
criteria for declaring it random-like are a far cry from the distinguishers that
cryptographers use to separate deterministic sequences from random sequences.
As Avi Wigderson [38, page 6] put it,

5

“Randomness is in the eye of the beholder, or more precisely, in its
computational capabilities ... a phenomenon (be it natural or arti-
ﬁcial) is deemed “random enough,” or pseudorandom, if the class of
observers/applications we care about cannot distinguish it from ran-
dom!”

Many examples of generators that appear random to observers with restricted
computational powers are known.
In particular, pseudorandom generators for
polynomial size constant depth circuits have been constructed in [1]; later, this
work was greatly simpliﬁed and improved in [28]. O’Connor [30] proved that an
inﬁnite binary sequence appears random to all ﬁnite-state machines if and only
if it is ∞-distributed. Pseudorandom generators for space-bounded computation
have been constructed in [29]. It is conceivable that McCulloch-Pitts dynamical
systems could fool neurologists into ﬁnding their trajectories unpredictable just as
they ﬁnd normal electroencephalograms unpredictable. Proving this in a formal
setting with a suitable deﬁnition of ‘neurologists’ is an interesting challenge.

A variation on our theme comes from the idea that in a brain of n neurons, only
m neurons may be visible to the observer and the remaining n − m are hidden
from view. Formally, given positive integers m, n such that m ≤ n and given a
McCulloch-Pitts dynamical system Φ : {0, 1}n → {0, 1}n, we may consider the
mapping Φm : {0, 1}n → {0, 1}m such that Φm(x) is the m-bit preﬁx of Φ(x).
Can such mappings provide pseudorandomness? Our Theorem 1 shows that the
answer is negative when m = n; one of the reviewers argued that, under the
usual assumption that one-way functions exist, the answer is close to aﬃrmative
when m = 1. Here is the argument: Every one-way function f (as every boolean
function) can be computed by a threshold circuit [32, Chapter 7]. When this circuit
has n gates and depth d, it can be imbedded in a McCulloch-Pitts dynamical
system Φ : {0, 1}n → {0, 1}n, where the results of its computation show up with
the time delay of d units. Now f is represented in the d-fold iteration of Φ and
there is an appropriate projection π, the hard-core bit [11], such that the sequence
π(x), π(f (x)), π(f (f (x))), . . . is pseudorandom.

Statistical properties of Φ1 have been studied in [14, Section 4.2]. For instance,
there is a McCulloch-Pitts dynamical system Φ : {0, 1}37 → {0, 1}37 such that the
restriction of the trajectory of Φ on the ﬁrst bit passes all ten statistical tests of
the battery SmallCrush implemented in the software library TestU01 of [8, 9].

Acknowledgments

This research was undertaken, in part, thanks to funding from the Canada Re-

6

search Chairs program and from the Natural Sciences and Engineering Research
Council of Canada. We are grateful to P´eter G´acs for helpful comments on a draft
of this note and to Avi Wigderson for telling us about Nisan’s papers [28, 29]. We
also thank the two anonymous reviewers for their thoughtful comments that made
us improve the presentation considerably.

References

[1] M. Ajtai and A. Wigderson, Deterministic simulation of probabilistic constant
depth circuits, 26th Annual Symposium on Foundations of Computer Science,
pp. 11–19, IEEE, 1985.

[2] S. Altunay, Z. Telatar, and O. Erogul, Epileptic EEG detection using the
linear prediction error energy, Expert Systems with Applications 37 (2010),
5661–5665.

[3] B. Applebaum, Y. Ishai, E. Kushilevitz, Cryptography by cellular automata

or how fast can complexity emerge in nature? in: ICS (pp. 1–19), 2010.

[4] W.A. Chaovalitwongse, Optimization and Data Mining in Epilepsy Re-
search: A Review and Prospective, in: Handbook of Optimization in Medicine
(P.M. Pardalos and H.E. Romeijn, eds.), Springer, 2009, pp. 1–32.

[5] V. Chv´atal and M. Goldsmith, Can brains generate random numbers?

arXiv:1208.6451 [math.DS].

[6] T. Cover, Geometrical and statistical properties of systems of linear inequali-
ties with applications in pattern recognition, IEEE Transactions on Electronic
Computers (1965), 326–334.

[7] F.L. Da Silva et al., Epilepsies as dynamical diseases of brain systems: basic
models of the transition between normal and epileptic activity, Epilepsia 44
(2003), 72–83.

[8] P. L’Ecuyer, R. Simard, TestU01: A C library for empirical testing of random
number generators, ACM Transactions on Mathematical Software 33 (2007),
Article 22, 40 pages.

[9] P. L’Ecuyer, R. Simard, TestU01: A software library in ANSI C for empirical
testing of random number generators. Users guide, compact version. (Version:
August 17, 2009).
http://www.iro.umontreal.ca/~simardr/testu01/guideshorttestu01.pdf

7

[10] Y.M. Elyada, D. Horn, Can dynamic neural ﬁlters produce pseudo-random
sequences? in: Articial neural networks: Biological inspirations - ICANN
2005 (pp. 211–216). Springer, 2005

[11] O. Goldreich, L.A. Levin, A hard-core predicate for all one-way functions,
Proceedings of the 21st annual ACM symposium on theory of computing (pp.
25–32), 1989.

[12] O. Goldreich, Foundations of cryptography. Basic tools, Cambridge University

Press, Cambridge, 2001.

[13] O. Goldreich, S. Goldwasser, S. Micali, How to construct random functions,

Journal of the ACM 33 (1986), 792–807.

[14] M. Goldsmith, Neural networks as pseudorandom number generators (Unpub-

lished doctoral dissertation), Concordia University, 2015.

[15] W. Hoeﬀding, Probability inequalities for sums of bounded random variables,

Journal of the American Statistical Association 58 (1963), 13 – 30.

[16] L.D. Iasemidis et al., Dynamical resetting of the human brain at epileptic
seizures: application of nonlinear dynamics and global optimization tech-
niques, IEEE Transactions on Biomedical Engineering 51 (2004), 493–506.

[17] L.G. Khachiyan, A polynomial algorithm in linear programming. (Russian),

Doklady Akademii Nauk SSSR 244 (1979), 1093–1096.

[18] D.E. Knuth, The Art of Computer Programming, Volume 2: Seminumerical

Algorithms, Addison-Wesley Professional, 2014.

[19] M. Krause, S. Lucks, Pseudorandom functions in T C 0 and cryptographic
limitations to proving lower bounds, Computational complexity 10 (2001),
297–313.

[20] K. Lehnertz, et al., Nonlinear EEG analysis in epilepsy: Its possible use for
interictal focus localization, seizure anticipation, and prevention, Journal of
Clinical Neurophysiology 18 (2001) 209–222.

[21] A. Liu, et al., Detection of neonatal seizures through computerized EEG
analysis, Electroencephalography and clinical neurophysiology 82 (1992) 30–
37.

[22] W.S. McCulloch, W.Pitts, A logical calculus of the ideas immanent in nervous

activity, Bulletin of Mathematical Biophysics 5 (1943) 115–133.

8

[23] S. Muroga, Threshold logic and its applications, Wiley-Interscience [John Wi-

ley & Sons], New York-London-Sydney, 1971.

[24] M. Naor, B. Pinkas, O. Reingold, Distributed pseudo-random functions and

KDCs, in: Advances in cryptology - EUROCRYPT99 (pp. 327–346), 1999.

[25] M. Naor, O. Reingold, Synthesizers and their application to the parallel con-
struction of pseudo-random functions, 36th Annual Symposium on Founda-
tions of Computer Science, pp. 170–181, IEEE, 1995.

[26] M. Naor, O. Reingold, Number-theoretic constructions of eﬃcient pseudo-

random functions, Journal of the ACM 51 (2004), 231–262.

[27] J.B. Nielsen, A threshold pseudorandom function construction and its appli-
cations, in: Advances in cryptology - Crypto 2002 (pp. 401–416), Springer,
2002.

[28] N. Nisan, Pseudorandom bits for constant depth circuits, Combinatorica 11

(1991), 63–70.

[29] N. Nisan, Pseudorandom generators for space-bounded computation, Combi-

natorica 12 (1992), 449–461.

[30] M.G. O’Connor, An unpredictability approach to ﬁnite-state randomness,

Journal of Computer and System Sciences 37 (1988), 324–336.

[31] H. Ocak, Automatic detection of epileptic seizures in EEG using discrete
wavelet transform and approximate entropy, Expert Systems with Applications
36 (2009), 2027–2036

[32] I. Parberry, Circuit complexity and neural networks, MIT press, 1994.

[33] J. Radon, Mengen konvexer k¨orper, die einen gemeinsamen punkt enthalten,

Mathematische Annalen 83 (1921), 113–115.

[34] F. Rosenblatt, The perceptron — a perceiving and recognizing automation.

Cornell Aeronautical Laboratory, Ithaca. Report 85-460-1, 1957.

[35] N. Sauer, On the density of families of sets, Journal of Combinatorial Theory

Series A 13 (1972), 145–147.

[36] S. Shelah, A combinatorial problem; stability and order for models and the-
ories in inﬁnitary languages, Paciﬁc Journal of Mathematics 41 (1972), 247–
261.

9

[37] A.W. Tucker, Linear inequalities and convex polyhedral sets, Proc. 2nd symp.

linear programming (pp. 569–602), 1955.

[38] A. Wigderson, Randomness and Pseudorandomness, The Institute Letter ,

Summer 2009, pp. 1,6,7.
http://www.ias.edu/ﬁles/pdfs/publications/letter-2009-summer.pdf

[39] R.O. Winder, Partitions of N -space by hyperplanes, SIAM Journal on Applied

Mathematics 14 (1966), 811–818.

10

