6
1
0
2

 
r
a

 

M
9
1

 
 
]
L
C
.
s
c
[
 
 

1
v
2
4
0
6
0

.

3
0
6
1
:
v
i
X
r
a

Globally Normalized Transition-Based Neural Networks

Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta,

Kuzman Ganchev, Slav Petrov and Michael Collins

Google Inc

New York, NY

{andor,chrisalberti,djweiss,severyn,apresta,kuzman,slav,mjcollins}@google.com

Abstract

achieves

state-of-the-art

We introduce
a globally normalized
transition-based neural network model
that
part-of-
speech tagging, dependency parsing and
sentence compression results. Our model
is a simple feed-forward neural network
that operates on a task-speciﬁc transition
system, yet achieves comparable or better
accuracies than recurrent models. The
key insight
is based on a novel proof
illustrating the label bias problem and
showing that globally normalized models
can be strictly more expressive than
locally normalized models.

1 Introduction

network

of

natural

have

language

approaches

taken
Neural
processing
the ﬁeld
(NLP) by storm.
In particular, variants of
long short-term memory (LSTM) networks
(Hochreiter and Schmidhuber, 1997)
have
produced impressive results on some of
the
as
classic NLP tasks
part-of-speech
such
tagging (Ling et al., 2015),
syntactic parsing
(Vinyals et al., 2015) and semantic role labeling
(Zhou and Xu, 2015). One might speculate that
it is the recurrent nature of these models that
enables these results.

In this work we demonstrate that simple
feed-forward networks without any recurrence
can achieve comparable or better accuracies
than LSTMs, as long as they are globally
normalized.
Our model, described in de-
tail
in Section 2, uses a transition system
(Nivre, 2006) and feature embeddings as intro-
duced by Chen and Manning (2014). We do not
use any recurrence, but perform beam search
for maintaining multiple hypotheses and intro-

duce global normalization with a conditional ran-
dom ﬁeld (CRF) objective (Bottou et al., 1997;
Le Cun et al., 1998; Lafferty et al., 2001) to over-
come the label bias problem that locally normal-
ized models suffer from. Since we use beam
inference, we approximate the partition func-
tion by summing over the elements in the beam,
and use early updates (Collins and Roark, 2004;
Zhou et al., 2015). We compute gradients based
on this approximate global normalization and per-
form full backpropagation training of all neural
network parameters based on the CRF loss.

We revisit the label bias problem in Section 3
and provide a novel proof that globally normal-
ized models are strictly more expressive than lo-
cally normalized models. Lookahead features
can partially mitigate this discrepancy, but can-
not fully compensate for it—a point to which we
return later. To empirically demonstrate the ef-
fectiveness of global normalization, we evaluate
our model on part-of-speech tagging, syntactic de-
pendency parsing and sentence compression (Sec-
tion 4). Our model achieves state-of-the-art ac-
curacy on all of these tasks, matching or outper-
forming LSTMs while being signiﬁcantly faster.
In particular for dependency parsing on the Wall
Street Journal we achieve the best-ever published
unlabeled attachment score of 94.41%.

As discussed in more detail

in Section 5,
we also outperform previous structured training
approaches used for neural network transition-
based parsing.
Our ablation experiments
show that we outperform Weiss et al. (2015) and
Alberti et al. (2015) because we do global back-
propagation training of all model parameters,
while they ﬁx the neural network parameters when
training the global part of their model. We
also outperform Zhou et al. (2015) despite using a
smaller beam. To shed additional light on the la-
bel bias problem in practice, we provide a sentence

compression example where the local model com-
pletely fails. We then demonstrate that a globally
normalized parsing model without any lookahead
features is almost as accurate as our best model,
while a locally normalized model loses more than
10% absolute in accuracy because it cannot effec-
tively incorporate evidence as it becomes avail-
able.

2 Model

At its core, our model is an incremental transition-
based parser (Nivre, 2006). To apply it to different
tasks we only need to adjust the transition system
and the input features.

2.1 Transition System
Given an input x, most often a sentence, we deﬁne:

• A set of states S.
• A special start state s† ∈ S.
• A set of allowed decisions A(s) for all s ∈ S.
• A transition function t(s, d) returning a new

state s′ for any decision d ∈ A(s).

We drop the dependence on x for brevity. We will
use a function ρ(s, d; θ) to compute the score of
decision d in state s. The vector θ contains the
model parameters and we assume that ρ(s, d; θ) is
differentiable with respect to θ.

Throughout this work we will use transition sys-
tems in which all complete structures for the same
input x have the same number of decisions n(x)
(or n for brevity). In dependency parsing for ex-
ample, this is true for both the arc-standard and
arc-eager transition systems (Nivre, 2006), where
for a sentence x of length m, the number of deci-
sions for any complete parse is n(x) = 2 × m.1
A complete structure is then a sequence of deci-
sion/state pairs (s1, d1) . . . (sn, dn) such that s1 =
s†, di ∈ S(si) for i = 1 . . . n, and si+1 =
t(si, di). We use the notation d1:j to refer to a de-
cision sequence d1 . . . dj.

We assume that there is a one-to-one mapping
between decision sequences d1:j and states sj: that
is, we essentially assume that a state encodes the
entire history of decisions. Thus, each state can be
reached by a unique decision sequence from s†.2
We will use decision sequences d1:j and states in-
terchangeably:
in a slight abuse of notation, we
1Note that this is not true for the swap transition system

deﬁned in Nivre (2009).

2It is straightforward to extend the approach to make use
of dynamic programming in the case where the same state
can be reached by multiple decision sequences.

deﬁne ρ(d1:j , d; θ) to be equal to ρ(s, d; θ) where
s is the state reached by decisions d1:j.

The scoring function ρ(s, d; θ) can be deﬁned
in a number of ways.
In this work, following
Chen and Manning (2014), Weiss et al. (2015),
and Zhou et al. (2015), we deﬁne it via a feed-
forward neural network as

ρ(s, d; θ) = φ(s; θ(l)) · θ(d).

Here θ(l) are the parameters of the neural network,
excluding the parameters at the ﬁnal layer. θ(d) are
the ﬁnal layer parameters for decision d. φ(s; θ(l))
is the representation for state s computed by the
neural network under parameters θ(l). Note that
the score is linear in the parameters θ(d). We next
describe how softmax-style normalization can be
performed at the local or global level.

2.2 Global vs. Local Normalization
In the Chen and Manning (2014) style of greedy
neural network parsing, the conditional probabil-
ity distribution over decisions dj given context
d1:j−1 is deﬁned as

p(dj|d1:j−1; θ) =

exp ρ(d1:j−1, dj; θ)

ZL(d1:j−1; θ)

,

(1)

where

ZL(d1:j−1; θ) = X

exp ρ(d1:j−1, d′; θ).

d′∈A(d1:j−1)

Each ZL(d1:j−1; θ) is a local normalization term.
The probability of a sequence of decisions d1:n is

pL(d1:n) =

n

Y
j=1

p(dj|d1:j−1; θ)

=

exp Pn
Qn

j=1 ρ(d1:j−1, dj; θ)
j=1 ZL(d1:j−1; θ)

.

(2)

Beam search can be used to attempt to ﬁnd the
maximum of (2) with respect to d1:n.

In contrast, a Conditional Random Field (CRF)

deﬁnes a distribution pG(d1:n) as follows:

pG(d1:n) =

exp Pn

j=1 ρ(d1:j−1, dj ; θ)

ZG(θ)

,

(3)

where

ZG(θ) = X

exp

d′
1:n∈Dn

n

X
j=1

ρ(d′

1:j−1, d′

j; θ)

and Dn is the set of all valid sequences of deci-
sions of length n. ZG(θ) is a global normalization
term. The inference problem is now to ﬁnd

argmax
d1:n∈Dn

pG(d1:n) = argmax
d1:n∈Dn

n

X
j=1

ρ(d1:j−1, dj; θ).

Beam search can again be used to approximately
ﬁnd the argmax.

2.3 Training
Training data consists of inputs x paired with gold
1:n. We use stochastic gradi-
decision sequences d∗
ent descent on the negative log-likelihood of the
data under the model. Under a locally normalized
model, the negative log-likelihood is

Llocal(d∗

1:n; θ) = − ln pL(d∗

1:n; θ) =

(4)

−

n

X
j=1

ρ(d∗

1:j−1, d∗

j ; θ) +

n

X
j=1

ln ZL(d∗

1:j−1; θ),

whereas under a globally normalized model it is

Lglobal(d∗

1:n; θ) = − ln pG(d∗

1:n; θ) =

−

n

X
j=1

ρ(d∗

1:j−1, d∗

j ; θ) + ln ZG(θ).

(5)

A signiﬁcant practical advantange of the locally
normalized cost (4) is that it factorizes into n in-
dependent terms, each of which can be computed
exactly and minimized separately. By contrast, the
1:n ∈ Dn that
ZG term in (5) contains a sum over d′
is in many cases intractable.

early

updates

To make learning tractable with the glob-
ally normalized model, we use beam search
and
(Collins and Roark, 2004;
Zhou et al., 2015). As the training sequence is
being decoded, we keep track of the location of
the gold path in the beam. If the gold path falls
out of the beam at step j, a stochastic gradient
step is taken on the following objective:

Lglobal−beam(d∗

1:j; θ) =

−

j

X

i=1

ρ(d∗

1:i−1, d∗

i ; θ) + ln X
∈Bj

d′

1:j

exp

j

X

i=1

ρ(d′

1:i−1, d′

i; θ).(6)

Here the set Bj contains all paths in the beam at
step j, together with the gold path preﬁx d∗
1:j. It
is straightforward to derive gradients of the loss
in (6) and to back-propagate gradients to all levels
of a neural network deﬁning the score ρ(s, d; θ).
If the gold path remains in the beam throughout
decoding, a gradient step is performed using Bn,
the beam at the end of decoding.

3 The Label Bias Problem

Intuitively, we would like the model
to be
able to revise an earlier decision made during
search, when later evidence becomes available that
rules out the earlier decision as incorrect. At
ﬁrst glance, it might appear that a locally nor-
malized model used in conjunction with beam
search or exact search is able to revise ear-
lier decisions. However the label bias prob-
lem (see Lafferty et al. (2001), Bottou (1991),
Bottou and LeCun (2005)) means that locally nor-
malized models often have a very weak ability to
revise earlier decisions.

This section gives a more formal perspective
on the label bias problem than in previous work,
through a proof that globally normalized models
are strictly more expressive than locally normal-
ized models. The proof makes use of an example
that gives an illustration of the label bias problem.

Global Models can be Strictly More Expressive
than Local Models Consider a tagging problem
where the task is to map an input sequence x1:n
to a decision sequence d1:n. First, consider a lo-
cally normalized model where we restrict the scor-
ing function to access only the ﬁrst i input sym-
bols x1:i when scoring decision di. We will re-
turn to this restriction soon. The scoring function
ρ can be an otherwise arbitrary function of the tu-
ple hd1:i−1, di, x1:ii:
n

pL(d1:n|x1:n) =

Y
i=1

pL(di|d1:i−1, x1:i)

=

exp Pn
Qn

i=1 ρ(d1:i−1, di, x1:i)

i=1 ZL(d1:i−1, x1:i)

.

.

Second, consider a globally normalized model

pG(d1:n|x1:n) =

exp Pn

i=1 ρ(d1:i−1, di, x1:i)

ZG(x1:n)

This model again makes use of a scoring function
ρ(d1:i−1, di, x1:i) restricted to the ﬁrst i input sym-
bols when scoring decision di.

Deﬁne PL to be the set of all possible distribu-
tions pL(d1:n|x1:n) under the local model obtained
as the scores ρ vary. Similarly, deﬁne PG to be the
set of all possible distributions pG(d1:n|x1:n) un-
der the global model. Here a “distribution” is a
function from a pair (x1:n, d1:n) to a probability
p(d1:n|x1:n). Our main result is the following:
Theorem 3.1

PL is a strict subset of PG, that is PL ( PG.

To prove this we will ﬁrst prove that PL ⊆ PG.
This step is straightforward. We then show that
PG * PL; that is, there are distributions in PG
that are not in PL. The proof that PG * PL gives
a clear illustration of the label bias problem.

Proof that PL ⊆ PG: We need to show that
for any locally normalized distribution pL, we can
construct a globally normalized model pG such
that pG = pL. Consider a locally normalized
model with scores ρ(d1:i−1, di, x1:i). Deﬁne a
global model pG with scores

ρ′(d1:i−1, di, x1:i) = log pL(di|d1:i−1, x1:i).

Then it is easily veriﬁed that

pG(d1:n|x1:n) = pL(d1:n|x1:n)

for all x1:n, d1:n. (cid:3)

In proving PG * PL we will use a simple prob-
lem where every example seen in training or test
data is one of the following two tagged sentences:

x1x2x3 = a b c, d1d2d3 = A B C
x1x2x3 = a b e, d1d2d3 = A D E

(7)

Note that the input x2 = b is ambiguous: it can
take tags B or D. This ambiguity is resolved when
the next input symbol, c or e, is observed.

follows.

Now consider a globally normalized model,
scores ρ(d1:i−1, di, x1:i) are de-
where the
the set
ﬁned as
{(A, B), (B, C), (A, D), (D, E)} of bigram tag
transitions seen in the data. Similarly, deﬁne E
as the set {(a, A), (b, B), (c, C), (b, D), (e, E)} of
(word, tag) pairs seen in the data. We deﬁne

Deﬁne T

as

ρ(d1:i−1, di, x1:i)
= α × J(di−1, di) ∈ T K + α × J(xi, di) ∈ EK

(8)

where α is the single scalar parameter of the
model, and JπK = 1 if π is true, 0 otherwise.

Proof that PG * PL: We will construct a glob-
ally normalized model pG such that there is no lo-
cally normalized model such that pL = pG.

Under the deﬁnition in (8), it is straightforward

to show that

lim
α→∞

pG(A B C|a b c) = lim
α→∞

pG(A D E|a b e) = 1.

In

contrast,

any
ρ(d1:i−1, di, x1:i), we must have

under

deﬁnition

for

pL(A B C|a b c) + pL(A D E|a b e) ≤ 1

(9)

follows

because

pL(A D E|a b e)

pL(A B C|a b c)

This
=
pL(A|a) × pL(B|A, a b) × pL(C|A B, a b c)
pL(A|a) ×
and
pL(D|A, a b) × pL(E|A D, a b e).
The in-
equality pL(B|A, a b) + pL(D|A, a b) ≤ 1 then
immediately implies (9).

=

is

to

the

Under

impossible

given
a

(9)
locally

pG(A B C|a b c)

It follows that for sufﬁciently large values of α,
we have pG(A B C|a b c) + pG(A D E|a b e) > 1,
it
and
de-
normalized model with
ﬁne
pL(A B C|a b c)
and
=
pL(A D E|a b e) = pG(A D E|a b e). (cid:3)
that

scores
ρ(d1:i−1, di, x1:i) depend only on the ﬁrst
i
input symbols,
the globally normalized model
is still able to model the data in (7), while the
locally normalized model fails (see Eq. 9). The
ambiguity at input symbol b is naturally resolved
when the next symbol (c or e) is observed, but
the locally normalized model is not able to revise
its prediction.

restriction

It is easy to ﬁx the locally normalized model
the example in (7) by allowing scores
for
ρ(d1:i−1, di, x1:i+1) that take into account the in-
put symbol xi+1. Such lookahead is common in
practice, but insufﬁcient in general. For every
amount of lookahead k, we can construct exam-
ples that cannot be modeled with a locally nor-
malized model by duplicating the middle input
b in (7) k + 1 times. Only a local model with
scores ρ(d1:i−1, di, x1:n) that considers the entire
input can capture any distribution p(d1:n|x1:n):
in this case the decomposition pL(d1:n|x1:n) =
Qn
i=1 pL(di|d1:i−1, x1:n) makes no independence
assumptions.

However, increasing the amount of context used
as input comes at a cost, requiring more powerful
learning algorithms, and potentially more train-
ing data. For a detailed analysis of the trade-
offs between structural features in CRFs and more
powerful local classiﬁers without structural con-
straints, see Liang et al. (2008);
in these exper-
iments local classiﬁers are unable to reach the
performance of CRFs on problems such as pars-
ing and named entity recognition where structural
constraints are important. Note that there is noth-
ing to preclude an approach that makes use of both
global normalization and more powerful scoring
functions ρ(d1:i−1, di, x1:n), obtaining the best of
both worlds. The experiments that follow make
use of both.

Method

Linear CRF
Ling et al. (2015)

Our Local (B=1)
Our Local (B=8)
Our Global (B=8)

En
WSJ

97.17
97.78

97.44
97.45
97.44

En-Union

News Web QTB

Ca

Ch

Cz

CoNLL ’09

En

Ge

Ja

Sp

97.60 94.58 96.04
97.44 94.03 96.18

98.81 94.45 98.90 97.50 97.14 97.90 98.79
98.77 94.38 99.00 97.60 97.84 97.06 98.71

97.66 94.46 96.59
97.69 94.46 96.64
97.77 94.80 96.86

98.91 94.56 98.96 97.36 97.35 98.02 98.88
98.88 94.56 98.96 97.40 97.35 98.02 98.89
99.03 94.72 99.02 97.65 97.52 98.37 98.97

Avg

-

97.17
97.16

97.29
97.30
97.47

Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL’09.

4 Experiments

To demonstrate the ﬂexibility and modeling power
of our approach, we provide experimental results
on a diverse set of structured prediction tasks. We
ﬁrst direct our attention to POS tagging, then to
syntactic dependency parsing and ﬁnally to sen-
tence compression.

While directly optimizing the global model (5)
works well, we found that training the model in
two steps achieves the same precision much faster:
we ﬁrst pretrain the network using the local ob-
jective (4), and then perform additional training
steps using the global objective (6). We pretrain
all layers except the softmax layer in this way. We
purposefully abstain from complicated hand en-
gineering of input features, which might improve
performance further (Durrett and Klein, 2015).

4.1 Part of Speech Tagging
Part of speech (POS) tagging is a classic NLP task,
where modeling the structure of the output is im-
portant for achieving state-of-the-art performance.

Data & Evaluation. We conducted exper-
iments on a number of different datasets:
(1) English Wall Street Journal
(WSJ) part
the Penn Treebank (Marcus et al., 1993)
of
with standard POS tagging splits;
(2) En-
glish “Treebank Union” multi-domain corpus
containing data from the OntoNotes corpus
the English Web
version 5 (Hovy et al., 2006),
Treebank
and
the updated and corrected Question Treebank
(Judge et al., 2006) with
to
Weiss et al. (2015); and (3) CoNLL ’09 multi-
lingual shared task (Hajiˇc et al., 2009).

(Petrov and McDonald, 2012),

identical

setup

Model Conﬁguration.
Inspired by the inte-
grated POS tagging and parsing transition system
of Bohnet and Nivre (2012), we employ a simple
transition system that uses only a SHIFT action and
predicts the POS tag of the current word on the
buffer as it gets shifted to the stack. We extract the

following features on a window ±3 tokens cen-
tered at the current focus token: word, cluster,
character n-gram up to length 3. We also extract
the tag predicted for the previous 4 tokens. The
network in these experiments has a single hidden
layer with 256 units on WSJ and Treebank Union
and 64 on CoNLL’09.

Results.
In Table 1 we compare our model to
a linear CRF and to the compositional character-
to-word LSTM model of Ling et al. (2015). The
CRF is a ﬁrst-order linear model with exact infer-
ence and the same emission features as our model.
It additionally also has transition features of the
word, cluster and character n-gram up to length 3
on both endpoints of the transition. The results for
Ling et al. (2015) were solicited from the authors.
Our local model already compares favorably
against these methods on average. Using beam
search with a locally normalized model does not
help, but with global normalization it leads to a
7% reduction in relative error, empirically demon-
strating the effect of label bias.
It is also inter-
esting to note that the set of character ngrams fea-
ture is very important, increasing average accuracy
on the CoNLL’09 datasets by about 0.5% abso-
lute. This shows that character-level modeling can
also be done with a simple feed-forward netowork
without recurrence.

4.2 Dependency Parsing
In dependency parsing the goal is to produce a di-
rected tree representing the syntactic structure of
the input sentence.

Data & Evaluation. We use the same corpora
as in our POS tagging experiments, except that
we use the standard parsing splits of the WSJ. We
convert the English constituency trees to Stanford
style dependencies (De Marneffe et al., 2006) us-
ing version 3.3.0 of the converter. For English,
we use predicted POS tags (the same POS tags
are used for all models) and exclude punctua-

Method

Martins et al. (2013)
Zhang and McDonald (2014)
Weiss et al. (2015)
Alberti et al. (2015)

Our Local (B=1)
Our Local (B=32)
Our Global (B=32)

WSJ

UAS

LAS

92.89 90.55
93.22 91.02
93.99 92.05
94.23 92.36

93.17 91.18
93.58 91.66
94.41 92.55

Union-News
UAS
LAS

93.10 91.13
93.32 91.48
93.91 92.25
94.10 92.55

93.11 91.46
93.65 92.03
94.44 92.93

Union-Web
UAS
LAS

Union-QTB
UAS
LAS

88.23
88.65
89.29
89.55

88.42
88.96
90.17

85.04
85.59
86.44
86.85

85.58
86.17
87.54

94.21
93.37
94.17
94.74

92.49
93.22
95.40

91.54
90.69
92.06
93.04

90.38
91.17
93.64

Table 2: Final English dependency parsing test set results (without tri-training for any method).

Method

Catalan

UAS LAS

Chinese

UAS LAS

Czech

UAS LAS

English

UAS LAS

German

UAS LAS

Japanese
UAS LAS

Spanish
UAS LAS

Best Shared Task Result

-

87.86

-

79.17

-

80.38

-

89.88

-

87.48

-

92.57

-

87.64

Ballesteros et al. (2015)
90.22 86.42
Zhang and McDonald (2014) 91.41 87.91
Lei et al. (2014)
91.33 87.22
92.44 89.60
Bohnet and Nivre (2012)
Alberti et al. (2015)
92.31 89.17

Our Local (B=1)
Our Local (B=16)
Our Global (B=16)

91.24 88.21
91.91 88.93
92.67 89.83

80.64 76.52
82.87 78.57
81.67 76.71
82.52 78.51
83.57 79.90

81.29 77.29
82.22 78.26
84.72 80.85

79.87 73.62
86.62 80.59
88.76 81.77
88.82 83.73
88.45 83.57

85.78 80.63
86.25 81.28
88.94 84.56

90.56 88.01
92.69 90.01
92.75 90.00
92.87 90.60
92.70 90.56

91.44 89.29
92.16 90.05
93.22 91.23

88.83 86.10
89.88 87.38
90.81 87.81
91.37 89.38
90.58 88.20

89.12 86.95
89.53 87.4
90.91 89.15

93.47 92.55
92.82 91.87
94.04 91.84
93.67 92.63
93.99 93.10

93.71 92.85
93.61 92.74
93.65 92.84

90.38 86.59
90.82 87.34
91.16 87.38
92.24 89.60
92.26 89.33

91.01 88.14
91.64 88.88
92.62 89.95

Table 3: Final CoNLL ’09 dependency parsing test set results.

tion from the evaluation, as is standard. For the
CoNLL ’09 datasets we follow standard practice
and include all punctuation in the evaluation. We
follow Alberti et al. (2015) and use our own pre-
dicted POS tags so that we can include a k-best tag
feature (see below) but use the supplied predicted
morphological features. We report unlabeled and
labeled attachment scores (UAS/LAS).

Model Conﬁguration. Our model conﬁguration
is basically the same as the one originally pro-
posed by Chen and Manning (2014) and then re-
ﬁned by Weiss et al. (2015). In particular, we use
the arc-standard transition system and extract the
same set of features as prior work: words, part of
speech tags, and dependency arcs and labels in the
surrounding context of the state, as well as k-best
tags as proposed by Alberti et al. (2015). We use
two hidden layers of 1,024 dimensions each.

Results. Tables 2 and Table 3 show our ﬁnal
parsing results and a comparison to the best sys-
tems from the literature. We obtain the best ever
published results on almost all datasets, including
the WSJ. The results in Table 2 are without tri-
training. When we use tri-training, our WSJ accu-
racy improves to 94.61/92.78 (UAS/LAS), which
compares favorably to the 94.26/92.41 reported
by Weiss et al. (2015) with tri-training. As we

show in Section 5, these gains can be attributed
to the full backpropagation training that differenti-
ates our approach from that of Weiss et al. (2015)
and Alberti et al. (2015). Our results also signiﬁ-
cantly outperform the LSTM-based approaches of
Dyer et al. (2015) and Ballesteros et al. (2015).

4.3 Sentence Compression

Our ﬁnal structured prediction task is extractive
sentence compression.

&

a

Evaluation. We

Data
follow
Filippova et al. (2015), where
large news
collection is used to heuristically generate com-
pression instances. Our ﬁnal corpus contains
about 2.3M compression instances: we use 2M
examples for training, 130k for development and
160k for the ﬁnal test. We report per-token F1
score and per-sentence accuracy (A),
i.e. per-
centage of instances that fully match the golden
compressions. Following Filippova et al. (2015)
we also run a human evaluation on 200 sentences
where we ask the raters to score compressions for
readability (read) and informativeness (info)
on a scale from 0 to 5.

Model Conﬁguration. The transition system
for sentence compression is similar to POS tag-
ging: we scan sentences from left-to-right and la-

Method

Generated corpus Human eval
info

read

F1

A

Filippova et al. (2015)
Automatic

Our Local (B=1)
Our Local (B=8)
Our Global (B=8)

35.36

82.83

-

30.51
31.19
35.16

-

78.72
75.69
81.41

4.66
4.31

4.58

-

4.03
3.77

4.03

-

4.67

4.07

Table 4: Sentence compression results on News data. Auto-
matic refers to application of the same automatic extraction
rules used to generate the News training corpus.

bel each token as keep or drop. We extract fea-
tures from words, POS tags, and dependency la-
bels from a window of tokens centered on the in-
put, as well as features from the history of predic-
tions. We use a single hidden layer of size 400.

Results. Table 4 shows our sentence compres-
sion results. Our globally normalized model again
signiﬁcantly outperforms the local model. Beam
search with a locally normalized model suffers
from severe label bias issues that we discuss on
a concrete example in Section 5. We also com-
pare to the best sentence compression system from
Filippova et al. (2015), a 3-layer stacked LSTM
which uses dependency label information. The
LSTM and our global model perform on par on
both the automatic evaluation as well as the hu-
man ratings, but our model is roughly 100× faster.
All compressions kept approximately 42% of the
tokens on average and all the models are signiﬁ-
cantly better than the automatic extractions (p <
0.05).

5 Discussion

We derived a proof for the label bias problem
and the advantages of global models. We then
emprirically veriﬁed this theoretical superiority
by demonstrating state-of-the-art performance on
three different tasks. Our experiments showed
consistent improvements in accuracy for globally
normalized models over locally normalized mod-
els with beam search. In this section we situate and
compare our model to previous work and provide
two examples of the label bias problem in practice.

5.1 Related Neural CRF Work
Neural network models have been been combined
with conditional
random ﬁelds and globally
normalized models before. Bottou et al. (1997)
and Le Cun et al. (1998) describe global
train-

Method

Local (B=1)
Local (B=16)

Global (B=16) {θ(d)}
Global (B=16) {W2, θ(d)}
Global (B=16) {W1, W2, θ(d)}
Global (B=16) (full)

UAS

LAS

92.85
93.32

93.45
94.01
94.09
94.38

90.59
91.09

91.21
91.77
91.81
92.17

Table 5: WSJ dev set scores for successively deeper levels
of backpropagation. The full parameter set corresponds to
backpropagation all the way to the embeddings. Wi: hidden
layer i weights.

ing of neural network models for structured
prediction problems.
Peng et al. (2009) add
a non-linear neural network layer to a linear-
chain CRF and Do and Artires (2010) apply
a similar approach to more general Markov
network structures.
and
Zheng et al. (2015) introduce recurrence into the
model and Huang et al. (2015) ﬁnally combine
CRFs and LSTMs. These neural CRF models are
limited to sequence labeling tasks where exact
inference is possible, while our model works well
when exact inference is intractable.

Yao et al. (2014)

5.2 Related Transition-Based Parsing Work

on

early work

neural-networks

Zhou et al. (2015)

For
for
transition-based parsing, see Henderson (2003;
2004). Our work is closest
to the work of
Weiss et al. (2015),
and
Watanabe and Sumita (2015); in these approaches
global normalization is added to the local model
of Chen and Manning (2014).
Empirically,
Weiss et al. (2015) achieves the best performance,
even though their model keeps the parameters of
the locally normalized neural network ﬁxed and
only trains a perceptron that uses the activations
as features. Their model is therefore limited in
its ability to revise the predictions of the locally
normalized model. In Table 5 we show that full
backpropagation training all the way to the word
embeddings is very important and signiﬁcantly
contributes to the performance of our model. We
also compared training under the CRF objective
with a Perceptron-like hinge loss between the
gold and best elements of the beam. When we
limited the backpropagation depth to training only
the top layer θ(d), we found negligible differences
in accuracy: 93.20% and 93.28% for the CRF
objective and hinge loss respectively. However,

Method

Predicted compression

pL pG

Local (B=1)
Local (B=8)
Global (B=8)

In Pakistan, former leader Pervez Musharraf has appeared in court for the ﬁrst time, on treason charges. 0.13 0.05
In Pakistan, former leader Pervez Musharraf has appeared in court for the ﬁrst time, on treason charges. 0.16 < 10−4
In Pakistan, former leader Pervez Musharraf has appeared in court for the ﬁrst time, on treason charges. 0.06 0.07

Table 6: Example sentence compressions where the label bias of the locally normalized model leads to a breakdown during
beam search. The probability of each compression under the local (pL) and global (pG) models shows that only the global
model can properly represent zero probability for the empty compression.

when training with full backpropagation the CRF
accuracy is 0.2% higher and training converged
more than 4× faster.

Zhou et al. (2015) perform full backpropaga-
tion training like us, but even with a much
larger beam,
their performance is signiﬁcantly
lower than ours. We also apply our model
to two additional
tasks, while they experi-
ment only with dependency parsing.
Finally,
Watanabe and Sumita (2015) introduce recurrent
components and additional techniques like max-
violation updates for a corresponding constituency
parsing model. In contrast, our model does not re-
quire any recurrence or specialized training.

5.3 Label Bias in Practice
We observed several instances of severe label bias
in the sentence compression task. Although us-
ing beam search with the local model outperforms
greedy inference on average, beam search leads
the local model to occasionally produce empty
compressions (Table 6).
It is important to note
that these are not search errors: the empty com-
pression has higher probability under pL than the
prediction from greedy inference. However, the
more expressive globally normalized model does
not suffer from this limitation, and correctly gives
the empty compression almost zero probability.

We also present some empirical evidence that
the label bias problem is severe in parsing. We
trained models where the scoring functions in
parsing at position i in the sentence are limited to
considering only tokens x1:i; hence unlike the full
parsing model, there is no ability to look ahead
in the sentence when making a decision.3 The
result for a greedy model under this constraint
is 76.96% UAS; for a locally normalized model
with beam search is 81.35%; and for a globally
normalized model is 93.60%. Thus the globally
normalized model gets very close to the perfor-

3This setting may be important in some applications,
where for example parse structures for sentence preﬁxes are
required, or where the input is received one word at a time
and online processing is beneﬁcial.

mance of a model with full lookahead, while the
locally normalized model with a beam gives dra-
matically lower performance. In our ﬁnal exper-
iments with full lookahead, the globally normal-
ized model achieves 94.01% accuracy, compared
to 93.07% accuracy for a local model with beam
search. Thus adding lookahead allows the lo-
cal model to close the gap in performance to the
global model; however there is still a signiﬁcant
difference in accuracy, which may in large part be
due to the label bias problem.

A number of authors have considered modiﬁed
training procedures for greedy models, or for lo-
cally normalized models. Daum´e III et al. (2009)
introduce Searn, an algorithm that allows a
classiﬁer making greedy decisions to become
more robust
to errors made in previous deci-
sions. Goldberg and Nivre (2013) describe im-
provements to a greedy parsing approach that
makes use of methods from imitation learn-
ing (Ross et al., 2011) to augment
the training
set. Note that these methods are focused on
greedy models:
they are unlikely to solve the
label bias problem when used in conjunction
with beam search, given that
the problem is
one of expressivity of the underlying model.
More recent work (Yazdani and Henderson, 2015;
Vaswani and Sagae, 2016) has augmented locally
normalized models with correctness probabilities
or error states, effectively adding a step after every
decision where the probability of correctness of
the resulting structure is evaluated. This gives con-
siderable gains over a locally normalized model,
although performance is lower than our full glob-
ally normalized approach.

6 Conclusions

We presented a simple and yet powerful model ar-
chitecture that produces state-of-the-art results for
POS tagging, dependency parsing and sentence
compression. Our model combines the ﬂexibil-
ity of transition-based algorithms and the model-
ing power of neural networks. Our results demon-

strate that feed-forward network without recur-
rence can outperform recurrent models such as
LSTMs when they are trained with global normal-
ization. We further support our empirical ﬁndings
with a proof showing that global normalization
helps the model overcome the label bias problem
from which locally normalized models suffer.

Acknowledgements

We would like to thank Ling Wang for training
his C2W part-of-speech tagger on our setup, and
Emily Pitler, Ryan McDonald, Greg Coppola and
Fernando Pereira for tremendously helpful discus-
sions. Finally, we are grateful to all members of
the Google Parsing Team.

References
[Alberti et al.2015] Chris Alberti, David Weiss, Greg
Improved
Coppola, and Slav Petrov.
transition-based parsing and tagging with neural net-
works. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1354–1359.

2015.

[Ballesteros et al.2015] Miguel Ballesteros, Chris Dyer,
and Noah A. Smith. 2015.
Improved transition-
based parsing by modeling characters instead of
words with LSTMs. pages 349–359.

[Bohnet and Nivre2012] Bernd Bohnet and Joakim
Nivre. 2012. A transition-based system for joint
part-of-speech tagging and labeled non-projective
dependency parsing.
In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 1455–1465.

[Bottou and LeCun2005] L´eon Bottou and Yann Le-
Cun. 2005. Graph transformer networks for image
recognition. Bulletin of the International Statistical
Institute (ISI).

[Bottou et al.1997] L´eon Bottou, Yann Le Cun, and
Yoshua Bengio. 1997. Global training of docu-
ment processing systems using graph transformer
networks. In Proceedings of Computer Vision and
Pattern Recognition (CVPR), pages 489–493.

[Bottou1991] L´eon Bottou.

1991. Une approche
th´eorique de lapprentissage connexionniste: Appli-
cations `a la reconnaissance de la parole. Ph.D. the-
sis, Doctoral dissertation, Universite de Paris XI.

[Chen and Manning2014] Danqi Chen and Christo-
pher D. Manning. 2014. A fast and accurate de-
pendency parser using neural networks. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing, pages 740–750.

[Collins and Roark2004] Michael Collins and Brian
Roark. 2004. Incremental parsing with the percep-
tron algorithm.
In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL’04), pages 111–118.

[Daum´e III et al.2009] Hal Daum´e III, John Langford,
and Daniel Marcu. 2009. Search-based structured
prediction. Machine Learning Journal (MLJ).

[De Marneffe et al.2006] Marie-Catherine De Marn-
effe, Bill MacCartney, and Christopher D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of Fifth In-
ternational Conference on Language Resources and
Evaluation, pages 449–454.

[Do and Artires2010] Trinh Minh Tri Do and Thierry
Artires. 2010. Neural conditional random ﬁelds. In
International Conference on Artiﬁcial Intelligence
and Statistics, volume 9, pages 177–184.

[Durrett and Klein2015] Greg Durrett and Dan Klein.
2015. Neural crf parsing.
In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing, pages
302–312.

[Dyer et al.2015] Chris Dyer, Miguel Ballesteros,
Wang Ling, Austin Matthews, and Noah A. Smith.
2015. Transition-based dependency parsing with
stack long short-term memory.
In Proceedings of
the 53rd Annual Meeting of the Association for
Computational Linguistics, pages 334–343.

[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A. Colmenares, Łukasz Kaiser, and
Oriol Vinyals. 2015. Sentence compression by dele-
tion with lstms.
In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 360–368.

[Goldberg and Nivre2013] Yoav Goldberg and Joakim
Nivre. 2013. Training deterministic parsers with
non-deterministic oracles. Transactions of the Asso-
ciation for Computational Linguistics, 1:403–414.

[Hajiˇc et al.2009] Jan Hajiˇc, Massimiliano Cia-
ramita, Richard Johansson, Daisuke Kawahara,
Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Mey-
ers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek,
Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue,
and Yi Zhang. 2009. The conll-2009 shared task:
Syntactic and semantic dependencies in multi-
ple languages.
In Proceedings of the Thirteenth
Conference on Computational Natural Language
Learning: Shared Task, pages 1–18.

[Henderson2003] James Henderson. 2003.

Inducing
history representations for broad coverage statistical
parsing.
In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 24–31.

[Henderson2004] James Henderson. 2004. Discrimi-
native training of a neural network statistical parser.
In Proceedings of the 42nd Meeting of the Associa-
tion for Computational Linguistics (ACL’04), pages
95–102.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and J¨urgen Schmidhuber. 1997. Long short-term
memory. Neural computation, 9(8):1735–1780.

[Hovy et al.2006] Eduard Hovy, Mitchell Marcus,
Martha Palmer, Lance Ramshaw,
and Ralph
Weischedel. 2006. Ontonotes: The 90% solution.
In Proceedings of the Human Language Technology
the NAACL, Short Papers, pages
Conference of
57–60.

[Huang et al.2015] Zhiheng Huang, Wei Xu, and Kai
Yu. 2015. Bidirectional LSTM-CRF models for se-
quence tagging. arXiv preprint arXiv:1508.01991.

[Judge et al.2006] John Judge, Aoife Cahill, and Josef
van Genabith. 2006. Questionbank: Creating a cor-
pus of parse-annotated questions. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 497–504.

[Lafferty et al.2001] John Lafferty, Andrew McCallum,
and Fernando Pereira. 2001. Conditional random
ﬁelds: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, pages 282–289.

[Le Cun et al.1998] Yann Le Cun, L´eon Bottou, Yoshua
Bengio, and Patrick Haffner. 1998. Gradient based
learning applied to document recognition. Proceed-
ings of IEEE, 86(11):2278–2324.

[Lei et al.2014] Tao Lei, Yu Xin, Yuan Zhang, Regina
Barzilay, and Tommi Jaakkola. 2014. Low-rank
tensors for scoring dependency structures. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1381–
1391.

[Liang et al.2008] Percy Liang, Hal Daum´e, III, and
Dan Klein. 2008. Structure compilation: Trading
structure for features. In Proceedings of the 25th In-
ternational Conference on Machine Learning, pages
592–599.

[Ling et al.2015] Wang Ling, Chris Dyer, Alan W
Black, Isabel Trancoso, Ramon Fermandez, Silvio
Amir, Luis Marujo, and Tiago Luis. 2015. Finding
function in form: Compositional character models
for open vocabulary word representation.
In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1520–
1530.

[Marcus et al.1993] Mitchell P. Marcus, Beatrice San-
torini, and Mary Ann Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313–
330.

[Martins et al.2013] Andre Martins, Miguel Almeida,
and Noah A. Smith. 2013. Turning on the turbo:
Fast third-order non-projective turbo parsers.
In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages 617–
622.

[Nivre2006] Joakim Nivre. 2006.

Inductive Depen-

dency Parsing. Springer-Verlag New York, Inc.

[Nivre2009] Joakim Nivre. 2009. Non-projective de-
pendency parsing in expected linear time.
In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 351–359.

[Peng et al.2009] Jian Peng, Liefeng Bo, and Jinbo Xu.
2009. Conditional neural ﬁelds.
In Advances in
Neural Information Processing Systems 22, pages
1419–1427.

[Petrov and McDonald2012] Slav Petrov and Ryan Mc-
Donald. 2012. Overview of the 2012 shared task
on parsing the web. Notes of the First Workshop
on Syntactic Analysis of Non-Canonical Language
(SANCL).

[Ross et al.2011] St´ephane Ross, Geoffrey J. Gordon,
and J. Andrew Bagnell. 2011. No-regret reduc-
tions for imitation learning and structured predic-
tion. AISTATS.

[Vaswani and Sagae2016] Ashish Vaswani and Kenji
Sagae.
2016. Efﬁcient structured inference for
transition-based parsing with neural networks and
error states. Transactions of the Association for
Computational Linguistics, to appear.

[Vinyals et al.2015] Oriol Vinyals, Łukasz Kaiser,
Terry Koo, Slav Petrov, Ilya Sutskever, and Geof-
frey Hinton. 2015. Grammar as a foreign language.
In Advances in Neural
Information Processing
Systems 28, pages 2755–2763.

[Watanabe and Sumita2015] Taro Watanabe and Ei-
ichiro Sumita. 2015. Transition-based neural con-
stituent parsing. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing, pages 1169–1179.

[Weiss et al.2015] David Weiss, Chris Alberti, Michael
Collins, and Slav Petrov. 2015. Structured training
for neural network transition-based parsing. In Pro-
ceedings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 323–333.

[Yao et al.2014] Kaisheng Yao, Baolin Peng, Geoffrey
Zweig, Dong Yu, Xiaolong Li, and Feng Gao. 2014.
Recurrent conditional random ﬁeld for language un-
derstanding. In IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP
’14).

[Yazdani and Henderson2015] Majid Yazdani

and
James Henderson.
Incremental recurrent
neural network dependency parser with search-
based discriminative training. In Proceedings of the
Nineteenth Conference on Computational Natural
Language Learning, pages 142–152.

2015.

[Zhang and McDonald2014] Hao Zhang and Ryan Mc-
Donald. 2014. Enforcing structural diversity in
cube-pruned dependency parsing.
In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, pages 656–661.

[Zheng et al.2015] Shuai Zheng, Sadeep Jayasumana,
Bernardino Romera-Paredes, Vibhav Vineet,
Zhizhong Su, Dalong Du, Chang Huang, and Philip
H. S. Torr. 2015. Conditional random ﬁelds as re-

current neural networks. In The IEEE International
Conference on Computer Vision (ICCV).

[Zhou and Xu2015] Jie Zhou and Wei Xu. 2015. End-
to-end learning of semantic role labeling using re-
current neural networks. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing, pages
1127–1137.

[Zhou et al.2015] Hao Zhou, Yue Zhang, and Jiajun
Chen.
2015. A neural probabilistic structured-
prediction model for transition-based dependency
parsing. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1213–1222.

