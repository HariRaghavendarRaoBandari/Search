Neural Discourse Relation Recognition with Semantic Memory

Biao Zhang1,2, Deyi Xiong2 and Jinsong Su1
Xiamen University, Xiamen, China 3610051
Soochow University, Suzhou, China 2150062

zb@stu.xmu.edu.cn, jssu@xmu.edu.cn

dyxiong@suda.edu.cn

6
1
0
2

 
r
a

 

M
2
1

 
 
]
L
C
.
s
c
[
 
 

1
v
3
7
8
3
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Humans comprehend the meanings and relations of
discourses heavily relying on their semantic mem-
ory that encodes general knowledge about concepts
and facts.
Inspired by this, we propose a neural
recognizer for implicit discourse relation analysis,
which builds upon a semantic memory that stores
knowledge in a distributed fashion. We refer to this
recognizer as SeMDER. Starting from word embed-
dings of discourse arguments, SeMDER employs a
shallow encoder to generate a distributed surface
representation for a discourse. A semantic encoder
with attention to the semantic memory matrix is
further established over surface representations. It
is able to retrieve a deep semantic meaning repre-
sentation for the discourse from the memory. Us-
ing the surface and semantic representations as in-
put, SeMDER ﬁnally predicts implicit discourse re-
lations via a neural recognizer. Experiments on the
benchmark data set show that SeMDER beneﬁts
from the semantic memory and achieves substan-
tial improvements of 2.56% on average over current
state-of-the-art baselines in terms of F1-score.

1 Introduction
Discourse relation recognition (DRR) that automatically
identiﬁes the logical relation of a coherent text is very impor-
tant for discourse-level comprehension. It is relevant to a va-
riety of nature language processing tasks such as summariza-
tion [Yoshida et al., 2014], machine translation [Guzm´an et
al., 2014], question answering [Jansen et al., 2014] and infor-
mation extraction [Cimiano et al., 2005]. Although explicit
DRR has recently achieved remarkable success [Miltsakaki
et al., 2005; Pitler et al., 2008], implicit DRR still remains a
serious challenge due to the absence of discourse connectives.
However, even if discourse connectives are not provided,
humans can still easily succeed in recognizing the relations
of discourse arguments. One reason for this, according to
cognitive psychology, would be that humans have a semantic
memory in mind, which helps them comprehend word senses
and further argument meanings via composition. After un-
derstanding what two arguments of a discourse convey, hu-
mans can easily interpret the discourse relation of the two

Figure 1: Overall architecture for SeMDER model. We use
the shallow and deep yellow color to indicate the surface and
semantic representation respectively.

arguments. This semantic memory, as discussed by Tulv-
ing [1972], refers to general knowledge including “words and
other verbal symbols, their meaning and referents, about rela-
tions among them, and about rules, formulas, and algorithms
for manipulating them”. It can be retrieved to help disam-
biguation and comprehension whenever the barrier of cogni-
tion occurs.

Consider the implicit discourse relation between the fol-

lowing two sentences:

(1) I was prepared to be in a very bad mood tonight.
Now, I feel maybe there’s a little bit of euphoria.

It is difﬁcult for conventional discourse relation recognizers
to identify the relation between the two sentences as there
is little signiﬁcant surface information for use. However, if
the recognizer obtains the knowledge of the antonymous re-
lationship between the meaning of “bad mood” and that of
“euphoria”, it will be easy to infer the COMPARISON relation
between the two sentences. This semantic knowledge can be
stored in an external memory for a discourse recognizer just
like the semantic memory for humans.

Inspired by the semantic memory in cognitive neuro-
science [Yee et al., 2014] as well as memory network [Weston
et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] and
attentional mechanisms [Mnih et al., 2014; Bahdanau et al.,
2014; Xu et al., 2015], we propose a neural network with se-

DiscourseWordEmbeddingSemanticMemorycareswhothemoverbidwesaycompetitorsourSemanticRepresentationDiscourseArg1:ourcompetitorssayweoverbidthemArg2:whocaresRelation:ComparisonNeuralRecognizerSemanticEncoderSurfaceRepresentationShallowEncodermantic memory for implicit DRR, which refers to SeMDER.
The philosophy of SeMDER includes: (1) the external se-
mantic memory should be distributed as this allows easy com-
putation; (2) the semantic memory should be easily accessed
and retrieved; and (3) the retrieved content should be inte-
grated into the comprehension of meanings of discourse argu-
ments and their relations. In order to meet these requirements,
we use a distributed matrix that encodes semantic knowledge
of words as our external memory. The distributed memory
is retrieved via an attentive reader. The retrieved distributed
knowledge is incorporated into semantic representations of
discourse arguments. Practically, we build a neural network
that is composed of three essential components: a shallow en-
coder, a semantic encoder and a neural recognizer. The neural
network is visualized in Figure 1. In particular,
• Shallow encoder: we feed word embeddings of dis-
course arguments into a shallow encoder [Zhang et al.,
2015] to obtain shallow representations of arguments.
Due to their shallow property, we refer to them as sur-
face representations (see Section 3.1);
• Semantic encoder: we retrieve the semantic memory
via an attention model. The retrieved content, together
with surface representations, are incorporated into the
semantic encoder to obtain deep semantic representa-
tions (see Section 3.2);
• Neural recognizer: both surface and semantic represen-
tations are feed into a neural recognizer to predict the
corresponding discourse relations (see Section 3.3).

Our contributions are twofold. First, we propose a neural
network architecture for implicit DRR with an encoded se-
mantic memory that enhances representations of arguments.
To the best of our knowledge, we are the ﬁrst to explore se-
mantic memory for DRR via attentional mechanisms. Sec-
ond, we conduct a series of experiments for English implicit
DRR on the PDTB-style corpus to evaluate the effective-
ness of our proposed neural network and semantic memory.
Experiment results show that our network achieves substan-
tial improvements against several strong baselines in term of
F1 score. Extensive analysis on the attention further indi-
cates that our model can recognize some important relation-
relevant words, which we conjecture is the main reason for
our success.

2 Related Work
The release of Penn Discourse Treebank (PDTB) [Prasad et
al., 2008] opens the door to machine learning based implicit
DRR. A variety of machine learning strategies have been pre-
sented previously, including feature engineering, connective
predicting, data selection and discourse representation via
neural networks.

Research on feature engineering exploits powerful and dis-
criminative features for implicit DRR. In this respect, Pilter
et al. [2009] investigate several linguistically informed fea-
tures, such as polarity tags, verb classes, modality, context
and lexical features. Lin et al. [2009] further consider contex-
tual words, word pairs and parse trees for feature engineering.
Later, several more powerful features have been developed:

aggregated word pairs [McKeown and Biran, 2013], Brown
clusters and coreference patterns [Rutherford and Xue, 2014].
With these features, Park and Cardie [2012] perform feature
set optimization for better feature combination.

The major difference between explicit and implicit DRR is
the presence of discourse connectives, the most salient fea-
tures for DRR. Therefore, if we ﬁnd a way to predict con-
nectives for implicit discourses, we can transform implicit
DRR into explicit DRR. Along this line, Zhou et al. [2010]
use a language model to automatically insert discourse con-
nectives, while Patterson and Kehler [2013] use a classiﬁer to
predict the presence or omission of a lexical connective. Dif-
ferent from this prediction strategy, Hong et al. [2012] lever-
age discourse connectives as a bridge between explicit and
implicit relations and adopt an unsupervised cross-argument
inference mechanism.

Yet another strategy is data selection, where explicit dis-
course instances that are similar to the implicit ones are found
and added to training corpus. Different data selection meth-
ods for implicit DRR can be classiﬁed into the following cat-
instance typicality [Wang et al., 2012], multi-task
egories:
learning [Lan et al., 2013], domain adaptation [Braud and De-
nis, 2014; Ji et al., 2015], semi-supervised learning [Hernault
et al., 2010; Fisher and Simmons, 2015] and explicit dis-
course connective classiﬁcation [Rutherford and Xue, 2015].
The third strategy is to learn representations of disourse ar-
guments using neural networks for relation recognition, fol-
lowing remarkable success of neural networks in various nat-
ural language processing tasks.
In this respect, Braud and
Denis [2015] investigates the usefulness of word representa-
tions. Speciﬁcally, two different neural network models have
been developed for implicit DRR: recursive neural network
for entity-augmented distributed semantics [Ji and Eisenstein,
2015] and shallow convolutional neural network for discourse
representation [Zhang et al., 2015]. The former incorporates
coreferent entity mentions into compositional distributed rep-
resentations, while the latter develops a pure neural network
model for discourse representations in implicit DRR. Nor-
mally, entities utilized in the former heavily depend on the
availability and robustness of an upstream coreference sys-
tem, and the latter only learns shallow representations for dis-
course arguments. Instead, our proposed model does not rely
on any linguistic resources and incorporates a semantic mem-
ory to obtain deep semantic representations over shallow rep-
resentations in [Zhang et al., 2015]. Additionally, since the
semantic memory is represented as a distributed matrix, our
model is more robust and adaptable.

The exploration of semantic memory for implicit DRR is
inspired by recent developments in cognitive neuroscience.
Yee et al. [2014] show how this memory is organized and re-
trieved in brain. In order to explore semantic memory in neu-
ral networks, we borrow ideas from recently introduced mem-
ory networks [Weston et al., 2014; Sukhbaatar et al., 2015;
Kumar et al., 2015] to organize semantic memory as a dis-
tributed matrix and use an attention model to retrieve this dis-
tributed memory. The adaptation and utilization of semantic
memory into implicit DRR, to the best of our knowledge, has
never been investigated before.

3 The SeMDER Model
This section elaborates the proposed SeMDER model. We
will ﬁrst present the shallow encoder which converts a dis-
course into a distributed embedding. The semantic encoder,
where the semantic memory is incorporated via an attention
model is then described. After that, we explain how the neural
recognizer classiﬁes discourse relations. We also discuss the
objective function and the procedure for parameter learning
in this section.

3.1 Shallow Encoder
To obtain surface representations for discourses, we employ a
shallow convolutional neural network (SCNN) [Zhang et al.,
2015] as our shallow encoder. SCNN is speciﬁcally designed
on the PDTB corpus, where implicit discourse relations are
annotated between two neighboring arguments, namely Arg1
and Arg2 (see the example in Figure 1). Given an argument
which consists of n words, SCNN represents each word as a
d-dimensional dense, real-valued vector xi ∈ Rd1 and con-
catenates them into a word embedding matrix:

X = (x1, x2, . . . , xn)

(1)
where X ∈ Rd1×n forms the input layer of SCNN. All word
vectors in vocabulary V are stacked into a parameter matrix
L ∈ Rd1×|V | (|V | is the vocabulary size), which will be tuned
in the training phase.

To represent a discourse argument c, SCNN extracts ma-
jor information inside X through three convolution operations
avg, min and max deﬁned as follows:

n(cid:88)

i

cavg
r =

1
n

Xr,i

(2)

(3)

cmin
r = min (Xr,1, Xr,2, . . . , Xr,n)

cmax
r = max (Xr,1, Xr,2, . . . , Xr,n)

(4)
where r indicates the row of X. The argument c is thereby
represented as the concatenation of these convolutional fea-
tures:

c =(cid:2)cavg; cmax; cmin(cid:3)

(5)
SCNN further obtains the representation p ∈ R6d1 of a dis-
course by performing nonlinear transformations on the con-
catenation of two argument embeddings cArg1 and cArg2 gen-
erated in Eq. 5 as follows:

p = g([cArg1; cArg2]), g(x) =

tanh(x)
||tanh(x)||

(6)

Despite its simplicity, SCNN outperforms several feature-
based systems. This is the reason that we choose it as
our shallow encoder to obtain surface representations of dis-
courses. However, the lack of deep knowledge in SCNN lim-
its its further development. We therefore introduce a deep se-
mantic encoder over the shallow encoder, which will be elab-
orated in the next section.

Figure 2:
Illustration of the semantic encoder. We use gray
color to indicate representations in the attention space. The
dashed red box shows the bilinear-style computation for at-
tention weights.

3.2 Semantic Encoder
Upon the surface representations, we further build a semantic
encoder to incorporate a semantic memory to strengthen dis-
course comprehension. The semantic memory in SeMDER is
represented as a distributed matrix M ∈ Rm×d2 , where d2 is
the dimension of word embedding in the memory. Each row
in the matrix indicates one word in discourse arguments (thus
typically m ≤ n). We assume that the semantic and syntactic
attributes of words have already been encoded into this ma-
trix. Therefore, incorporating this memory information into
discourse representations will be beneﬁcial for implicit DRR
task.

Figure 2 gives an illustration of the procedure for incorpo-
rating the semantic memory. Speciﬁcally, given the surface
representation p for a discourse and the semantic memory ma-
trix M, we stack an attention layer to project them onto the
same space, which we call attention space. The projection is
done as follows:

pa = f (Wpp + ba)

(7)

Ma = f (WmM T + ba)

(8)
where the subscript a denotes the attention space, pa and Ma
are the attentional representations for p and M respectively.
Wp ∈ Rda×6d1, Wm ∈ Rda×d2 are transformation matrices,
ba ∈ Rda is the bias term, da is the dimensionality of the
attention space, and f (·) is an element-wise activation func-
tion such as tanh(·), which is used throughout our model.
The arrows marked by “ 1(cid:13)” in Figure 2 show this projection
process.

Note that we differentiate the transformation matrix Wp in
Eq. 7 to the Wm in Eq. 8, since the surface representation
and semantic memory are from different semantic spaces.
However, we share the same bias term for them. This will
force our model to learn to encode attention semantics into
the transformation matrices, rather than the biases.

After obtaining the attentional representations for the dis-
course and semantic memory, we further estimate how useful
each word memory cell i in the semantic memory (i.e., the
ith row in M) is to the corresponding discourse. This can be

SurfaceRepresentationSemanticMemorycareswhothemoverbidwesaycompetitorsourAttentionWeightsSemanticRepresentationWeightSumWeightMatrix1(cid:13)1(cid:13)2(cid:13)2(cid:13)3(cid:13)calculated by a match score:

si = g(pa, Ma,i)

(9)
where g(·) is the scoring function. Since we are only inter-
ested in words occurred in the corresponding discourse, our
attention schema is somewhat like a local attention. As dis-
cussed in [Luong et al., 2015], a general scoring function is
much better for the local attention. Thus, we use a variant of
the general function as our scoring function (see the red box
in Figure 2):

g(pa, Ma,i) = paWsMa,i

(10)
where Ws ∈ Rda×da is the bilinear scoring matrix, in which
each element (see the red node in Figure 2) represents an
interaction between the corresponding dimension of pa and
Ma,i.

We further normalize the match score vector in Eq. 9 to
generate a probabilistic attention distribution over words in
the semantic memory:

(cid:80)m

exp(si)
j=1 exp(sj)

αi =

(11)

Intuitively, the probability αi (a.k.a attention weight) reﬂects
the importance of the word Mi in representing the whole dis-
course with respect to the ﬁnal discourse relation recognition.
Recall the above-mentioned example (1), if the importance of
words “bad mood” and “euphoria” is recognized, there would
be more chance that the ﬁnal recognizer succeeds.

Based on this attention distribution, we can compute the
semantic representation for a discourse as a weighted sum of
words in the semantic memory according to α (see the arrows
marked by “ 3(cid:13)” in Figure 2):

pk =

αjMj

(12)

j=1

As shown in Eq. 12, the semantic representation is directly
retrieved from the semantic memory.
It encodes semantic
knowledge of words in discourse arguments that can help dis-
course relation recognition.

3.3 Neural Recognizer
Up to now, we have inferred both the surface and semantic
representation for a discourse. To recognize the discourse re-
lation, we further stack a Softmax layer upon these two rep-
resentations:

yp = h(Wr,pp + Wr,kpk + br)

(13)
where h(·) is the softmax function, Wr,p ∈ Rl×6d1, Wr,k ∈
Rl×d2 and br ∈ Rl are the parameter matrices and bias term
respectively, and l indicates the number of discourse rela-
tions.

3.4 Objective Function and Parameter Learning
Given a training corpus which contains T instances
{(x, y)}T
t=1, we employ the following cross-entropy error to

m(cid:88)

access how well the predicted relation yp represents the gold
relation y,

yj × log (yp,j)

(14)

E(yp, y) = − l(cid:88)
T(cid:88)

j

1
T

t=1

Therefore, the joint training objective of SeMDER is deﬁned
as follows:

J(θ) =

E(y(t)

p , y(t)) + R(θ)

(15)

where R(θ) is the regularization term with respect to θ. To-
wards the parameters θ, we divide them into three different
sets:

: discourse
Wr,p, Wr,k and br;

• θL : word embedding matrix L;
• θR
relation recognition parameters
• θM : memory-related parameters Wp, Wm, Ws and ba;
these parameters are regularized with corresponding

All
weights1:

R(θ) =

(cid:107)θL(cid:107)2 +

λL
2

λR
2

(cid:107)θR(cid:107)2 +

λM
2

(cid:107)θM(cid:107)2

(16)

Notice that although we can ﬁne-tune the semantic mem-
ory in an end-to-end manner, we do not do that in our model.
This is because we hope that the semantic and syntactic at-
tributes encoded in the semantic memory can be preserved
throughout our neural network.

We apply Limited-memory Broyden-Fletcher-Goldfarb-
Shanno (L-BFGS) algorithm to optimize each parameter. In
order to run the L-BFGS algorithm, we need to solve two
problems: parameter initialization and partial gradient calcu-
lation.

In the phase of parameter initialization, θR and θM are ran-
domly set according to a normal distribution (µ = 0, σ =
0.01). For the word embedding θL, we use the toolkit
Word2Vec2 to perform pretraining on a large-scale unlabeled
data. This word embedding will be further ﬁne-tuned in our
SeMDER model to capture much more semantics related to
discourse relations.

The partial gradient for parameter θj is computed as fol-

lows:

∂J
∂θj

=

1
T

∂E(y(t)
p , y(t))
∂θj

+ λjθj

(17)

This gradient will be feed into the toolkit libLBFGS3 for pa-
rameter updating in our practical implementation.
4 Experiments
In this section, we conducted a series of experiments on En-
glish implicit DRR task. We begin with a brief review of
the PDTB dataset. Then, we describe our experiment setup.
Finally, we present experiment results and give an in-depth
analysis on the attention.

1The bias terms b is not regularized in practice.
2https://code.google.com/p/word2vec/
3http://www.chokkan.org/software/liblbfgs/

T(cid:88)

t=1

Relation

COM
CON
EXP
TEM

Positive/Negative Sentences
Train
Test

Dev

1942/1942
3342/3342
7004/7004
760/760

197/986
295/888
671/512
64/1119

152/894
279/767
574/472
85/96l

Table 1: Statistics of implicit discourse relations for the train-
ing (Train), development (Dev) and test (Test) sets in PDTB
corpus.

4.1 Dataset
We used PDTB 2.0 corpus4 [Prasad et al., 2008] (PDTB
thereafter), which is the largest hand-annotated discourse
corpus. Discourse relations are annotated in a predicate-
argument view in PDTB, where each discourse connective
is treated as a predicate that takes two text spans as its argu-
ments. The relation tags in PDTB are arranged in a three-
level hierarchy, where the top level consists of four major se-
mantic classes: TEMPORAL (TEM), CONTINGENCY (CON),
EXPANSION (EXP) and COMPARISON (COM). Because the
top-level relations are general enough to be annotated with a
high inter-annotator agreement and are common to most the-
ories of discourse, in our experiments we only use this level
of annotations.

PDTB contains discourse annotations over 2,312 Wall
Street Journal articles, and is organized in different sections.
Following previous work [Pitler et al., 2009; Zhou et al.,
2010; Lan et al., 2013; Zhang et al., 2015], we used sec-
tions 2-20 as our training set, sections 21-22 as the test set.
Sections 0-1 were used as the development set for hyperpa-
rameter optimization. We formulated the task as four separate
one-against-all binary classiﬁcation problems: each top level
class vs. the other three discourse relation classes. We also
balanced the training set by resampling training instances in
each class until the number of positive and negative instances
are equal. In contrast, all instances in the test and develop-
ment set are kept in nature. The statistics of various data sets
is listed in Table 1.
4.2 Setup
We selected the GoogleNews-vectors-negative3005 as our ex-
ternal semantic memory. This data contains 300-dimensional
vectors (thus, d2 = 300) for 3 million words and phrases.
It is trained on part of Google News dataset (about 100 bil-
lion words). The wide coverage and newswire domain of its
training corpus as well as the syntactic property of word2vec
models make this vector a good choice for the semantic mem-
ory.

We tokenized all datasets using Stanford NLP Toolkit6,
and employed a large-scale unlabeled data7 including 1.02M

4http://www.seas.upenn.edu/ pdtb/
5https://drive.google.com/ﬁle/d/0B7XkCwpI5KDYNlNUTTlSS

21pQmM/edit?pref=2&pli=1

6http://nlp.stanford.edu/software/corenlp.shtml
7This data contains the training and development set for implicit
DRR, as well as the English sentences in the FBIS corpus and the
English sentences in Hansards part of LDC2004T07 corpus.

sentences (33.5M words) for word embedding θL initializa-
tion. We optimized the hyperparameters d1, λL, λR, λM ac-
cording to previous work [Zhang et al., 2015] and prelim-
inary experiments on the development set. Finally, we set
d1 = 128, λL = 1e−5, λR = λM = 1e−4 for all the exper-
iments. With respect to da, we tried three different settings
da = 32, 64, 128.

To validate the effectiveness of SeMDER model, we com-

pared against the following baseline methods:

• SVM: a support vector machine (SVM) classiﬁer trained
with the labeled data in the training set. We used the
toolkit SVM-light8 to train the classiﬁer in our experi-
ments.

• SCNN: a shallow convolutional neural model proposed

by Zhang et al. [2015].

Features used in SVM experiments are taken from the state-
of-the-art implicit discourse relation recognition model, in-
cluding Bag of Words, Cross-Argument Word Pairs, Polarity,
First-Last, First3, Production Rules, Dependency Rules and
Brown cluster pair [Rutherford and Xue, 2014]. Addition-
ally, in order to collect bag of words, production rules, de-
pendency rules, and cross-argument word pairs, we used a
frequency cutoff of 5 to remove rare features, following Lin
et al. [2009].

4.3 Classiﬁcation Results
Because of the imbalance nature in our test set, we choose F1
score as our major evaluation metric. The performance of dif-
ferent models is presented in Table 2, which, overall, shows
that SeMDER outperforms the two baselines, achieving im-
provements in F1 score of 1.14% on COM, 1.66% on CON,
1.36% on EXP and 5.62% on TEM over the best baseline re-
sults. We further observe that the improvements mainly result
from high precision for COM, CON and TEM, while high re-
call for EXP. This is reasonable since the EXP relation owns
the largest number of instances in our data.

As the neural baseline, SCNN outperforms SVM on CON,
EXP and TEM, but fails on COM. The SeMDER with se-
mantic memory, however, consistently surpasses SVM and
SCNN in all discourse relations. This suggests that the in-
corporated semantic memory is helpful for recognizing cor-
rect discourse relations. Additionally, for SeMDER, increas-
ing the attention space dimensionality da from 32 to 128 im-
proves the performance in most cases.

Yet another interesting observation from Table 2 is that the
improvement of SeMDER over the two baselines for rela-
tion TEM is the biggest. The gain over SVM is 11.4% and
5.6% over SCNN. This improvement is largely due to high
precisions. As the number of instances in relation TEM is
the smallest (see Table 1), we argue that the traditional neu-
ral network models may suffer from overﬁtting in this case.
However, our SeMDER enhanced with the semantic memory
is capable of generalization that alleviates this overﬁtting is-
sue.

8http://svmlight.joachims.org/

Model
SVM
SCNN

SeMDER

Model
SVM
SCNN

SeMDER

P

22.79
22.00
22.18
23.33
25.71

da
-
-
32
64
128
(a) COM vs Other
da
-
-
32
64
128
(c) EXP vs Other

65.89
56.29
54.80
54.79
54.98

P

R

64.47
67.76
73.68
61.84
53.95

R

58.89
91.11
99.48
99.65
100.0

F1
33.68
33.22
34.09
33.87
34.82

F1
62.19
69.59
70.67
70.70
70.95

Model
SVM
SCNN

SeMDER

Model
SVM
SCNN

SeMDER

P

39.14
39.80
41.14
39.82
42.07

da
-
-
32
64
128
(b) CON vs Other
da
-
-
32
64
128
(d) TEM vs Other

15.10
20.22
21.79
23.01
34.78

P

R

72.40
75.29
74.91
80.65
74.19

R

68.24
62.35
60.00
61.18
37.65

F1
50.82
52.04
53.11
53.32
53.70

F1
24.73
30.54
31.97
33.44
36.16

Table 2: Classiﬁcation results of different models on implicit DRR. P=Precision, R=Recall, and F1=F1 score. The best F1
scores are highlighted in bold.

Relation Example

Top Words

COM

CON

EXP

TEM

[people think of the steel business as an old and mundane
smokestack business]Arg1, [they ’re dead wrong]Arg2
[three minutes into the massage , the man curled up , began
shaking and turned red]Arg1, [paramedics were called]Arg2
[numerous injuries were reported]Arg1, [some buildings col-
lapsed , gas and water lines ruptured and ﬁres raged]Arg2
[warner sued sony and guber-peters late last week]Arg1, [sony
and guber-peters have countersued]Arg2

wrong, people, dead, think, smokestack

shaking, turned, paramedics, massage, curled

injuries, were, collapsed, raged, ruptured

have, countersued, late, week, last

Table 3: Attention examples selected from the test set (we set da = 128 for all relations). The top words are arranged in the
order of attention weights.

discourse relation-relevant information into semantic repre-
sentations of discourses, which, to some extend, simulates
the cognition process of humans. Experiment results show
that our model outperforms several strong baselines, and fur-
ther analysis reveals that our model can indeed detect some
relation-relevant words.

In the future, we would like to exploit different types of se-
mantic memory, e.g., a distributed memory on ontology con-
cepts and relations. We also want to explore different atten-
the concat and dot in [Luong et al.,
tion architectures, e.g.
2015]. Furthermore, we are interested in adapting our model
to other similar classiﬁcation tasks, such as sentiment classi-
ﬁcation, movie review classiﬁcation and nature language in-
ference.

4.4 Attention Analysis
We would like to know more about what role the semantic
memory plays in our model, especially what the model learns
from this semantic memory. Analyzing semantic representa-
tions is relatively meaningless. Therefore we turn to look into
words with high attention weights for the answer.

We present one example per discourse relation from the
test set in Table 3, where words assigned with the top-5 atten-
tion weights are listed separately. Consider the example for
COM, our model retrieves the words “wrong, people, dead,
think, smokestack”, which roughly reﬂect the discourse mean-
ing that people think smokestack, dead wrong. Obviously,
these words are crucial for discourse comprehension. These
examples display that SeMDER prefers to retrieve from the
semantic memory relation-relevant words that strongly indi-
cate the corresponding relations, which we think is the main
reason for the success of SeMDER.

5 Conclusion and Future Work
In this paper, we have presented a neural discourse relation
recognizer with a distributed semantic memory for implicit
DRR. The semantic memory encodes semantic knowledge of
words in discourse arguments and helps disambiguation and
comprehension. We employ an attention model to retrieve

References
[Bahdanau et al., 2014] Dzmitry Bahdanau, Kyunghyun Cho, and
Yoshua Bengio. Neural machine translation by jointly learning
to align and translate. 2014.

[Braud and Denis, 2014] Chlo´e Braud and Pascal Denis. Combin-
ing natural and artiﬁcial examples to improve implicit discourse
relation identiﬁcation. In Proc. of COLING, pages 1694–1705,
August 2014.

[Cimiano et al., 2005] Philipp Cimiano, Uwe Reyle, and Jasmin
ˇSari´c. Ontology-driven discourse analysis for information ex-
traction. Data & Knowledge Engineering, 55:59–83, 2005.

[Fisher and Simmons, 2015] Robert Fisher and Reid Simmons.
In

Spectral semi-supervised discourse relation classiﬁcation.
Proc. of ACL-IJCNLP, pages 89–93, July 2015.

[Guzm´an et al., 2014] Francisco Guzm´an, Shaﬁq Joty, Llu´ıs
M`arquez, and Preslav Nakov. Using discourse structure improves
machine translation evaluation. In Proc. of ACL, pages 687–698,
June 2014.

[Hernault et al., 2010] Hugo Hernault, Danushka Bollegala, and
Mitsuru Ishizuka. A Semi-Supervised Approach to Improve
Classiﬁcation of Infrequent Discourse Relations Using Feature
Vector Extension. In Proc. of EMNLP, 2010.

[Hong et al., 2012] Yu Hong, Xiaopei Zhou, Tingting Che, Jianmin
Yao, Qiaoming Zhu, and Guodong Zhou. Cross-argument in-
In Proc. of
ference for implicit discourse relation recognition.
CIKM, pages 295–304, 2012.

[Jansen et al., 2014] Peter Jansen, Mihai Surdeanu, and Peter
Clark. Discourse complements lexical semantics for non-factoid
answer reranking. In Proc. of ACL, pages 977–986, June 2014.

[Ji and Eisenstein, 2015] Yangfeng Ji and Jacob Eisenstein. One
vector is not enough: Entity-augmented distributed semantics for
discourse relations. TACL, pages 329–344, 2015.

[Ji et al., 2015] Yangfeng Ji, Gongbo Zhang, and Jacob Eisenstein.
Closing the gap: Domain adaptation from explicit to implicit dis-
course relations. In Proc. of EMNLP, pages 2219–2224, Septem-
ber 2015.

[Kumar et al., 2015] Ankit Kumar, Ozan Irsoy, Jonathan Su, James
Bradbury, Robert English, Brian Pierce, Peter Ondruska, Ishaan
Gulrajani, and Richard Socher. Ask me anything: Dynamic mem-
ory networks for natural language processing. CoRR, 2015.

[Lan et al., 2013] Man Lan, Yu Xu, and Zhengyu Niu. Leveraging
Synthetic Discourse Data via Multi-task Learning for Implicit
In Proc. of ACL, pages 476–
Discourse Relation Recognition.
485, Soﬁa, Bulgaria, August 2013.

[Lin et al., 2009] Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
Recognizing implicit discourse relations in the Penn Discourse
Treebank. In Proc. of EMNLP, pages 343–351, 2009.

[Luong et al., 2015] Minh-Thang Luong, Hieu Pham, and Christo-
pher D. Manning. Effective approaches to attention-based neural
machine translation. Proc. of EMNLP, 2015.

[McKeown and Biran, 2013] Kathleen McKeown and Or Biran.
Aggregated word pair features for implicit discourse relation dis-
ambiguation. In Proc. of ACL, pages 69–73, 2013.

[Miltsakaki et al., 2005] Eleni Miltsakaki, Nikhil Dinesh, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. Experiments on
sense annotations and sense disambiguation of discourse connec-
tives. In Proc. of TLT2005, 2005.

[Mnih et al., 2014] Volodymyr Mnih, Nicolas Heess, Alex Graves,
and koray kavukcuoglu. Recurrent models of visual attention. In
Proc. of NIPS, pages 2204–2212. 2014.

[Park and Cardie, 2012] Joonsuk Park and Claire Cardie. Improv-
ing Implicit Discourse Relation Recognition Through Feature Set
Optimization. In Proc. of SIGDIAL, pages 108–112, Seoul, South
Korea, July 2012.

[Patterson and Kehler, 2013] Gary Patterson and Andrew Kehler.
In Proc. of

Predicting the presence of discourse connectives.
EMNLP, pages 914–923, 2013.

[Pitler et al., 2008] Emily Pitler, Mridhula Raghupathy, Hena
Mehta, Ani Nenkova, Alan Lee, and Aravind K Joshi. Easily
identiﬁable discourse relations. Technical Reports (CIS), page
884, 2008.

[Pitler et al., 2009] Emily Pitler, Annie Louis, and Ani Nenkova.
Automatic sense prediction for implicit discourse relations in
text. In Proc. of ACL-AFNLP, pages 683–691, August 2009.

[Prasad et al., 2008] Rashmi Prasad, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L
Webber. The penn discourse treebank 2.0. In LREC. Citeseer,
2008.

[Rutherford and Xue, 2014] Attapol Rutherford and Nianwen Xue.
Discovering implicit discourse relations through brown cluster
pair representation and coreference patterns. In Proc. of EACL,
pages 645–654, April 2014.

[Rutherford and Xue, 2015] Attapol Rutherford and Nianwen Xue.
Improving the inference of implicit discourse relations via clas-
sifying explicit discourse connectives. In Proc. of NAACL-HLT,
pages 799–808, May–June 2015.

[Sukhbaatar et al., 2015] Sainbayar Sukhbaatar, arthur szlam, Ja-
son Weston, and Rob Fergus. End-to-end memory networks. In
Proc. of NIPS, pages 2431–2439. 2015.

[Tulving, 1972] Endel Tulving. Episodic and semantic memory. In
Endel Tulving and W. Donaldson, editors, Organization of Mem-
ory, pages 381–403. Academic Press, New York, 1972.

[Wang et al., 2012] Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li.
Implicit discourse relation recognition by selecting typical train-
ing examples. In Proc. of COLING, pages 2757–2772, 2012.

[Weston et al., 2014] Jason Weston, Sumit Chopra, and Antoine

Bordes. Memory networks. CoRR, abs/1410.3916, 2014.

[Xu et al., 2015] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S.
Zemel, and Yoshua Bengio. Show, attend and tell: Neural im-
age caption generation with visual attention. In Proc. of ICML,
pages 2048–2057, 2015.

[Yee et al., 2014] Eiling Yee, Evangelia G Chrysikou, and Sharon L
Thompson-Schill. The cognitive neuroscience of semantic mem-
ory, 2014.

[Yoshida et al., 2014] Yasuhisa Yoshida, Jun Suzuki, Tsutomu Hi-
rao, and Masaaki Nagata. Dependency-based discourse parser
for single-document summarization. In Proc. of EMNLP, pages
1834–1839, October 2014.

[Zhang et al., 2015] Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie
Lu, Hong Duan, and Junfeng Yao. Shallow convolutional neural
network for implicit discourse relation recognition. In Proc. of
EMNLP, September 2015.

[Zhou et al., 2010] Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man
Lan, Jian Su, and Chew Lim Tan. Predicting discourse con-
nectives for implicit discourse relation recognition. In Proc. of
COLING, pages 1507–1514, 2010.

