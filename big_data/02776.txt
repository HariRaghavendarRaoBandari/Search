Implicit Discourse Relation Classiﬁcation via Multi-Task Neural Networks

Yang Liu1, Sujian Li1,2, Xiaodong Zhang1 and Zhifang Sui1,2
1 Key Laboratory of Computational Linguistics, Peking University, MOE, China
2 Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China

{cs-ly, lisujian, zxdcs, szf}@pku.edu.cn

6
1
0
2

 
r
a

M
9

 

 
 
]
L
C
.
s
c
[
 
 

1
v
6
7
7
2
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Without discourse connectives, classifying implicit dis-
course relations is a challenging task and a bottleneck
for building a practical discourse parser. Previous re-
search usually makes use of one kind of discourse
framework such as PDTB or RST to improve the clas-
siﬁcation performance on discourse relations. Actually,
under different discourse annotation frameworks, there
exist multiple corpora which have internal connections.
To exploit the combination of different discourse cor-
pora, we design related discourse classiﬁcation tasks
speciﬁc to a corpus, and propose a novel Convolutional
Neural Network embedded multi-task learning system
to synthesize these tasks by learning both unique and
shared representations for each task. The experimental
results on the PDTB implicit discourse relation classiﬁ-
cation task demonstrate that our model achieves signif-
icant gains over baseline systems.

Introduction

Discourse relations (e.g., contrast and causality) support a
set of sentences to form a coherent text. Automatically iden-
tifying these discourse relations can help many downstream
NLP tasks such as question answering and automatic sum-
marization.

Under certain circumstances, these relations are in the
form of explicit markers like “but” or “because”, which is
relatively easy to identify. Prior work (Pitler, Louis, and
Nenkova 2009) shows that where explicit markers exist, re-
lation types can be disambiguated with F1 scores higher than
90%. However, without an explicit marker to rely on, classi-
fying the implicit discourse relations is much more difﬁcult.
The fact that these implicit relations outnumber the explicit
ones in naturally occurring text makes the classiﬁcation of
their types a key challenge in discourse analysis.

The major line of research work approaches the implicit
relation classiﬁcation problem by extracting informed fea-
tures from the corpus and designing machine learning al-
gorithms (Pitler, Louis, and Nenkova 2009; Lin, Kan, and
Ng 2009; Louis et al. 2010). An obvious challenge for clas-
sifying discourse relations is which features are appropri-
ate for representing the sentence pairs. Intuitively, the word
Copyright c(cid:13) 2016, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

pairs occurring in the sentence pairs are useful, since they
to some extent can represent semantic relationships between
two sentences (for example, word pairs appearing around
the contrast relation often tend to be antonyms). In earlier
studies, researchers ﬁnd that word pairs do help classifying
the discourse relations. However, it is strange that most of
these useful word pairs are composed of stopwords. Ruther-
ford and Xue (2014) point out that this counter-intuitive
phenomenon is caused by the sparsity nature of these word
pairs. They employ Brown clusters as an alternative abstract
word representation, and as a result, they get more intuitive
cluster pairs and achieve a better performance.

Another problem in discourse parsing is the coexistence
of different discourse annotation frameworks, under which
different kinds of corpora and tasks are created. The well-
known discourse corpora include the Penn Discourse Tree-
Bank (PDTB) (Prasad et al. 2007) and the Rhetorical Struc-
ture Theory - Discourse Treebank (RST-DT) (Mann and
Thompson 1988). Due to the annotation complexity, the size
of each corpus is not large enough. Further, these corpora
under different annotation frameworks are usually used sep-
arately in discourse relation classiﬁcation, which is also the
main reason of sparsity in discourse relation classiﬁcation.
However, these different annotation frameworks have strong
internal connections. For example, both the Elaboration and
Joint relations in RST-DT have a similar sense as the Expan-
sion relation in PDTB. Based on this, we consider to design
multiple discourse analysis tasks according to these frame-
works and synthesize these tasks with the goal of classifying
the implicit discourse relations, ﬁnding more precise repre-
sentations for the sentence pairs. The most inspired work to
us is done by (Lan, Xu, and Niu 2013) where they regard
implicit and explicit relation classiﬁcation in PDTB frame-
work as two tasks and design a multi-task learning method
to obtain higher performance.

In this paper, we propose a more general multi-task learn-
ing system for implicit discourse relation classiﬁcation by
synthesizing the discourse analysis tasks within different
corpora. To represent the sentence pairs, we construct the
convolutional neural networks (CNNs) to derive their vector
representations in a low dimensional latent space, replacing
the sparse lexical features. To combine different discourse
analysis tasks, we further embed the CNNs into a multi-task
neural network and learn both the unique and shared repre-

sentations for the sentence pairs in different tasks, which can
reﬂect the differences and connections among these tasks.
With our multi-task neural network, multiple discourse tasks
are trained simultaneously and optimize each other through
their connections.

Prerequisite

As stated above, to improve the implicit discourse relation
classiﬁcation, we make full use of the combination of differ-
ent discourse corpora. In our work, we choose three kinds
of discourse corpora: PDTB, RST-DT and the natural text
with discourse connective words. In this section, we brieﬂy
introduce the corpora.

PDTB The Penn Discourse Treebank (PDTB) (Prasad et
al. 2007), which is known as the largest discourse corpus, is
composed of 2159 Wall Street Journal articles. PDTB adopts
the predicate-argument structure, where the predicate is the
discourse connective (e.g. while) and the arguments are two
text spans around the connective. In PDTB, a relation is ex-
plicit if there is an explicit discourse connective presented in
the text; otherwise, it is implicit. All PDTB relations are hi-
erarchically organized into 4 top-level classes: Expansion,
Comparison, Contingency, and Temporal and can be fur-
ther divided into 16 types and 23 subtypes. In our work, we
mainly experiment on the 4 top-level classes as in previous
work (Lin, Kan, and Ng 2009).

RST-DT RST-DT is based on the Rhetorical Structure
Theory (RST) proposed by (Mann and Thompson 1988) and
is composed of 385 articles. In this corpus, a text is repre-
sented as a discourse tree whose leaves are non-overlapping
text spans called elementary discourse units (EDUs). Since
we mainly focus on discourse relation classiﬁcation, we
make use of the discourse dependency structure (Li et al.
2014) converted from the tree structures and extracted the
EDU pairs with labeled rhetorical relations between them.
In RST-DT, all relations are classiﬁed into 18 classes. We
choose the highest frequent 12 classes and get 19,681 rela-
tions.

Raw Text with Connective Words There exists a large
amount of raw text with connective words in it. As we know,
these connective words serve as a natural means to con-
nect text spans. Thus, the raw text with connective words is
somewhat similar to the explicit discourse relations in PDTB
without expert judgment and can also be used as a spe-
cial discourse corpus. In our work, we adopt the New York
Times (NYT) Corpus (Sandhaus 2008) with over 1.8 mil-
lion news articles. We extract the sentence pairs around the
35 commonly-used connective words, and generate a new
discourse corpus with 40,000 relations after removing the
connective words. This corpus is not veriﬁed by human and
contains some noise, since not all the connective words re-
ﬂect discourse relations or some connective words may have
different meanings in different contexts. However, it can still
help training a better model with a certain scale of instances.

Multi-Task Neural Network for Discourse

Parsing

Motivation and Overview
Different discourse corpora are closely related, though un-
der different annotation theories. In Table 1, we list some
instances which have similar discourse relations in nature
but are annotated differently in different corpora. The sec-
ond row belongs to the Elaboration relation in RST-DT. The
third and fourth row are both Expansion relations in PDTB:
one is implicit and the other is explicit with the connective
“in particular”. The ﬁfth row is from the NYT corpus and
directly uses the word “particularly” to denote the discourse
relation between two sentences. From these instances, we
can see they all reﬂect the similar discourse relation that the
second argument gives more details of the ﬁrst argument. It
is intuitive that the classiﬁcation performance on these in-
stances can be boosted from each other if we appropriately
synthesize them. With this idea, we propose to adopt the
multi-task learning method and design a speciﬁc discourse
analysis task for each corpus.

According to the principle of multi-task learning, the
more related the tasks are, the more powerful the multi-task
learning method will be. Based on this, we design four dis-
course relation classiﬁcation tasks.
Task 1 Implicit PDTB Discourse Relation Classiﬁcation
Task 2 Explicit PDTB Discourse Relation Classiﬁcation
Task 3 RST-DT Discourse Relation Classiﬁcation
Task 4 Connective Word Classiﬁcation

The ﬁrst two tasks both classify the relation between two
arguments in the PDTB framework. The third task is to pre-
dict the relations between two EDUs using our processed
RST-DT corpus. The last one is designed to predict a correct
connective word to a sentence pair by using the NYT corpus.
We deﬁne the task of classifying implicit PDTB relations as
our main task, and the other tasks as the auxiliary tasks. This
means we will focus on learning from other tasks to improve
the performance of our main task in our system. It is noted
that we call the two text spans in all the tasks as arguments
for convenience.

Next, we will introduce how we tackle these tasks. In our
work, we propose to use the convolutional neural networks
(CNNs) for representing the argument pairs. Then, we em-
bed the CNNs into a multi-task neural network (MTNN),
which can learn the shared and unique properties of all the
tasks.
CNNs: Modeling Argument Pairs
Figure 1 illustrates our proposed method of modeling the ar-
gument pairs. We associate each word w with a vector repre-
sentation xw ∈ RDe, which is usually pre-trained with large
unlabeled corpora. We view an argument as a sequence of
these word vectors, and let x1
i ) be the vector of the i-
th word in argument Arg1 (Arg2). Then, the argument pair
can be represented as,

i (x2

Arg1 : [x1
Arg2 : [x2

1, x1
1, x2

2,··· , x1
2,··· , x2

]

]

(1)
(2)

m1

m2

Data Source
RST-DT

Discourse Relation
Elaboration

Argument 1
it added 850 million Canadian dollars

PDTB

PDTB

Expansion(implicit)

Expansion(explicit)

Income from continuing operations was up
26%
as in summarily sacking exchange controls

NYT Corpus

particularly

Native plants seem to have a built-in tolerance
to climate extremes and have thus far done
well

Argument 2
reserves now amount to 61% of its total less-
developed-country exposure
Revenue rose 24% to $6.5 billion from $5.23
billion
and in particular slashing the top rate of in-
come taxation to 40%
particularly ﬁne show-offs have been the but-
terﬂy weeds, boneset and baptisia

Table 1: Discourse Relation Examples in Different Corpora.

where Arg1 has m1 words and Arg2 has m2 words.

Figure 1: Neural Networks For Modeling the Argument Pair.

Generally, let xi:i+j relate to the concatenation of word
vectors xi, xi+1,··· , xi+j. A convolution operation in-
volves a ﬁlter w, which is applied to a window of h words to
produce a new feature. For our speciﬁc task of capturing the
relation between two arguments, each time we take h words
from each arguments, concatenate their vectors, and apply
the convolution operation on this window pair. For example,
a feature cij is generated from a window pair composed by
words x1
j:j+h−1 from Arg2,
(3)
where b is a bias term and f is a non-linear function, for
which in this paper we use tanh.

i:i+h−1 from Arg1 and words x2

cij = f (w · [x1

i:i+h−1, x2

j:j+h−1] + b)

The ﬁlter is applied to each possible window pair of the
two arguments to produce a feature map c, which is a two-
dimensional matrix. Since the arguments may have different
lengths, we use an operation called “dynamic pooling” to
capture the most salient features in c, generating a ﬁxed-
size matrix p ∈ Rnp×np. In order to do this, matrix c will be
divided into np roughly equal parts. Every maximal value in
the rectangular window is selected to form a np × np grid.
During this process, the matrix p will lose some information
compared to the original matrix c. However, this approach
can capture c’s global structure. For example, the upper left
part of p will be constituted by word pair features reﬂecting
the relationship between the beginnings of two arguments.
This property is useful to discourse parsing, because some
prior research (Lin, Kan, and Ng 2009) has pointed out that
word position in one argument is important for identifying
the discourse relation.

With multiple ﬁlters like this, the argument pairs can be
modeled as a three-dimensional tensor. We ﬂatten it to a vec-
tor p ∈ Rnp×np×nf and use it as the representation of the
argument pair, where nf is the number of ﬁlters.

Multi-Task Neural Networks: Classifying
Discourse Relations

Multi-task learning (MTL) is a kind of machine learning ap-
proach, which trains both the main task and auxiliary tasks
simultaneously with a shared representation learning the
commonality among the tasks. In our work, we embed the
convolutional neural networks into a multi-task learning sys-
tem to synthesize the four tasks mentioned above. We map
the argument pairs of different tasks into low-dimensional
vector representations with the proposed CNN. To guarantee
the principle that these tasks can optimize each other with-
out bringing much noise, each task owns a unique represen-
tation of the argument pairs, meanwhile, there is a special
shared representation connecting all the tasks. The architec-
ture of our multi-task learning system is shown in Figure 2.
For clarity, the diagram depicts only two tasks. It should be
aware that the number of tasks is not limited to two.

Figure 2: Architecture of Multi-task Neural Networks for
Discourse Relation Classiﬁcation.

For task t, the argument pair s = (Arg1, Arg2) is
mapped into a unique vector pt and a shared vector ps,
where N N denotes the convolutional neural networks for

Arg2Argument PairWindow PairFeature Map(cid:1849)(cid:1855)(cid:1867)(cid:1866)(cid:1874) Vector Representation(cid:1830)(cid:1877)(cid:1866)(cid:1853)(cid:1865)(cid:1861)(cid:1855)(cid:1842)(cid:1867)(cid:1867)(cid:1864)(cid:1861)(cid:1866)(cid:1859) Arg1Arg2(cid:1875)(cid:1855)(cid:1867)(cid:1866)(cid:1874) Arg1Task	2NN	ShareArg2PairRepresentationRepresentationVector(cid:1849)1(cid:1872)(cid:3404)1 (cid:1849)1(cid:1872)(cid:3404)2 (cid:1849)2(cid:1872)(cid:3404)1 (cid:1849)2(cid:1872)(cid:3404)2 (cid:1868)(cid:1871)  (cid:1868)1 (cid:1868)2  (cid:1869)2 (cid:1869)1 (cid:1871)(cid:1858)1 (cid:1871)(cid:1858)2 Task	1Results(cid:1864)1 (cid:1864)2 Arg1Task	2NN	ShareArg2Arguments PairArguments Pair RepresentationTask-specific Representation(cid:1849)1(cid:1872)(cid:3404)1 (cid:1849)1(cid:1872)(cid:3404)2 (cid:1849)2(cid:1872)(cid:3404)1 (cid:1849)2(cid:1872)(cid:3404)2 Task	1Classification Results(cid:1869)1 (cid:1868)(cid:1871) (cid:1868)2 (cid:1868)1 (cid:1871)(cid:1858)1 (cid:1869)2 (cid:1871)(cid:1858)2 (cid:1864)1 (cid:1864)2 modeling the argument pair,

pt = N Nt(Arg1, Arg2)
ps = N N (Arg1, Arg2)

(4)
(5)

These two vectors are then concatenated and mapped into
a task-speciﬁc representation qt by a nonlinear transforma-
tion,

qt = f (wt

1[pt, ps] + bt
1)

(6)

where wt is the transformation matrix and bt
term.

1 is the bias

After acquiring qt, we use several additional surface-level
features, which have been proven useful in a bunch of ex-
isting work (Lin, Kan, and Ng 2009; Rutherford and Xue
2014). We notate the feature vector for task t as sf t. Then,
we concatenate sf t with qt and name it rt. Since all the
tasks are related with classiﬁcation, we set the dimension of
the output vector for task t as the predeﬁned class number
nt. Next, we take rt as input and generate the output vector
lt through a sof tmax operation with the weight matrix w2
t
and the bias bt
2,

rt = [qt, sf t]

lt = sof tmax(wt

2rt + bt
2)

(7)
(8)

where the i-th dimension of lt can be interpreted as the con-
ditional probability that an instance belongs to class i in task
t.

This network architecture has various good properties.
The shared representation makes sure these tasks can effec-
tively learn from each other. Meanwhile, multiple CNNs for
modeling the argument pairs give us the ﬂexibility to assign
different hyper-parameters to each task. For example, PDTB
is built on sentences while RST-DT is on elementary dis-
course units, which are usually shorter than sentences. Un-
der the proposed framework, we can assign a larger window
size to PDTB related tasks and a smaller window size to the
RST-DT related task, for better capturing their discourse re-
lations.
Additional Features When classifying the discourse rela-
tions, we consider several surface-level features, which are
supplemental to the automatically generated representations.
We use different features for each task, considering their
speciﬁc properties. These features include:
• The ﬁrst and last words of the arguments (For task 1)
• Production rules extracted from the constituent parse trees

of the arguments (For task 1,2,4)

• Whether two EDUs are in the same sentence (For task 3)

Model Training
We deﬁne the ground-truth label vector gt for each instance
in task t as a binary vector. If the instance belongs to class
i, only the i-th dimension gt[i] is 1 and the other dimen-
sions are set to 0. In our MTNN model, all the tasks are
classiﬁcation problems and we adopt cross entropy loss as

the optimization function. Given the neural network param-
eters Θ and the word embeddings Θe, the objective function
for instance s can be written as,

J(Θ, Θe) = − nt(cid:88)

gt[i]lt[i]

(9)

i

We use mini-batch stochastic gradient descent (SGD) to
train the parameters Θ and Θe. Referring to the training pro-
cedure in (Liu et al. 2015), we select one task in each epoch
and update the model according to its task-speciﬁc objective.
To avoid over-ﬁtting, we use different learning rates to
train the neural network parameters and the word embed-
dings , which are denoted as λ and λe. To make the most
of all the tasks, we expect them to reach their best perfor-
mance at roughly the same time. In order to achieve this, we
assign different regulative ratio µ and µe to different tasks,
adjusting their learning rates λ and λe. That is, for task t, the
update rules for Θ and Θe are,

Θ ← Θ + µtλ
Θe ← Θe + µt

eλe

∂J(Θ)

∂Θ
∂J(Θe)

∂Θe

(10)

(11)

It is worth noting that, to avoid bringing noise to the main
task, we let µe of the auxiliary tasks to be very small,
preventing them from changing the word embeddings too
much.

Experiments

Datasets
As introduced above, we use three corpora: PDTB, RST-DT,
and the NYT corpus, in our experiments to train our multi-
task neural network.

Relation
Comparison
Contingency
Expansion
Temporal
Total

Train
1855
3235
6673
582
12345

Dev
189
281
638
48
1156

Test
145
273
538
55
1011

Table 2: Distribution of Implicit Discourse Relations in
PDTB.

Since our main goal is to conduct implicit discourse rela-
tion classiﬁcation (the main task), Table 2 summarizes the
statistics of the four top-level implicit discourse relations
in PDTB. We follow the setup of previous studies (Pitler,
Louis, and Nenkova 2009), splitting the dataset into a a train-
ing set, development set, and test set. Sections 2-20 are used
to train classiﬁers, Sections 0-1 to develop feature sets and
tune models, and Section 21-22 to test the systems.

For Task 2, all 17,469 explicit relations in sections 0-24 in
PDTB are used. Table 3 shows the distribution of these ex-
plicit relations on four classes. For Task 3, we convert RST-
DT trees to discourse dependency trees according to (Li et
al. 2014) and get direct relations between EDUs, which is

Relation
Comparison
Contingency

Freq. Relation
Temporal
5397
3104
Expansion

Freq.
2925
6043

Table 3: Distribution of Explicit Discourse Relations in
PDTB.

more suitable for the classiﬁcation task. We choose the 12
most frequent coarse-grained relations shown in Table 4,
generating a corpus with 19,681 instances.

Relation
Elaboration
Attribution
Joint
Same-unit
Contrast
Explanation

Freq. Relation
7675 Background
2984 Cause
1923
1357
1090
959

Evaluation
Enablement
Temporal
Comparison

Freq.
897
867
582
546
499
302

Table 4: Distribution of 12 Relations Used in RST-DT.

For Task 4, we use the Standford parser (Klein and Man-
ning 2003) to segment sentences. We select the 35 most fre-
quent connectives in PDTB, and extract instances containing
these connectives from the NYT corpus based on the same
patterns as in (Rutherford and Xue 2015). We then manu-
ally compile a set of rules to remove some noisy instances,
such as those with too short arguments. Finally, we obtain a
corpus with 40,000 instances by random sampling. Due to
space limitation, we only list the 10 most frequent connec-
tive words in our corpus in Table 5.

Relation
Because
If
Until
In fact
Indeed

Pct.

Relation

22.52% For example
8.65% As a result
9.45% So
9.25% Unless
8.02% In the end

Pct.
5.92%
4.30%
3.26%
2.69%
2.59%

Table 5: Percentage of 10 Frequent Connective Words Used
in NYT Corpus.

Model Conﬁguration
We use word embeddings provided by GloVe (Pennington,
Socher, and Manning 2014), and the dimension of the em-
beddings De is 50. We ﬁrst train these four tasks separately
to roughly set their hyper-parameters. Then, we more care-
fully tune the multi-task learning system based on the perfor-
mance of our main task on the development set. The learning
rates are set as λ = 0.004, λe = 0.001.

Each task has a set of hyper-parameters, including the
window size of CNN h, the pooling size np, the number of
ﬁlters nf , dimension of the task-speciﬁc representation nr,
and the regulative ratios µ and µe. All the tasks share a win-
dow size, a pooling size and a number of ﬁlters for learning
the shared representation, which are denoted as hs, ns
f .
p, ns
The detailed settings are shown in Table 6.

Task

1
2
3
4

h
5
5
4
4

np
10
10
8
10

nf
80
80
100
100

nr
20
20
30
40

µ
1.0
1.5
2.0
2.0

µe
1.0
0.15
0.2
0.2

hs

6

ns
p

10

ns
f

40

Table 6: Hyper-parameters for the MTL system.

Evaluation and Analysis
We mainly evaluate the performance of the implicit PDTB
relation classiﬁcation, which can be seen as a 4-way clas-
siﬁcation task. For each relation class, we adopt the com-
monly used metrics, Precision, Recall and F1, for perfor-
mance evaluation. To evaluate the whole system, we use the
metrics of Accuracy and macro-averaged F 1.
Analysis of Our Model First of all, we evaluate the com-
bination of different tasks. Table 7 shows the detailed results.
For each relation, we ﬁrst conduct the main task (denoted as
1) through implementing a CNN model and show the results
in the ﬁrst row. Then we combine the main task with one of
the other three auxiliary tasks (i.e., 1+2, 1+3, 1+4) and their
results in the next three rows. The ﬁnal row gives the per-
formance using all the four tasks (namely, ALL). In general,
we can see that when synthesizing all the tasks, our MTL
system can achieve the best performance.

Relation

Expansion

Comparison

Temporal

Contingency

Tasks
1
1+2
1+3
1+4
ALL
1
1+2
1+3
1+4
ALL
1
1+2
1+3
1+4
ALL
1
1+2
1+3
1+4
ALL

Precision

59.47
60.64
60.35
60.00
64.13
34.65
30.00
40.37
35.94
30.63
35.29
36.36
37.50
60.00
42.42
42.93
40.34
42.50
47.11
59.20

Recall
74.72
71.00
71.56
77.51
76.77
30.35
22.76
30.34
15.86
33.79
10.91
21.82
16.36
10.91
25.45
30.04
35.17
37.36
41.76
37.73

F1
66.23
65.41
65.48
67.64
69.88
32.35
25.88
34.65
22.01
32.13
16.67
27.27
22.79
18.46
31.82
35.35
37.57
39.77
44.27
46.09

Table 7: Results on 4-way Classiﬁcation of Implicit Rela-
tions in PDTB.

More speciﬁcally, we ﬁnd these tasks have different inﬂu-
ence on different discourse relations. Task 2, the classiﬁca-
tion of explicit PDTB relations, has slight or even negative
impact on the relations except the Temporal relation. This re-
sult is consistent with the conclusion reported in (Sporleder
and Lascarides 2008), that there exists difference between
explicit and implicit discourse relations and more corpus
of explicit relations does not deﬁnitely boost the perfor-
mance of implicit ones. Task 4, the classiﬁcation of connec-

tive words, besides having the similar effects, is observed to
be greatly helpful for identifying the Contingency relation.
This may be because the Contingency covers a wide range
of subtypes and the ﬁne-grained connective words in NYT
corpus can give some hints of identifying this relation. On
the contrary, when training with the task of classifying RST-
DT relations (Task 3), the result gets better on Comparison,
however, the improvement on other relations is less obvious
than when using the other two tasks. One possible reason for
this is the deﬁnitions of Contrast and Comparison in RST-
DT are similar to Comparison in PDTB, so these two tasks
can more easily learn from each other on these classes. Im-
portantly, when synthesizing all the tasks in our model, ex-
cept the result on Comparison relation experiences a slight
deterioration, the classiﬁcation performance generally gets
better.

System

(Rutherford and Xue 2015)

Proposed STL
Proposed MTL

Accuracy

57.10
52.82
57.27

F1
40.50
37.65
44.98

Table 8: General Performances of Different Approaches on
4-way Classiﬁcation Task.

Comparison with Other Systems We compare the gen-
eral performance of our model with a state-of-the-art sys-
tem in terms of accuracy and macro-average F1 in Table
8. Rutherford and Xue (2015) elaborately select a com-
bination of various lexical features, production rules, and
Brown cluster pairs, feeding them into a maximum entropy
classiﬁer. They also propose to gather weakly labeled data
based on the discourse connectives for the classiﬁer and
achieve state-of-the-art results on 4-way classiﬁcation task.
We can see our proposed MTL system achieves higher per-
formance on both accuracy and macro-averaged F1. We also
compare the general performance between our MTL system
and the Single-task Learning (STL) system which is only
trained on Task 1. The result shows MTL raises the Accu-
racy from 52.82 to 57.27 and the F 1 from 37.65 to 44.98.
Both improvements are signiﬁcant under one-tailed t-test
(p < 0.05).

System

(Zhou et al. 2010)

(Park and Cardie 2012)
(Ji and Eisenstein 2015)

(R&X 2015)
Proposed STL
Proposed MTL

Comp.
31.79
31.32
35.93
41.00
37.10
37.91

Cont.
47.16
49.82
52.78
53.80
51.73
55.88

Expa.

-
-
-

69.40
67.53
69.97

Temp.
20.30
26.57
27.63
33.30
29.38
37.17

Table 9: General Performances of Different Approaches on
Binary Classiﬁcation Task.

For a more direct comparison with previous results, we
also conduct experiments based on the setting that the task
as four binary one vs. other classiﬁers. The results are pre-
sented in Table 9. Three additional systems are used as base-
lines. Park and Cardie (2012) design a traditional feature-

based method and promote the performance through opti-
mizing the feature set. Ji and Eisenstein (2015) used two re-
cursive neural networks on the syntactic parse tree to induce
the representation of the arguments and the entity spans.
Zhou et al. (2010) ﬁrst predict connective words on a unla-
beled corpus, and then use these these predicted connectives
as features to recognize the discourse relations.

The results show that the multi-task learning system is es-
pecially helpful for classifying the Contingency and Tempo-
rary relation. It increases the performance on Temporary re-
lation from 33.30 to 37.17, achieving a substantial improve-
ment. This is probably because this relation suffers from the
lack of training data in STL, and the use of MTL can learn
better representations for argument pairs, with the help of
auxiliary tasks. The Comparison relation beneﬁts the least
from MTL. Previous work of (Rutherford and Xue 2014)
suggests this relation relies on the syntactic information of
two arguments. Such features are captured in the upper layer
of our model, which can not be optimized by multiple tasks.
Generally, our system achieves the state-of-the-art perfor-
mance on three discourse relations (Expansion, Contingency
and Temporary).

Related Work

The supervised method often approaches discourse analysis
as a classiﬁcation problem of pairs of sentences/arguments.
The ﬁrst work to tackle this task on PDTB were (Pitler,
Louis, and Nenkova 2009). They selected several surface
features to train four binary classiﬁers, each for one of the
top-level PDTB relation classes. Although other features
proved to be useful, word pairs were the major contributor to
most of these classiﬁers. Interestingly, they found that train-
ing these features on PDTB was more useful than training
them on an external corpus. Extending from this work, Lin,
Kan, and Ng (2009) further identiﬁed four different feature
types representing the context, the constituent parse trees,
the dependency parse trees and the raw text respectively. In
addition, Park and Cardie (2012) promoted the performance
through optimizing the feature set. Recently, McKeown and
Biran (2013) tried to tackle the feature sparsity problem
by aggregating features. Rutherford and Xue (2014) used
brown cluster to replace the word pair features, achieving
the state-of-the-art classiﬁcation performance. Ji and Eisen-
stein (2015) used two recursive neural networks to represent
the arguments and the entity spans and use the combination
of the representations to predict the discourse relation.

There also exist some semi-supervised approaches which
exploit both labeled and unlabeled data for discourse rela-
tion classiﬁcation. Hernault, Bollegala, and Ishizuka (2010)
proposed a semi-supervised method to exploit
the co-
occurrence of features in unlabeled data. They found this
method was especially effective for improving accuracy for
infrequent relation types. Zhou et al. (2010) presented a
method to predict the missing connective based on a lan-
guage model trained on an unannotated corpus. The pre-
dicted connective was then used as a feature to classify the
implicit relation. An interesting work is done by (Lan, Xu,
and Niu 2013), where they designed a multi-task learning

method to improve the classiﬁcation performance by lever-
aging both implicit and explicit discourse data.

In recent years, neural network-based methods have
gained prominence in the ﬁeld of natural language process-
ing (Kim 2014; Cao et al. 2015). Some multi-task neural
networks are proposed. For example, Collobert et al. (2011)
designed a single sequence labeler for multiple tasks, such as
Part-of-Speech tagging, chunking, and named entity recog-
nition. Very recently, Liu et al. (2015) proposed a repre-
sentation learning algorithm based on multi-task objectives,
successfully combining the tasks of query classiﬁcation and
web search.

Conclusion

Previous studies on implicit discourse relation classiﬁcation
always face two problems: sparsity and argument represen-
tation. To solve these two problems, we propose to use dif-
ferent kinds of corpus and design a multi-task neural net-
work (MTNN) to synthesize different corpus-speciﬁc dis-
course classiﬁcation tasks. In our MTNN model, the con-
volutional neural networks with dynamic pooling are devel-
oped to model the argument pairs. Then, different discourse
classiﬁcation tasks can derive their unique and shared rep-
resentations for the argument pairs, through which they can
optimize each other without bringing useless noise. Experi-
ment results demonstrate that our system achieves state-of-
the-art performance. In our future work, we will design a
MTL system based on the syntactic tree, enabling each task
to share the structural information.

Acknowledgments

We thank all the anonymous reviewers for their insight-
ful comments on this paper. This work was partially sup-
ported by National Key Basic Research Program of China
(2014CB340504), National Natural Science Foundation of
China (61273278 and 61572049). The correspondence au-
thor of this paper is Sujian Li.

References

[2015] Cao, Z.; Wei, F.; Dong, L.; Li, S.; and Zhou, M. 2015.
Ranking with recursive neural networks and its application
to multi-document summarization. In Proceedings of AAAI.
[2011] Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.;
Kavukcuoglu, K.; and Kuksa, P. 2011. Natural language
processing (almost) from scratch. The Journal of Machine
Learning Research 12:2493–2537.
[2010] Hernault, H.; Bollegala, D.; and Ishizuka, M. 2010.
A Semi-Supervised Approach to Improve Classiﬁcation of
Infrequent Discourse Relations Using Feature Vector Exten-
sion. In Proceedings of EMNLP, 399–409.
2015. One vector is
[2015] Ji, Y., and Eisenstein, J.
not enough: Entity-augmented distributed semantics for dis-
course relations. Transactions of the Association for Com-
putational Linguistics 3:329–344.
[2014] Kim, Y. 2014. Convolutional neural networks for
In Proceedings of EMNLP, 1746–
sentence classiﬁcation.
1751.

[2003] Klein, D., and Manning, C. D. 2003. Accurate unlex-
icalized parsing. In Proceedings of ACL, 423–430.
[2013] Lan, M.; Xu, Y.; and Niu, Z.-Y. 2013. Leveraging
synthetic discourse data via multi-task learning for implicit
discourse relation recognition. In Proceedings of ACL, 476–
485.
[2014] Li, S.; Wang, L.; Cao, Z.; and Li, W. 2014. Text-
level discourse dependency parsing. In Proceedings of ACL,
volume 1, 25–35.
[2009] Lin, Z.; Kan, M.-Y.; and Ng, H. T. 2009. Recognizing
implicit discourse relations in the penn discourse treebank.
In Proceedings of EMNLP, 343–351.
[2015] Liu, X.; Gao, J.; He, X.; Deng, L.; Duh, K.; and Wang,
Y.-Y. 2015. Representation learning using multi-task deep
neural networks for semantic classiﬁcation and information
retrieval. In Proceedings NAACL, 912–921.
[2010] Louis, A.; Joshi, A.; Prasad, R.; and Nenkova, A.
2010. Using entity features to classify implicit discourse
relations. In Proceedings of SigDial, 59–62.
[1988] Mann, W. C., and Thompson, S. A. 1988. Rhetori-
cal structure theory: Toward a functional theory of text or-
ganization. Text-Interdisciplinary Journal for the Study of
Discourse 8(3):243–281.
[2013] McKeown, K., and Biran, O. 2013. Aggregated word
pair features for implicit discourse relation disambiguation.
In Proceedings of ACL, 69–73.
[2012] Park, J., and Cardie, C. 2012. Improving implicit dis-
course relation recognition through feature set optimization.
In Proceedings of SigDial, 108–112.
[2014] Pennington, J.; Socher, R.; and Manning, C. D. 2014.
Glove: Global vectors for word representation. In Proceed-
ings of EMNLP, 1532–1543.
[2009] Pitler, E.; Louis, A.; and Nenkova, A. 2009. Au-
tomatic sense prediction for implicit discourse relations in
text. In Proceedings of ACL, 683–691.
[2007] Prasad, R.; Miltsakaki, E.; Dinesh, N.; Lee, A.; Joshi,
A.; Robaldo, L.; and Webber, B. L. 2007. The penn dis-
course treebank 2.0 annotation manual.
[2014] Rutherford, A., and Xue, N. 2014. Discovering im-
plicit discourse relations through brown cluster pair repre-
sentation and coreference patterns. In Proceedings of EACL,
645–654.
[2015] Rutherford, A., and Xue, N. 2015.
Improving the
inference of implicit discourse relations via classifying ex-
In Proceedings of NAACL,
plicit discourse connectives.
799–808.
[2008] Sandhaus, E.
2008. The new york times anno-
tated corpus. Linguistic Data Consortium, Philadelphia
6(12):e26752.
[2008] Sporleder, C., and Lascarides, A. 2008. Using auto-
matically labelled examples to classify rhetorical relations:
An assessment. Natural Language Engineering 14(03):369–
416.
[2010] Zhou, Z.-M.; Xu, Y.; Niu, Z.-Y.; Lan, M.; Su, J.; and
Tan, C. L. 2010. Predicting discourse connectives for im-

plicit discourse relation recognition. In International Con-
ference on Computational Linguistics, 1507–1514.

