6
1
0
2

 
r
a

 

M
7
1

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
6
5
3
5
0

.

3
0
6
1
:
v
i
X
r
a

A Stationary Accumulated Projection Method for

Linear System of Equations

Wujian Penga,∗, Shuhua Zhangb

aDepartment of Math. and Info. Sciences, Zhaoqing Univ., Zhaoqing, China,526061

b Department of Math., Tianjin Univ. of Finance and Economics, Tianjin, China,300204

Abstract

It is shown in this paper that, almost all current prevalent iterative methods
for solving linear system of equations can be classiﬁed as what we called
extended Krylov subspace methods. In this paper a new type of iterative
methods are introduced which do not depend on any Krylov subspaces. This
type of methods are based on the so-called accumulated projection tech-
nique proposed by authors.
It overcomes some shortcomings of classical
Row-Projection technique and takes full advantages of the linear system.
Comparing with traditional Krylov subspace methods which always depend
on the matrix-vector multiplication with some ﬁxed matrix, the newly intro-
duced method (SAP) uses diﬀerent projection matrices which diﬀer in each
step in the iteration process to form an approximate solution. More impor-
tantly some particular accelerative schemes (named as MSAP1 and MSAP2)
are introduced to improve the convergence of the SAP method. Numerical
experiments show some surprisingly improved convergence behavior; some
superior experimental behavior of MSAP methods over GMRES and block-
Jacobi are demonstrated in some situations.

Keywords:
2000 MSC: 65F10, 15A06

Iterative method; Accumulated projection; Krylov subspace

∗Corresponding Author
Email addresses: wpeng@zqu.edu.cn (Wujian Peng), shuhua55@126.com (Shuhua

Zhang)

Preprint submitted to Elsevier

March 18, 2016

1. Introduction

Linear systems of the form

Ax = b

(1)

where A ∈ Rn×n is nonsingular arise from tremendous mathematical applica-
tions and are the fundamental objects of almost every computational process.
From the very ancient Gaussian elimination to state-of-the-art methods like
GMRES, PCG, Bicgstab ([1, 2, 15]) as well as Multigrid method ([8, 9]),
numerous solvers of linear systems have been introduced and studied in ex-
treme detail. Basically all solvers fall into two categories: Direct methods
and iterative methods.

Although some state-of-the-art direct methods can be applied to solve sys-
tems with pretty large amount of unknowns ([3, 6]) in some situations, for
even larger scale sparse systems (say, with unknowns up to a few millions) one
can resort to the LGO-based solver ([10, 11]) recently introduced by authors,
iterative methods are the only option available for many practical problems.
For example, detailed three-dimensional multiphysics simulations lead to lin-
ear systems comprising hundreds of millions or even billions of unknowns,
systems with several millions of unknowns are now routinely encountered in
many applications, making the use of iterative methods virtually mandatory.

Traditional iterative methods are classiﬁed as stationary and non-stationary

methods. Stationary methods usually take the form:

xk+1 = Gxk + v, k = 0, 1, 2,· · · .

(2)

where v is a ﬁxed vector and x0 as the ﬁrst guess.

Excellent books covering the detailed analysis of error and convergence

of these methods include works by Varga [14] and David Young [17], etc.

More recent iterative methods can be classiﬁed as Krylov subspace methods

(or non-stationary methods in some literature)[13]. Krylov subspace methods
take the following form

xk = x0 + yk, k = 1, 2,· · ·

(3)

where x0 is an initial guess and the correction vector yk belongs to a so-called
Krylov subspace

Km(G, r0) ≡ span{r0, Gr0, G2r0,· · · , Gm−1r0}.

2

By assuming diﬀerent strategies for seeking yk from Km(G, r0) with G usually
taken as A or A′, one gets a variety of iterative methods such as CG, GMRES,
BiCG, FOM, MNRES, SYMMLQ, etc.([7, 13, 15]).

As a matter of fact, if we would refer extended Krylov subspace methods
as those at each stage of iteration either the approximate solution or the
correction vectors always come from Krylov subspaces with a few (one or
two) ﬁxed generator matrices (By a “generator” matrix to Krylov subspace
Km(A, v) we mean matrix A here), then the traditional stationary iterative
methods such as Jacobi, Gauss-Seidal and SOR can also be classiﬁed as
extended Krylov subspace methods. Since for example one can easily see
from (2) that

xk+1 = v + Gv + G2v + G3v + · · · + Gkv + Gk+1x0 ≡ yk+1 + zk+1

where yk+1 = v + Gv + G2v + G3v + · · · + Gkv ∈ Kk+1(G, v) and zk+1 =
Gk+1x0 ∈ Kk+2(G, x0) and x0 is the initial guess to the system. It has been
shown in [5] that another well-known type of methods, the row projection
method(or ART method called in CT-related techniques, a generalization of
RP can also be seen in [12]), can also be put into the form of (2), and thus
they still belong to the category of extended Krylov subspace methods.

Krylov subspace methods can be very eﬀective when used in case the
condition number of the coeﬃcient matrix A is relatively small. However in
case of A having large condition numbers, they are not eﬀective any more or
even fail to converge.

Problems with current Krylov subspace methods lie on the fact that the
successive corrections to previous approximation come from Krylov subspace
with a ﬁxed “generator” matrix and usually some ﬁxed starting vector (say
vector v in above). If we take a look at the structure of Krylov subspace
Km(A, v) = span{v, Av, A2v · · · , Am−1v}, we can see that the base vectors
of this subspace always have the form Akv, which are increasingly closer to
the subspaces formed by the eigenvectors corresponding to the eigenvalues
with the largest magnitudes. For the sake of simplicity, we will refer them
as generalized eigenspace denoted by Ls(A), i.e,

Ls(A) = span{v1, v2,· · · , vs}

(4)

where Avi = λi and |λi| ≥ |λi+1|, for i = 1, 2,· · · , n. It is thus ineﬃcient
to ﬁnd a good “approximation” to the error vector em ≡ x − x0 in such
a subspace when em contains rich eigenvector components corresponding to

3

the smallest eigenvalues in magnitude. Especially when vector em is almost
perpendicular to the Krylov subspace Km(A, v).

It is thus always desirable for us to use some types of preconditioning
when we apply Krylov subspace iterative methods to solve linear system of
equations, especially for large scale computing. Though numerous precon-
ditioning techniques are exploited in recent decades and some of them turn
out to be extremely eﬃcient in some special situations, there does not exist a
simple preconditioning technique which can be applied in general cases. An-
other important factor is, all preconditioning techniques can be traced back
to certain algebraic iterative schemes ([4, 15, 16]).

Our motivation here is to develop a set of purely algebraic algorithms
that can in someway overcome the diﬃculties arising in the Krylov subspace
methods. Instead of seeking corrections from certain Krylov subspaces when
solving system (1), our new approach always tries to get a sequence of pro-
jection vectors {vk} of the solution x with each vk guaranteed to be closer to
x than its predecessor vk−1 by the so-called “accumulated projection” tech-
nique. More importantly We will also develop some accelerating techniques
to improve the convergence of our iterative methods.

2. An Accumulated Projection Idea

Let’s start from a simple projection idea. Assume x is the solution to (1)
and we can get the projection vector p of x onto some subspace easily. Of
course p can be used as an approximation to x and the error vector e = x− p
satisﬁes

Ae = r

where r = b − Ap.
To get a projection vector p of x onto any subspace W , we need the
ix where vi (i = 1,· · · , m) are the base
information of all inner products v′
vectors of W , i.e., W = span{v1, v2,· · · , vm}. From system (1) we see that
actually any groups of row vectors {Ai1, Ai2, Ai3,· · · , Aim} can be used as the
base vectors where Aik stands for the ikth row vector of matrix A. To get a
better approximation xl+1 to x provided that xl is given, people usually use
the residual equation

(5)
to ﬁnd a correction vector y so that xl+1 = xl + y is “closer” to x in some
measurement while y is obtained in the same way as xl is calculated, as done
in Row Projection Methods [5].

Ael = rl

4

Our idea here goes like the following.

Instead of using the residual
equations to get a correction vector, we use the given approximation pl (on
subspace Wl) and use it as a base vector to form another subspace Wl+1 to-
gether with a group of diﬀerent row vectors selected from matrix A since p′
lx
can also be calculated when pl is formed. The projection vector pl+1 of x onto
Wl+1 usually have a larger length and is thus “closer” to x than pl, which is
proved in Lemma 2.1 and Lemma 2.2. This process can be repeated until pl
reaches its limit position x (actually ¯x, the projection of x on to ran(A′)).
The major features of this approach are: First it takes full advantage of all
given information about the exact solution x (the projection information of
x on each base vector Ai ( Aix = bi)) as well as every approximating vec-
tor xl (x′xl is recorded and used in later calculations); secondly the whole
process uses only the original data of the system to reach a sequence of ap-
proximation xl which approaches to x steadily, i.e., the correction vector
(which is implicitly calculated) does not rely on the residual equations. It
is by this reason we name it as a “stationary” method. By this way we can
avoid the negative impact of successive matrix-vector multiplication between
a few ﬁxed generator matrices and some starting vector (i.e. the term Akv
in Krylov subspace construction); Thirdly, there does not exist any constant
iterative matrix ( like matrix G in (2) ) between adjacent approximations xl
and xl+1.

Assume x′vi = bi, (i = 1, 2) with b1 6= 0 and ||vi|| = 1 (i = 1, 2), we wish

to ﬁnd a real number t such that the function f (t) deﬁned by

f (t) = |x′v|
||v||

(6)

is maximized among all possible vectors in the form v = v1 +tv2. As a matter
of fact, this optimization problem is equivalent to searching a vector from
subspace span{v1, v2} so that it is as close to x as possible.
problem lies on the following conclusion.

It is easy to see from analysis that the answer to the above optimization

Lemma 2.1. Let x′vi = bi, (i = 1, 2) with |b1| ≥ |b2| and ||vi|| = 1 (i = 1, 2),
and α = v′

. Then

1v2. Let s = b2−αb1
b1−αb2

f (s) ≡ |x′(v1 + sv2)|
||v1 + sv2||

= max
t∈R

|x′(v1 + tv2)|
||v1 + tv2||

.

(7)

5

Furthermore

Proof. Let

We have

Thus

f (s) ≥ max{|b1|,|b2|}

(8)

g(t) =

.

x′(v1 + tv2)
||v1 + tv2||
b1 + tb2

g(t) =

√1 + 2αt + t2

.

g ′(t) =

b2(1 + 2αt + t2) − (b1 + tb2)(α + t)

(1 + 2αt + t2)3/2

=

=

(1 + 2αt + t2)3/2

b2 − αb1 − (b1 − αb2)t
(b1 − αb2)(s − t)
(1 + 2αt + t2)3/2

Let g ′(t) = 0 we have the solution as t =

b2 − αb1
b1 − αb2 ≡ s, i.e., s is an extreme

point for function f (t).
case 1. b1 > αb2, we have g ′(t) > 0 if (t < s) and g ′(t) < 0 if t > s. That
means g(t) reaches the maximal value at s. Since g(t) → −b2 when t → −∞
and g(t) → b2 when t → +∞, we have g(s) ≥ g(t) > −b2 for all t < s and
b2 < g(t) ≤ g(s) for all t > s, thus function f (t) = |g(t)| reaches its maximal
value at s.
case 2. b1 < αb2, we have g ′(t) < 0 if (t < s) and g ′(t) > 0 if t > s. That
means g(t) reaches the minimal value at s. Since g(t) → −b2 when t → −∞
and g(t) → b2 when t → +∞, we have g(s) ≤ g(t) < −b2 for all t < s and
b2 > g(t) ≥ g(s) for all t > s, thus we have f (t) = |g(t)| reaches its maximal
value at s.
Thus in both cases we have f (s) > |b2|. Since f (0) = |g(0)| = |b1| and f (s)
is the maximal value of f (t), thus we also have f (s) > |b1|. See ﬁgure 1. ✷
Remark: Assuming b1 6= 0, f (s) can be rewritten as following (by replac-
ing s as s = b2−αb1
b1−αb2

)

f (s) =

|b1 + sb2|

√1 + 2αs + s2

=

√1 − 2αr + r2

√1 − α2

|b1| = |b1|s1 +

(r − α)2
1 − α2

(9)

where r = b2/b1.

6

case1: b1 = 2, b2 = 1, alpha = 0.24

case2: b1 = −2, b2 = 1, alpha = 0.24

2.5

2

1.5

1

0.5

0

−0.5

−1
−50

1

0.5

0

−0.5

−1

−1.5

−2

−2.5

−3
−50

0

50

0

50

(a) case 1

(b) case 2

Figure 1: Graph of g(t)

In view of (9), f (s) → ∞ when α → 1 ( assuming that r is independent
of α). It is thus attempting for us to increase the length of p1(x) based on
0x = b1 by carefully selecting suitable
previous projection direction p0 with p′
vector d with d′x = b2 easily obtained so that α = p′
0p1 is as close as possible
to 1 (i.e., the angle between p0 and p1 should be very small). However this
seems to be very hard and thus we turn to an easier scheme to fulﬁll our
task—we will use subspaces on which projections of x are easily available.
For this purpose we now generalize our conclusion in Lemma 2.1 into the
following statement.

Lemma 2.2. Let x, vi ∈ Rn (i = 1, 2,· · · , m), and W = span{v1, v2,· · · , vm}.
Let ¯x be the projection of x onto space W . Then

¯x′x
||¯x||

= max
v∈W

.

|x′v|
||v||

Proof. Without loss of generality we can assume ||x|| = 1. By the deﬁnition
of angles between vectors we have

f (v) = |x′v|
||v||

= |x′v|
||v||||x||

= | cos < x, v > |

where < x, v > denotes the angle between vector x and v. Obviously f (v)
reaches its maximum value if and only if < x, v > is minimized, which is true
only when v lies on the projection of x onto subspace W .
✷

7

By using this result, one can always expect a searching direction d on
which vector x has a projection vector with larger length than any given base
vectors of subspace W = span{v1, v2,· · · , vm} with x′vi (i = 1, 2,· · · , m)
given. Since we have n vectors Ai (i = 1, 2, 3,· · · , n) to form subspaces of
Rn, this give us plenty of choices when it comes to construct subspaces.
More importantly we can use parallel process to construct these subspaces
and ﬁgure out projections of x on each of them. Instead of using successive
“partial” projections which did not adequately make use of current system
information, all these projections of x can be used to construct a better
approximation to the current system.

2.1. An Accumulated Projection Algorithm

In this subsection we present a basic algorithm for calculating a projec-
tion vector p of x to the system (1) based on current system data, i.e., the
coeﬃcient matrix A and the right-hand side vector b.

In preparation, we begin with the division of all row vectors of A into
groups of vectors {Gi}k
1, with each group Gi contains mi vectors, where
mi (i = 1,· · · , k) are relatively small integers satisfying mi < m, ∀1 ≤ i ≤
k. m is a suitable integer so that the QR factorization of matrix Ai (a
submatrix of A) formed by all vectors in group Gi is applicable; in case of
sparse coeﬃcient matrix, QS factorization process based on LGO method
[11] can be used and thus m can be relatively large (say, up to O(105) or
even larger). The right-hand side vector b is divided correspondingly into
vectors bi (i = 1,· · · , k). One thing needs to be mentioned here is that we
assume two adjacent groups Gi and Gi+1 contain about half of their vectors
in common and any row vector in A must lie in at least one of the groups, we
will refer this group {Gi} as an overlapped division of A. A non-overlapped
division of A means the intersection of any two groups in the division is
empty.

Our approach is to use a sequential projection process to get a ﬁnal pro-
jection vector p of x. We begin with an initial projection vector p0 of x and
let it combine with all row vectors in the ﬁrst group Gi to form a subspace
W1 of Rn, and then ﬁnd the projection vector p1 of x in W1. p1 is then used
to combine with all row vectors in the next group G2 to form a subspace W2
so that a projection vector p2 of x in W2 can be obtained. The above process
is repeated until all groups are handled so that the ﬁnal projection vector pk
are available. The following algorithm gives the details.

8

Algorithm 1. (An accumulated projection method-AP) The following proce-
dure produces an approximate vector p to the solution vector x which satisﬁes
Ax = b.
Step 1. Divide matrix A into k blocks: A1, A2,· · · , Ak, divide b correspond-
ingly: b = b1, b2,· · · , bk ( blocks Ai and Ai+1 may contain common
row vectors).
Step 2. Initialize p0 as p0 = αA′b and c0 = α||b||2, where α = ||b||2/||A′b||2.
Step 3. For i = 1 to k

Step 3.1 Construct matrix Wi = [pi−1, A′
Step 3.2 Compute the projection vector pi of x onto subspace ran(Wi)

i] and vector l = [ci−1, b′

i]′.

and the scalar ci(= x′pi) as
i Wi)−1W ′
i x

pi = Wi(W ′

and

ci = l′(W ′

i Wi)−1l.

Step 3.3 Go to next i.

Step 4. Output p(= pk) and c(= ck).

It should be mentioned here that the AP algorithm depicts a successive
projection process over subspace ran(Wi) = span{pi−1, v1, v2,· · · , vmi} (i =
1,· · · , k), where v1, v2,· · · , vmi denotes the row vectors of submatrix Ai of
A, and pi is the projection of x over subspace ran(Wi) with p0 stands for the
initial searching direction (usually a projection vector of x). Obviously we
have ||pi+1|| ≥ ||pi|| for 1 ≤ i ≤ k by Lemma 2.2.
Hence the whole AP process can be written in the matrix form as p =
Pkx where Pi (i = 1,· · · , k) represents the projection matrix over subspace
ran(Wi). It is easy to see that Pi depends on vector x. As a matter of fact,
Pk has the form

Pk = Wk(W ′

kWk)−1W ′
k
where Wk = [pk−1, A′
k). Here and after we always
k], assuming pk−1 /∈ ran(A′
use ran(A) to denote the range of matrix A, i.e, the subspace formed by all
column vectors of matrix A.

As a straightforward application, Algorithm 1 can be used to solve the

linear system (1) as stated in the next algorithm.

Algorithm 2. (Stationary Accumulated Projection Method-SAP). Let A ∈
Rm×n , b ∈ Rn with m ≤ n. ǫ be a given tolerance. The following procedure
produces an approximation p to the vector x ∈ Rn satisfying Ax = b.

9

Step1. Initialize s as s = 0, vector x0 as x0 = αA′b, c0 = αb′b, tol = ||b −

Ax0||/||b||, where α = ||b||2/||A′b||2.

Step2. While tol > ǫ

Step2.1 Use Algorithm 1 to get a projection vector xs+1 of x and
cs+1(= x′xs+1) with p and c in step 2 replaced by xs and cs
respectively.

Step2.2 Calculate tol = ||b − Axs||/||b||;
Step2.3 s = s + 1;

Step3. Output p(= xs) and c(= cs).

Remark: In actual implementation of SAP algorithm, in order to eﬀectively
obtain the projection of x over each subspace ran(Wi) through Algorithm
1, one can store the resulting QR or LGO factors ( Qi and Ri for QR, or
Qi and Si for LGO respectively) of all submatrix A′
i (i = 1, 2,· · · , k) once in
advance and reuse them in later projections. Although the projection ma-
trix Pi varies constantly, the projection vector can always be obtained in an
economic count of ﬂops, as it can be seen in later sections.

The convergence of this algorithm is put forward to the next section. We
need to point out that each sweep in step 2 is a projection process with
projection matrix Ps (s = 1, 2,· · · k) varies. Figure 2 shows the comparison
between approximate solutions at diﬀerent iterations by this algorithm, and

Figure 2: Comparison of approx. solns at diﬀerent iteration numbers

Table 1 gives the needed iteration for a convergent solution under given

10

tolerance, where the coeﬃcient matrix A is chosen as A = diag(−1, 2,−1)
with A ∈ R100×100 and the block size is chosen as 20 when applying Algorithm
2 in this case.

Table 1: SAP–iteration numbers needed for convergence

tolerance

iter#

10−3
724

10−4
872

10−5
1020

10−6
1169

10−7
1317

3. Error Analysis

In this section we present some analysis results for AP process and the
SAP algorithms. We need to mention here that unlike classical Krylov sub-
space methods, the SAP method proposed here can actually be used to solve
any under-determined systems.

3.1. AP analysis

We ﬁrst present some analysis about the AP process described in Algo-

rithm 1.

Lemma 3.1. Assume that matrix A ∈ Rm×n (m ≤ n) has full row rank,
x ∈ Rn and b ∈ Rm where m ≤ n satisfy Ax = b. Let A be divided into
k)′ with Ai ∈ Rmi×n, and b
2,· · · , A′
k submatrices by its rows: A = (A′
1 be the vector
is divided as b = (b′
sequence produced by AP process (Algorithm 1).

k)′ correspondingly. Let {pi}k

1, A′

1, b′

2,· · · , b′

(1) There holds for every i = 1, 2,· · · k
(x − pi, ps) = 0,

(s = i, i − 1).

(2) Vector pi+1 − pi (i = 0, 1,· · · , k − 1) is orthogonal to pi, i.e.

(pi+1 − pi, pi) = 0

(3) There holds for i = 1, 2,· · · , k

||pi||2 + ||pi+1 − pi||2 = ||pi+1||2

(10)

(11)

(12)

11

(4) For every s(1 ≤ s ≤ k), there holds

||ps||2 = ||p0||2 +

s

Xi=1

||pi − pi−1||2

(13)

Proof. (1) We ﬁrst show that (x − p0, p0) = 0. As a matter of fact, since

α = b′b/(b′AA′b), we have

1b = αb′b − αb′b = 0
(x − p0, p0) = (x − αA′
From the fact that pi is the projection of x over subspace ran(Wi) with
Wi = [pi−1, A′

1b1, αA′b) = αx′A′

1b − α2b′A1A′

i], for any i(1 ≤ i ≤ k) we must have

(x − pi, pi) = 0 and (x − pi, pi−1) = 0

since both pi and pi−1 belong to Wi.
(2) Note that from (10) we have

(pi+1 − pi, pi) = ((x − pi) − (x − pi+1), pi) = (x − pi, pi) − (x − pi+1, pi) = 0,
which yields (11).

(3) From (11) we have

||pi+1 − pi||2 = (pi+1 − pi, pi+1 − pi)
= (pi+1 − pi, pi+1)
= (pi+1, pi+1) − (pi, pi+1)
= (pi+1, pi+1) − (pi, (pi+1 − pi) + pi)
= (pi+1, pi+1) − (pi, pi)
= ||pi+1||2 − ||pi||2

from which (12) comes immediately.

(4) Equation (13) follows from the recursive application of (12):

||ps||2 = ||ps−1||2 + ||ps − ps−1||2

= ||ps−2||2 + ||ps−1 − ps−2||2 + ||ps − ps−1||2
· · ·= ||p0||2 + ||p1 − p0||2 + ||p2 − p1||2 + · · · + ||ps − ps−1||2.

Proof is completed

✷
Lemma 3.1 actually tells the fact that the “length” (norm) sequence
1 actually forms a monotonically increasing

1 of projection vector {pi}k

{||pi||}k

12

sequence, and obviously ||x|| is actually one of its upper bounds. In order
to ﬁnd out how fast this sequence is increasing, we need to ﬁgure out the
detailed information of each ||pi|| (i = 1, 2,· · · , k). The following conclusion
answers this question.

Lemma 3.2. Assume the same assumption in Lemma 3.1. Then pi+1 has
the following expression

pi+1 = αipi + A′

i+1u

u = ˜Ai+1(bi+1 − αiAi+1pi)
x′pi − (Ai+1pi)′ ˜Ai+1bi+1
˜Ai+1Ai+1pi
ipi − p′
p′

iA′

i+1

αi =

˜Ai+1 = (Ai+1A′

i+1)−1.

where u is

and

and

Furthermore

(14)

(15)

(16)

||pi+1||2 = αi||pi||2 + b′

i+1

˜Ai+1bi+1 − α2

i (Ai+1pi)′ ˜Ai+1(Ai+1pi)

(17)

Proof.
It is valid to express pi+1 in the form like (14) for some u ∈ Rmi+1 since
pi+1 ∈ ran(Wi+1), where mi is the number of rows in submatrix Ai.

Since pi+1 is the projection of x over subspace Wi+1, we have

which leads to

Ai+1(x − pi+1) = 0

from which comes (15).

bi+1 − αiAi+1pi − Ai+1A′

i+1u = 0

Similarly, by p′

i(x − pi+1) = 0 we have

x′pi − αip′

ipi + u′Ai+1pi = 0,

replacing u by (15) yields (16).

Finally from (14) we have

||pi+1||2 = (αipi + A′

= α2

i+1u)′(αipi + A′

i+1u)

i p′

ipi + 2αip′

iA′

i+1u + u′Ai+1A′

i+1u.

(18)

13

Since

and

2αip′

iA′

i+1u = 2αip′
= 2αip′

iA′
iA′

i+1

i+1

˜Ai+1(bi+1 − αiA′
˜Ai+1bi+1 − 2α2

i+1pi)

i (Ai+1pi)′ ˜Ai+1(Ai+1pi)

u′Ai+1A′

i+1u = (bi+1 − αiAi+1pi)′ ˜Ai+1(bi+1 − αiAi+1pi)

= b′

˜Ai+1bi+1 − 2αib′
i (Ai+1pi)′ ˜Ai+1(Ai+1pi),

i+1

i+1
+ α2

˜Ai+1Ai+1pi

(19)

(20)

equation (17) comes from (18) (19) (20) combined.

✷
Lemma 3.2 describes one way of constructing pi+1, and detailed informa-
tion about pi+1 is revealed by (17). However a more direct approach can be
used to evaluate the diﬀerence of the norms between two consecutive projec-
tions pi+1 and pi. These can be shown in the following conclusion.

Lemma 3.3. Assume the same assumption in lemma 3.1. Let I be the iden-
tity matrix in Rn. Then pi+1 has the following expression

pi+1 = pi + ¯A′

i+1v

and

||pi+1||2 − ||pi||2 = (bi+1 − (p′

ix)d)′( ¯Ai+1 ¯A′

i+1)−1(bi+1 − (p′

ix)d)

where ¯Ai+1 is a rank-one modiﬁcation of submatrix Ai+1 as

¯Ai+1 = Ai+1 − dp′

i = Ai+1(I − uiu′
i)

(21)

(22)

(23)

with ui = pi/||pi||, d ∈ Rmi+1 a vector taken as d = Ai+1pi+1/||pi||2 and v is
deﬁned as

v = ( ¯Ai+1 ¯A′
assuming the related inverse exists.

i+1)−1(bi+1 − (x′pi)d)

Proof.
Since pi+1 is the projection of x over subspace Wi+1 = ran([pi, A′
be constructed as follows.

i+1]), it can

First we modify row vectors in Ai+1 so that they are orthogonal to vector

pi, this can be depicted as a rank-one modiﬁcation to Ai+1 as

¯Ai+1 = Ai+1 − dp′
i,

14

where d can be obtained from the fact that
¯Ai+1pi = 0

which leads to

hence

and

Ai+1pi − dp′

ipi = 0,

d = Ai+1pi/(p′

ipi),

¯Ai+1 = Ai+1 − dp′

i = Ai+1 − Ai+1pip′

i/(p′

ipi) = Ai+1(I − uiu′
i),

where ui = pi/||pi||.

Next we calculate the projection vector ˜pi+1 of x over ran( ¯Ai+1) as

where v can be derived from the fact that

˜pi+1 = ¯A′

i+1v,

¯Ai+1(x − ˜pi+1) = 0,

which leads to

v = ( ¯Ai+1 ¯A′

i+1)−1(bi+1 − (p′

ix)d)

assuming ( ¯Ai+1 ¯A′

i+1)−1 exists.

Since ˜pi+1 = ¯Ai+1v is the projection of x over ran( ¯A′

i+1) and ¯Ai+1pi = 0,

we must have (pi, ˜pi+1) = 0. Therefore

||pi+1||2 − ||pi||2 = ||˜pi+1||2
= v′ ¯Ai+1 ¯A′
i+1v
ix)d)′( ¯Ai+1 ¯A′
= (bi+1 − (p′

i+1)−1(bi+1 − (p′

ix)d).

noting that matrix ( ¯Ai+1 ¯A′
metric).

i+1)−1 is symmetric (actually positive deﬁnite sym-
✷.
Remark : It can be shown that the length diﬀerence between pi+1 and pi

can also be written as

||pi+1||2 − ||pi||2 = ˜x′G˜x

(24)
where G = ( ¯Ai+1 ¯Ai+1)−1 and ˜x = ¯x− (x′u)u, where ¯x denotes the projection
of x on ran(A′) and (x′u)u is the projection of x (as well as ¯x) on the direction
of u = pi/||pi||.
matrix ( ¯Ai+1 ¯A′
essary conditions for these to hold true.

Note that in the above lemma, we need to assume the existence of each
i+1)−1. The following conclusion gives the suﬃcient and nec-

15

Lemma 3.4. Let A ∈ Rm×n(m ≤ n) and rank(A) = m, u ∈ Rn be a unit
vector in Rn. Let ¯A = A(I − uu′) and G = ¯A ¯A′, where I denote the identity
matrix in Rn. Then G is nonsingular if and only if u /∈ ran(A′).
Proof.
Note that G = ¯A ¯A′ is invertible if and only if ¯A is of full row rank.

(Necessity) Assume G is invertible, we need to show that u /∈ ran(A′). If
this is not the case, i.e., u ∈ ran(A′), then there is a v ∈ Rn ( v 6= 0) such
that u = A′v. Thus

¯A′v = (A(I − uu′))′v = (A − Auu′)′u = A′v − uu′A′v = u − u(u′u) = 0

since ||u|| = 1. This means ¯A is not of full rank, hence G is singular, a
contradiction with our assumption.
(Suﬃciency). Assume u /∈ ran(A′), we need to show that G is invertible.
As a matter of fact, if G is not invertible, then ¯A is not of full-row rank.
Therefore there exists a nonzero vector v ∈ Rm such that ¯A′v = 0. That
means

0 = (A(I − uu′))′v = A′v − uu′A′v = A′v − αu

where α = u′(A′v) is a scalar. It is easy to see from here that α 6= 0, otherwise
we would have A′v = 0 which means A is not of full row rank. Hence
u = A′v/α, i.e., u ∈ ran(A′), this is contradictory with the assumption. ✷
Lemma 3.5. Assume the same assumption in Lemma 3.1. Vector sequence
p0,p1,· · · , pk are produced in one AP process, then

||pi|| ≤ ||pi+1||

(i = 0, 1, 2,· · · , k)

(25)

and the equal sign holds if and only if

Ai+1pi = bi+1

Inequality (25) comes from (12) directly. We now prove the neces-

Proof.
sary condition for ||pi+1|| = ||pi||.
(Necessity) Note that if ||pi+1|| = ||pi|| holds , by (12) we must have pi+1 = pi.
Also from (14) we know that

pi+1 = αipi + A′

i+1u,

16

thus

Multiplying both sides of (26) by Ai+1 we have

A′
i+1u = pi+1 − αipi = (1 − αi)pi.

Ai+1A′

i+1u = (1 − αi)Api.

Note that from (15) we have

Ai+1A′

i+1u = bi+1 − αiApi.

Combining (27) and (28) yields

Api = bi+1.

(26)

(27)

(28)

(Suﬃciency)Now we prove pi+1 = pi under the assumption Api = bi+1.

As a matter of fact, in view of (15) and (16) we only need to show that

αi = 1

in this case.

Since (x − pi, pi) = 0, we have

x′pi = p′

ipi.

By using Ai+1pi = bi+1 we obtain

p′
iA′

i+1

˜Ai+1Ai+1pi = bi+1 ˜Ai+1bi+1

Hence from (16) we have

(29)

(30)

αi =

p′
ipi − b′
p′
ipi − b′
This completes the proof of the suﬃcient condition.

x′pi − (Ai+1pi)′ ˜Ai+1bi+1
˜Ai+1Ai+1pi
p′
ipi − p′

iA′

i+1

=

˜Ai+1bi+1
˜Ai+1bi+1

= 1

i+1

i+1

✷
Based on the above error analysis about AP process, a practical AP

algorithm can be implemented as follows.

Algorithm 3. (An accumulated projection method-AP) The following proce-
dure produces an approximate vector p to the solution vector x which satisﬁes
Ax = b.

17

Step 1. Divide matrix A into k blocks by its row vectors: A1, A2,· · · , Ak, di-
vide b correspondingly: b1, b2,· · · , bk(Note, Ai and Ai+1 may contains
some common row vectors).
Step 2. Initialize p0 as p0 = αA′b and c0 = α||b||2, where α = ||b||2/||A′b||2.
Step 3. For i = 1 to k

Step 3.1 Do QR factorization on submatrix Ai : A′
Step 3.2 Compute vector ˜bi = (R′
Step 3.3 Compute projection vector xi of x over ran(A′
Step 3.4 Store orthogonal matric Qi and vector ˜bi.
Step 3.5 Go to next i.

i)−1bi.

i = QiRi.

i): xi = Q′
i

˜bi.

Step 4. For i = 1 to k

Step 4.1 Compute projection vector ˜pi−1 of pi−1 on ran(A′

i):

˜pi−1 = Qi(Q′

ipi−1).

Step 4.2 Compute vector ¯pi−1 = pi−1 − ˜pi−1.
Step 4.3 Compute the projection vector ˜xi of x on direction ¯pi−1:

˜xi = β ¯pi−1, where β = (ci−1 − ˜b′

iQ′

ipi−1)/(¯p′

i−1 ¯pi−1).

Step 4.4 Compute the projection vector pi of x over ran([pi−1, A′

i]):

pi = xi + ˜xi

and the inner product between pi and x:
˜bi + β(ci−1 − ˜biQ′

ci = ˜b′
i

ipi−1)

Step 4.5 Go to next i.

Step 5. Output p(= pk) and c(= ck).

3.2. Convergence analysis of SAP

We now turn to the convergence of SAP (Algorithm 2). We ﬁrst have the

following conclusion.
Theorem 3.6. Let {xs}t
the SAP process before convergence reached. Then there exists

1 be any approximating vector sequence produced by

||x1|| < ||x2|| < · · · < ||xt|| < ||¯x||

(31)

18

Proof.
Let {ps
j}k
j=1 be the projection vector in the s-th AP iteration in SAP algo-
rithm. Then we have xi = pi
k (i = 1, 2,· · · ,∞). Since xi 6= ¯x for any i < t,
6= bj.
from Lemma 3.5 we can always ﬁnd some integer j such that Ajxi
Assuming j is the ﬁrst of this kind, then start from the s + 1-th AP iteration
in SAP algorithm, we have pi+1
l = xi (l = 1, 2,· · · , j − 1) and by Lemma 3.1
we have

Note that we always have ||pi+1
||xi|| < ||pi+1

j

j

||

j

||xi|| < ||pi+1
|| ≤ ||pi+1
|| ≤ ||xi+1||,

k

|| = ||xi+1||, thus
(i = 1, 2,· · · , t)

and since ¯x is the projection of x over ran(A′) while xi is the projection of
some subspace Wk of ran(A′), by Lemma 2.2 we always have ||xi|| < ||¯x|| for
any i (1 ≤ i ≤ t). These complete the proof.
✷
In order to prove the convergence of SAP method, we need the following
conclusion.
Lemma 3.7. Let {ys}∞
1 , and sup-
pose lims→∞ ys = y. Then y = ¯x, where ¯x is the projection of x satisfying
Ax = b.

1 be any convergent subsequence of {xs}∞

k

k

Proof.
Since ys = xts for some integer ts and xs = ps
we have
||ys+1||2 − ||ys||2 = ||pts+1
= ||pts+1
> ||pts+1
= ||pts+1
= ˜x′G˜x
0 || = ys/||ys|| and G = ( ¯A1 ¯A′

k (s = 1, 2,· · · ), by Lemma 3.1
||2 − ||pts
k ||2
||2 − ||pts+1
||2
1 − pts+1
||2
||2 − ||pts+1
||2

0 /||pts

1)−1 with

where ˜x = ¯x − x′u0u0, u0 = pts
¯A1 = A1(I − u0u′
0). i.e., we have

(32)
If y 6= ¯x, taking the limits on both sides of (32) for s approaching to inﬁnity
we have

||ys+1||2 − ||ys||2 > ˜x′G˜x

1

0

0

0

0 = lim

s→∞||ys+1||2 − ||ys||2 > lim

s→∞

˜x′G˜x > 0

since G is symmetric positive deﬁnite and ˜x 6= 0. This is a contradiction and
thus we must have y = ¯x.

✷

19

Theorem 3.8. Let {xs}∞
the SAP process. Then

1 be the approximating vector sequence produced by

lim
s→∞

xs = ¯x

where ¯x stands for the projection of x onto subspace ran(A′) of Rn. Partic-
ularly if m = n and A is nonsingular, then we have ¯x = x.

Proof.

1

From Theorem 3.6 we see that sequence {xs}∞

is a bounded
xs does not exist, then it has at least two diﬀerent cluster
sequence. If lim
s→∞
points z1 and z2 such that there are two subsequences of {xs}∞
1 approaches
to z1 and z2. However by Lemma 3.7 we have z1 = ¯x and z2 = ¯x. This is a
contradiction. Hence we must have

lim
s→∞

xs = ¯x.

Proof is completed.

✷

4. Some Acceleration Strategies

We have observed from the preceding section that the convergence speed
of the simple iterative algorithm may not be very satisfactory in general.
In this section we are to design some accelerative approaches for the SAP
algorithm.

4.1. Increase the Block Size

An apparent approach is to simply increase the size of each block. The
following table (Table 2) shows the iteration numbers needed for a convergent
solution when A = diag(−1, 2,−1) ∈ R100×100 and the tolerance is set at 10−5
for the relative residual error. One can see that the number of iterations may
drastically decrease when the size of blocks is slightly increased. Unlike
GMRES(m) with restarting where m stands for the inner iteration numbers
for each outer iteration and m has to be very small comparing to the size of
systems, this approach is viable since the size of each block can be selected
much larger ( in case of sparse systems, one can choose the size of each block
as large as O(105) when LGO-based QS decomposition method [11] is used
to orthogonalize the block submatrices.

20

Table 2: block-SAP– iteration numbers needed for convergence

block size

10

iter#

11404

15
2994

20
1020

25
443

30
222

35
104

40
57

50
27

4.2. A Modiﬁed SAP Approach

Another option for accelerating the convergence is to add one simple step
at the beginning of each loop in Algorithm 2 step 2. Speciﬁcally instead of
using xk as the initial approximate solution to start another AP process, we
ﬁrst get the projection vector p of x onto the subspace W = span{xk−1, xk}
as well as c = x′p and then use it to replace xk and ck in Algorithm 2. The
details come as follows.

Algorithm 4. (A Modiﬁed Stationary Accumulated Projection Method-MSAP
version 1). Let A ∈ Rn×n, b ∈ Rn. ǫ be a given tolerance. The following
procedure produces an approximation p to the solution x of system Ax = b.

Step1. Initialize s as s = 0, vector x0 as x0 = αA′b, c0 = αb′b, t = ||b −

Ax0||/||b|| where α = ||b||2/||A′b||2. Let p = x0 and c = c0.

Step2. While t > ǫ

Step1. Use Algorithm 3 to get a projection vector xs+1 of x and cs+1(=
x′xs+1) with p and c the initial projection vector and the corre-
sponding scalar taken as xs and cs respectively;

step 2.2 Calculate the projection p of x onto subspace W = span{xs, xs−1}

and scalar c = x′p. Rename p as xs+1 and cs+1 respectively;

step 2.3 Calculate t = ||b − Axs+1||/||b||;
step 2.4 s = s + 1;

Step3. Output p = (xs) and c = (cs).

The following table (Table 3) shows the needed iteration numbers when run-
ning the same example in subsection 4.1.
It is easy to see that this sim-
ple acceleration technique works very well (comparing with the numbers in
Table 2).

It is attempting to increase the dimension of the subspace W in Algorithm
4 step 2.2 to get a better convergence speed. Unfortunately this seems not
work since the “distance” between {xs}m
1 are not far enough and thus the

21

Table 3: MSAP–iteration numbers needed for convergence
50
block size
15

10
2134

15
403

20
134

iter#

25
69

30
38

35
34

40
18

submatrix formed by these vectors tends to be very ill-conditioned, which
eventually makes the idea not work well. The following subsection depicts
an alternative option for this idea.

4.3. A Varying Subspace Method

It is attractive to use a subspace W with larger dimension than that of
subspace W in Algorithm 4 step 2.2 to develop an accelerative method for
SAP. However we have noted that as iteration goes on, the matrix formed
by the successive SAP projection tends to be ill-conditioned. Hence we plan
to use a more ﬂexible strategy to handle the ill-conditioned systems. Our
intension is to use a detector to check the conditioning of an intermediate
matrix H, and then arrange the dimension of the subspace W accordingly.
The details are described in the following algorithm.

Algorithm 5. (A Modiﬁed Stationary Accumulated Projection Method-MSAP
version 2). Let A ∈ Rn×n, b ∈ Rn.
ǫ be a given tolerance. The fol-
lowing procedure produces an approximation p to the solution x of system
Ax = b.
Step1. Initialize s as s = 0, vector x0 as x0 = αA′b, c0 = αb′b, t = ||b −
Ax0||/||b|| where α = ||b||2/||A′b||2. Let p = x0 and c = c0, and m be
a small predetermined integer.

Step2. While t > ǫ

Step 2.1. Use Algorithm 3 to get a projection vector pn of x and
cn(= x′xs+1) with p and c as the initial projection vector and
the corresponding scalar.

Step 2.2. Store pn as a row vector in matrix H and cn into a column

vector L.

Step 2.3. If H contains m row vectors

Step 2.3.1 if H is well-conditioned,

Step 2.3.1.1 update xs+1 as the projection of x on subspace

W = ran(H) and the scalar cs+1 = x′xs+1

22

Step 2.3.1.2 remove the ﬁrst row vector of H and the ﬁrst

element in vector L correspondingly;

step 2.3.2 if H is ill-conditioned

Step 2.3.2.1 update xk+1 as the projection of x on sub-
space W = span(xs, pn) and the scalar cs+1 = x′xs+1.
Step 2.3.2.2 remove all but the ﬁrst row vectors of H and

the elements in vector L correspondingly.

step 2.4 if H contains less than m row vectors, update xs+1 as the
projection of x on subspace W = span(xs, pn) and the scalar
cs+1 = x′xs+1.

step 2.5 Calculate t = ||b − Axs||/||b||;
step 2.6 Set s = s + 1, p = xs+1 and c = cs+1.

Step3. Output p(= xs) and cs.

The following table (Table 4) shows the astonishing acceleration speed
of convergence when we use Algorithm 5 to solve the same problem in the
preceding subsection.

Table 4: MSAP2–iteration numbers needed for convergence
50
block size
7

15
102

10
185

30
16

35
14

iter#

20
42

25
30

40
10

5. Numerical Experiments

In this section we will show some application of the aforementioned SAP
methods and we compare the results with those produced by GMRES—a
benchmark Krylov subspace method. We use MSAP and GMRES to calcu-
late the systems.

As the ﬁrst example, we use the two-point boundary value problem

(cid:26) (a(t)u′(t))′ + b(t)u(t) = f (t),

u(0) = u(1) = 0.

t ∈ (0, 1)

(33)

This equation represents some important practical problems such as chord
balancing, elastic beam problems, etc. We use ﬁnite element method to get

23

the numerical solution to the system, which ends up with a linear system of
equations in the form of (1) with n unknowns, where n stands for the number
of grids which divide the interval (0, 1) into n + 1 equal-sized subintervals
(xi, xi+1) (i = 1, 2,· · · n). We use the linear interpolation function at each
grid point to construct the ﬁnite element space V h
0 , test functions are also
from V h
0 .

In our test we take a(t) = 1+t, b(t) = t and f (t) is taken so that the exact
solution to the system is u(t) = t(1 − t)e2+t. By using the aforementioned
ﬁnite element space V h
0 , we get a linear system of equation Ax = b with A as
a symmetric tridiagonal matrix in Rn×n, where n is taken as 200. The block
size for the MSAP method (version 2) is set to be the square root of the
restart number m of GMRES(m) multiplied by the number of unknowns n
so that submatrices of A have roughly the same number of non-zero elements
as those in Krylov subspace matrices formed in GMRES process.

Table 5 shows the comparison of iteration numbers needed for conver-
gence, the relative error in terms of ||x − approx.x||/||x|| (where approx.x
stands for the approximate solutions obtained by using GMRES and MSAP2
respectively) and the relative residual error in terms of ||b − A ∗ approx.x||/||b||
is used for the convergence criteria. We also observed that the time cost in
these example also show some advantage of MSAP over GMRES as shown
in Table 5.

Table 5: Comparison between MSAP and GMRES

settings

iter. #

time(in s)

rel. error

msap
blk size

20
30
40
50
60
70
80

gmres msap
restart m=5
200
200
50
33
22
17
13

2
5
8
13
18
25
32

gmres
(out,in)
(2000,2)
(2000,5)
(1415,7 )
(538,3 )
(282,18)
(148,24 )
(91,28 )

msap

gmres msap

gmres

1.2690
0.6340
0.1320
0.0800
0.0510
0.0380
0.0380

0.6700
1.6430
1.8180
1.3780
1.1880
1.1750
0.9950

7.02e-7
3.49e-7
9.57e-7
3.32e-7
2.41e-7
5.06e-7
3.01e-8

6.96e-5
5.68e-5
4.92e-5
4.4e-5
4.01e-5
3.71e-5
3.48e-5

From the above table it seems that MSAP has a better relative error level
than that of GMRES at the same relative residual level. We have to point out

24

here that the construction of this test system is made so that the solution has
rich eigenvector components corresponding to the smallest eigenvalues of the
coeﬃcient matrix A. In case the condition number cond(A) is relatively small
(say, less than O(103)), GMRES outperforms MSAP in terms of time costs
and ﬂops in our tests, while in case the condition number of the coeﬃcient
matrices are larger than O(103), MSAP generally outperforms GMRES in
most of our test cases.

Table 7 is a comparison between MSAP and block Jacobi method for a
system with coeﬃcient matrix A ∈ R200×200. The block size for both methods
are chosen as exactly the same. One can see a much less iteration number
needed for MSAP than that of block Jacobi method in each case of block
size, while the relative errors obtained by MSAP are much better than those
obtained by block Jacobi method.

blk size

time(in s)

Table 6:

iteration numbers needed for convergence, tol =10−5
rel. error

iteration#

Jacobi msap
5.666
12.166
3.68
2.769
1.489
1.547
1.018
0.587
0.379
0.781
0.222
0.589
0.446
0.134
0.102
0.408

Jacobi msap
7836
1745
830
5347
390
4082
3316
185
130
2806
85
2440
2159
55
45
1946

Jacobi

6.9553e-005
5.6761e-005
4.921e-005
4.3997e-005
4.0135e-005
3.7142e-005
3.4765e-005
3.2554e-005

msap

7.0191e-007
3.4931e-007
9.5735e-007
3.3189e-007
2.4148e-007
5.0644e-007
3.0083e-008
3.132e-008

10
15
20
25
30
35
40
45

6. Comments and Summary

In this paper we present a new type of iterative methods for solving linear
system of equations. This might be the ﬁrst type of methods which do not
belong to the category of extended Krylov subspace methods as we mentioned
above.
It can overcome some shortcomings of Krylov subspace methods
and exhibit better performance in our test problems. We need to mention
that convergence speed of these algorithms deteriorates when the number of
subdividing blocks of the coeﬃcient matrix exceeds 20, a remedy to this is

25

to embed an inner loop in the AP process, which will cause more ﬂops but
the obtained time eﬃciency payoﬀ this costs in our tests. A relevant issue is
the study of detailed error analysis for the SAP algorithm which may leads
to a deep insight error estimation for each AP process in terms of subspace
distance. We need to mention here that the SAP algorithm is nothing but a
“horizontal” application of the AP process, i.e., the AP processes are always
applied to the original linear system instead of residual equations. We ﬁnd
that a vertical application of AP process is also possible and the results are
to appear in our later work.

References

[1] O. Axelsson. A survey of preconditioned iterative methods for linear

systems of equationns. BIT, 25:166–187, 1985.

[2] O. Axelsson. Iterative Solution Methods. Cambridge University Press,

1994.

[3] R. Barrett, M. Berry, T. F. Chan, J. Demmel, J. Donato, J. Dongarra,
V. Eijkhout, R. Pozo, C. Romine, and H. Van der Vorst. Templates for
the Solution of Linear Systems: Building Blocks for Iterative Methods,
2nd Edition. SIAM, Philadelphia, PA, 1994.

[4] M. Benzi. Preconditioning techniques for large linear systems: A survey.

Journal of Computational Physics, 182:418–477, 2002.

[5] R. Bramley and A. Sameh. Row projection methods for large nonsym-

metric linear systems. SIAM J. on Scientiﬁc Computing, 13(1), 1992.

[6] I.S. Duﬀ. Sparse numerical linear algebra: direct methods and precon-
ditioning. In I.S. Duﬀ and G.A. Watson, editors, The State of the Art
in Numerical Analysis, pages 27–62. Oxford University Press, 1997.

[7] G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns

Hopkins University Press, Baltimore and London, 1996.

[8] W. Hackbusch. Multi-Grid Methods and Applications. Springer-Verlag,

Berlin, 1985.

[9] W. Hackbusch. Iterative Solution of Large Sparse Systems of Equations.

Springer-Verlag, New York, 1994.

26

[10] W. Peng. A lgo-based elimination solver for large scale linear system of
equations. Numerical Mathematics– A Journal of Chinese Universities,
(in press).

[11] W. Peng and B. N. Datta. A sparse qs-decomposition for large sparse
linear system of equations.
In Y. Huang et al., editor, Domain De-
composition Methods in Science and Engineering XIX, Lecture Notes
in Computational Science and Engineering, volume 78, pages 431–438.
Spring-Verlag, 2011.

[12] W. Peng and Z. Wang. A line projection method for solving linear system
of equations. Paciﬁc Journal of Applied Mathematics, 5(1):17–28, 2013.

[13] Y. Saad. Iterative methods for sparse linear systems (2nd ed.). SIAM.,

2003.

[14] R.S Varga. Matrix Iterative Analysis. Prentice-Hall, Englewood Cliﬀs,

NJ, 1962.

[15] Henk A. Van Der Vorst.

Iterative Krylov Methods for Large Linear

Systems. Cambridge University Press, 2003.

[16] P. Wesseling. An Introduction To Multigrid Methods. John Wiley &

Sons, 1992.

[17] D. M. Young. Iterative Solution of Large Linear Systems. Academic

Press, New York, 1971.

27

SAP solutions at different iterations with A= diag(−1,2,−1)_n, n=100 

 

x
sap50
sap100
sap150
sap1000

0.2

0.4

0.6

0.8

1

10

8

6

4

2

0

−2

 
0

