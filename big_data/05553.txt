6
1
0
2

 
r
a

 

M
7
1
 
 
]

.

O
C
h
t
a
m

[
 
 

1
v
3
5
5
5
0

.

3
0
6
1
:
v
i
X
r
a

Non-backtracking random walks and a weighted Ihara’s theorem

Mark Kempton∗

Abstract

We study the mixing rate of non-backtracking random walks on graphs by looking at non-backtracking
walks as walks on the directed edges of a graph. A result known as Ihara’s Theorem relates the adjacency
matrix of a graph to a matrix related to non-backtracking walks on the directed edges. We prove a
weighted version of Ihara’s Theorem which relates the transition probability matrix of a non-backtracking
walk to the transition matrix for the usual random walk. This allows us to determine the spectrum of
the transition probability matrix of a non-backtracking random walk in the case of regular graphs and
biregular graphs. As a corollary, we obtain a result of Alon et. al.
in [1] that in most cases, a non-
backtracking random walk on a regular graph has a faster mixing rate than the usual random walk. In
addition, we obtain an analogous result for biregular graphs.

1

Introduction

A random walk on a graph G is a random process on the vertices of G in which, at each step in the walk, we
choose uniformly at random among the neighbors of the current vertex. Random walks have been studied
extensively, and are used in a variety of algorithms involving graphs. For a comprehensive survey on random
walks on graphs, see [13], and for applications of spectral techniques to random walk theory, see [5]. Random
walks on graphs have the useful property that given any initial distribution on the vertex set, the random
walk converges to a unique stationary distribution as long as the graph is connected and not bipartite. The
speed at which this convergence takes place is referred to as the mixing rate of the random walk. In a graph
where a random walk has a fast mixing rate, vertices can be sampled quickly using this random process,
making this a useful tool in theoretical computer science.

A non-backtracking random walk on a graph is a random walk with the added condition that, on a
given step, we are not allowed to return to the vertex visited on the previous step. Viewed as a walk on
vertices, a non-backtracking random walk loses the property of being a Markov chain, making its analysis
somewhat more diﬃcult. However, their study has received increased interest in recent years. Recently,
Angel, Friedman, and Hoory [2] studied non-backtracking walks on the universal cover of a graph. Fitzner
and Hofstad [7] studied the convergence of non-backtracking random walks on lattices and tori. Krzakala
et. al.
[11] use a matrix related to non-backtracking walks to study spectral clustering algorithms. Most
pertinent to the current paper, Alon, Benjamini, Lubetzky, and Sodin [1] studied the mixing rate of a
non-backtracking walk for regular graphs. In particular, they prove that in most cases, a non-backtracking
random walk on a regular graph has a faster mixing rate than a random walk allowing backtracking.

In this paper, we study the mixing rate for a non-backtracking random walk, with the goal of removing
the condition of regularity needed in the results of Alon et. al. in [1]. We take a diﬀerent approach than Alon
et. al. by looking at the non-backtracking walk as a walk along directed edges of a graph, as is done in [2].
This allows us to turn the non-backtracking random walk into a Markov chain, but on a larger state space,
which in turn allows us to determine the stationary distribution to which a non-backtracking walk converges
for a general graph, whether or not it is regular. In the case of regular graphs, our approach allows us to
compute the spectrum of the transition probability matrix for a non-backtracking random walk, expressed
in terms of the eigenvalues of the adjacency matrix. This allows for easy comparison of the mixing rates of
a non-backtracking random walk, and an ordinary random walk. As a corollary, this gives us an alternate
proof of the result in [1] for regular graphs. Our approach gives more information than the approach in [1],

∗Center of Mathematical Sciences and Applications, Harvard University, Cambridge, MA. Email: mkemp-

ton@cmsa.fas.harvard.edu

1

in that the full spectrum of the transition probability matrix is given. In addition, we are able to compute
the spectrum of the non-backtracking transition probability matrix for biregular graphs. As a corollary, we
generalize the result in [1] for regular graphs to an analogous result for biregular graphs.

A key component in our proof is a weighted version of a result known as Ihara’s Theorem, also called
the Ihara zeta identity, which relates an operator indexed by the directed edge set of a graph to an operator
indexed by the vertex set of the graph. Ihara’s Theorem was ﬁrst considered in the study of number theoretic
zeta functions on graphs, and was ﬁrst proved for regular graphs by Ihara in 1966 (see [9]). Numerous other
proofs have been given since, along with generalizations to irregular graphs, by Hashimoto ([8], 1989), Bass
([3], 1992), Stark and Terras ([15], 1996), Kotani and Sunada ([10], 2000), and others. We will give an
elementary proof of Ihara’s Theorem that, to our knowledge, is original. In addition, we follow ideas similar
to those in [10] to obtain a version of Ihara’s Theorem with weights that allows us to study the relevant
transition probability matrices for random walks.

The remainder of this paper is organized as follows. In section 2, we give the necessary background and
preliminary information on random walks, and develop the corresponding theory for non-backtracking walks,
including the convergence of a non-backtracking walk to a stationary distribution for a general graph. We
accomplish this via walks on the directed edges of a graph. We also investigate bounds obtained from the
normalized Laplacian for a directed graph. We also give the relevant background on Ihara’s Theorem, and
a new elementary proof. In section 3, we prove our weighted version of Ihara’s formula. Finally, in section
4, we use this formula to obtain the spectrum of the transition probability matrix for a non-backtracking
random walk for regular and biregular graphs. This gives a new proof of the result of Alon et. al. concerning
the mixing rate of a non-backtracking random walk on a regular graph, and generalizes this result to the
class of biregular graphs.

2 Preliminaries

2.1 Random walks

Throughout this paper, we will let G = (V, E) denote a graph with vertex set V and (undirected) edge set
E, and we will let n = |V | and m = |E|. A random walk on a graph is a sequence (v0, v1, ..., vk) of vertices
vi ∈ V where vi is chosen uniformly at random among the neighbors of vi−1. Random walks on graphs are
well-studied, and considerable literature exists about them. See in particular [5] and [13] for good surveys,
especially in the use of spectral techniques in studying random walks on graphs.

The adjacency matrix A of G is the n × n matrix with rows and columns indexed by V given by

A(u, v) =(1

0

if u ∼ v
otherwise.

It is a well-known fact that the (u, v) entry of Ak is the number of walks of length k starting at vertex u and
ending at vertex v. Deﬁne D to be the n × n diagonal matrix with rows and columns indexed by V with
D(v, v) = dv, where dv denotes the degree of vertex v. A random walk on a graph G is a Markov process
with transition probability matrix P = D−1A, so

P (u, v) =( 1

du
0

if u ∼ v
otherwise.

Given any starting probability distribution f0 on the vertex set V , the resulting expected distribution fk
after applying k random walk steps is given by fk = f0P k. Here we are considering f0 and fk as row vectors
in Rn.

Note that, in general, P is not symmetric for an irregular graph, but is similar to the symmetric matrix
D−1/2AD−1/2. Thus, the eigenvalues of P are real, and if we order them as µ1 ≥ µ2 ≥ ··· ≥ µn, then it is
easy to see that µ1 = 1 with eigenvector 1, and µn ≥ −1. By Perron-Frobenius theory, if the matrix P is
irreducible, then we have that µ2 < 1, and if P is aperiodic, then µn > −1. The matrix P being irreducible
and aperiodic corresponds to the graph G being connected and non-bipartite.

2

The stationary distribution for a random walk on G is given by

π(v) =

dv

vol(G)

.

The stationary distribution has the important property the πP = π, so that a random walk with initial
distribution π will stay at π at each step. An important fact about the stationary distribution is that if G
is a connected graph that is not bipartite, then for any initial distribution f0 on V (G), we have

(f0P t)(v) = π(v)

lim
t→∞

for all v (see [13]).

Knowing that a random walk will converge to some stationary distribution, a fundamental question to
consider is to determine how quickly the random walk approaches the stationary distribution, or in other
words, to determine the mixing rate. In order to make this question precise, we need to consider how to
measure the distance between two distribution vectors.

Several measures for deﬁning the mixing rate of a random walk have been given (see [5]). Classically, the

mixing rate is deﬁned in terms of the pointwise distance (see [13]). That is, the mixing rate is

max

u,v (cid:12)(cid:12)P t(u, v) − π(v)(cid:12)(cid:12)

1/t

.

ρ = lim sup
t→∞

Note that a small mixing rate corresponds to fast mixing. Alternatively, the mixing rate can be considered
in terms of the standard L2 (Euclidean) norm, the relative pointwise distance, the total variation distance, or
the χ-squared distance. In general, these measures can yield diﬀerent distances, but spectral bounds on the
mixing rate are essentially the same for each. See [5] for a detailed comparison of each. For our purposes,
we will primarily be concerned with the χ-squared distance, which will be deﬁned below.

The mixing rate of a random walk is directly related to the eigenvalues of P .

Theorem 1 (Corollary 5.2 of [13]). Let G be a connected non-bipartite graph with transition probability
matrix P , and let the eigenvalues of P be 1 = µ1 > µ2 ≥ ··· ≥ µn > −1. Then the mixing rate is
max{µ2,|µn|}.

Thus, the smaller the eigenvalues of P , the faster the random walk converges to its stationary distribution.

2.2 Non-backtracking random walks
A non-backtracking random walk on G is a sequence (v0, v1, ..., vk) of vertices vi ∈ V where vi+1 is chosen
In other words, a non-
randomly among the neighbors of vi such that vi+1 6= vi−1 for i = 1, ..., k − 1.
backtracking random walk is a random walk in which a step is not allowed to go back to the immediately
previous state. A non-backtracking random walk on a graph is not a Markov chain since, in any given state,
we need to remember the previous step in order to take the next step. In order for this to be well-deﬁned,
we assume throughout the remainder of the paper that the minimim degree of G is at least 2.

Deﬁne P (k) to be the n × n transition probability matrix for a k-step non-backtracking random walk on
the vertices. That is P (k)(u, v) is the probability that a non-backtracking random walk starting at vertex
u ends up at vertex v after k steps. Note that P (1) = P , where P = D−1A is the transition matrix for an
ordinary random walk on G. However, P (k) is not simply P k since a non-backtracking random walk is not
a Markov chain.

This process can be turned into a Markov chain, however, by changing the state space from the vertices
of the graph to the directed edges of the graph. That is, replace each edge in E with two directed edges (one
in each direction). Then the non-backtracking random walk is a sequence of directed edges (e1, e2,··· , ek)
where if ei = (vj, vk), and ei+1 = (vr, vs) then vk = vr and vs 6= vj. That is, the non-backtracking condition
restricts the walk from moving from an edge to the edge going in the opposite direction. Denote the set of
directed edges by −→E . The transition probability matrix for this process we will call ˜P . Observe that

˜P ((u, v), (x, y)) =( 1

dv−1
0

if v = x and y 6= u
otherwise.

3

Note that ˜P is a 2m × 2m matrix. Note also that ˜P k is the transition matrix for a walk with k steps on the
directed edges.

Lemma 1. Given any graph G, the matrix ˜P as deﬁned above is doubly stochastic.

Proof. Observe ﬁrst that the rows of the matrix ˜P sum to 1, as it is a transition probability matrix. In
addition, the columns of ˜P sum to 1. To see this, consider the column indexed by the directed edge (u, v).
The entry of this column corresponding to the row indexed by (x, y) is
dy−1 if y = u and if v 6= x. Since
y = u this is equal to

du−1 . Otherwise, the entry is 0. Thus the column sum is

1

1

as claimed.

Deﬁne the distribution ˜π : −→E → R by

1

du − 1

=

du − 1
du − 1

= 1

Xx∼u

x6=v

˜π =

1

vol(G)

where 1 is the vector of length 2m with each entry equal to 1.

Lemma 2. Let ˜f0 : −→E → R be any distribution on the directed edges of G. If the matrix ˜P is irreducible

and aperiodic, then

˜f0 ˜P k −→ ˜π

as k → ∞.
Proof. It follows from Lemma 1 that ˜π is a stationary distribution for ˜P . This follows because, since the
columns of ˜P sum to 1, we have

˜π ˜P = ˜π.

Therefore, if the sequence ˜f0 ˜P k converges, it must converge to ˜π. Now, ˜P being irreducible and aperiodic
are precisely the conditions for this to converge.

Let f be a probability distribution on the vertices of G. Then f can be turned into a distribution ˜f on

−→E as follows. Deﬁne

˜f ((u, v)) =

1
du

f (u).

Conversely, given a distribution ˜g on −→E , deﬁne a distribution g on the vertices by

g(u) = X(u,v)∈−→E

˜g(u, v).

Thus, given any starting distribution f0 : V → R on the vertex set of G, we can compute the distribution
after k non-backtracking random walk steps fk : V → R as follows. First compute the distribution ˜f0 on the
˜fk(u, v). The following
directed edges as above, then compute ˜fk = ˜f0 ˜P k, then fk is given by fk(u) =Pv∼u
proposition tells us that this converges to the same stationary distribution as an ordinary random walk on
a graph.
Theorem 2. Given a graph G and a starting distribution f0 : V → R on the vertices of G, deﬁne fk = f0P (k)
to be the distribution on the vertices after k non-backtracking random walk steps. Deﬁne the distribution
π : V → R by π(v) = dv
vol(G) (note that this is the stationary distribution for an ordinary random walk on G).
Then if the matrix ˜P is irreducible and aperiodic, then for any starting distribution f0 on V , we have

fk −→ π as k → ∞.

4

Proof. As described above, take the distribution f0 on vertices to the corresponding distribution ˜f0 on
directed edges. Then deﬁne ˜fk = ˜f0 ˜P k. Then by Lemma 2, ˜fk converges to ˜π. Now ˜π = 1
vol G , and observe
that

π(u) =

du

vol(G)

1

vol(G)

= Xv∼u

= Xv∼u

˜π((u, v)).

So pulling the distribution ˜π on directed edges back to a distribution on the vertices yields π. Thus the
result follows.

Deﬁnition 1. The χ-squared distance for measuring convergence of a random walk is deﬁned by

∆′(t) = max

y∈V (G)

 Xx∈V (G)

( ˜P t(y, x) − ˜π(x))2

˜π(x)

.

1/2




Notice that since ˜π = 1/ vol(G),

∆′(t)2 = max

y

= max

y

1
2mk(χy ˜P t − ˜π)k2
1
2mk(χy − ˜π) ˜P tk2

Theorem 3. Let µ1 = 1, µ2,··· , µ2m be the eigenvalues of ˜P . Then the convergence rate for the non-
backtracking random walk with respect to the χ-squared distance is bounded above by maxi6=1 |µi|.
Proof. We have

∆′(t)2 = max

y

1
2mk(χy − ˜π) ˜P tk2.

Observe that χu − ˜π is orthogonal ˜π, which is the eigenvector for µ1, so we see that

Therefore,

∆′(t) ≤

1
2m

max

i6=1 |µi|t.

lim
t→∞

(∆′(t))1/t ≤ max

i6=1 |µi|.

2.3 Non-backtracking Walks as Walks on a Directed Graph
The transition probability matrix ˜P for the walk on directed edges can be thought of as a transition matrix
for a random walk on a directed line graph of the graph G. In this way, theory for random walks on directed
graphs can be applied to analyze non-backtracking random walks. Random walks on directed graphs have
been studied by Chung in [4] by way of a directed version of the normalized graph Laplacian matrix. In
[4], the Laplacian for a directed graph is deﬁned as follows. Let P be the transition probability matrix
for a random walk on the directed graph, and let φ be its Perron vector, that is, φP = φ. Then let Φ be
the diagonal matrix with the entries of φ along the diagonal. Then the Laplacian for the directed graph is
deﬁned as

L = I −

Φ1/2P Φ−1/2 + Φ−1/2P ∗Φ1/2

.

2

This produces a symmetric matrix that thus has real eigenvalues. Those eigenvalues are then related to the
convergence rate of a random walk on the directed graph. In particular, the convergence rate is bounded
above by 2λ−1

1 (− log minx φ(x)), where λ1 is the second smallest eigenvalue of L (see Theorem 7 of [4]).

Applying this now to non-backtracking random walks, deﬁne ˜P as before. Then as seen above, φ is the
constant vector with φ(v) = 1/ vol(G) for all v. Then the directed Laplacian for a non-backtracking walk
becomes

˜P + ˜P ∗

.

2

˜L = I2m =

5

Then Theorem 1 of [4], applied to the matrix ˜L as deﬁned, gives the Rayleigh quotient for a function
f : −→E → C by

˜R(f ) =

f∗ ˜Lf
f∗f

=

1
2

P(u,v)∈−→E (G) P(v,w)

w6=u

(f (u, v) − f (v, w))2 ˜P ((u, v), (v, w))
P(u,v)∈−→E (G)

f (u, v)2

.

From this it is clear that ˜L is positive semideﬁnite with smallest eigenvalue λ0 = 0.
If 0 = λ0 ≤ λ1 ≤
··· ≤ λ2m−1 are the eigenvalues of ˜L, then Theorem 7 from [4] implies that the convergence rate for the
corresponding random walk is bounded above by

2 log vol(G)

λ1

.

We remark that for an ordinary random walk on an undirected graph G, the convergence rate is also on
the order of 1/λ1(L), where L now denotes the normalized Laplacian of the undirected graph G. Note that

where R(f ) = Puv∈E(G)(f (u) − f (v))2

Pv∈V (G) f (v)2dv

λ1(L) =

R(f )

inf

f :V (G)→R

f⊥D1

denotes the Rayleigh quotient with respect to L, and

λ1( ˜L) =

˜R(f )

inf

f :−→E (G)→R

f⊥1

with ˜R given above.

The following result shows that the Laplacian bound does not give an improvement for non-backtracking

random walks over ordinary random walks.
Proposition 1. Let G be any graph, and let L be the normalized graph Laplacian and ˜L the non-backtracking
Laplacian deﬁned above. Then we have

λ1( ˜L) ≤ λ1(L).

Proof. Let f : V (G) → R be the function orthogonal to D1 that achieves the minimum in the Rayleigh
quotient for L. So

Xv∈V (G)

f (v)dv = 0 and λ1(L) = Puv∈E(G)(f (u) − f (v))2

.

Pv∈V (G) f (v)2dv

Deﬁne f′ : −→E → R by f′(u, v) = f (u). Observe that

X(u,v)∈−→E (G)

f′(u, v) = X(u,v)∈−→E (G)

f (u) = Xu∈V (G)

f (u)du = 0.

6

So f′ is orthogonal to 1. Therefore

λ1( ˜L) ≤ ˜R(f′) =

=

1
2

1
2

P(u,v)∈−→E (G) P(v,w)

w6=u

P(u,v) P(v,w)

w6=u

f′(u, v)2

(f′(u, v) − f′(v, w))2 ˜P ((u, v), (v, w))
P(u,v)∈−→E (G)
(f (u) − f (v))2
P(u,v)

f (u)2

dv−1

1

1

=

(f (u) − f (v))2
2 P(u,v)
f (u)2du
Pu∈V (G)
= P{u,v}∈E(G)
Pu∈V (G)

(f (u) − f (v))2
f (u)2du

= R(f ) = λ1(L).

Ihara’s Theorem

2.4
The transition probability matrix ˜P deﬁned above is a weighted version of an important matrix that comes
up in the study of zeta functions on ﬁnite graphs. We deﬁne B to be the 2m × 2m matrix with rows and
columns indexed by the set of directed edges of G as follows.

B((u, v), (x, y)) =(1

0

if v = x and y 6= u
otherwise.

The matrix B can be thought of as a non-backtracking edge adjacency matrix, and the entries of Bk
describe the number of non-backtracking walks of length k from one directed edge to another, in the same
way that the entries of powers of the adjacency matrix, Ak, count the number of walks of length k from one
vertex to another. The expression det(I − uB) is closely related to zeta functions on ﬁnite graphs which. A
result known as Ihara’s Theorem further relates such zeta functions to a determinant expression involving
the adjacency matrix. While we will not go into zeta functions on ﬁnite graphs in this paper, the following
result equivalent to Ihara’s theorem will be of interest to us.

Ihara’s Theorem. For a graph G on n vertices and m edges, let B be the matrix deﬁned above, let A denote
the adjacency matrix, D the diagonal degree matrix, and I the identity. Then

det(I − uB) = (1 − u2)m−n det(I − uA + u2(D − I)).

We remark that the expression det(I − uB) is the characteristic polynomial of B evaluated at 1/u. In
this way the complete spectrum of the matrix B is given by the reciprocals of the roots of the polynomial
(1 − u2)m−n det(I − uA + u2(D − I)). Numerous proofs of this result exist in the literature [9, 8, 3, 15, 10].
For completeness, we will include here an elementary proof that uses only basic linear algebra. To the
knowledge of the author, this proof is original. To begin, we will need a lemma giving a well-known property
of determinants.

Lemma 3. Let M be a k × l matrix, N a l × k matrix, and A an invertible k × k matrix. Then

det(A + M N ) = det(A) det(I + N A−1M ).

7

Proof. Note that

(cid:20)I N
I(cid:21)(cid:20)

0

I

A−1M A−1(A + M N )(cid:21)(cid:20)I −N

I (cid:21) =(cid:20)I + N A−1M 0
I(cid:21) .

A−1M

0

0

Taking determinants of both sides gives the result.

Proof of Ihara’s Theorem. Deﬁne S to be the 2m × n matrix

S((u, v), x) =(1

0

if v = x
otherwise

so S is the endpoint incidence operator. Deﬁne T to be the n × 2m matrix given by

T (x, (u, v)) =(1

0

if u = x
otherwise

so T is the starting point incidence operator. We will also deﬁne τ to be the 2m × 2m matrix giving the
reversal operator that switches a directed edge with its opposite. That is,

τ ((a, b), (c, d)) =(1

0

if b = c, a = d
otherwise

Now, a straightforward computation veriﬁes that

and

Then from Lemma 3 and (1) we obtain

B = ST − τ,

A = T S,

D = T τ S.

det(I − uB) = det(I − u(ST − τ ))
= det(I + uτ − uST )
= det(I + uτ ) det(I − uT (I + uτ )−1S)

where u is chosen so that the matrix I + uτ is inverivle.

(1)

(2)

(3)

Observe that τ 2 = I, so that (I − uτ )(I + uτ ) = (1− u2)I, so (I + uτ )−1 = 1

1−u2 (I − uτ ). Thus, applying

(2) and (3), the above becomes

det(I − uB) = det(I + uτ ) det(I −
= det(I + uτ ) det(I −
= det(I + uτ )

1

u

u

1 − u2 T (I − uτ )S)
1 − u2 (T S − uT τ S))

(1 − u2)n det((1 − u2)I − uA + u2D)

= (1 − u2)m−n det(I − uA + u2(D − I))

where the last step is obtained by observing that det(I + uτ ) = (1 − u2)m. This is the desired equality for
our choice of u. This is a polynomial of ﬁnite degree in u, and there are inﬁnitely many u that make I + uτ
invertible, so the equality holds for all u.

8

3 A weighted Ihara’s theorem

In this section, we will give a weighted version of Iharra’s Theorem. The proof presented in the previous
section does not lend itself well to generalization to the weighted setting, so we will not follow that strategy.
Rather, we will follow the main ideas of the proof of Ihara’s theorem found in [10] to obtain our weighted
version of this result.

To each vertex x ∈ V (G) we assign a weight w(x) 6= 0, and let W be the n × n diagonal matrix given
by W (x, x) = w(x). Deﬁne S and T to be the matrices from the proof of Ihara’s Theorem in the previous
section, and deﬁne ˜S = SW and ˜T = W T . So ˜S is the weighted version of the endpoint vertex-edge incidence
operator, and ˜T is the weighted version of the starting point vertex-edge incidence operator. Deﬁne τ from
the proof of Ihara’s Theorem, and deﬁne ˜τ to be the weighted version of τ , that is

˜τ ((a, b), (c, d)) =(w(b)2

0

if b = c, a = d
otherwise

Finally, deﬁne the 2m × 2m matrix ˜P by

˜P ((a, b), (c, d)) =(w(b)2

0

if b = c, a 6= d
otherwise.

(4)

Then ˜P is the weighted version of the non-backtracking edge adjacency matrix B seen above in Ihara’s
theorem, with w(b)2 the weight on edge (a, b). We remark that if we take w(x) = 1/√dx − 1 for each
x ∈ V (G), then ˜P is exactly the transition probability matrix for a non-backtracking random walk on the
directed edges of G deﬁned in Section 2.2. This case is our primary focus, but we note that our computations
apply for any arbitrary positive weights assigned to the vertices.

Now, a straightforward computation veriﬁes that

˜P = ˜S ˜T − ˜τ

(5)

and

(6)
We will deﬁne ˜A = W AW . Note that ˜A(u, v) = w(u)w(v), so this is the adjacency matrix for the weighted
graph with edge weights w(u)w(v). The matrix ˜A is similar to W 2A, so when w(x) = 1/√dx − 1, this is the

matrix whose entries are the transition probabilities for a single step of a non-backtracking random walk G.

˜T ˜S = W AW.

From (5) and (6) we obtain the following equations.

(I − u ˜P )(I − u˜τ ) = I − u ˜S ˜T + u2 ˜S ˜T ˜τ − u2˜τ 2
(I − u˜τ )(I − u ˜P ) = I − u ˜S ˜T + u2˜τ ˜S ˜T − u2˜τ 2

(7)
(8)

We deﬁne ˜D to be the diagonal n × n matrix ˜D(x, x) = Pv∼x w(x)2w(v)2 and observe that ˜T ˜τ ˜S = ˜D.

It then follows that

(cid:16)(I − u ˜P )(I − u˜τ ) + u2˜τ 2(cid:17) ˜S = ˜S(cid:16)I − u ˜A + u2 ˜D(cid:17)
˜T (cid:16)(I − u˜τ )(I − u ˜P ) + u2˜τ 2(cid:17) =(cid:16)I − u ˜A + u2 ˜D(cid:17) ˜T

(9)

(10)

We remark that in the proof in [10], they use the unweighted versions of each of these matrices, so τ rather
than ˜τ yields τ 2 = I. Hence S and T will factor through τ 2, so that the u2τ 2 term stays on the right hand side
of the above equations. Here we have ˜τ 2 is a 2m × 2m diagonal matrix with ˜τ 2((u, v), (u, v)) = w(u)2w(v)2.
Depending on the w(u)’s this matrix might not behave nicely with respect to the action of S and T , hence
the extra terms that need to stay on the left-hand side above. This diﬀerence from [10] is one of the primary
diﬃculties in generalizing this result.

We will now perform a change of basis to see how the operator (I − u ˜P )(I − u˜τ ) + u2 ˜τ 2 behaves with
respect to the decomposition of the space of functions f : −→E → C as the direct sum of Image ˜S and Ker ˜ST .

9

To this end, ﬁx any basis of the subspace Ker ˜ST , and let R be the 2m × (2m − n) matrix whose columns
are the vectors of that basis (note that ˜S has rank n). Deﬁne M =(cid:2) ˜S R(cid:3). This will be our change of basis
matrix. To obtain the inverse of M , form the matrix (cid:20) ( ˜ST ˜S)−1 ˜ST

(RT R)−1RT(cid:21) and observe that

(RT R)−1RT(cid:21)(cid:2) ˜S R(cid:3) =(cid:20) ( ˜ST ˜S)−1 ˜ST ˜S
(cid:20) ( ˜ST ˜S)−1 ˜ST

(RT R)−1RT ˜S (RT R)−1RT R(cid:21) =(cid:20)In

( ˜ST ˜S)−1 ˜ST R

0

0

I2m−n(cid:21) .

Therefore we have that M−1 =(cid:20) ( ˜ST ˜S)−1 ˜ST
(RT R)−1RT(cid:21).

Applying this change of basis, direct computation, applying (7) and (9), yields

(RT R)−1RT(cid:21)(cid:16)(I − u ˜P )(I − u˜τ ) + u2˜τ 2(cid:17)(cid:2) ˜S R(cid:3) =(cid:20)I − u ˜A + u2 ˜D −u ˜T R + u2 ˜T ˜τ R
(cid:20) ( ˜ST ˜S)−1 ˜ST

Therefore, the matrix (I−u ˜P )(I−u˜τ )+u2 ˜τ 2 is similar to the matrix(cid:20)I − u ˜A + u2 ˜D −u ˜T R + u2 ˜T ˜τ R

so they have the same determinant. Thus, we have proven a weighted version of Ihara’s Theorem, which we
state as the following.

(cid:21) .

(cid:21),

(11)

0

0

I

I

Theorem 4. Let G be a graph on n vertices and m edges, and assign an arbitrary positive weight w(x) > 0
assigned to each vertex x. Let ˜P be the 2m× 2m weighted non-backtracking edge adjacency matrix with edge
weight w(v)2 assigned to edge (u, v) as deﬁned in (4). Let ˜A be the weighted n × n adjacency matrix with
edge weight w(u)w(v) assigned to each edge. Let ˜τ be the weighted reversal operator deﬁned above, and ˜D
the n × n diagonal matrix with ˜D(x, x) =Pv∼x w(x)2w(v)2 as deﬁned above. Then we have

det(cid:16)(I − u ˜P )(I − u˜τ ) + u2˜τ 2(cid:17) = det(I − u ˜A + u2 ˜D).

As a corollary to the decomposition in equation (11), if we take w(x) = 1 for all x, then ˜τ 2 = I, and the

usual unweighted Ihara’s Theorem falls out immediately.

If we take w(x) =

1√dx−1 , then ˜P becomes the transition probability matrix for the non-backtracking
walk on directed edges, and ˜A(u, v) =
. This is clearly similar to the matrix (D − I)−1A. So
in this case ˜A is similar to the matrix whose entries are the transition probabilities for a single step in a
non-backtracking random walk. (Note, however, that (D − I)−1A is not the transition probability matrix
for a non-backtracking random walk.)

√(du−1)(dv−1)

1

4 The mixing rate of non-backtracking random walks

4.1 An alternate proof for regular graphs

Applying the results of the previous section to regular graphs yields a diﬀerent proof of the results from [1]
on the mixing rate of non-backtracking random walks on regular graphs.

Let G be a regular graph where each vertex has degree d. Then choosing w(x) = 1/√d − 1 for all x yields
gives us that ˜P is the transition probability matrix for the non-backtracking random walk on G. We remark
that, from the previous section, we have ˜τ = 1
(d−1)2 I. Therefore,
the decomposition in (11) becomes

d−1 A, and ˜D = d

(d−1)2 I, ˜A = 1

d−1 τ , ˜τ 2 = 1

(I − u ˜P )(I − u˜τ ) ∼"I − u

(d−1)2(cid:17) I# .
Noting that ˜τ can be thought of as block diagonal with m blocks of the form (cid:20)

∗
(cid:16)1 − u2

d−1 A + u2
d−1 I

0

then taking determinants, we ﬁnd that

det(I − u ˜P )(cid:18)1 −

u2

(d − 1)2(cid:19)m

=(cid:18)1 −

u2

(d − 1)2(cid:19)2m−n

det(cid:18)I −

u
d − 1

10

(cid:21),
1/(d − 1)

0

0

1/(d − 1)
I(cid:19)

u2
d − 1

A +

and hence

det(I − u ˜P ) = 1 −(cid:18) u

d − 1(cid:19)2!m−n n

Yi=1(cid:18)1 −

λi
d − 1

u +

1

d − 1

u2(cid:19)

where the product ranges over all the eigenvalues λi of the adjacency matrix A for i = 1,··· , n. As remarked
previously, the left hand side det(I − u ˜P ) is the characteristic polynomial of ˜P evaluated at 1/u, so from
this we obtain the spectrum of ˜P .
Theorem 5. Let G be a d-regular graph with m edges and n vertices, and let ˜P be the 2m × 2m transition
probability matrix for a non-backtracking random walk as deﬁned above. Then the eigenvalues of ˜P are

±

1

d − 1

,

λi ±pλ2

2(d − 1)

i − 4(d − 1)

, (i = 1,··· , n)

where λi ranges over the eigenvalues of the adjacency matrix A, and ±1/(d− 1) each have multiplicity m− n.

From this we obtain the result from [1].

Corollary 1. Let G be a non-bipartite, connected d-regular graph on n vertices for d ≥ 3, and let ρ and ˜ρ
denote the mixing rates of simple and non-backtracking random walk on G, respectively. Let λ be the second
largest eigenvalue of the adjacency matrix of G in absolute value.

If λ ≥ 2√d − 1, then

If λ < 2√d − 1 and d = no(1), then

d

2(d − 1) ≤

˜ρ
ρ ≤ 1.

+ o(1).

˜ρ
ρ

=

d

2(d − 1)

λ+√λ2−4(d−1)

in
Proof. We remark that the expression
[1] for the mixing rate of a non-backtracking random walk on a regular graph, and we may proceed with
the analysis of the convergence rate in the same way they do. The convergence rate is given by the second
largest eigenvalue of ˜P , which will be obtained setting λ to be the second largest eigenvalue of A. Let µ be
this eigenvalue.

is precisely the expression derived by Alon et al.

2(d−1)

For 2√d − 1 ≤ λ ≤ d we have

λ

2(d − 1)

<

λ +pλ2 − 4(d − 1)

2(d − 1)

λ
d

.

≤

d is the second largest eigenvalue of the transition probability matrix P for the

λ

d . Since λ

So
usual walk, the ﬁrst case follows.

2(d−1) ≤ µ ≤ λ
For λ < 2√d − 1, µ is complex, and we obtain
=(cid:18)

λ +pλ2 − 4(d − 1)

2(d − 1)

2

|µ|2 =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

λ

2(d − 1)(cid:19)2

+ p4(d − 1) − λ2
2(d − 1)

!2

=

1

d − 1

so |µ| = 1√d−1

.

We remark that in this case that λ < 2√d − 1, a classic result of Nilli ([14]) related to the Alon-Boppana

Theorem implies that we are never too far below this bound. Indeed, the result states that if G is d-regular
. With the restriction that d = no(1), then the

with diameter at least 2(k + 1), then λ ≥ 2√d − 1 − 2√d−1−1
diameter is at least (1 − o(1)) logd−1 n, and so λ > (1 − o(1))2√d − 1, and the second case follows.

k+1

11

4.2 Biregular graphs

1

A graph G is called (c, d)-biregular if it is bipartite and each vertex in one part of the bipartition has degree c,
and each vertex of the other part has degree d. In the weighted Ihara’s Theorem, we have ˜τ 2((u, v), (u, v) =
(du−1)(dv−1) , so in the case where G is (c, d)-biregular, then we have ˜τ 2 =
(c−1)(d−1) I. So since ˜τ 2 is a
multiple of the identity, as with regular graphs, in the decomposition (11), the u2˜τ 2 term can be taken to the
other side of the equation. Note that ˜D is diagonal with ˜D(u, u) =Pv∼u
(c−1)(d−1) if u has
(c−1)(d−1) if u has degree d. Then ˜D − ˜τ 2 is diagonal with entry
(c−1)(d−1) = 1
d−1
c−1 . Hence the decomposition (11) becomes
(c−1))(d−1) −
c−1 I(cid:21)
I − u(cid:20)
0
1

(du−1)(dv−1) =
(c−1)(d−1) −

(cid:21) + u2(cid:20) 1
d−1 I
0

d−1 M T

degree c, or

c−1 M

or

0

0

d

d

1

1

1

1

1

c

c

1

(c−1)(d−1) = 1
(I − u ˜P )(I − u˜τ ) ∼


∗
(c−1)(d−1)(cid:17) I

u2




(cid:16)1 −

where A =(cid:20) 0 M

0(cid:21) is the adjacency matrix of G.

M T

Note that ˜τ is similar to a block diagonal matrix with blocks of the form (cid:20)

0

1/(d − 1)

1/(c − 1)

0

(cid:21), so

taking the determinant above we obtain

0

det(I − u ˜P )(cid:18)1 −

=

u2

(c − 1)(d − 1)(cid:19)m
(c − 1)(d − 1)(cid:19)2m−n

u2

so

(cid:18)1 −

det(cid:18)I − u(cid:20)
(c − 1)(d − 1)(cid:19)m−n
det(I − u ˜P ) =(cid:18)1 +
c−1(cid:17) I
d−1(cid:17) I
We will look at the matrix 
(cid:16)1 + u2
c−1 M
(cid:16)1 + u2

d−1 M T

u2

u

u

0
1

c−1 I(cid:21)(cid:19)

0

1

c−1 M

1

0

d−1 M T
det
(cid:16)1 + u2
d−1(cid:17) I

d−1 M T

u

(cid:21) + u2(cid:20) 1
d−1 I
0
c−1(cid:17) I
c−1 M
(cid:16)1 − u2


u

size r, and the second part has size s, where without loss of generality, r > s. By row reduction, this has
the same determinant as the matrix

. Suppose the ﬁrst part in the bipartition of G has

which is

0

(cid:16)1 + u2


d − 1(cid:19)r
d − 1(cid:19)r−s

u2

u2

(cid:18)1 +
=(cid:18)1 +

d−1(cid:17) I

u

c−1 M
c−1(cid:17) I − 1
1+ u2

d−1

(cid:16)1 − u2

(c−1)(d−1) M T M


u2

det (cid:18)1 +

u2

c − 1(cid:19) I −

1 + u2
d−1

1

1

det(cid:18)(cid:18)1 +

u2

c − 1(cid:19)(cid:18)1 +

M T M!

u2

(c − 1)(d − 1)
u2
d − 1(cid:19) I −

(c − 1)(d − 1)

M T M(cid:19) .

Now, the above determinant is given by the product of the eigenvalues of the matrix. Observe that if λ

is an eigenvalue of the adjacency matrix A, then λ2 is an eigenvalue of M T M . Therefore, in all we have

det(I − u ˜P ) =(cid:18)1 −
×

s

u2

(c − 1)(d − 1)(cid:19)m−n(cid:18)1 +
Yi=1(cid:18)(cid:18)1 +

c − 1(cid:19)(cid:18)1 +

u2

u2

d − 1(cid:19) −

u2

d − 1(cid:19)r−s

λ2
i u2

(c − 1)(d − 1)(cid:19)

12

where the product ranges over the s largest eigenvalues of A (or in other words, λ2
of M T M ). Therefore the characteristic polynomial is given by

i ranges of the s eigenvalues

det(uI − ˜P ) =(cid:18)u2 −

1

(c − 1)(d − 1)(cid:19)m−n(cid:18)u2 +

1

d − 1(cid:19)r−s

×

s

Yi=1(cid:18)(cid:18)u2 +

1

c − 1(cid:19)(cid:18)u2 +

1

d − 1(cid:19) −

i u2
λ2

(c − 1)(d − 1)(cid:19) .

Thus we can explicitly obtain the eigenvalues of ˜P .

Theorem 6. Let G be a (c, d)-biregular graph, let the part with degree c have size r, and the part with degree
d have size s, and assume without loss of generality that r ≥ s. Suppose G has n vertices and m edges. Then
the eigenvalues of the non-backtracking transition probability matrix ˜P deﬁned above are

1

±

p(c − 1)(d − 1)

as well as the 4 roots of the polynomial

with multiplicity m − n each , ±

1

√d − 1

i with multiplicity r − s each

u4 +(cid:18) 1
(c − 1)

+

1

(d − 1) −

λ2
i

(c − 1)(d − 1)(cid:19) u2 +

1

(c − 1)(d − 1)

for each value of λi ranging over the s positive eigenvalues of the adjacency matrix A. These roots are

±s λ2

i − (c − 1) − (d − 1) ±p(λ2

i − (c − 1) − (d − 1))2 − 4(c − 1)(d − 1)
2(c − 1)(d − 1)

, (i = 1,··· , s).

(12)

We can now give a version of Corollary 1 for (c, d)-biregular graphs.

Corollary 2. Let G be a (c, d)-biregular graph with c, d ≥ 2. Let ρ = λ2/cd be the square of the second
largest eigenvalue of the transition probability matrix P for a random walk on G, and let ˜ρ = |µ|2 be the
square of the second largest modulus of an eigenvalue of ˜P . Let λ be the second largest eigenvalue of the
adjacency matrix of G. Then we have the following cases.

If λ > √c − 1 + √d − 1, then

cd

2(c − 1)(d − 1) 1 −

c − 1 + d − 1

c − 1 + 2p(c − 1)(d − 1) + d − 1! ≤

˜ρ
ρ ≤ 1.

If λ < √c − 1 + √d − 1 and both c and d are no(1), then

˜ρ
ρ ≤

cd

2(c − 1)(d − 1)

+ o(1).

Proof. We need to compare the eigenvalues of ˜P to the eigenvalues of P = D−1A = (cid:20) 0

1
d M T

that for λ an eigenvalue of A, we have

1
c M

0 (cid:21). Note

which implies M y = λx and M T x = λy. Then observe

(cid:20) 0 M

0(cid:21)(cid:20)x

M T

y(cid:21) = λ(cid:20)x
y(cid:21)

(cid:20) 0

d M T

1

1
c M

0 (cid:21)" 1√c x

1√d

y# =" 1

d√c M T x# =

c√d
1

M y

λ

√cd" 1√c x
y# ,
1√d

so the eigenvalues of P are λ/√cd where λ ranges over the eigenvalues of A. Note that the largest eigenvalue
of A is √cd.

13

Let µ equal the expression (12), and consider the following cases.

If √c − 1 +√d − 1 ≤ λ ≤

√cd, then µ is real. Direct computation veriﬁes that, evaluating the expression
(12) at λ = √cd yields µ = 1 = λ/√cd and µ < λ/√cd for λ in this range. Therefore, in this case the
eigenvalue of ˜P always has smaller absolute value than the corresponding eigenvalue of P , implying ˜ρ ≤ ρ.
The lower bound follows from (12) ignoring the square root inside. Thus the ﬁrst case follows.

If λ < √c − 1 + √d − 1, then µ is complex, and direct computation shows

so

|µ| =

|µ|2 =

1

,

p(c − 1)(d − 1)
((c − 1)(d − 1))1/4

1

A version of the Alon-Boppana Theorem exists for (c, d)-biregular graphs as well, proven by Feng and Li

in [6] (see also [12]).

Theorem 7 ([6]). Let G be a (c, d)-biregular graph, and let λ be the second largest eigenvalue of the adjacency
matrix A of G. Then

where the diameter of G is greater than 2(k + 1).

λ2 ≥(cid:16)√c − 1 + √d − 1(cid:17)2

2p(c − 1)(d − 1) − 1

k

−

Observe that certainly the diameter is at least logcd n, so that the condition on the degrees and Theorem

7 imply that

As |µ|2 =

1√(c−1)(d−1)

, so this gives the result for the second case.

λ2 ≥ 2p(c − 1)(d − 1)(1 − o(1)).

5 Conclusion

We have looked at non-backtracking random walks from the point of view of walking along directed edges.
For the special cases of regular and biregular graphs, our weighted version of Ihara’s Theorem (Theorem
4) has given us the comlete spectrum of the transition probability matrix for the non-bakctracking walk,
allowing for easy comparison between the non-backracking mixing rate, and the mixing rate of the usual
random walk. Clearly, it would be desirable to extend these reults to more general classes of graphs. The
diﬃculty in applying Theorem 4 directly is with the term involving ˜τ 2. As seen in section 3, ˜τ 2 is a 2m× 2m
diagonal matrix with

˜τ 2 ((u, v), (u, v)) =

1

(du − 1)(dv − 1)

.

In the case of regular and biregular graphs, this expression is constant (we get 1/(d− 1) and 1/(c− 1)(d− 1)
for the d-regular and (c, d)-biregular cases respectively), making ˜τ 2 simply a multiple of the identity. This
allows the diﬃculty to be handled relatively easily. Regular and biragular graphs are in fact the only graphs
for which ˜τ 2 is a multiple of the identity, suggesting that these exact techniques will not work as nicely on
more general classes of graphs. If a cleaner version of Theorem 4 could be proven, then, aside from being in-
teresting in its own rite, it could potentially be used to extend our results on non-backtracking random walks.

Acknowledgment. The author would like to thank Fan Chung for numerous helpful discussions through-

out the process of writing this paper.

References

[1] N. Alon, I. Benjamini, E. Lubetzky, and S. Sodin, Non-backtracking random walks mix faster, Commu-

nications in Contemporary Mathematics, 09, (2007) 585.

14

[2] O. Angel, J. Friedman, and S. Hoory, The non-backtracking spectrum of the universal cover of a graph,

Transactions of the American Mathematical Society, 326(6), (2015) 4287–4318.

[3] H. Bass, the Ihara-Selberg zeta function of a tree lattice, Internat. J. Math., 3 (1992), 717–797.

[4] F. Chung, Laplacians and the Cheeger inequality for directed graphs, Annals of Combinatorics, 9 (2005),

1–19.

[5] F. Chung, Spectral Graph Theory, AMS Publications, 1997.

[6] K. Feng and W.-C. W. Li, Spectra of hypergraphs and applications, Journal of Number Theory, 60(1),

(1996) 1–22.

[7] R. Fitzner and R. van der Hofstad, Non-backtracking random walk, J. Stat. Phys., 150(2), (2013) 264–

284.

[8] K. Hishimoto, Artin-type L-functinos and the density theorem for prime cycles on ﬁnite graphs, Internat.

J. Math., 3 (1992), 809–826.

[9] Y. Ihara, On discrete subgroups of the two by two projective linear group over p-adic ﬁelds, J. Math.

Soc. Japan, 18 (1966), 219–235.

[10] M. Kotani and T. Sunada, Zeta functions of ﬁnite graphs, J. Math. Sci. Univ. Tokyo , 7 (2000), 7-25.

[11] F. Krzakala, C. Moore, E. Mossel, J. Neeman, A. Sly, L. Zdeborova, and P. Zhang, Spectral redemption
in clustering sparse networks, Proceedings of the National Academy of Sciences, 110(52), (2013) 20935–
20940.

[12] W.-C. W. Li and P. Sol´e, Spectra of regular graphs and hypergraphs and orthogonal polynomials.

European Journal of Combinatorics, 17(5), (1996) 461–477.

[13] L. Lov´asz, Random walks on graphs: a survey, Combinatorics, Paul Erd˝os is Eighty (Volume 2),

Keszthely (Hungary) (1993) pp.1-46.

[14] A. Nilli, On the second eigenvallue of a graph. Discrete Math., 91 (1991), 207-210.

[15] H. M. Stark and A. A. Terras, Zeta functions of ﬁnite graphs and coverings, Advances in Mathematics,

121, (1996) 124–165.

15

